Document Title,Abstract,Year,PDF Link,label,code,time
Metrics that matter,"Within NASA, there is an increasing awareness that software is of growing importance to the success of missions. Much data has been collected, and many theories have been advanced on how to reduce or eliminate errors in code. However, learning requires experience. We document a new NASA initiative to build a centralized repository of software defect data; in particular, we document one specific case study on software metrics. Software metrics are used as a basis for prediction of errors in code modules, but there are many different metrics available. McCabe is one of the more popular tools used to produce metrics, but, other metrics can be more significant.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1199449,yes,yes,1487371926.829664
"Using product, process, and execution metrics to predict fault-prone software modules with classification trees","Software-quality classification models can make predictions to guide improvement efforts to those modules that need it the most. Based on software metrics, a model can predict which modules will be considered fault-prone, or not. We consider a module fault-prone if any faults were discovered by customers. Useful predictions are contingent on the availability of candidate predictors that are actually related to faults discovered by customers. With a diverse set of candidate predictors in hand, classification-tree modeling is a robust technique for building such software quality models. This paper presents an empirical case study of four releases of a very large telecommunications system. The case study used the regression-tree algorithm in the S-Plus package and then applied our general decision rule to classify modules. Results showed that in addition to product metrics, process metrics and execution metrics were significant predictors of faults discovered by customers",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=895475,yes,yes,1487371920.015751
The confounding effect of class size on the validity of object-oriented metrics,"Much effort has been devoted to the development and empirical validation of object-oriented metrics. The empirical validations performed thus far would suggest that a core set of validated metrics is close to being identified. However, none of these studies allow for the potentially confounding effect of class size. We demonstrate a strong size confounding effect and question the results of previous object-oriented metrics validation studies. We first investigated whether there is a confounding effect of class size in validation studies of object-oriented metrics and show that, based on previous work, there is reason to believe that such an effect exists. We then describe a detailed empirical methodology for identifying those effects. Finally, we perform a study on a large C++ telecommunications framework to examine if size is really a confounder. This study considered the Chidamber and Kemerer metrics and a subset of the Lorenz and Kidd metrics. The dependent variable was the incidence of a fault attributable to a field failure (fault-proneness of a class). Our findings indicate that, before controlling for size, the results are very similar to previous studies. The metrics that are expected to be validated are indeed associated with fault-proneness",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=935855,yes,yes,1487371919.316563
Empirical validation of object-oriented metrics on open source software for fault prediction,"Open source software systems are becoming increasingly important these days. Many companies are investing in open source projects and lots of them are also using such software in their own work. But, because open source software is often developed with a different management style than the industrial ones, the quality and reliability of the code needs to be studied. Hence, the characteristics of the source code of these projects need to be measured to obtain more information about it. This paper describes how we calculated the object-oriented metrics given by Chidamber and Kemerer to illustrate how fault-proneness detection of the source code of the open source Web and e-mail suite called Mozilla can be carried out. We checked the values obtained against the number of bugs found in its bug database - called Bugzilla - using regression and machine learning methods to validate the usefulness of these metrics for fault-proneness prediction. We also compared the metrics of several versions of Mozilla to see how the predicted fault-proneness of the software system changed during its development cycle.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1542070,yes,yes,1487371919.31656
An empirical study on object-oriented metrics,"The objective of this study is the investigation of the correlation between object-oriented design metrics and the likelihood of the occurrence of object oriented faults. Such a relationship, if identified, can be utilized to select effective testing techniques that take the characteristics of the program under test into account. Our empirical study was conducted on three industrial real-time systems that contain a number of natural faults reported for the past three years. The faults found in these three systems are classified into three types: object-oriented faults, object management faults and traditional faults. The object-oriented design metrics suite proposed by Chidamber and Kemerer (1994) is validated using these faults. Moreover, we propose a set of new metrics that can serve as an indicator of how strongly object-oriented a program is, so that the decision to adopt object oriented testing techniques can be made, to achieve more reliable testing and yet minimize redundant testing efforts",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=809745,yes,yes,1487371919.316557
Predicting fault-proneness using OO metrics. An industrial case study,"Software quality is an important external software attribute that is difficult to measure objectively. In this case study, we empirically validate a set of object-oriented metrics in terms of their usefulness in predicting fault-proneness, an important software quality indicator We use a set of ten software product metrics that relate to the following software attributes: the size of the software, coupling, cohesion, inheritance, and reuse. Eight hypotheses on the correlations of the metrics with fault-proneness are given. These hypotheses are empirically tested in a case study, in which the client side of a large network service management system is studied. The subject system is written in Java and it consists of 123 classes. The validation is carried out using two data analysis techniques: regression analysis and discriminant analysis",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=995794,yes,yes,1487371919.003099
Evaluating the impact of object-oriented design on software quality,"Describes the results of a study where the impact of object-oriented (OO) design on software quality characteristics is experimentally evaluated. A suite of Metrics for OO Design (MOOD) was adopted to measure the use of OO design mechanisms. Data collected on the development of eight small-sized information management systems based on identical requirements were used to assess the referred impact. Data obtained in this experiment show how OO design mechanisms such as inheritance, polymorphism, information hiding and coupling, can influence quality characteristics like reliability or maintainability. Some predictive models based on OO design metrics are also presented",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=492446,yes,yes,1487371919.003097
Prediction of fault-proneness at early phase in object-oriented development,"To analyse the complexity of object-oriented software, several metrics have been proposed. Among them, Chidamber and Kemerer's (1994) metrics are well-known object-oriented metrics. Also, their effectiveness has been empirically evaluated from the viewpoint of estimating the fault-proneness of object-oriented software. In the evaluations, these metrics were applied, not to the design specification but to the source code, because some of them measure the inner complexity of a class, and such information cannot be obtained until the algorithm and the class structure are determined at the end of the design phase. However, the estimation of the fault-proneness should be done in the early phase so as to effectively allocate effort for fixing the faults. This paper proposes a new method to estimate the fault-proneness of an object class in the early phase, using several complexity metrics for object-oriented software. In the proposed method, we introduce four checkpoints into the analysis/design/implementation phase, and we estimate the fault-prone classes using applicable metrics at each checkpoint",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=776386,yes,yes,1487371919.003093
A study on fault-proneness detection of object-oriented systems,"Fault-proneness detection in object-oriented systems is an interesting area for software companies and researchers. Several hundred metrics have been defined with the aim of measuring the different aspects of object-oriented systems. Only a few of them have been validated for fault detection, and several interesting works with this view have been considered. This paper reports a research study starting from the analysis of more than 200 different object-oriented metrics extracted from the literature with the aim of identifying suitable models for the detection of the fault-proneness of classes. Such a large number of metrics allows the extraction of a subset of them in order to obtain models that can be adopted for fault-proneness detection. To this end, the whole set of metrics has been classified on the basis of the measured aspect in order to reduce them to a manageable number; then, statistical techniques were employed to produce a hybrid model comprised of 12 metrics. The work has focused on identifying models that can detect as many faulty classes as possible and, at the same time, that are based on a manageably small set of metrics. A compromise between these aspects and the classification correctness of faulty and non-faulty classes was the main challenge of the research. As a result, two models for fault-proneness class detection have been obtained and validated",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=914976,yes,yes,1487371919.00309
A validation of object-oriented design metrics as quality indicators,"This paper presents the results of a study in which we empirically investigated the suite of object-oriented (OO) design metrics introduced in (Chidamber and Kemerer, 1994). More specifically, our goal is to assess these metrics as predictors of fault-prone classes and, therefore, determine whether they can be used as early quality indicators. This study is complementary to the work described in (Li and Henry, 1993) where the same suite of metrics had been used to assess frequencies of maintenance changes to classes. To perform our validation accurately, we collected data on the development of eight medium-sized information management systems based on identical requirements. All eight projects were developed using a sequential life cycle model, a well-known OO analysis/design method and the C++ programming language. Based on empirical and quantitative analysis, the advantages and drawbacks of these OO metrics are discussed. Several of Chidamber and Kemerer's OO metrics appear to be useful to predict class fault-proneness during the early phases of the life-cycle. Also, on our data set, they are better predictors than â€œtraditionalâ€?code metrics, which can only be collected at a later phase of the software development processes",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=544352,yes,yes,1487371918.691442
Assessing the applicability of fault-proneness models across object-oriented software projects,"A number of papers have investigated the relationships between design metrics and the detection of faults in object-oriented software. Several of these studies have shown that such models can be accurate in predicting faulty classes within one particular software product. In practice, however, prediction models are built on certain products to be used on subsequent software development projects. How accurate can these models be, considering the inevitable differences that may exist across projects and systems? Organizations typically learn and change. From a more general standpoint, can we obtain any evidence that such models are economically viable tools to focus validation and verification effort? This paper attempts to answer these questions by devising a general but tailorable cost-benefit model and by using fault and design data collected on two mid-size Java systems developed in the same environment. Another contribution of the paper is the use of a novel exploratory analysis technique - MARS (multivariate adaptive regression splines) to build such fault-proneness models, whose functional form is a-priori unknown. The results indicate that a model built on one system can be accurately used to rank classes within another system according to their fault proneness. The downside, however, is that, because of system differences, the predicted fault probabilities are not representative of the system predicted. However, our cost-benefit model demonstrates that the MARS fault-proneness model is potentially viable, from an economical standpoint. The linear model is not nearly as good, thus suggesting a more complex model is required.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1019484,yes,yes,1487371918.691433
A comprehensive empirical validation of design measures for object-oriented systems,"This paper aims at empirically exploring the relationships between existing object-oriented coupling, cohesion, and inheritance measures and the probability of fault detection in system classes during testing. The underlying goal of such a study is to better understand the relationship between existing design measurement in OO systems and the quality of the software developed. Results show that many of the measures capture similar dimensions in the data set, thus reflecting the fact that many of them are based on similar principles and hypotheses. Besides the size of classes, the frequency of method invocations and the depth of inheritance hierarchies seem to be the main driving factors of fault-proneness",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=731251,yes,yes,1487371918.403117
Predicting fault-prone classes with design measures in object-oriented systems,"The paper aims at empirically exploring the relationships between existing object oriented coupling, cohesion, and inheritance measures and the probability of fault detection in system classes during testing. The underlying goal of such a study is to better understand the relationship between existing product measurement in OO systems and the quality of the software developed. It is shown that by using a subset of existing measures, accurate models can be built to predict in which classes most of the faults are likely to lie in. By inspecting 48% of the classes, it is possible to find 95% of the faults. Besides the size of classes, the frequency of method invocations and the depth of inheritance hierarchies seem to be the main driving factors of fault proneness",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=730898,yes,yes,1487371918.403114
Predicting fault-prone software modules in telephone switches,"An empirical study was carried out at Ericsson Telecom AB to investigate the relationship between several design metrics and the number of function test failure reports associated with software modules. A tool, ERIMET, was developed to analyze the design documents automatically. Preliminary results from the study of 130 modules showed that: based on fault and design data one can satisfactorily build, before coding has started, a prediction model for identifying the most fault-prone modules. The data analyzed show that 20 percent of the most fault-prone modules account for 60 percent of all faults. The prediction model built in this paper would have identified 20 percent of the modules accounting for 47 percent of all faults. At least four design measures can alternatively be used as predictors with equivalent performance. The size (with respect to the number of lines of code) used in a previous prediction model was not significantly better than these four measures. The Alberg diagram introduced in this paper offers a way of assessing a predictor based on historical data, which is a valuable complement to linear regression when prediction data is ordinal. Applying the method described in this paper makes it possible to use measures at the design phase to predict the most fault-prone modules",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=553637,yes,yes,1487371918.118545
Predicting fault incidence using software change history,"This paper is an attempt to understand the processes by which software ages. We define code to be aged or decayed if its structure makes it unnecessarily difficult to understand or change and we measure the extent of decay by counting the number of faults in code in a period of time. Using change management data from a very large, long-lived software system, we explore the extent to which measurements from the change history are successful in predicting the distribution over modules of these incidences of faults. In general, process measures based on the change history are more useful in predicting fault rates than product metrics of the code: For instance, the number of times code has been changed is a better indication of how many faults it will contain than is its length. We also compare the fault rates of code of various ages, finding that if a module is, on the average, a year older than an otherwise similar module, the older module will have roughly a third fewer faults. Our most successful model measures the fault potential of a module as the sum of contributions from all of the times the module has been changed, with large, recent changes receiving the most weight",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=859533,yes,yes,1487371916.527531
Code churn: a measure for estimating the impact of code change,"This study presents a methodology that will produce a viable fault surrogate. The focus of the effort is on the precise measurement of software development process and product outcomes. Tools and processes for the static measurement of the source code have been installed and made operational in a large embedded software system. Source code measurements have been gathered unobtrusively for each build in the software evolution process. The measurements are synthesized to obtain the fault surrogate. The complexity of sequential builds is compared and a new measure, code churn, is calculated. This paper demonstrates the effectiveness of code complexity churn by validating it against the testing problem reports",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=738486,yes,yes,1487371916.527527
Use of relative code churn measures to predict system defect density,"Software systems evolve over time due to changes in requirements, optimization of code, fixes for security and reliability bugs etc. Code churn, which measures the changes made to a component over a period of time, quantifies the extent of this change. We present a technique for early prediction of system defect density using a set of relative code churn measures that relate the amount of churn to other variables such as component size and the temporal extent of churn. Using statistical regression models, we show that while absolute measures of code chum are poor predictors of defect density, our set of relative measures of code churn is highly predictive of defect density. A case study performed on Windows Server 2003 indicates the validity of the relative code churn measures as early indicators of system defect density. Furthermore, our code churn metric suite is able to discriminate between fault and not fault-prone binaries with an accuracy of 89.0 percent.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1553571,yes,yes,1487371916.399931
How good is your blind spot sampling policy,"Assessing software costs money and better assessment costs exponentially more money. Given finite budgets, assessment resources are typically skewed towards areas that are believed to be mission critical. This leaves blind spots: portions of the system that may contain defects which may be missed. Therefore, in addition to rigorously assessing mission critical areas, a parallel activity should sample the blind spots. This paper assesses defect detectors based on static code measures as a blind spot sampling method. In contrast to previous results, we find that such defect detectors yield results that are stable across many applications. Further, these detectors are inexpensive to use and can be tuned to the specifics of the current business situations.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1281737,yes,yes,1487371915.856622
Inheritance-based object-oriented software metrics,"There is no software metrics based on object-oriented programming languages (OOPLs) developed to help object-oriented software development. A graph-theoretical complexity metric to measure object-oriented software complexity is described. It shows that inheritance has a close relation with the object-oriented software complexity, and reveals that misuse of repeated (multiple) inheritance will increase software complexity and be prone to implicit software errors. An algorithm to support this software metric is presented. Its time complexity is O(<e1>n</e1><sup>3</sup>)",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=271895,no,no,1487371926.829675
Modelling and analysing fault propagation in safety-related systems,"A formal specification for analysing and implementing multiple fault diagnosis software is proposed in this paper. The specification computes all potential fault sources that correspond to a set of triggered alarms for a safety-related system, or part of a system. The detection of faults occurring in a safety-related system is a fundamental function that needs to be addressed efficiently. Safety monitors for fault diagnosis have been extensively studied in areas such as aircraft systems and chemical industries. With the introduction of intelligent sensors, diagnosis results are made available to monitoring systems and operators. For complex systems composed of thousands of components and sensors, the diagnosis of multiple faults and the computational burden of processing test results are substantial. This paper addresses the multiple fault diagnosis problem for zero-time propagation using a fault propagation graph. Components represented as nodes in a fault propagation graph are allocated with alarms. When faults occur and are propagated some of these alarms are triggered. The allocation of alarms to nodes is based on a severity analysis performed using a form of failure mode and effect analysis on components in the system.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1270740,no,no,1487371926.829674
The feasibility of applying object-oriented technologies to operational flight software,"As object-oriented technologies move from the laboratory to the mainstream, companies are beginning to realize cost benefits in terms of software reuse and reduced development time. These benefits have been elusive to developers of real-time flight software. Issues such as processing latencies, validated performance of mission critical functions, and integration of legacy code have inhibited the effective use of object-oriented technologies in this domain. Emerging design languages, development tools, and processes offer the potential to address the application of object technologies to real-time operational flight software development. This paper examines emerging object-based technologies and assess their applicability to operational flight software. It includes an analysis that compares and contrasts the current Comanche software development process with one based on object-oriented concepts",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=886959,no,no,1487371926.829673
Fault tolerant avionics display system,"Discusses the fault-tolerant requirements of avionics display systems, and an open architecture which permits implementing these requirements. The fault tolerant requirements include the classes of hardware and software faults. Within each class, the requirements are broken down into fault types the system must tolerate. The designers of both hardware and software must develop and integrated approach to implementing fault detection, damage assessment, fault isolation, and recovery for the fault types. Active matrix (AM) liquid crystal displays (LCDs) used with high-performance graphics and video processors provide the capability needed to achieve these fault-tolerant designs",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=177198,no,no,1487371926.829672
Robustness and diagnosability of OO systems designed by contracts,"While there is a growing interest in component-based systems for industrial applications, little effort has so far been devoted to quality evaluation of these systems. This paper presents the definition of measures for two quality factors, namely robustness and â€œdiagnosabilityâ€?for the special case of object-oriented (OO) systems, for which thee approach known as design-by-contract has been used. The main steps in constructing these measures are given, from informal definitions of the factors to be measured to the mathematical model of the measures. To fix the parameters, experimental studies have been conducted, essentially based on applying mutation analysis in the OO context. Several measures are presented that reveal and estimate the contribution of the contracts' quality and density to the overall quality of a system in terms of robustness and diagnosability",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915535,no,no,1487371926.82967
Mapping tasks into fault tolerant manipulators,"The application of robots in critical missions in hazardous environments requires the development of reliable or fault tolerant manipulators. In this paper we define fault tolerance as the ability to continue the performance of a task after immobilization of a joint due to failure. Initially, no joint limits are considered, in which case we prove the existence of fault tolerant manipulators and develop an analysis tool to determine the fault tolerant work space. We also derive design templates for spatial fault tolerant manipulators. When joint limits are introduced, analytic solutions become infeasible but instead a numerical design procedure can be used, as is illustrated through an example",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=351405,no,no,1487371926.829669
"Design, implementation and performance evaluation of a new digital distance relaying algorithm","This paper presents the design, simulation, implementation and performance evaluation of a computationally efficient and accurate digital distance relaying algorithm. Published historical data were used in the first phase for validation purpose. Sample results illustrating highly accurate fault impedance estimates for various conditions are reported. The second phase uses voltage and current signals generated by Alternative Transients Program (ATP) and a sample power system for various first-zone, second-zone and third-zone faults. Results of these studies confirming stability and computational efficiency of the algorithm are presented and discussed. In the third phase, a prototype of the relay was developed and tested using real-time fault data generated from physical models of the transmission lines. Oscillographs for these conditions were recorded. Results of these tests indicating high speed relay operation are also discussed. The performance evaluation studies reported in this paper conclusively demonstrate that the new algorithm provides fast and accurate fault impedance estimates for the three-zone protection of transmission lines",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=486132,no,no,1487371926.829668
Mainframe Implementation With Off-The-Shelf LSI Modules,"Bit-sliced microprocessor parallel design for Sperry Univac 1108 improves performance, reduces build cost, and detects most faults.",1978,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1647039,no,no,1487371926.829666
Predicting maintainability with object-oriented metrics -an empirical comparison,"<div style=""font-variant: small-caps; font-size: .9em;"">First Page of the Article</div><img class=""img-abs-container"" style=""width: 95%; border: 1px solid #808080;"" src=""/xploreAssets/images/absImages/01287246.png"" border=""0"">",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1287246,no,no,1487371926.829662
Assessing usability through perceptions of information scent,Information scent is an establish concept for assessing how users interact with information retrieval systems. This paper proposes two ways of measuring user perceptions of information scent in order to assess the product quality of Web or Internet information retrieval systems. An empirical study is presented which validates these measures through an evaluation based on a live e-commerce application. This study shows a strong correlation between the measures of perceived scent and system usability. Finally the wider applicability of these methods is discussed.,2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357919,no,no,1487371926.36014
Early metrics for object oriented designs,"To produce high quality object-oriented systems, a strong emphasis on the development process is necessary. This implies two implicit and complementary goals. First, to ensure a full control over the whole process, enabling accurate cost and delay estimation, resource efficient management, and a better overall understanding. Second, to improve quality all along the system lifecycle at development and maintenance stage. On the client side, a steady control over the development process implies a better detection and elimination of faults, raising the viability and usability of the system. This paper introduces a realistic example of metrics integration in the development process of object-oriented software. By addressing early stages of the design (ie. class diagram), we can anticipate design-level errors or warnings thus enabling a reduction of immediate and further costs and problems. Metrics used are issued from state of the art object-oriented research, and integrated in the widespread unified process of software development.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1428417,no,no,1487371926.360138
Relating operational software reliability and workload: results from an experimental study,"This paper presents some results from an experimental and analytical study to investigate the failure behavior of operational software for several types of workload. The authors are primarily interested in the so-called highly reliable software where enough evidence exists to indicate a lack of known faults. For purposes of this study, a â€œgoldâ€?version of a well known program was used in the experimental study. Judiciously selected errors were introduced, one at a time, and each mutated program was executed for three types of workload: constant; random; and systematically varying. Analytical analyses of the resulting failures were undertaken and are summarized here. The case when workload is random is of particular interest in this paper. An analytical model for the resulting failure phenomenon based on the Wald equation is found to give excellent results",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=500658,no,no,1487371926.360137
Software fault tolerance for a flight control system,"The aim of software fault tolerance is to introduce programming techniques which will allow the embedded software to maintain performance in the presence of hardware faults which include data, address and control bus corruptions. A case study is described in which the navigation and control software of a remotely piloted vehicle (RPV) is subjected to such transient fault conditions. The embedded software was designed to detect and recover from such faults. Various aspects of the design, fault conditions, experimental setup and results are discussed",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=82204,no,no,1487371926.360135
Prediction of software faults using fuzzy nonlinear regression modeling,"Software quality models can predict the risk of faults in modules early enough for cost-effective prevention of problems. This paper introduces the fuzzy nonlinear regression (FNR) modeling technique as a method for predicting fault ranges in software modules. FNR modeling differs from classical linear regression in that the output of an FNR model is a fuzzy number. Predicting the exact number of faults in each program module is often not necessary. The FNR model can predict the interval that the number of faults of each module falls into with a certain probability. A case study of a full-scale industrial software system was used to illustrate the usefulness of FNR modeling. This case study included four historical software releases. The first release's data were used to build the FNR model, while the remaining three releases' data were used to evaluate the model. We found that FNR modeling gives useful results",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=895473,no,no,1487371926.360134
Fault emulation: A new methodology for fault grading,"In this paper, we introduce a method that uses the field programmable gate array (FPGA)-based emulation system for fault grading. The real-time simulation capability of a hardware emulator could significantly improve the performance of fault grading, which is one of the most time consuming tasks in the circuit design and test process. We employ a serial fault emulation algorithm enhanced by two speed-up techniques. First, a set of independent faults can be injected and emulated at the same time. Second, multiple dependent faults can be simultaneously injected within a single FPGA-configuration by adding extra circuitry. Because the reconfiguration time of mapping the numerous faulty circuits into the FPGA's is pure overhead and could be the bottleneck of the entire process, using extra circuitry for injecting a large number of faults can reduce the number of FPGA-reconfigurations and, thus, improving the performance significantly. In addition, we address the issue of handling potentially detected faults in this hardware emulation environment by using the dual-railed logic. The performance estimation shows that this approach could be several orders of magnitude faster than the existing software approaches for large sequential designs",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=790625,no,no,1487371926.360133
NFTAPE: a framework for assessing dependability in distributed systems with lightweight fault injectors,"Many fault injection tools are available for dependability assessment. Although these tools are good at injecting a single fault model into a single system, they suffer from two main limitations for use in distributed systems: (1) no single tool is sufficient for injecting all necessary fault models; (2) it is difficult to port these tools to new systems. NFTAPE, a tool for composing automated fault injection experiments from available lightweight fault injectors, triggers, monitors, and other components, helps to solve these problems. We have conducted experiments using NFTAPE with several types of lightweight fault injectors, including driver-based, debugger-based, target-specific, simulation-based, hardware-based, and performance-fault injections. Two example experiments are described in this paper. The first uses a hardware fault injector with a Myrinet LAN; the other uses a Software Implemented Fault Injection (SWIFI) fault injector to target a space-imaging application",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=839467,no,no,1487371926.360131
A multiple-case study of software effort estimation based on use case points,"Through industry collaboration we have experienced an increasing interest in software effort estimation based on use cases. We therefore investigated one promising method, the use case points method, which is inspired by function points analysis. Four companies developed equivalent functionality, but their development processes varied, ranging from a light, code-and-fix process with limited emphasis on code quality, to a heavy process with considerable emphasis on analysis, design and code quality. Our effort estimate, which was based on the use case points method, was close to the actual effort of the company with the lightest development process; the estimate was 413 hours while actual effort of the four companies ranged from 431 to 943 hours. These results show, that the use case points method needs modification to better handle effort related to the development process and the quality of the code.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541849,no,no,1487371926.360129
Maintenance metrics for the object oriented paradigm,"Software metrics have been studied in the procedural paradigm as a quantitative means of assessing the software development process as well as the quality of software products. Several studies have validated that various metrics are useful indicators of maintenance effort in the procedural paradigm. However, software metrics have rarely been studied in the object oriented paradigm. Very few metrics have been proposed to measure object oriented systems, and the proposed ones have not been validated. This research concentrates on several object oriented software metrics and the validation of these metrics with maintenance effort in two commercial systems",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=263801,no,no,1487371926.360127
An overview of the fault protection design for the attitude control subsystem of the Cassini spacecraft,"This paper describes Cassini's fault tolerance objectives, and how those objectives have influenced the design of its attitude control subsystem (ACS). Particular emphasis is placed on the architecture of the software algorithms that are used to detect, locate, and recover from ACS faults, and the integration of these algorithms into the rest of the object-oriented ACS flight software. The interactions of these ACS â€œfault protectionâ€?algorithms with Cassini's system-level fault protection algorithms is also described. We believe that the architecture of the ACS fault protection algorithms provides significant performance and operability improvements over the fault protection algorithms that have flown on previous interplanetary spacecraft",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=703535,no,no,1487371926.360125
Design and development of a digital multifunction relay for generator protection,This paper presents the design and development of a rotor earth fault protection function as part of a multifunction generator protection relay. The relay design is based on a low frequency square wave injection method in detecting rotor earth faults. The accuracy of rotor earth fault resistance measurement is improved by applying piecewise quadratic approximation to the nonlinear gain characteristic of the measurement circuit. The paper also presents the hardware and software architecture of the relay,2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=929370,no,no,1487371925.8962
"Design, implementation and performance evaluation of a new digital distance relaying algorithm","This paper presents the design, simulation, implementation and performance evaluation of a computationally efficient and accurate digital distance relaying algorithm. Published historical data were used in the first phase for validation purposes. Sample results illustrating highly accurate fault impedance estimates for various conditions are reported. The second phase uses voltage and current signals generated by the Alternative Transients Program (ATP) and a sample power system for various first-zone, second-zone and third-zone faults. Results of these studies, confirming the stability and computational efficiency of the algorithm, are presented and discussed. In the third phase, a prototype of the relay was developed and tested using real-time fault data generated from physical models of the transmission lines. Oscillographs for these conditions were recorded. Results of these tests indicating high speed relay operation are also discussed. The performance evaluation studies reported in this paper conclusively demonstrate that the new algorithm provides fast and accurate fault impedance estimates for the three-zone protection of transmission lines",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=515272,no,no,1487371925.896199
Process fault detection using constraint suspension,"In the paper the authors describe two tools for use in an intelligent fault-detection system. The first tool is a method for fault localisation, called constraint suspension, which helps us find and identify the parts suspension, which helps us find and identify the parts of the process which are responsible for any inconsistencies in the measurements. The second tool gives us a flexible framework in which we can build a software model of the process in terms of hierarchies of component parts or objects, it is called object orientation. We use these tools to describe algorithms for automatic fault detection in a physical process plant. The algorithms are tested on a simulation of a liquid level control system.",1987,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4642442,no,no,1487371925.896197
Design and implementation of a pluggable fault tolerant CORBA infrastructure,"In this paper we present the design and implementation of a Pluggable Fault Tolerant CORBA Infrastructure that provides fault tolerance for CORBA applications by utilizing the pluggable protocols framework that is available for most CORBA ORBS. Our approach does not require modification to the CORBA ORB, and requires only minimal modifications to the application. Moreover; it avoids the difficulty of retrieving and assigning the ORB state, by incorporating the fault tolerance mechanisms into the ORB. The Pluggable Fault Tolerant CORBA Infrastructure achieves performance that is similar to, or better than, that of other Fault Tolerant CORBA systems, while providing strong replica consistency.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1015513,no,no,1487371925.896196
Test case prioritization: a family of empirical studies,"To reduce the cost of regression testing, software testers may prioritize their test cases so that those which are more important, by some measure, are run earlier in the regression testing process. One potential goal of such prioritization is to increase a test suite's rate of fault detection. Previous work reported results of studies that showed that prioritization techniques can significantly improve rate of fault detection. Those studies, however, raised several additional questions: 1) Can prioritization techniques be effective when targeted at specific modified versions; 2) what trade-offs exist between fine granularity and coarse granularity prioritization techniques; 3) can the incorporation of measures of fault proneness into prioritization techniques improve their effectiveness? To address these questions, we have performed several new studies in which we empirically compared prioritization techniques using both controlled experiments and case studies",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=988497,no,no,1487371925.896194
Substituting Voas's testability measure for Musa's fault exposure ratio,"This paper analyzes the relationship between Voas's (1991) software testability measure and Musa's (1987) fault exposure ratio, K. It has come to our attention that industrial users of Musa's model employ his published, experimental value for K, 4.2Ã—10<sup>-7</sup>, for their projects, instead of creating their own K estimate for the system whose reliability is being assessed. We provide a theoretical argument for how a slight modification to Voas's formulae for determining a predicted minimal fault size can provide a predicted average fault size. The predicted average fault size can be used as a substitute for 4.2Ã—10<sup>-7</sup>, and in our opinion is a more plausible choice for K",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=542188,no,no,1487371925.896193
A foundation for adaptive fault tolerance in software,"Software requirements often change during the operational lifetime of deployed systems. To accommodate requirements not conceived during design time, the system must be able to adapt its functionality and behavior. The paper examines a formal model for reconfigurable software processes that permits adaptive fault tolerance by adding or removing specific fault tolerance techniques during runtime. A distributed software-implemented fault tolerance (SIFT) environment for managing user applications has been implemented using ARMOR processes that conform to the formal model of reconfigurability. Because ARMOR processes are reconfigurable, they can tailor the fault tolerance services that they provide to themselves and to the user applications. We describe two fault tolerance techniques: microcheckpointing and assertion checking, that have been incorporated into ARMOR process via reconfigurations to the original ARMOR design. Experimental evaluations of the SIFT environment on a testbed cluster at the Jet Propulsion Laboratory demonstrate the effectiveness of these two fault tolerance techniques in limiting data error propagation among the ARMOR processes. These experiments validate the concept of using an underlying reconfigurable process architecture as the basis for implementing replaceable error detection and recovery services.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1194806,no,no,1487371925.896191
Identification of test process improvements by combining fault trigger classification and faults-slip-through measurement,"Successful software process improvement depends on the ability to analyze past projects and determine which parts of the process that could become more efficient. One source of such an analysis is the faults that are reported during development. This paper proposes how a combination of two existing techniques for fault analysis can be used to identify where in the test process improvements are needed, i.e. to pinpoint which activities in which phases that should be improved. This was achieved by classifying faults after which test activities that triggered them and which phase each fault should have been found in, i.e. through a combination of orthogonal defect classification (ODC) and faults-slip-through measurement. As a part of the method, the paper proposes a refined classification scheme due to identified problems when trying to apply ODC classification schemes in practice. The feasibility of the proposed method was demonstrated by applying it on an industrial software development project at Ericsson AB. The obtained measures resulted in a set of quantified and prioritized improvement areas to address in consecutive projects.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541824,no,no,1487371925.89619
Fault-tolerant computer systems,"The paper reviews the methods by which reliable processing and control can be achieved using fault-tolerant digital computers. The motivation for employing such systems is discussed, together with an indication of current and potential areas of application. The features of fault-tolerant computers are described in general terms, together with a system design procedure specific to the development of a reliable computer. The adherence to a well structured design methodology is particularly important in a fault-tolerant computer to ensure an initially fault-free system. In order to follow this design procedure an intimate knowledge of the following subject areas is required: fault classification, redundancy techniques and their relative merits and reliability modelling and analysis. These topics are covered in the paper, with particular emphasis being placed upon the implementation of hardware, software, data and time-redundancy techniques. Examples of fault-tolerant computers proposed and produced in the last decade are described. The availability of large scale integrated circuits (LSI) and in particular microprocessors will have a profound effect on the development and application of fault-tolerant computers. The implications of using LSI in this area are therefore discussed including a brief description of two fundamentally different approaches to the realisation of a fault tolerant microcomputer.",1981,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4644988,no,no,1487371925.896188
Reengineering analysis of object-oriented systems via duplication analysis,"All software systems, no matter how they are designed, are subject to continuous evolution and maintenance activities to eliminate defects and extend their functionalities. This is particularly true for object-oriented systems, where we may develop different software systems using the same internal library or framework. These systems may evolve in quite different directions in order to cover different functionalities. Typically, there is the need to analyze their evolution in order to redefine the library or framework boundaries. This is a typical problem of software reengineering analysis. In this paper, we describe metrics, based on duplication analysis, that contribute to the process of reengineering analysis of object-oriented systems. These metrics are the basic elements of a reengineering analysis method and tool. Duplication analyses at the file, class and method levels have been performed. A structural analysis using metrics that capture similarities in class structure has been also exploited. In order to identify the best approach for the reengineering analysis of object-oriented systems, a comparison between the two approaches is described. In this paper, a case study based on real cases is presented, in which the results obtained by using a reengineering process with and without the analysis tool are described. The purpose of this study is to discover which method is the most powerful and how much time reduction can be obtained by its use.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=919132,no,no,1487371925.896186
Rapid detection of faults for safety critical aircraft operation,"Fault diagnosis typically assumes a sufficiently large fault signature and enough time for a reliable decision to be reached. However, for a class of safety critical faults on commercial aircraft engines, prompt detection is paramount within a millisecond range to allow accommodation to avert undesired engine behavior. At the same time, false positives must be avoided to prevent inappropriate control action. To address these issues, several advanced features were developed that operate on the residuals of a model based detection scheme. We show that these features pick up system changes reliably within the required time. A bank of binary classifiers determines the presence of the fault as determined by a maximum likelihood hypothesis test. We show performance results for four different faults at various levels of severity and show performance results throughout the entire flight envelope on a high fidelity aircraft engine model.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1368144,no,no,1487371925.429999
Robust prediction of fault-proneness by random forests,"Accurate prediction of fault prone modules (a module is equivalent to a C function or a C+ + method) in software development process enables effective detection and identification of defects. Such prediction models are especially beneficial for large-scale systems, where verification experts need to focus their attention and resources to problem areas in the system under development. This paper presents a novel methodology for predicting fault prone modules, based on random forests. Random forests are an extension of decision tree learning. Instead of generating one decision tree, this methodology generates hundreds or even thousands of trees using subsets of the training data. Classification decision is obtained by voting. We applied random forests in five case studies based on NASA data sets. The prediction accuracy of the proposed methodology is generally higher than that achieved by logistic regression, discriminant analysis and the algorithms in two machine learning software packages, WEKA [I. H. Witten et al. (1999)] and See5. The difference in the performance of the proposed methodology over other methods is statistically significant. Further, the classification accuracy of random forests is more significant over other methods in larger data sets.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1383136,no,no,1487371925.429998
OBDD-based evaluation of reliability and importance measures for multistate systems subject to imperfect fault coverage,"Algorithms for evaluating the reliability of a complex system such as a multistate fault-tolerant computer system have become more important. They are designed to obtain the complete results quickly and accurately even when there exist a number of dependencies such as shared loads (reconfiguration), degradation, and common-cause failures. This paper presents an efficient method based on ordered binary decision diagram (OBDD) for evaluating the multistate system reliability and the Griffith's importance measures which can be regarded as the importance of a system-component state of a multistate system subject to imperfect fault-coverage with various performance requirements. This method combined with the conditional probability methods can handle the dependencies among the combinatorial performance requirements of system modules and find solutions for multistate imperfect coverage model. The main advantage of the method is that its time complexity is equivalent to that of the methods for perfect coverage model and it is very helpful for the optimal design of a multistate fault-tolerant system.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1542055,no,no,1487371925.429996
An industrial case study of the verification and validation activities,"The aim of verification and validation is to ensure the quality of a software product. The main fault detection techniques used are software inspections and testing. Software inspections aim at finding faults in the beginning of the software life-cycle and testing in the end. It is, however, difficult to know what kind of faults that are found, the severity of these, how much time it will take to correct them etc. Hence, it is important for a software organization to know what faults that can be found in inspections and testing, respectively. We report on a case study over 2 years in a large project. The purpose of the case study is to investigate the trade-off between inspections and testing. A measure of goodness is introduced, which measures if faults could have been found earlier in the process. The measure was successfully used to illustrate the effect on when faults are found concerning a process change, a project decision, and extensions of developed test cases. An increased focus on the development of low level design specifications before the coding activity, and testing improvements in early phases were concluded to be important process improvements for the organization. The results also concern how much resources that are used in the various development phases, from requirements development to system verification, and the severity of faults that are found in the various phases. The results will be used as input to a quality improvement program in the company.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1232470,no,no,1487371925.429995
Model- and knowledge-based fault detection and diagnosis of gas transmission networks,"This paper describes an expert system for online fault detection and diagnosis of gas transmission networks, combining model- and knowledge-based methods. It consists of a set of hierarchically structured components which include signal processing, Luenberger-type state observation, rule-based knowledge processing as well as an advanced user interface. The diagnosis system was tested with real measurement data from a medium sized gas distribution network. Its real-time capability and effectiveness for basic fault detection purposes was demonstrated by industrial applications",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=384834,no,no,1487371925.429993
Performance analysis of a generalized concurrent error detection procedure,"A general procedure for error detection in complex systems, called the data block capture and analysis monitoring process, is described and analyzed. It is assumed that, in addition to being exposed to potential external fault sources, a complex system will in general always contain embedded hardware and software fault mechanisms which can cause the system to perform incorrect computations and/or produce incorrect output. Thus, in operation, the system continuously moves back and forth between error and no-error states. These external fault sources or internal fault mechanisms are extremely difficult to detect. The data block capture and analysis monitoring process is concerned with detecting deviations from the normal performance of the system, known as errors, which are symptomatic of fault conditions. The process consists of repeatedly recording a fixed amount of data from a set of predetermined observation lines of the system being monitored (i.e. capturing a block of data) and then analyzing the captured block in an attempt to determine whether the system is functioning correctly",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=46280,no,no,1487371925.429992
A change impact model for changeability assessment in object-oriented software systems,"Growing maintenance costs have become a major concern for developers and users of software systems. Changeability is an important aspect of maintainability, especially in environments where software changes are frequently required. In this work, the assumption that high-level design has an influence on maintainability is carried over to changeability and investigated for quality characteristics. The approach taken to assess the changeability of an object-oriented (OO) system is to compute the impact of changes made to classes of the system. A change impact model is defined at the conceptual level and mapped on the C++ language. In order to assess the practicality of the model on large industrial software systems, an experiment involving the impact of one change is carried out on a telecommunications system. The results suggest that the software can easily absorb this kind of change and that well chosen conventional OO design metrics can be used as indicators of changeability",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=756690,no,no,1487371925.429991
Induction machine condition monitoring with higher order spectra,"This paper describes a novel method of detecting and unambiguously diagnosing the type and magnitude of three induction machine fault conditions from the single sensor measurement of the radial electromagnetic machine vibration. The detection mechanism is based on the hypothesis that the induction machine can be considered as a simple system, and that the action of the fault conditions are to alter the output of the system in a characteristic and predictable fashion. Further, the change in output and fault condition can be correlated allowing explicit fault identification. Using this technique, there is no requirement for a priori data describing machine fault conditions, the method is equally applicable to both sinusoidally and inverter-fed induction machines and is generally invariant of both the induction machine load and speed. The detection mechanisms are rigorously examined theoretically and experimentally, and it is shown that a robust and reliable induction machine condition-monitoring system has been produced. Further, this technique is developed into a software-based automated commercially applicable system",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=873211,no,no,1487371925.429989
Fault injection experiment results in space borne parallel application programs,"Development of the REE Commercial-Off-The-Shelf (COTS) based space-borne supercomputer requires a detailed knowledge of system behavior in the presence of Single Event Upset (SEU) induced faults. When combined with a hardware radiation fault model and mission environment data in a medium grained system model, experimentally obtained fault behavior data can be used to: predict system reliability, availability and performance; determine optimal fault detection methods and boundaries; and define high ROI fault tolerance strategies. The REE project has developed a fault injection suite of tools and a methodology for experimentally determining system behavior statistics in the presence of application level SEU induced transient faults. Initial characterization of science data application code for an autonomous Mars Rover geology application indicates that this code is relatively insensitive to SEUs and thus can be made highly immune to application level faults with relatively low overhead strategies.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1035379,no,no,1487371925.429987
Reliable distributed sorting through the application-oriented fault tolerance paradigm,"A fault-tolerant parallel sorting algorithm developed using the application-oriented fault tolerance paradigm is presented. The algorithm is tolerant of one processor/link failure in an <e1>n</e1>-cube. The addition of reliability to the sorting algorithm results in a performance penalty. Asymptotically, the fault-tolerant algorithm is less costly than host sorting. Experimentally it is shown that fault-tolerant sorting quickly becomes more efficient that host sorting when the bitonic sort/merge is considered. The main contribution is the demonstration that the application-oriented fault tolerance paradigm is applicable to problems of a noniterative-convergent nature",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=149960,no,no,1487371925.429985
The design of reliable devices for mission-critical applications,"Mission-critical applications require that any failure that may lead to erroneous behavior and computation is detected and signaled as soon as possible in order not to jeopardize the entire system. Totally self-checking (TSC) systems are designed to be able to autonomously detect faults when they occur during normal circuit operation. Based on the adopted TSC design strategy and the goal pursued during circuit realization (e.g., area minimization), the circuit, although TSC, may not promptly detect the fault depending on the actual number of input configurations that serve as test vectors for each fault in the network. If such a number is limited, although TSC it may be improbable that the fault is detected once it occurs, causing detection and aliasing problems. The paper presents a design methodology, based on a circuit re-design approach and an evaluation function, for improving a TSC circuit promptness in detecting faults' occurrence, a property we will refer to as TSC quality.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1246540,no,no,1487371924.965116
An evaluation of software design using the DEMETER tool,"The purpose of this study was to investigate the provision of measures suitable for evaluating software designs. These would allow some degree of predictability in estimating the quality of a coded software product. Experiments were conducted to test hypotheses concerning the evaluation of software design metrics related to code complexity. Three medium-sized software projects were used. Metrics from the design stage of these projects were collected by using a software package called DEMETER. This tool was developed specifically to provide the researcher with a usable method of collecting data for calculating design metrics. Data analysis was performed to identify relationships between the measures of design quality and software coded complexity represented by control flow as well as data complexity. The results indicated that reducing the number of interconnections between software units, together with an increase in the relationships of the elements within a module (controlled by the flow of global data), improves the resultant software.<<ETX>>",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=254075,no,no,1487371924.965115
Statistical analysis on a case study of load effect on PSD technique for induction motor broken rotor bar fault detection,Broken rotor bars in an induction motor create asymmetries and result in abnormal amplitude of the sidebands around the fundamental supply frequency and its harmonics. Monitoring the power spectral density (PSD) amplitudes of the motor currents at these frequencies can be used to detect the existence of broken rotor bar faults. This paper presents a study on an actual three-phase induction motor using the PSD analysis as a broken rotor bar fault detection technique. The distributions of PSD amplitudes of experimental healthy and faulty motor data sets at these specific frequencies are analyzed statistically under different load conditions. Results indicate that statistically significant conclusions on broken rotor bar detection can vary significantly under different load conditions and under different inspected frequencies. Detection performance in terms of the variation of PSD amplitudes is also investigated as a case study.,2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1234558,no,no,1487371924.965113
Error and failure analysis of a UNIX server,"This paper presents a measurement-based dependability study of a UNIX server. The event logs of a UNIX server are collected to form the dependability data basis. Message logs spanning approximately eleven months were collected for this study. The event log data are classified and categorized to calculate parameters such as MTBF and availability. Component analysis is also performed to identify modules that are prone to errors in the system. Next, the system error activity proceeding each system failure is analyzed to identify error patterns that may be precursors of the observed failure events. Lastly, the error/failure results from the measurement are reviewed in the perspective of the fault/error assumptions made in several popular fault injection studies",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=731618,no,no,1487371924.965112
Applications of Global Positioning System (GPS) in geodynamics: with three examples from Turkey,"Global Positioning System (GPS) has been very useful tool for the last two decades in the area of geodynamics because or the validation of the GPS results by the Very Long Baseline Interferometry (VLBI) and Satellite Laser Ranging (SLR) observations. The modest budget requirement and the high accuracy relative positioning availability of GPS increased the use of it in determination of crustal and/or regional deformations. Since the civilian use the GPS began in 1980, the development on the receiver and antenna technology with the ease of use software packages reached to a well known state, which may be named as a revolution in the Earth Sciences among other application fields. Analysis of a GPS network can also give unknown information about the fault lines that can not be seen from the ground surface. Having information about the strain accumulation along the fault line may allow us to evaluate future probabilities of regional earthquake hazards and develop earthquake scenarios for specific faults. In this study, the use of GPS in geodynamical studies will be outlined throughout the instrumentation, the measurements, and the methods utilized. The preliminary results of three projects, sponsored by the Scientific & Technical Research Council of Turkey (TUBITAK) and Istanbul Technical University (ITU) which have been carried out in Turkey using GPS will be summarized. The projects are mainly aimed to determine the movements along the fault zones. Two of the projects have been implemented along the North Anatolian Fault Zone. (NAFZ), one is in Mid-Anatolia region, and the ther is in Western Marmara region. The third project has been carried out in the Fethiye-Burdur region. The collected GPS data were processed by the GAMIT/GLOBK software The results are represented as velocity vectors obtained using the yearly combinations of the daily measured GPS data.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1512597,no,no,1487371924.965111
An effective fault-tolerant routing methodology for direct networks,"Current massively parallel computing systems are being built with thousands of nodes, which significantly affect the probability of failure. M. E. Gomex proposed a methodology to design fault-tolerant routing algorithms for direct interconnection networks. The methodology uses a simple mechanism: for some source-destination pairs, packets are first forwarded to an intermediate node, and later, from this node to the destination node. Minimal adaptive routing is used along both subpaths. For those cases where the methodology cannot find a suitable intermediate node, it combines the use of intermediate nodes with two additional mechanisms: disabling adaptive routing and using misrouting on a per-packet basis. While the combination of these three mechanisms tolerates a large number of faults, each one requires adding some hardware support in the network and also introduces some overhead. In this paper, we perform an in-depth detailed analysis of the impact of these mechanisms on network behaviour. We analyze the impact of the three mechanisms separately and combined. The ultimate goal of this paper is to obtain a suitable combination of mechanisms that is able to meet the trade-off between fault-tolerance degree, routing complexity, and performance.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1327925,no,no,1487371924.965109
An expert fault manager using an object meta-model,"This paper describes the design and implementation of an expert fault management (EFM) system, based on an object-oriented meta-model, which can isolate causes of performance problems in a distributed environment. Error diagnosis can integrate application, system, and network related causes and identify the root cause, as well as affected applications. Diagnosis can be proactive or reactive. The implementation is based on the GEN-X expert system, and intelligent agents written in PERL. It is self-configuring, and has demonstrated it's ability to detect problems earlier and more accurately than humans",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=527349,no,no,1487371924.965108
Software recreate problems estimated to range 10-20 percent: A case study on two operating system products,"Software recreates are necessitated due to inadequate diagnostic capability following a failure. They impact the service process and the perception of availability, but have never been adequately quantified. This paper develops a technique to make the key measurements of: percent recreate, arrival rate and open time, from problem service data without requiring any additional instrumentation. The study is conducted over an 18 month period on two operating system products, that are among the best in the industry for diagnosis and service. The results provide the first insight into the problem and some accurate baselines",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=624284,no,no,1487371924.965106
Metric Analysis and Data Validation Across Fortran Projects,"The desire to predict the effort in developing or explain the quality of software has led to the proposal of several metrics in the literature. As a step toward validating these metrics, the Software Engineering Laboratory has analyzed the Software Science metrics, cyclomatic complexity, and various standard program measures for their relation to 1) effort (including design through acceptance testing), 2) development errors (both discrete and weighted according to the amount of time to locate and frix), and 3) one another. The data investigated are collected from a production Fortran environment and examined across several projects at once, within individual projects and by individual programmers across projects, with three effort reporting accuracy checks demonstrating the need to validate a database. When the data come from individual programmers or certain validated projects, the metrics' correlations with actual effort seem to be strongest. For modules developed entirely by individual programmers, the validity ratios induce a statistically significant ordering of several of the metrics' correlations. When comparing the strongest correlations, neither Software Science's E metric, cyclomatic complexity nor source lines of code appears to relate convincingly better with effort than the others",1983,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1703112,no,no,1487371924.965105
Evolutionary neural networks: a robust approach to software reliability problems,"In this empirical study, from a large data set of software metrics for program modules, thirty distinct partitions into training and validation sets are automatically generated with approximately equal distributions of fault prone and not fault prone modules. Thirty classification models are built for each of the two approaches considered-discriminant analysis and the evolutionary neural network (ENN) approach-and their performances on corresponding data sets are compared. The lower error proportions for ENNs on fault prone, not fault prone, and overall classification were found to be statistically significant. The robustness of ENNs follows from their superior performance on the range of data configurations used. It is suggested that ENNs can be effective in other software reliability problem domains, where they have been largely ignored",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=630844,no,no,1487371924.965102
Predicting software development errors using software complexity metrics,"Predictive models that incorporate a functional relationship of program error measures with software complexity metrics and metrics based on factor analysis of empirical data are developed. Specific techniques for assessing regression models are presented for analyzing these models. Within the framework of regression analysis, the authors examine two separate means of exploring the connection between complexity and errors. First, the regression models are formed from the raw complexity metrics. Essentially, these models confirm a known relationship between program lines of code and program errors. The second methodology involves the regression of complexity factor measures and measures of errors. These complexity factors are orthogonal measures of complexity from an underlying complexity domain model. From this more global perspective, it is believed that there is a relationship between program errors and complexity domains of program structure and size (volume). Further, the strength of this relationship suggests that predictive models are indeed possible for the determination of program errors from these orthogonal complexity domains",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=46879,no,no,1487371924.501196
An empirical comparison of software fault tolerance and fault elimination,"The authors compared two major approaches to the improvement of software-software fault elimination and software fault tolerance-by examination of the fault detection (and tolerance, where applicable) of five techniques: run-time assertions, multiversion voting, functional testing augmented by structural testing, code reading by stepwise abstraction, and static data-flow analysis. The focus was on characterizing the sets of faults detected by the techniques and on characterizing the relationships between these sets of faults. Two categories of questions were investigated: (1) comparison between fault elimination and fault tolerance techniques and (2) comparisons among various testing techniques. The results provide information useful for making decisions about the allocation of project resources, show strengths and weaknesses of the techniques studies, and indicate directions for future research",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=67598,no,no,1487371924.501194
Optimal cost-effective design of parallel systems subject to imperfect fault-coverage,"Computer-based systems intended for critical applications are usually designed with sufficient redundancy to be tolerant of errors that may occur. However, under imperfect fault-coverage conditions (such as the system cannot adequately detect, locate, and recover from faults and errors in the system), system failures can result even when adequate redundancy is in place. Because parallel architecture is a well-known and powerful architecture for improving the reliability of fault-tolerant systems, this paper presents the cost-effective design policies of parallel systems subject to imperfect fault-coverage. The policies are designed by considering (1) cost of components, (2) failure cost of the system, (3) common-cause failures, and (4) performance levels of the system. Three kinds of cost functions are formulated considering that the total average cost of the system is based on: (1) system unreliability, (2) failure-time of the system, and (3) total processor-hours. It is shown that the MTTF (mean time to failure) of the system decreases by increasing the spares beyond a certain limit. Therefore, this paper also presents optimal design policies to maximize the MTTF of these systems. The results of this paper can also be applied to gracefully degradable systems.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181898,no,no,1487371924.501193
An automatic restructuring approach preserving the behavior of object-oriented designs,"Work on restructuring object-oriented designs involves metrics for quality estimation, and automated transformations. However, these factors have been treated almost independently of each other. A long-term goal is to define behavior-preserving design transformations, and automate the transformations using metrics. This paper describes an automatic restructuring approach that preserves the behavior of an object-oriented design. Cohesion and coupling metrics, based on abstract models that represent design components and their relationships, are defined to quantify designs and provide criteria for comparing alternative designs. Primitive operations and semantics for restructuring are defined to validate the preservation of design behavior This approach devises a fitness function using cohesion and coupling metrics, and restructures object-oriented designs by applying a genetic algorithm using the fitness function. We empirically evaluate the capability of this approach by applying it to the designs of Java programs and compare our results with a simulated annealing-based approach. Results from our experiments demonstrate that this approach may be useful in improving object-oriented designs automatically.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=991507,no,no,1487371924.501191
COFTA: hardware-software co-synthesis of heterogeneous distributed embedded systems for low overhead fault tolerance,"Embedded systems employed in critical applications demand high reliability and availability in addition to high performance. Hardware-software co-synthesis of an embedded system is the process of partitioning, mapping, and scheduling its specification into hardware and software modules to meet performance, cost, reliability, and availability goals. In this paper, we address the problem of hardware-software co-synthesis of fault-tolerant real-time heterogeneous distributed embedded systems. Fault detection capability is imparted to the embedded system by adding assertion and duplicate-and-compare tasks to the task graph specification prior to co-synthesis. The dependability (reliability and availability) of the architecture is evaluated during co-synthesis. Our algorithm, called COFTA (Co-synthesis Of Fault-Tolerant Architectures), allows the user to specify multiple types of assertions for each task. It uses the assertion or combination of assertions which achieves the required fault coverage without incurring too much overhead. We propose new methods to: 1) Perform fault tolerance based task clustering, which determines the best placement of assertion and duplicate-and-compare tasks, 2) Derive the best error recovery topology using a small number of extra processing elements, 3) Exploit multidimensional assertions, and 4) Share assertions to reduce the fault tolerance overhead. Our algorithm can tackle multirate systems commonly found in multimedia applications. Application of the proposed algorithm to a large number of real-life telecom transport system examples (the largest example consisting of 2,172 tasks) shows its efficacy. For fault secure architectures, which just have fault detection capabilities, COFTA is able to achieve up to 48.8 percent and 25.6 percent savings in embedded system cost over architectures employing duplication and task-based fault tolerance techniques, respectively. The average cost overhead of COFTA fault-secure architectures over simplex architectures is only 7.3 percent. In case of fault-tolerant architectures, which cannot only detect but also tolerate faults, COFTA is able to achieve up to 63.1 percent and 23.8 percent savings in embedded system cost over architectures employing triple-modular redundancy, and task-based fault tolerance techniques, respectively. The average cost overhead of COFTA fault-tolerant architectures over simplex architectures is only 55.4 percent",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=762534,no,no,1487371924.50119
Assessing reliability risk using fault correction profiles,"Building on the concept of the fault correction profile - a set of functions that predict fault correction events as a function of failure detection events - introduced in previous research, we define and apply reliability risk metrics that are derived from the fault correction profile. These metrics assess the threat to reliability of an unstable fault correction process. The fault correction profile identifies the need for process improvements and provides information for developing fault correction strategies. Applying these metrics to the NASA Goddard Space Flight Center fault correction process and its data, we demonstrate that reliability risk can be measured and used to identify the need for process improvement.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1281738,no,no,1487371924.501189
"Testability, failure rates, detectability, trustability and reliability","Discusses the relationship between several statistical measures of program dependability, including failure rates and testability. This is done by describing these concepts within the framework of a confidence-based measure called trustability. Suppose that M is a testing method, F is a class of faults and P is a class of programs. Suppose that the probability of a fault from F causing a failure is at least D when a program pâˆˆP is tested according to M, if in fact p contains a fault of type F. Then D is called the detectability of M with respect to F and P. If we test a program using a method with detectability D, and see no faults, then we can conclude with risk at most 1-D that the program has no faults, i.e. we can have confidence at least C=D that the program is fault-free for the associated fault class F. If we have confidence at least C that a program has no faults, then we say that the program has trustability C with respect to F. More refined measures of trustability can be defined which also take fault class frequencies into account. Testability is defined to be the probability of finding a fault in a program p, if p contains a fault. The probability that a program will fail when it is tested over its operational distribution is called its failure rate. Trustability is confidence in the absence of faults and reliability is the probability of a program operating without failure. Trustability and reliability coincide if the class of faults for which we have a certain level of trustability is the class of common case faults",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=318456,no,no,1487371924.501187
CVS release history data for detecting logical couplings,"The dependencies and interrelations between classes and modules affect the maintainability of object-oriented systems. It is therefore important to capture weaknesses of the software architecture to make necessary corrections. We describe a method for software evolution analysis. It consists of three complementary steps, which form an integrated approach for the reasoning about software structures based on historical data: 1) the quantitative analysis uses version information for the assessment of growth and change behavior; 2) the change sequence analysis identifies common change patterns across all system parts; and 3) the relation analysis compares classes based on CVS release history data and reveals the dependencies within the evolution of particular entities. We focus on the relation analysis and discuss its results; it has been validated based on empirical data collected from a concurrent versions system (CVS) covering 28 months of a picture archiving and communication system (PACS). Our software evolution analysis approach enabled us to detect shortcomings of PACS such as architectural weaknesses, poorly designed inheritance hierarchies, or blurred interfaces of modules.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1231205,no,no,1487371924.501185
OOA metrics for the Unified Modeling Language,"UML is the emerging standard for expressing OOA/OOD models. New metrics for object oriented analysis models are introduced, and existing ones are adapted to the entities and concepts of UML. In particular, these metrics concern UML use case diagrams and class diagrams used during the OOA phase. The proposed metrics are intended to allow an early estimate of development efforts, implementation time and cost of the system under development, and to measure its object orientedness and quality since the beginning of the analysis phase. The proposed metric suite is described in detail, and its relations with proposed metrics found in the literature are highlighted. Some measurements on three software projects are given",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=665739,no,no,1487371924.501183
Software reliability growth model considering testing profile and operation profile,"The testing and operation environments may be essentially different, thus the fault detection rate (FDR) of testing phase is different from that of the operation phase. In this paper, based on the representative model, G-O model, of nonhomogeneous Poisson process (NHPP), a transformation is performed between the FDR of the testing phase to that of the operation considering the profile differences of the two phases, and then a software reliability growth model (SRGM) called TO-SRGM describing the differences of the FDR between the testing phase and the operation phase is proposed. Finally, the parameters of the model are estimated using the least squares estimate (LSE) based on normalized failure data. Experiment results show that the goodness-of-fit of the TO-SRGM is better than that of the G-0 model and the PZ-SRGM on the normalized failure data set.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510059,no,no,1487371924.50118
Software reuse metrics for object-oriented systems,"The importance of software measurement is increasing leading to development of new measurement techniques. Reusing existing software components is a key feature in increasing software productivity. It is one of the key elements in object-oriented programming, which reduces the cost and increases the quality of the software. An important feature of C++ called templates support generic programming, which allows the programmer to develop reusable software modules such as functions, classes, etc. The need for software reusability metrics is particularly acute for an organization in order to measure the degree of generic programming included in the form of templates in code. This research addresses this need and introduces a new set of metrics for object-oriented software. Two metrics are proposed for measuring amount of genericty included in the code and then analytically evaluated against Weyuker's set of nine axioms. This set of metrics is then applied to standard projects and accordingly ways in which project managers can use these metrics are suggested.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1563143,no,no,1487371924.038515
Compiler assisted fault detection for distributed-memory systems,"Distributed-memory systems provide the most promising performance to cost ratio for multiprocessor computers due to their scalability. However the issues of fault detection and fault tolerance are critical in such systems since the probability of having faulty components increases with the number of processors. We propose a methodology for fault detection through compiler support. More specifically, we augment the single-program multiple-data (SPMD) execution model to duplicate selected data items in such a way that during execution, whenever a value of a duplicated data is computed, the owners of the data are tested. The proposed compiler assisted fault detection technique does not require any specialized hardware and allows for a selective choice of redundancy at compile time",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=296667,no,no,1487371924.038514
Analyzing software quality with limited fault-proneness defect data,"Assuring whether the desired software quality and reliability is met for a project is as important as delivering it within scheduled budget and time. This is especially vital for high-assurance software systems where software failures can have severe consequences. To achieve the desired software quality, practitioners utilize software quality models to identify high-risk program modules: e.g., software quality classification models are built using training data consisting of software measurements and fault-proneness data from previous development experiences similar to the project currently under-development. However, various practical issues can limit availability of fault-proneness data for all modules in the training data, leading to the data consisting of many modules with no fault-proneness data, i.e., unlabeled data. To address this problem, we propose a novel semi-supervised clustering scheme for software quality analysis with limited fault-proneness data. It is a constraint-based semi-supervised clustering scheme based on the k-means algorithm. The proposed approach is investigated with software measurement data of two NASA software projects, JM1 and KC2. Empirical results validate the promise of our semi-supervised clustering technique for software quality modeling and analysis in the presence of limited defect data. Additionally, the approach provides some valuable insight into the characteristics of certain program modules that remain unlabeled subsequent to our semi-supervised clustering analysis.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1581286,no,no,1487371924.038512
Detection of faults in induction motors using artificial neural networks,"When faults begin to develop, the dynamic processes in an induction motor change and this is reflected in the shape of the vibration spectrum. Thus, one can detect and identify machine faults by analyzing the vibration spectrum by examining whether any characteristics frequencies appear on the spectra. Here, the authors describe how fault detection and identification using such a vibration method on a induction motor was accomplished using a simple neural network program. Two machine faults, of bearing wear and unbalanced supply fault, are simulated and tested. Acceptable results are obtained and faults are classified accordingly",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=497719,no,no,1487371924.038511
Analyzing software measurement data with clustering techniques,"For software quality estimation, software development practitioners typically construct quality-classification or fault prediction models using software metrics and fault data from a previous system release or a similar software project. Engineers then use these models to predict the fault proneness of software modules in development. Software quality estimation using supervised-learning approaches is difficult without software fault measurement data from similar projects or earlier system releases. Cluster analysis with expert input is a viable unsupervised-learning solution for predicting software modules' fault proneness and potential noisy modules. Data analysts and software engineering experts can collaborate more closely to construct and collect more informative software metrics.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1274907,no,no,1487371924.038509
Classification-tree models of software-quality over multiple releases,"This paper presents an empirical study that evaluates software-quality models over several releases, to address the question, â€œHow long will a model yield useful predictions?â€?The classification and regression trees (CART) algorithm is introduced, CART can achieve a preferred balance between the two types of misclassification rates. This is desirable because misclassification of fault-prone modules often has much more severe consequences than misclassification of those that are not fault-prone. The case-study developed 2 classification-tree models based on 4 consecutive releases of a very large legacy telecommunication system. Forty-two software product, process and execution metrics were candidate predictors. Model 1 used measurements of the first release as the training data set; this model had 11 important predictors. Model 2 used measurements of the second release as the training data set; this model had 15 important predictors. Measurements of subsequent releases were evaluation data sets. Analysis of the models' predictors yielded insights into various software development practices. Both models had accuracy that would be useful to developers. One might suppose that software-quality models lose their value very quickly over successive releases due to evolution of the product and the underlying development processes. The authors found the models remained useful over all the releases studied",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=855532,no,no,1487371924.038508
A friend in need is a friend indeed [software metrics and friend functions],"Previous research has highlighted the extensive use of the C++ friend construct in both library-based and application-based systems. However, existing software metrics do not concentrate on measuring friendship accurately, a surprising omission given the debate friendship has caused in the object-oriented community. In this paper, a number of software metrics, that measure the extent to which friend class relationships are actually used in systems, are defined. These metrics are based on the interactions for which the friend construct is necessary, as well as the direction of this association between classes. Our results, in applying these metrics to the top 100 downloaded systems from sourceforge.net, indicate that up to 66% of friend class relationships in systems are redundant. Elsewhere, friend function declarations would have been more appropriate in many cases. In addition, it has been shown that friendship-based coupling contributes significantly to the high coupling of friend classes for only 25% of the systems studied.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541854,no,no,1487371924.038506
An approach for intelligent detection and fault diagnosis of vacuum circuit breakers,"In this paper, an approach for intelligent detection and fault diagnosis of vacuum circuit breakers is introduced, by which, the condition of a vacuum circuit breaker can be monitored on-line, and the detectable faults can be identified, located, displayed and saved for the use of analyzing their change tendencies. The main detecting principles and diagnostics are described. Both the hardware structure and software design are also presented.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=993739,no,no,1487371924.038505
Design of phase and amplitude comparators for transmission line protection,"This paper describes the design of poly-phase power system relays by using the amplitude and phase comparison techniques. Both techniques are used for designing phase-to-phase, three-phase and phase-to-ground relays. A new poly-phase amplitude method, which uses inputs from the phase comparator, is proposed for protecting power transmission lines during phase-to-phase and three-phase faults. A unique amplitude and phase comparison approach is introduced for use in a poly-phase phase-to-ground relaying unit. The proposed designs are suitable for implementation on microprocessors. Performance of the comparators was evaluated by using data simulated using EMTDC and some results are included in the paper",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=627135,no,no,1487371924.038503
Failure analysis of open faults by using detecting/un-detecting information on tests,"Recently, manufacturing defects including opens in the interconnect layers have been increasing. Therefore, a failure analysis for open faults has become important in manufacturing. Moreover, the failure analysis for open faults under BIST environment is demanded. Since the quality of the failure analysis is engaged by the resolution of locating the fault, we propose the method for locating single open fault at a stem, based on only detecting/un-detecting information on tests. Our method deduces candidate faulty stems based on the number of detections for single stuck-at fault at each fan-out branches, by performing single stuck-at fault simulation with both detecting and un-detecting tests. To improve the ability of locating the fault, the method reduces the candidate faulty stems based on the number of detections for multiple stuck-at faults at fanout branches of the candidate faulty stem, by performing multiple stuck-at fault simulation with detecting tests.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1376562,no,no,1487371924.038501
Detection of localised array fault from near field data,"The technique of localized fault diagnosis in an array antenna system is considered. Commonly available software provides transformation of near zone measured data to the far field pattern in the visible region only. It is shown that the reverse transformation of this far.field pattern in the visible region back to the aperture fails to detect the localized fault. However, in the case of the forward transform, if the far field pattern is computed not only in the visible region but also well into the invisible region, the detection of the localized fault is found to be possible by doing the reverse transformation of these data, which extends well into the invisible region. Since a commonly occurring fault in a passive array is the phase-only fault, the concept has been validated by a simulated example of this type of fault.<<ETX>>",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=175113,no,no,1487371923.577165
Software measurement data analysis using memory-based reasoning,"The goal of accurate software measurement data analysis is to increase the understanding and improvement of software development process together with increased product quality and reliability. Several techniques have been proposed to enhance the reliability prediction of software systems using the stored measurement data, but no single method has proved to be completely effective. One of the critical parameters for software prediction systems is the size of the measurement data set, with large data sets providing better reliability estimates. In this paper, we propose a software defect classification method that allows defect data from multiple projects and multiple independent vendors to be combined together to obtain large data sets. We also show that once a sufficient amount of information has been collected, the memory-based reasoning technique can be applied to projects that are not in the analysis set to predict their reliabilities and guide their testing process. Finally, the result of applying this approach to the analysis of defect data generated from fault-injection simulation is presented.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1180813,no,no,1487371923.577163
Refactoring the aspectizable interfaces: an empirical assessment,"Aspect oriented programming aims at addressing the problem of the crosscutting concerns, i.e., those functionalities that are scattered among several modules in a given system. Aspects can be defined to modularize such concerns. In this work, we focus on a specific kind of crosscutting concerns, the scattered implementation of methods declared by interfaces that do not belong to the principal decomposition. We call such interfaces aspectizable. All the aspectizable interfaces identified within a large number of classes from the Java Standard Library and from three Java applications have been automatically migrated to aspects. To assess the effects of the migration on the internal and external quality attributes of these systems, we collected a set of metrics and we conducted an empirical study, in which some maintenance tasks were executed on the two alternative versions (with and without aspects) of the same system. In this paper, we report the results of such a comparison.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1542065,no,no,1487371923.577162
Probabilistic evaluation of object-oriented systems,"The goal of this study is the development of a probabilistic model for the evaluation of flexibility of an object-oriented design. In particular, the model estimates the probability that a certain class of the system gets affected when new functionality is added or when existing functionality is modified. It is obvious that when a system exhibits a large sensitivity to changes, the corresponding design quality is questionable. Useful conclusions can be drawn from this model regarding the comparative evaluation of two or more object-oriented systems or even the assessment of several generations of the same system, in order to determine whether or not good design principles have been applied. The proposed model has been implemented in a Java program that can automatically analyze the class diagram of a given system.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357888,no,no,1487371923.577161
A software reliability growth model from testing to operation,"This paper presents a SRGM (software reliability growth model) from testing to operation based on NHPP (nonhomogeneous Poisson process). Although a few research projects have been devoted to the differences between testing environment and operational environment, consideration of the variation of environmental influence factors along testing time in the existing models is limited. The model in this paper is the first scheme of a few NHPP models which take environmental factors experimented from actual failure data as a function of testing time. FDR (fault detection rate) is usually used to measure the effectiveness of fault detection by test techniques and test cases. A bell-shaped FDR function is proposed which integrate both environmental factors and inherent FDR per fault. A NHPP SRGM from testing to operation that incorporates environmental factors called TOE-SRGM is built which integrates FDR of testing phase and the proposed FDR function of operational phase. TOE-SRGM is evaluated using a set of software failure data. The results show that TOE-SRGM fits the failure data better than G-O model and PZ-SRGM.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510175,no,no,1487371923.577159
Uncertainty model for product environmental performance scoring,"As product designers begin to make design decisions motivated by environmental performance, it is critical for environmental experts to present designers with useful information about the quality of both performance measures and the data on which the measures are based. In this paper, we discuss sources of uncertainty in environmental performance measurement using Motorola's Green Design Advisor environmental scoring software. We present a probabilistic method for measuring both data and model uncertainty in Green Design Advisor scores. Using Monte Carlo simulation, this model was applied to two Motorola product designs to generate probability distributions for the scores. The products have the same functionality, but one was designed with environmental performance in mind. The results of this study indicate that the uncertainty model can distinguish between Green Design Advisor scores which differ by 10% or more. In addition, approximation formulas were developed which give results similar to the simulation results, with a computation time short enough to be useful for an environmental scoring software",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=675059,no,no,1487371923.577158
Identifying fault prone modules: an empirical study in telecommunication system,"Telecommunication systems are becoming more dependent on software-intensive products and systems. Poor software quality can threaten safety, risk a company's business, or alienate potential customers. It is no longer acceptable to ignore software quality until just prior to a product's release. This study identifies troublesome modules in a large telecommunication system. For this, we propose the software metrics of the CHILL language, which are used to develop the telecommunication software. We present the identification method of fault-prone software modules of telecommunication software using neural networks. We investigate the relationship between the proposed metrics and the change request frequency of software modules which are found during the development phase. Using the neural network model, we classify software modules as either fault prone or not fault prone based on the proposed metrics. We obtained the experimental results that the total fitting rate of 52 testing data sets was 96.2%. Therefore, for newly added software modules, we can predict whether the software module is fault prone or not",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=665798,no,no,1487371923.577157
Application transparent fault management in fault tolerant Mach,"A general purpose operating system fault management mechanism, the sentry, has been defined and implemented for the Mach 3.0 microkernel running a UNIX 4.3 BSD server. The value of a mechanism in the operating system domain is usually judged by two criteria: the suitability of the mechanism to support a wide range of policies and the performance cost of the mechanism. Similarly, in fault detection and recovery there are a relatively large number of strategies which can be mapped onto mechanisms and policies for fault tolerance. To highlight the properties of the sentry mechanism for fault management, the suitability and performance of the proposed mechanism are being evaluated for sample fault detection policies and for sample fault recovery policies. In the fault detection domain use of the mechanism to support assertion type policy is presented and evaluated through an example. Two recovery policies have been chosen and evaluated: checkpoint/restart and checkpoint/restart/journaling.",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=627303,no,no,1487371923.577155
On the correlation between code coverage and software reliability,"We report experiments conducted to investigate the correlation between code coverage and software reliability. Black-, decision-, and all-use-coverage measures were used. Reliability was estimated to be the probability of no failure over the given input domain defined by an operational profile. Four of the five programs were selected from a set of Unix utilities. These utilities range in size from 121 to 8857 lines of code, artificial faults were seeded manually using a fault seeding algorithm. Test data was generated randomly using a variety of operational profiles for each program. One program was selected from a suite of outer space applications. Faults seeded into this program were obtained from the faults discovered during the integration testing phase of the application. Test cases were generated randomly using the operational profile for the space application. Data obtained was graphed and analyzed to observe the relationship between code coverage and reliability. In all cases it was observed that an increase in reliability is accompanied by an increase in at least one code coverage measure. It was also observed that a decrease in reliability is accompanied by a decrease in at least one code coverage measure. Statistical correlations between coverage and reliability were found to vary between -0.1 and 0.91 for the shortest two of the five programs considered; for the remaining three programs the correlations varied from 0.89 to 0.99",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=497650,no,no,1487371923.577153
Application of metrics to object-oriented designs,"The benefits of object-oriented software development are now widely recognized. However, methodologies that are used for the object-oriented software development process are still in their infancy. There is a lack of methods available to assess the quality of the various components that are derived during the development process. In this paper, we describe a method to assess the quality of object-oriented designs. We utilize a basic set of object-oriented metrics that is proposed by Shyam Chidamber et al. (1991 and 19994). We perform experimental tests on a set of object-oriented designs using the NOC metric. Also, we refer to our ongoing work to provide automated assistance to help restructure the design based on the metric findings",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=682165,no,no,1487371923.577151
Re-engineering fault tolerance requirements: a case study in specifying fault tolerant flight control systems,We present a formal specification of fault tolerance requirements for an analytical redundancy based fault tolerant flight control system. The development of the specification is driven by the performance and fault tolerance requirements contained in the US Air Force military specification MIL-F-9490D. The design constraints imposed to the system from adopting the analytical redundancy approach are captured within the specification. We draw some preliminary conclusions from our study,2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=948564,no,no,1487371923.117525
Observation and analysis of multiple-phase grounding faults caused by lightning,"This paper describes four phase and five-phase grounding faults caused by lightning on a 154-kV overhead transmission line. The authors measured insulator voltages and currents flowing along the ground wire and the tower. In addition, they photographed lightning strokes to the transmission line and flashovers between the arcing horns and examined the fault phases at substations. The paper analyzes insulator voltage waveforms using the Electromagnetic Transients Program (EMTP) and estimates the fault processes",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=484035,no,no,1487371923.117523
A DSP-based FFT-analyzer for the fault diagnosis of rotating machine based on vibration analysis,"A DSP-based measurement system dedicated to the vibration analysis on rotating machines was designed and realized. Vibration signals are on-line acquired and processed to obtain a continuous monitoring of the machine status. In case of fault, the system is capable of isolating the fault with a high reliability. The paper describes in detail the approach followed to built up fault and unfault models together with the chosen hardware and software solutions. A number of tests carried out on small-size three-phase asynchronous motors highlights high promptness in detecting faults, low false alarm rate, and very good diagnostic performance",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=928883,no,no,1487371923.117522
Extending software quality assessment techniques to Java systems,"The paper presents extensions to Bell Canada source code quality assessment suite (DATRIX tm) for handling Java language systems. Such extensions are based on source code object metrics, including Java interface metrics, which are presented and explained in detail. The assessment suite helps to evaluate the quality of medium-large software systems identifying parts of the system which have unusual characteristics. The paper also studies and reports the occurrence of clones in medium-large Java software systems. Clone presence affects quality since it increases a system size and often leads to higher maintenance costs. The clone identification process uses Java specific metrics to determine similarities between methods throughout a system. The results obtained from experiments with software evaluation and clone detection techniques, on over 500 KLOC of Java source code, are presented",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=777743,no,no,1487371923.117521
FD-HGAC: a hybrid heuristic/genetic algorithm hardware/software co-synthesis framework with fault detection,"Embedded real-time systems are becoming increasingly complex. To combat the rising design cost of those systems, co-synthesis tools that map tasks to systems containing both software and specialized hardware have been developed. As system transient fault rates increase due to technology scaling, embedded systems must be designed in fault tolerant ways to maintain system reliability. This paper presents and analyzes FD-HGAC, a tool using a genetic algorithm and heuristics to design real-time systems with partial fault detection. Results of numerous trials of the tool are shown to produce systems with average 22% detection coverage that incurs no cost or performance penalty.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1466435,no,no,1487371923.117519
A metric based technique for design flaws detection and correction,"During the evolution of object-oriented (OO) systems, the preservation of correct design should be a permanent quest. However, for systems involving a large number of classes and which are subject to frequent modifications, the detection and correction of design flaws may be a complex and resource-consuming task. Automating the detection and correction of design flaws is a good solution to this problem. Various authors have proposed transformations that improve the quality of an OO system while preserving its behavior. In this paper, we propose a technique for automatically detecting situations where a particular transformation can be applied to improve the quality of a system. The detection process is based on analyzing the impact of various transformations on software metrics using quality estimation models",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=802337,no,no,1487371923.117518
Predictive modeling of software quality for very large telecommunications systems,"Society's reliance on large complex telecommunications systems mandates high reliability. Controlling faults in software requires that one can predict problems early enough to take preventive action. Software metrics are the basis for such predictions, and thus, many organization are collecting volumes of software metric data. Collecting software metrics is not enough. One must translate measurements into predictions. This study systematically presents a methodology for developing models that predict software quality factors. The individual details of this methodology may be familiar, but the whole modeling process must be integrated to produce successful predictions of software quality. We use two example studies to illustrate each step. One predicted the number of faults to be discovered in each module, and the other predicted whether each module would be considered fault-prone. The examples were based on the same data set, consisting of a sample from a very large telecommunications system. The sample of modules represented about 1.3 million lines of code",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=540273,no,no,1487371923.117516
Reliable distributed sorting through the application-oriented fault tolerance paradigm,The design and implementation of a reliable version of the distributed bitonic sorting algorithm using the application-oriented fault tolerance paradigm on a commercial multicomputer is described. Sorting assertions in general are discussed and the bitonic sort algorithm is introduced. Faulty behavior is discussed and a fault-tolerant parallel bitonic sort developed using this paradigm is presented. The error coverage and the response of the fault-tolerant algorithm to faulty behavior are presented. Both asymptotic complexity and the results of run-time experimental measurements on an Ncube multicomputer are given. The authors demonstrate that the application-oriented fault tolerance paradigm is applicable to problems of a noniterative nature,1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=37983,no,no,1487371923.117514
Modeling the effects of combining diverse software fault detection techniques,"Considers what happens when several different fault-finding techniques are used together. The effectiveness of such multi-technique approaches depends upon a quite subtle interplay between their individual efficacies. The modeling tool we use to study this problem is closely related to earlier work on software design diversity which showed that it would be unreasonable even to expect software versions that were developed truly independently to fail independently of one another. The key idea was a â€œdifficulty functionâ€?over the input space. Later work extended these ideas to introduce a notion of â€œforcedâ€?diversity. In this paper, we show that many of these results for design diversity have counterparts in diverse fault detection in a single software version. We define measures of fault-finding effectiveness and diversity, and show how these might be used to give guidance for the optimal application of different fault-finding procedures to a particular program. The effects on reliability of repeated applications of a particular fault-finding procedure are not statistically independent; such an incorrect assumption of independence will always give results that are too optimistic. For diverse fault-finding procedures, it is possible for effectiveness to be even greater than it would be under an assumption of statistical independence. Diversity of fault-finding procedures is a good thing and should be applied as widely as possible. The model is illustrated using some data from an experimental investigation into diverse fault-finding on a railway signalling application",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=888629,no,no,1487371923.117512
Defect-based reliability analysis for mission-critical software,"Most software reliability methods have been developed to predict the reliability of a program using only data gathered during the resting and validation of a specific program. Hence, the confidence that can be attained in the reliability estimate is limited since practical resource constraints can result in a statistically small sample set. One exception is the Orthogonal Defect Classification (ODC) method, which uses data gathered from several projects to track the reliability of a new program, Combining ODC with root-cause analysis can be useful in many applications where it is important to know the reliability of a program for a specific type of a fault. By focusing on specific classes of defects, it becomes possible to (a) construct a detailed model of the defect and (b) use data from a large number of programs. In this paper, we develop one such approach and demonstrate its application to modeling Y2K defects",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=884761,no,no,1487371923.11751
Software architecture analysis-a case study,"Presents a case study that evaluates two software quality attributes: performance and availability. We use three programs based on two architectural styles: pipe-filter and batch-sequential. The objective of this study is to identify the crucial factors that might have an influence on these quality attributes from the software architecture perspective. The benefit of this study is that early quality prediction can be facilitated by an analysis of the software architecture. The results from this study show that it is feasible to select a better architectural style based on variations in the execution environment to attain higher availability and/or better performance. Moreover, we demonstrate the effects of these variations on the quality measurements",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=812714,no,no,1487371922.657659
A controlled experiment assessing test case prioritization techniques via mutation faults,"Regression testing is an important part of software maintenance, but it can also be very expensive. To reduce this expense, software testers may prioritize their test cases so that those that are more important are run earlier in the regression testing process. Previous work has shown that prioritization can improve a test suite's rate of fault detection, but the assessment of prioritization techniques has been limited to hand-seeded faults, primarily due to the belief that such faults are more realistic than automatically generated (mutation) faults. A recent empirical study, however, suggests that mutation faults can be representative of real faults. We have therefore designed and performed a controlled experiment to assess the ability of prioritization techniques to improve the rate of fault detection techniques, measured relative to mutation faults. Our results show that prioritization can be effective relative to the faults considered, and they expose ways in which that effectiveness can vary with characteristics of faults and test suites. We also compare our results to those collected earlier with respect to the relationship between hand-seeded faults and mutation faults, and the implications this has for researchers performing empirical studies of prioritization.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510136,no,no,1487371922.657658
"An ordinal-time reliability model applied to ""Big-Bang"" suite-based testing","System testing is often performed by means of a comprehensive test suite that spans the functional requirements of a software product (""Big Bang"" testing). This suite is then run repeatedly after each modification or fix until a satisfactory pass rate is achieved. Such testing does not lend itself to treatment with traditional reliability models which, in keeping with their origins in hardware, assume that each fault is fixed upon discovery and that the relevant temporal variable is calendar or execution time. In this paper, a model using ordinal time, measured by baselined development configurations, is presented. The use of this model in combination with an empirical mapping of test case failures to product code faults allows for rapid tracking of testing progress. The use of the model as an aid to project management in the testing phase of a project is illustrated",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=990001,no,no,1487371922.657656
On integrating error detection into a fault diagnosis algorithm for massively parallel computers,"Scalable fault diagnosis is necessary for constructing fault tolerance mechanisms in large massively parallel multiprocessor systems. The diagnosis algorithm must operate efficiently even if the system consists of several thousand processors. We introduce an event-driven, distributed system-level diagnosis algorithm. It uses a small number of messages and is based on a general diagnosis model without the limitation of the number of simultaneously existing faults (an important requirement for massively parallel computers). The algorithm integrates both error detection techniques like âŒ©I'm aliveâŒ?messages, and built in hardware mechanisms. The structure of the implemented algorithm is presented and the essential program modules are described. The paper also discusses the use of test results generated by error detection mechanisms for fault localization. Measurement results illustrate the effect of the diagnosis algorithm, in particular the error detection mechanism by âŒ©I'm aliveâŒ? messages, on the application performance",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=395836,no,no,1487371922.657655
Getting started on metrics-Jet Propulsion Laboratory productivity and quality,"A description is given of the SPA (Software Product Assurance) Metrics Study, part of an effort to improve software quality and the productivity and predictability of software development. The first objective was to collect whatever data could be found from as many projects for which they were still available. Data were assembled on the basis of four basic parameters: source lines of code, dollars, work years, and defects. By using these four basic parameters, it was possible to construct quality and productivity baselines. Quality was defined as the number of defects per thousand lines of source code. Productivity was defined in two ways: dollars per source line of code and source lines of code per work month. Preliminary results of this study are presented",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=63616,no,no,1487371922.657653
Cost-effective graceful degradation in speculative processor subsystems: the branch prediction case,"We analyze the effect of errors in branch predictors, a representative example of speculative processor subsystems, to motivate the necessity for fault tolerance in such subsystems. We also describe the design of fault tolerant branch predictors using general fault tolerance techniques. We then propose a fault-tolerant implementation that utilizes the finite state machine (FSM) structure of the pattern history table (PHT) and the set of potential faulty states to predict the branch direction, yet without strictly identifying the correct state. The proposed solution provides virtually the same prediction accuracy as general fault tolerant techniques, while significantly reducing the incurred hardware overhead.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1240894,no,no,1487371922.657652
A fault model for fault injection analysis of dynamic UML specifications,"Verification and validation (V&V) tasks, as applied to software specifications, enable early detection of analysis and design flaws prior to implementation. Several fault injection techniques for software V&V are proposed at the code level. In this paper, we address V&V analysis methods based on fault injection at the software specification level. We present a fault model and a fault injection process for UML dynamic specifications. We use a case study based on a cardiac pacemaker for illustrating the developed approach.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=989460,no,no,1487371922.657651
Evaluating Web applications testability by combining metrics and analogies,"This paper introduces an approach to describe a Web application through an object-oriented model and to study application testability using a quality model focused on the use of object-oriented metrics and software analogies analysis. The proposed approach uses traditional Web and object-oriented metrics to describe structural properties of Web applications and to analyze them. These metrics are useful to measure some important software attributes, such as complexity, coupling, size, cohesion, reliability, defects density, and so on. Furthermore, the presented quality model uses these object-oriented metrics to describe applications in order to predict some software quality factors (such as test effort, reliability, error proneness, and so on) through an instance-based classification system. The approach uses a classification system to study software analogies and to define a set of information then used as the basis for applications quality factors prediction and evaluation. The presented approaches are applied into the WAAT (Web Applications Analysis and Testing) project",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1609664,no,no,1487371922.657649
Object-oriented analysis of COBOL,"The object-oriented paradigm is considered as the one which best guarantees the investments for renewal. It allows one to produce software with high degrees of reusability and maintainability, and satisfying certain quality characteristics. These features are not obviously automatically guaranteed by the simple adoption of an object-oriented programming language, so a process of re-analysis is needed. In this view, several methods for reengineering old applications according to the object-oriented paradigm have been defined and proposed. A method and tool (C<sub>2</sub>O<sup>2</sup>, COBOL to Object-Oriented) for analyzing COBOL applications in order to extract its object-oriented analysis is presented. The tool identifies classes and their relationships by means of a process of understanding and refinement in which COBOL data structures are analyzed, converted into classes, aggregated, and simplified semi-automatically. The algorithm is also capable of detecting data structures which may cause problems in the Year 2000, as demonstrated with an example",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=583029,no,no,1487371922.657638
Empirically based analysis of failures in software systems,"An empirical analysis of failures in software systems is used to evaluate several specific issues and questions in software testing, reliability analysis, and reuse. The issues examined include the following: diminishing marginal returns of testing; effectiveness of multiple fault-detection and testing phases; measuring system reliability versus function or component reliability; developer bias regarding the amount of testing that functions or components will receive; fault-proneness of reused versus newly developed software; and the relationship between degree of reuse and development effort and fault-proneness. Failure data from a large software manufacturer and a NASA production environment were collected and analyzed",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=58722,no,no,1487371922.657635
Analytical and empirical evaluation of software reuse metrics,"How much can be saved by using existing software components when developing new software systems? With the increasing adoption of reuse methods and technologies, this question becomes critical. However, directly tracking the actual cost savings due to reuse is difficult. A worthy goal would be to develop a method of measuring the savings indirectly by analyzing the code for reuse of components. The focus of the paper is to evaluate how well several published software reuse metrics measure the â€œtime, money and qualityâ€?benefits of software reuse. We conduct this evaluation both analytically and empirically. On the analytic front, we introduce some properties that should arguably hold of any measure of â€œtime, money and qualityâ€?benefit due to reuse. We assess several existing software reuse metrics using these properties. Empirically, we constructed a toolset (using GEN+S) to gather data on all published reuse metrics from CS+ code; then, using some productivity and quality data from â€œnearly replicatedâ€?student projects at the University of Maryland, we evaluate the relationship between the known metrics and the process data. Our empirical study sheds some light on the applicability of our different analytic properties, and has raised some practical issues to be addressed as we undertake broader study of reuse metrics in industrial projects",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=493415,no,no,1487371922.189611
Designing a service of failure detection in asynchronous distributed systems,"Even though introduced for solving the consensus problem in asynchronous distributed systems, the notion of unreliable failure detector can be used as a powerful tool for any distributed protocol in order to get better performance by allowing the usage of aggressive time-outs to detect failures of entities executing the protocol. We present the design of a Failure Detection Service (FDS) based on the notion of unreliable failure detectors introduced by T. Chandra and S. Toueg (1996). FDS is able to detect crashed objects and entities that permanently omit to send messages without imposing changes to the source code of the underlying protocols that use this service. Also, FDS provides an object oriented interface to its subscribers and, more important, it does not add network overhead if no entity subscribes to the service. The paper can be also seen as a first step towards a distributed implementation of a heartbeat-based failure management system as defined in fault-tolerant CORBA specification",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=922825,no,no,1487371922.189609
Validation of the fault/error handling mechanisms of the Teraflops supercomputer,"The Teraflops system, the world's most powerful supercomputer, was developed by Intel Corporation for the US Department of Energy (DOE) as part of the Accelerated Strategic Computing Initiative (ASCI). The machine contains more than 9000 Intel Pentium (R) Pro processors and performs over one trillion floating point operations per second. Complex hardware and software mechanisms were devised for complying with DOE's reliability requirements. This paper gives a brief description of the Teraflops system architecture and presents the validation of the fault/error handling mechanisms. The validation process was based on an enhanced version of the physical fault injection at the IC pin level. An original approach was developed for assessing signal sensitivity to transient faults and the effectiveness of the fault tolerance mechanisms. Several malfunctions were unveiled by the fault injection experiments. After corrective actions had been undertaken, the supercomputer performed according to the specification.",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=689489,no,no,1487371922.189608
An examination of fault exposure ratio,"The fault exposure ratio, K, is an important factor that controls the per-fault hazard rate, and hence, the effectiveness of the testing of software. The authors examine the variations of K with fault density, which declines with testing time. Because faults become harder to find, K should decline if testing is strictly random. However, it is shown that at lower fault densities K tends to increase. This is explained using the hypothesis that real testing is more efficient than strictly random testing especially at the end of the test phase. Data sets from several different projects (in USA and Japan) are analyzed. When the two factors, e.g., shift in the detectability profile and the nonrandomness of testing, are combined the analysis leads to the logarithmic model that is known to have superior predictive capability",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=256855,no,no,1487371922.189607
SVM-based approach for instrument fault accomodation in automotive systems,"The paper deals with the use of support vector machines (SVMs) in software-based instrument fault accommodation schemes. A performance comparisons between SVMs and artificial neural networks (ANNs) is also reported. As an example, a real case study on an automotive system is presented. The ANNs and SVMs regression capability are employed to accommodate faults that could occur on main sensors involved in the engine operating. The obtained results prove the good behaviour of both tools. Similar performances have been achieved in terms of accuracy.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1567582,no,no,1487371922.189605
Phase-only sidelobe sector nulling for a tapered array with failed elements,"The application of the phase-only sidelobe sector nulling method to re-optimize the pattern of a tapered array after the detection of element failures has been developed under a Hughes RF computer aided design project. This fault correction technique uses a nonlinear computer optimization code developed by Pierre and Lowe (1975) to reshape the pattern of a partially failed array by readjusting the phase weights of the remaining good elements. The sidelobe sector nulling can be utilized to suppress interference and sea clutter from sea surface or distant airborne jammers for a shipboard radar. However, if some elements of the array fail, then the nulling region will degrade significantly. By using the computer code a new set of phase weights can be found to compensate for the lost elements and maintain the sidelobe sector nulling performance.<<ETX>>",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=407825,no,no,1487371922.189604
Built-in diagnostics for advanced power management,"The Army's Diagnostic Analysis and Repair Tool Set (DARTS) is an advanced software product used to perform automated fault diagnostics that results in reduced logistics costs, decreased downtime and enhanced mission performance. DARTS enabled automated, knowledge based fault diagnostics to be embedded in the Advanced Modular Power Control System (AMPCS). AMPCS is an integrated hardware and software product for aerospace power management. DARTS was used in a concurrent engineering design environment as a computer aided engineering tool to optimize the fault detection and fault isolation characteristics of the AMPCS prototype design. Project results indicate a new method for linking the diagnostic knowledge base captured during design with the hardware under development to achieve automated fault isolation throughout the hardware life cycle. A successful demonstration of real-time fault diagnostics was conducted using the AMPCS prototype unit and the DARTS software in a hardware-in-the-loop laboratory system. This project reduced to practice the concept of automated, knowledge based diagnostics for electronic systems",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=381592,no,no,1487371922.189602
Genetic programming model for software quality classification,"We apply genetic programming techniques to build a software quality classification model based on the metrics of software modules. The model we built attempts to distinguish the fault-prone modules from non-fault-prone modules using genetic programming (GP). These GP experiments were conducted with a random subset selection for GP in order to avoid overfitting. We then use the whole fit data set as the validation data set to select the best model. We demonstrate through two case studies that the GP technique can achieve good results. Also, we compared GP modeling with logistic regression modeling to verify the usefulness of GP",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=966814,no,no,1487371922.189601
A software fault detection and recovery in CDMA systems,"This paper describes an approach to software fault detection and recovery in CDMA systems, which is based on the sanity monitoring and auditing techniques. Sanity monitoring is a fault tolerance technique in a distributed system. It detects and recovers from software faults. We discuss how to identify performance classes and fault types for processes, and how to locate and isolate faults. An audit program detects and recovers from data errors in communication systems such as telephone switches. It is adapted to distributed systems. We discuss data error detection and recovery methods for CDMA systems that also use OS primitives",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=655575,no,no,1487371922.189599
A metric-based approach to enhance design quality through meta-pattern transformations,"During the evolution of object-oriented legacy systems, improving the design quality is. most often a highly demanded objective. For such systems which have a large number of classes and are subject to frequent modifications, detection and correction of design defects is a complex task. The use of automatic detection and correction tools can be helpful for this task. Various research approaches have proposed transformations that improve the quality of an object-oriented systems while preserving its behavior This paper proposes a framework where a catalogue of object-oriented metrics can be used-as indicators for automatically detecting situations where a particular transformation can be applied to improve the quality of an object-oriented legacy system. The correction process is based on analyzing the impact of various meta-pattern transformations on these object-oriented metrics.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1192426,no,no,1487371922.189597
A knowledge based approach to fault detection and diagnosis in industrial processes: a case study,"The article proposes a scheme for on-line fault detection and diagnosis using an expert system. This approach is being tested at a beet-sugar factory in Spain. A great deal of critical situations may arise in the normal operation of such a process. They are now managed by plant operators and cover both cases: faulty equipment diagnosis, which requires module substitution, as well as the detection of major improper control actions that may cause plant problems in the long run. This prototype not only comprises aspects related with fault detection and diagnosis but also is responsible for supervisory tasks to improve the whole process performance. The paper includes a description of the system architecture, a detailed presentation of the fault detection and diagnosis modules, and some evaluation of the results obtained by the system operation at the factory",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=333083,no,no,1487371921.729486
Building a requirement fault taxonomy: experiences from a NASA verification and validation research project,"Fault-based analysis is an early lifecycle approach to improving software quality by preventing and/or detecting pre-specified classes of faults prior to implementation. It assists in the selection of verification and validation techniques that can be applied in order to reduce risk. This paper presents our methodology for requirements-based fault analysis and its application to National Aeronautics and Space Administration (NASA) projects. The ideas presented are general enough to be applied immediately to the development of any software system. We built a NASA-specific requirement fault taxonomy and processes for tailoring the taxonomy to a class of software projects or to a specific project. We examined requirement faults for six systems, including the International Space Station (ISS), and enhanced the taxonomy and processes. The developed processes, preliminary tailored taxonomies for critical/catastrophic high-risk (CCHR) systems, preliminary fault occurrence data for the ISS project, and lessons learned are presented and discussed.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1251030,no,no,1487371921.729485
Toward a software testing and reliability early warning metric suite,"The field reliability is measured too late for affordably guiding corrective action to improve the quality of the software. Software developers can benefit from an early warning of their reliability while they can still affordably react. This early warning can be built from a collection of internal metrics. An internal metric, such as the number of lines of code, is a measure derived from the product itself. An external measure is a measure of a product derived from assessment of the behavior of the system. For example, the number of defects found in test is an external measure. The ISO/IEC standard states that [i]nternal metrics are of little value unless there is evidence that they are related to external quality. Internal metrics can be collected in-process and more easily than external metrics. Additionally, internal metrics have been shown to be useful as early indicators of externally-visible product quality. For these early indicators to be meaningful, they must be related (in a statistically significant and stable way) to the field quality/reliability of the product. The validation of such metrics requires the convincing demonstration that (1) the metric measures what it purports to measure and (2) the metric is associated with an important external metric, such as field reliability, maintainability, or fault-proneness. Software metrics have been used as indicators of software quality and fault proneness. There is a growing body of empirical results that supports the theoretical validity of the use of higher-order early metrics, such as OO metrics defined by Chidamber-Kemerer (CK) and the MOOD OO metric suites as predictors of field quality. However, general validity of these metrics (which are often unrelated to the actual operational profile of the product) is still open to criticism.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1317422,no,no,1487371921.729484
Multivariate assessment of complex software systems: a comparative study,"Assessment of large complex systems requires robust modeling techniques. Multivariate models can be misleading if the underlying metrics are highly correlated. Munson and Khoshgoflaar propose using principal components analysis to avoid such problems. Even though many have used the technique, the advantages have not previously been empirically demonstrated, especially for large complex systems. Our case study illustrates that principal components analysis can substantially improve the predictive quality of a software quality model. This paper presents a case study of a sample of modules representing about 1.3 million lines of code, taken from a much larger real-time telecommunications system. This study used discriminant analyse's for classification of fault-prone modules, based on measurements of software design attributes and categorical variables indicating new, changed, and reused modules. Quality of fit and predictive quality were evaluated",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=479364,no,no,1487371921.729482
A memory-based reasoning approach for assessing software quality,"Several methods have been explored for assuring the reliability of mission critical systems (MCS), but no single method has proved to be completely effective. This paper presents an approach for quantifying the confidence in the probability that a program is free of specific classes of defects. The method uses memory-based reasoning techniques to admit a variety of data from a variety of projects for the purpose of assessing new systems. Once a sufficient amount of information has been collected, the statistical results can be applied to programs that are not in the analysis set to predict their reliabilities and guide the testing process. The approach is applied to the analysis of Y2K defects based on defect data generated using fault-injection simulation",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=960603,no,no,1487371921.729481
Experience from replicating empirical studies on prediction models,"When conducting empirical studies, replications are important contributors to investigating the generality of the studies. By replicating a study in another context, we investigate what impact the specific environment has, related to the effect of the studied object. In this paper, we define different levels of replication to characterise the similarities and differences between an original study and a replication, with particular focus on prediction models for the identification of fault-prone software components. Further, we derive a set of issues and concerns which are important in order to enable replication of an empirical study and to enable practitioners to use the results. To illustrate the importance of the issues raised, a replication case study is presented in the domain of prediction models for fault-prone software components. It is concluded that the results are very divergent, depending on how different parameters are chosen, which demonstrates the need for well-documented empirical studies to enable replication and use",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1011340,no,no,1487371921.729479
Forecasting field defect rates using a combined time-based and metrics-based approach: a case study of OpenBSD,"Open source software systems are critical infrastructure for many applications; however, little has been precisely measured about their quality. Forecasting the field defect-occurrence rate over the entire lifespan of a release before deployment for open source software systems may enable informed decision-making. In this paper, we present an empirical case study often releases of OpenBSD. We use the novel approach of predicting model parameters of software reliability growth models (SRGMs) using metrics-based modeling methods. We consider three SRGMs, seven metrics-based prediction methods, and two different sets of predictors. Our results show that accurate field defect-occurrence rate forecasts are possible for OpenBSD, as measured by the Theil forecasting statistic. We identify the SRGM that produces the most accurate forecasts and subjectively determine the preferred metrics-based prediction method and set of predictors. Our findings are steps towards managing the risks associated with field defects",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544734,no,no,1487371921.729478
An empirical study on identifying fault-prone module in large switching system,Software complexity metrics have been shown to be very closely related to the distribution of faults in software. This paper focuses on identification of fault-prone software modules based on discriminant analysis and classification for software that are developed by CHILL language. We define software complexity metrics for CHILL language. The technique is successful in classifying software modules with relatively low error rare. This procedure shows very useful method in the detection of software modules in the fault of programs with high potential,1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=648419,no,no,1487371921.729477
Robust sensor fault reconstruction for an inverted pendulum using right eigenstructure assignment,"This work presents a robust sensor fault reconstruction scheme applied to an inverted pendulum. The scheme utilized a linear observer, using right eigenstructure assignment in order to the effect of nonlinearities and disturbances on the fault reconstruction. The design method is adapted and modified from existing work in the literature. A suitable interface between the pendulum and a computer enabled the application. Very good results were obtained.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1387542,no,no,1487371921.729475
Using code metrics to predict maintenance of legacy programs: a case study,"The paper presents an empirical study on the correlation of simple code metrics and maintenance necessities. The goal of the work is to provide a method for the estimation of maintenance in the initial stages of outsourcing maintenance projects, when the maintenance contract is being prepared and there is very little available information on the software to be maintained. The paper shows several positive results related to the mentioned goal",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=972733,no,no,1487371921.729472
Evaluating the effect of inheritance on the modifiability of object-oriented business domain models,"The paper describes an experiment to assess the impact of inheritance on the modifiability of object oriented business domain models. This experiment is part of a research project on the quality determinants of early systems development artefacts, with a special focus on the maintainability of business domain models. Currently there is little empirical information about the relationship between the size, structural and behavioural properties of business domain models and their maintainability. The situation is different in object oriented software engineering where a number of experimental investigations into the maintainability of object oriented software have been conducted. The results of our experiment indicate that extensive use of inheritance leads to models that are more difficult to modify. These findings are in line with the conclusions drawn from three similar controlled experiments on inheritance and modifiability of object oriented software",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=914964,no,no,1487371921.276754
A tree-based classification model for analysis of a military software system,"Tactical military software is required to have high reliability. Each software function is often considered mission critical, and the lives of military personnel often depend on mission success. The paper presents a tree based modeling method for identifying fault prone software modules, which has been applied to a subsystem of the Joint Surveillance Target Attack Radar System, JSTPARS, a large tactical military system. We developed a decision tree model using software product metrics from one iteration of a spiral life cycle to predict whether or not each module in the next iteration would be considered fault prone. Model results could be used to identify those modules that would probably benefit from extra reviews and testing and thus reduce the risk of discovering faults later on. Identifying fault prone modules early in the development can lead to better reliability. High reliability of each iteration translates into a highly reliable final product. A decision tree also facilitates interpretation of software product metrics to characterize the fault prone class. The decision tree was constructed using the TREED-ISC algorithm which is a refinement of the CHAID algorithm. This algorithm partitions the ranges of independent variables based on chi squared tests with the dependent variable. In contrast to algorithms used by previous tree based studies of software metric data, there is no restriction to binary trees, and statistically significant relationships with the dependent variable are the basis for branching.",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=618605,no,no,1487371921.276752
Detection of fault-prone software modules during a spiral life cycle,"The article is an experience report on identifying fault prone modules in a subsystem of the Joint Surveillance Target Attack Radar System, JSTARS, a large tactical military system. The project followed the spiral life cycle model. The iterations of the system were developed in FORTRAN about one year apart. We developed a discriminant analysis model using software metrics from one iteration to predict whether or not each module in the next would be considered fault prone. Tactical military software is required to have high reliability. Each software function is often considered mission critical, and the lives of military personnel often depend on mission success. In our project, each iteration of a spiral life cycle development produced a system that was suitable for operational testing. A risk analysis based on operational testing guided development of the next iteration. Identifying fault prone modules early in the development of an iteration can lead to better reliability, The results confirm previously published studies that discriminant analysis can be a useful tool in identification of fault prone software modules. This study used consecutive iterations, first, to build, and then to evaluate the model. This model validation approach is more realistic than earlier studies which split data from one project to simulate two iterations. Model results could be used to identify those modules that would probably benefit from earlier reviews and testing, and thus, reduce the risk of unexpected problems with those modules",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=564990,no,no,1487371921.276751
A hierarchical object-oriented approach to fault tolerance in distributed systems,"We develop an object-oriented framework to support fault tolerance in distributed systems using nested atomic actions. The inherent properties of the object-oriented programming paradigm enhance the error detection and recovery capabilities of the fault tolerance schemes. In our approach, error detection is performed locally within the object, whereas the recovery from errors is accomplished either locally within the object or globally across the active objects. We develop a queue-based backward recovery scheme for the global object restoration which greatly reduces the performance and storage overhead when compared to the existing schemes. We illustrate our approach with the help of prototype implementations",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=624286,no,no,1487371921.27675
An automatic approach to object-oriented software testing and metrics for C++ inheritance hierarchies,"In this paper, we propose a concept named unit repeated inheritance (URI) to realize object-oriented testing and object-oriented metrics. The approach describes an inheritance level technique (ILT) as a guide to detect the software errors of the inheritance hierarchy and measure the software complexity of the inheritance hierarchy. The measurement of inheritance metrics and some testing criteria are formed based on the proposed mechanism. Thus, we use Lex and Yacc to construct a windowing tool which is used in conjunction with a conventional C++ programming environment to assist a programmer to analyze, test, and measure his/her C++ programs",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=652116,no,no,1487371921.276748
Can we rely on COTS microkernels for building fault-tolerant systems?,"This paper addresses the use of COTS microkernels in fault-tolerant, and, to some extent, safety-critical systems. The main issue is to assess the behavior of such components, upon which application software relies, in the presence of faults. Using fault injection, it is possible to classify the behavior of the functional primitives. From the results obtained fault containment mechanisms can be provided as a new API to complement the basic detection mechanisms of the microkernel. Some preliminary experiments with the Chorus microkernel are also reported",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=644723,no,no,1487371921.276747
Using coupling measurement for impact analysis in object-oriented systems,"Many coupling measures have been proposed in the context of object oriented (OO) systems. In addition, due to the numerous dependencies present in OO systems, several studies have highlighted the complexity of using dependency analysis to perform impact analysis. An alternative is to investigate the construction of probabilistic decision models based on coupling measurement to support impact analysis. In addition to providing an ordering of classes where ripple effects are more likely, such an approach is simple and can be automated. In our investigation, we perform a thorough analysis on a commercial C++ system where change data has been collected over several years. We identify the coupling dimensions that seem to be significantly related to ripple effects and use these dimensions to rank classes according to their probability of containing ripple effects. We then assess the expected effectiveness of such decision models",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=792645,no,no,1487371921.276746
Towards the Optimization of Automatic Detection of Design Flaws in Object-Oriented Software Systems,"In order to increase the maintainability and the flexibility of a software, its design and implementation quality must be properly assessed. For this purpose a large number of metrics and several higher-level mechanisms based on metrics are defined in literature. But the accuracy of these quantification means is heavily dependent on the proper selection of threshold values, which is oftentimes totally empirical and unreliable. In this paper we present a novel method for establishing proper threshold values for metrics-based rules used to detect design flaws in object-oriented systems. The method, metaphorically called ""tuning machine"", is based on inferring the threshold values based on a set of reference examples, manually classified in ""flawed"" respectively ""healthy"" design entities (e.g., classes, methods). More precisely, the ""tuning machine"" searches, based on a genetic algorithm, for those thresholds which maximize the number of correctly classified entities. The paper also defines a repeatable process for collecting examples, and discusses the encouraging and intriguing results while applying the approach on two concrete metrics-based rules that quantify two well-known design flaws i.e., ""God Class"" and ""Data Class"".",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1402118,no,no,1487371921.276744
Analyzing Java software by combining metrics and program visualization,"Shimba, a prototype reverse engineering environment, has been built to support the understanding of Java software. Shimba uses Rigi and SCED to analyze, visualize, and explore the static and dynamic aspects, respectively, of the subject system. The static software artifacts and their dependencies are extracted from Java byte code and viewed as directed graphs using the Rigi reverse engineering environment. The static dependency graphs of a subject system can be annotated with attributes, such as software quality measures, and then be analyzed and visualised using scripts through the end user programmable interface. Shimba has recently been extended with the Chidamber and Kemerer suite of object oriented metrics. The metrics measure properties of the classes, the inheritance hierarchy, and the interaction among classes of a subject system. Since Shimba is primarily intended for the analysis and exploration of Java software, the metrics have been tailored to measure properties of software systems using a reverse engineering environment. The static dependency graphs of the system under investigation are decorated with measures obtained by applying the object oriented metrics to selected software components. Shimba provides tools to examine these measures, to find software artifacts that have values that are in a given range, and to detect correlations among different measures. The object oriented analysis of the subject Java system can be investigated further by exporting the measures to a spreadsheet",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=827328,no,no,1487371921.276742
A temperature sensor fault detector as an artificial neural network application,The paper presents a fault detection method based on artificial neural network (ANN) capabilities applied in a particular case: a temperature virtual measurement system (VMS). A study concerning the ANN architectures and the ANN training rules involved in a fault detection procedure are also presented. The results obtained for the VMS based on a data acquisition board or on a GPIB controlled instrumentation highlight the general applicability of neural processing,1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=692515,no,no,1487371921.27674
A fault tolerance infrastructure for dependable computing with high-performance COTS components,"The failure rates of current COTS processors have dropped to 100 FITs (failures per 10<sup>9</sup> hours), indicating a potential MTTF of over 1100 years. However our recent study of Intel P6 family processors has shown that they have very limited error detection and recovery capabilities and contain numerous design faults (â€œerrataâ€?. Other limitations are susceptibility to transient faults and uncertainty about â€œwearoutâ€?that could increase the failure rate in time. Because of these limitations, an external fault tolerance infrastructure is needed to assure the dependability of a system with such COTS components. The paper describes a fault-tolerant â€œinfrastructureâ€?system of fault tolerance functions that makes possible the use of low-coverage COTS processors in a fault-tolerant, self-repairing system. The custom hardware supports transient recovery design fault tolerance, and self-repair by scaring and replacement. Fault tolerance functions are implemented by four types of hardware are processors of low complexity that are fault-tolerant. High error detection coverage, including design faults, is attained by diversity and replication",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=857581,no,no,1487371920.831072
Improving code churn predictions during the system test and maintenance phases,"We show how to improve the prediction of gross change using neural networks. We select a multiple regression quality model from the principal components of software complexity metrics collected from a large commercial software system at the beginning of the testing phase. Our measure of quality is based on gross change, and is collected at the end of the maintenance phase. This quality measure is attractive for study as it is both objective and easily obtained directly from the source code. Then, we train a neural network with the complete set of principal components. Comparisons of the two models, gathered from eight related software systems, shows that the neural network offers much improved predictive quality over the multiple regression model",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=336789,no,no,1487371920.831071
A technique for automated validation of fault tolerant designs using laser fault injection (LFI),"This paper describes the successful development and demonstration of a Laser Fault Injection (LFI) technique to inject soft, i.e., transient, faults into VLSI circuits in a precisely-controlled, non-destructive, non-intrusive manner for the purpose of validating fault tolerant design and performance. The technique described in this paper not only enables the validation of fault-tolerant VLSI designs, but it also offers the potential for performing automated testing of board-level and system-level fault tolerant designs including fault tolerant operating system and application software. The paper describes the results of LFI testing performed to date on test metal circuit structures, i.e., ring oscillators, flip-flops, and multiplier chains, and on an advanced RISC processor, with comprehensive on-chip concurrent error detection and instruction retry, in a working single board computer. Relative to rapid, low cost testing and validation of complex fault tolerant designs, with the automated laser system at the Laser Restructuring Facility at the University of South Florida Center for Microelectronics Research (USF/CMR), a design with 10000 test points could be tested and validated in under 17 minutes. In addition to describing the successful demonstration of the technique to date, the paper discusses some of the challenges that still need to be addressed to make the technique a truly practical fault tolerant design validation tool.",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=689466,no,no,1487371920.83107
An Optimal Approach to Fault Tolerant Software Systems Design,"A systematic method of providing software system fault recovery with maximal fault coverage subject to resource constraints of overall recovery cost and additional fault rate is presented. This method is based on a model for software systems which provides a measure of the fault coverage properties of the system in the presence of computer hardware faults. Techniques for system parameter measurements are given. An optimization problem results which is a doubly-constrained 0,1 Knapsack problem. Quantitative results are presented demonstrating the effectiveness of the approach.",1978,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1702554,no,no,1487371920.831068
Object oriented metrics useful in the prediction of class testing complexity,"Adequate metrics of object-oriented software enable one to determine the complexity of a system and estimate the effort needed for testing already in the early stage of system development. The metrics values enable to locate parts of the design that could be error prone. Changes in these parts could significantly, improve the quality of the final product and decrease testing complexity. Unfortunately only few of the existing Computer Aided Software Engineering tools (CASE) calculate object metrics. In this paper methods allowing proper calculation of class metrics for some commercial CASE tool have been developed. New metric, calculable on the basis of information kept in CASE repository and useful in the estimation of testing effort have also been proposed. The evaluation of all discussed metrics does not depend on object design method and on the implementation language",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=952447,no,no,1487371920.831067
Measurement-driven dashboards enable leading indicators for requirements and design of large-scale systems,"Measurement-driven dashboards provide a unifying mechanism for understanding, evaluating, and predicting the development, management, and economics of large-scale systems and processes. Dashboards enable interactive graphical displays of complex information and support flexible analytic capabilities for user customizability and extensibility. Dashboards commonly include software requirements and design metrics because they provide leading indicators for project size, growth, and stability. This paper focuses on dashboards that have been used on actual large-scale projects as well as example empirical relationships revealed by the dashboards. The empirical results focus on leading indicators for requirements and design of large-scale systems. In the first set of 14 projects focusing on requirements metrics, the ratio of software requirements to source-lines-of code averaged 1:46. Projects that far exceeded the 1:46 requirements-to-code ratio tended to be more effort-intensive and fault-prone during verification. In the second set of 16 projects focusing on design metrics, the components in the top quartile of the number of component internal states had 6.2 times more faults on average than did the components in the bottom quartile, after normalization by size. The components in the top quartile of the number of component interactions had 4.3 times more faults on average than did the components in the bottom quartile, after normalization by size. When the number of component internal states was in the bottom quartile, the component fault-proneness was low even when the number of component interactions was in the upper quartiles, regardless of size normalization. Measurement-driven dashboards reveal insights that increase visibility into large-scale systems and provide feedback to organizations and projects",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1509300,no,no,1487371920.831065
Reengineering the class-an object oriented maintenance activity,"When an Incremental Approach is used to develop an object-oriented system, there is a risk that the class design will deteriorate in quality with each increment. This paper presents a technique for detecting classes that may be prone to deteriorate, or if deterioration has occurred assists with reengineering those classes. Experience with applying this technique to an industrial software development project is also discussed",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=716634,no,no,1487371920.831064
A DSP-based FFT-analyzer for the fault diagnosis of rotating machine based on vibration analysis,"A DSP-based measurement system dedicated to the vibration analysis on rotating machines was designed and realized. Vibration signals are on-line acquired and processed to obtain a continuous monitoring of the machine status. In case of a fault, the system is capable of isolating the fault with a high reliability. The paper describes in detail the approach followed to built up fault and non-fault models together with the chosen hardware and software solutions. A number of tests carried out on small-size three-phase asynchronous motors highlight the excellent promptness in detecting faults, low false alarm rate, and very good diagnostic performance.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1177930,no,no,1487371920.831062
Availability requirement for a fault-management server in high-availability communication systems,"This paper investigates the availability requirement for the fault management server in high-availability communication systems. This study shows that the availability of the fault management server does not need to be 99.999% in order to guarantee a 99.999% system availability, as long as the fail-safe ratio (the probability that the failure of the fault management server does not bring down the system) and the fault coverage ratio (probability that the failure in the system can be detected and recovered by the fault management server) are sufficiently high. Tradeoffs can be made among the availability of the fault management server, the fail-safe ratio, and the fault coverage ratio to optimize system availability. A cost-effective design for the fault management server is proposed.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1211116,no,no,1487371920.831061
Integrated design method for probabilistic design,"This paper presents a rational design approach known as the integrated design method (IDM). By employing cost relationships within a probabilistic methodology, as is done in IDM, engineers have a new tool to objectively assess product cost, performance and reliability. The benefits of the method are: lower product cost; superior product performance; reduced product development time; and reduced product warranty and liability costs. To harness the full potential of IDM requires that users have an easy to use tool that is capable of performing all required analysis quickly, accurately, and with minimal user interaction. ProFORM is computational software, which focuses on integrating all the elements of the probabilistic analysis into a comprehensive package that is easily used and understood. Unlike other packages, ProFORM is integrated with the existing design tools in a seamless fashion. The numerical example at the end of the paper clearly demonstrates the advantages of IDM over the conventional design methods",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=653812,no,no,1487371920.831058
Dynamic coupling measures for object-oriented software,"The relationships between coupling and external quality factors of object-oriented software have been studied extensively for the past few years. For example, several studies have identified clear empirical relationships between class-level coupling and the fault-proneness of the classes. A common way to quantify the coupling is through static code analysis. However, the resulting static coupling measures only capture certain underlying dimensions of coupling. Other dependencies regarding the dynamic behavior of software can only be inferred from run-time information. For example, due to inheritance and polymorphism, it is not always possible to determine the actual receiver and sender classes (i.e., the objects) from static code analysis. This paper describes how several dimensions of dynamic coupling can be calculated by tracing the flow of messages between objects at run-time. As a first evaluation of the proposed dynamic coupling measures, fairly accurate prediction models of the change proneness of classes have been developed using change data from nine maintenance releases of a large SmallTalk system. Preliminary results suggest that dynamic coupling may also be useful for developing prediction models and tools supporting change impact analysis. At present, work on developing a dynamic coupling tracer and ripple-effect prediction models for Java programs is underway.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1011323,no,no,1487371920.404636
Measuring OO systems: a critical analysis of the MOOD metrics,"In parallel with the rise to prominence of the OO paradigm has come the acceptance that conventional software metrics are not adequate to measure object oriented systems. This has inspired a number of software practitioners and academics to develop new metrics that are suited to the OO paradigm. Arguably, the most thorough treatment of the subject is that of the MOOD team, under the leadership of Abreau. The MOOD metrics have been subjected to much empirical evaluation, with claims made regarding the usefulness of the metrics to assess external attributes such as quality and maintainability. We evaluate the MOOD metrics on a theoretical level and show that any empirical validation is premature, due to the majority of the MOOD metrics being fundamentally flawed. The metrics either fail to meet the MOOD team's own criteria or are founded on an imprecise, and in certain cases inaccurate, view of the OO paradigm. We propose our own solutions to some of these anomalies and clarify some important aspects of OO design, in particular those aspects that may cause difficulties when attempting to define accurate and meaningful metrics. The suggestions we make are not limited to the MOOD metrics but are intended to have a wider applicability in the field of OO metrics",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=779004,no,no,1487371920.404635
A multi-layered system of metrics for the measurement of reuse by inheritance,"In spite of the intense efforts of metrics research, the impact of object-oriented software metrics is, for the moment, still quite reduced. The cause of this fact lies not in an intrinsic incapacity of metrics to help in assessing and improving the quality of object-oriented systems, but in the unsystematic, dispersed and ambiguous manner of defining and using the metrics. In this paper, we define a multi-layered system of metrics that measures inheritance-based reuse, and we propose a number of metric definitions for the layers of this system. By defining and using such systems of metrics, we obtain a unitary approach of related measures and a systematic, yet flexible manner of defining new measures as part of a particular metrics system. Organising metric definitions in systems of metrics contributes to (a strongly needed) order among object-oriented metrics, thus increasing their reliability and usability",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=796478,no,no,1487371920.404634
Evaluating design metrics on large-scale software,"Design-quality metrics that are being developed for predicting the potential quality and complexity of the system being developed are discussed. The goal of the metrics is to provide timely design information that can guide a developer in producing a higher quality product. The metrics were tested on projects developed in a university setting with client partners in industry. To further validate the metrics, they were applied to professionally developed systems to test their predictive qualities. The results of applying two design metrics and a composite metric to data from a large-scale industrial project are presented.<<ETX>>",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=219620,no,no,1487371920.404632
Assessing uncertain predictions of software quality,"Many development organizations try to minimize faults in software as a means for improving customer satisfaction. Assuring high software quality often entails time-consuming and costly development processes. A software quality model based on software metrics can be used to guide enhancement efforts by predicting which modules are fault-prone. The paper presents a way to determine which predictions by a classification tree should be considered uncertain. We conducted a case study of a large legacy telecommunications system. One release was the basis for the training data set, and the subsequent release was the basis for the evaluation data set. We built a classification tree using the TREEDISC algorithm, which is based on chi-squared tests of contingency tables. The model predicted whether a module was likely to have faults discovered by customers, or not, based on software product, process, and execution metrics. We simulated practical use of the model by classifying the modules in the evaluation data set. The model achieved useful accuracy, in spite of the very small proportion of fault-prone modules in the system. We assessed whether the classes assigned to the leaves were appropriate by examining the details of the full tree, and found sizable subsets of modules with substantially uncertain classification. Discovering which modules have uncertain classifications allows sophisticated enhancement strategies to resolve uncertainties. Moreover, TREEDISC is especially well suited to identifying uncertain classifications",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=809737,no,no,1487371920.404631
Controlling overfitting in software quality models: experiments with regression trees and classification,"In these days of â€œfaster, cheaper, betterâ€?release cycles, software developers must focus enhancement efforts on those modules that need improvement the most. Predictions of which modules are likely to have faults during operations is an important tool to guide such improvement efforts during maintenance. Tree-based models are attractive because they readily model nonmonotonic relationships between a response variable and its predictors. However, tree-based models are vulnerable to overfitting, where the model reflects the structure of the training data set too closely. Even though a model appears to be accurate on training data, if overfitted it may be much less accurate when applied to a current data set. To account for the severe consequences of misclassifying fault-prone modules, our measure of overfitting is based on the expected costs of misclassification, rather than the total number of misclassifications. In this paper, we apply a regression-tree algorithm in the S-Plus system to the classification of software modules by the application of our classification rule that accounts for the preferred balance between misclassification rates. We conducted a case study of a very large legacy telecommunications system, and investigated two parameters of the regression-tree algorithm. We found that minimum deviance was strongly related to overfitting and can be used to control it, but the effect of minimum node size on overfitting is ambiguous",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915528,no,no,1487371920.404629
Dynamic metrics for object oriented designs,"As object-oriented (OO) analysis and design techniques become more widely used, the demand on assessing the quality of OO designs increases substantially. Recently, there has been much research effort devoted to developing and empirically validating metrics for OO design quality. Complexity, coupling, and cohesion have received a considerable interest in the field. Despite the rich body of research and practice in developing design quality metrics, there has been less emphasis on dynamic metrics for OO designs. The complex dynamic behavior of many real-time applications motivates a shift in interest from traditional static metrics to dynamic metrics. This paper addresses the problem of measuring the quality of OO designs using dynamic metrics. We present a metrics suite to measure the quality of designs at an early development phase. The suite consists of metrics for dynamic complexity and object coupling based on execution scenarios. The proposed measures are obtained from executable design models. We apply the dynamic metrics to assess the quality of a pacemaker application. Results from the case study are used to compare static metrics to the proposed dynamic metrics and hence identify the need for empirical studies to explore the dependency of design quality on each",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=809725,no,no,1487371920.404628
Modeling fault-prone modules of subsystems,"Software developers are very interested in targeting software enhancement activities prior to release, so that reworking of faulty modules can be avoided. Credible predictions of which modules are likely to have faults discovered by customers can be the basis for selecting modules for enhancement. Many case studies in the literature build models to predict which modules will be fault-prone without regard to the subsystems defined by the system's functional architecture. Our hypothesis is this: models that are specially built for subsystems will be more accurate than a system-wide model applied to each subsystem's modules. In other words, the subsystem that a module belongs to can be valuable information in software quality modeling. This paper presents an empirical case study which compared software quality models of an entire system to models of a major functional subsystem. The study, modeled a very large telecommunications system with classification trees built by the CART (classification and regression trees) algorithm. For predicting subsystem quality, we found that a model built with training data on the subsystem alone was more accurate than a similar model built with training data on the entire system. We concluded that the characteristics of the subsystem's modules were not similar to those of the system as a whole, and thus, information on subsystems can be valuable",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885877,no,no,1487371920.404626
Preparing measurements of legacy software for predicting operational faults,"Software quality modeling can be used by a software maintenance project to identify a limited set of software modules that probably need improvement. A model's goal is to recommend a set of modules to receive special treatment. The purpose of the paper is to report our experiences modeling software quality with classification trees, including necessary preprocessing of data. We conducted a case study on two releases of a very large legacy telecommunications system. A module was considered fault-prone if any faults were discovered by customers, and not fault-prone otherwise. Software product, process, and execution metrics were the basis for predictors. The TREEDISC algorithm for building classification trees was investigated, because it emphasizes statistical significance. Numeric data, such as software metrics, are not suitable for TREEDISC. Consequently, we transformed measurements into discrete ordinal predictors by grouping. This case study investigated the sensitivity of modeling results to various groupings. We found that robustness, accuracy, and parsimony of the models were influenced by the maximum number of groups. Models based on two sets of candidate predictors had similar sensitivity",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=792634,no,no,1487371920.404625
Improving tree-based models of software quality with principal components analysis,"Software quality classification models can predict which modules are to be considered fault-prone, and which are not, based on software product metrics, process metrics and execution metrics. Such predictions can be used to target improvement efforts to those modules that need them the most. Classification-tree modeling is a robust technique for building such software quality models. However, the model structure may be unstable, and accuracy may suffer when the predictors are highly correlated. This paper presents an empirical case study of four releases of a very large telecommunications system, which shows that the tree-based models can be improved by transforming the predictors with principal components analysis, so that the transformed predictors are not correlated. The case study used the regression-tree algorithm in the S-Plus package and then applied a general decision rule to classify the modules",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885872,no,no,1487371920.404622
An analysis of test data selection criteria using the RELAY model of fault detection,"RELAY is a model of faults and failures that defines failure conditions, which describe test data for which execution will guarantee that a fault originates erroneous behavior that also transfers through computations and information flow until a failure is revealed. This model of fault detection provides a framework within which other testing criteria's capabilities can be evaluated. Three test data selection criteria that detect faults in six fault classes are analyzed. This analysis shows that none of these criteria is capable of guaranteeing detection for these fault classes and points out two major weaknesses of these criteria. The first weakness is that the criteria do not consider the potential unsatisfiability of their rules. Each criterion includes rules that are sufficient to cause potential failures for some fault classes, yet when such rules are unsatisfiable, many faults may remain undetected. Their second weakness is failure to integrate their proposed rules",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=232020,no,no,1487371920.015752
Fault tolerance aspects of a highly reliable microprocessor-based water turbine governor,"A highly reliable fault tolerant microprocessor-based governor with high performance is being developed. In this system, techniques such as redundancy, fault detection, fault location, fault isolation, and self-recovery have been applied successfully to achieve high reliability. The hardware architecture of this system and its fault tolerance aspects are described. The software structure and the redundancy architecture are presented. The fault detection, the fault location, and the fault isolation techniques applied to this system are studied. The self-recovery and reconfiguration of the system and the switch-over control based on the fault detection results are discussed",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=124534,no,no,1487371920.015749
Evaluating the effectiveness of fault tolerance in replicated database management systems,"Database management systems (DBMS) achieve high availability and fault tolerance usually by replication. However fault tolerance does not come for free. Therefore, DBMSs serving critical applications with real time requirements must find a trade of between fault tolerance cost and performance. The purpose of this study is two-fold. It evaluates the effectiveness of DBMS fault tolerance in the presence of corruption in database buffer cache, which poses serious threat to the integrity requirement of the DBMSs. The first experiment of this study evaluates the effectiveness of fault tolerance, and the fault impact on database integrity, performance, and availability on a replicated DBMS, ClustRa, in the presence of software faults that corrupt the volatile data buffer cache. The second experiment identifies the weak data structure components in the data buffer cache that give fatal consequences when corrupted, and suggest the need for some forms of guarding them individually or collectively.",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=781065,no,no,1487371920.015748
System dependability evaluation via a fault list generation algorithm,"The size and complexity of modern dependable computing systems has significantly compromised the ability to accurately measure system dependability attributes such as fault coverage and fault latency. Fault injection is one approach for the evaluation of dependability metrics. Unfortunately, fault injection techniques are difficult to apply because the size of the fault set is essentially infinite. Current techniques select faults randomly resulting in many fault injection experiments which do not yield any useful information. This research effort has developed a new deterministic, automated dependability evaluation technique using fault injection. The primary objective of this research effort was the development and implementation of algorithms which generate a fault set which fully exercises the fault detection and fault processing aspects of the system. The theory supporting the developed algorithms is presented first. Next, a conceptual overview of the developed algorithms is followed by the implementation details of the algorithms. The last section of this paper presents experimental results gathered via simulation-based fault injection of an Interlocking Control System (ICS). The end result is a deterministic, automated method for accurately evaluating complex dependable computing systems using fault injection",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=536240,no,no,1487371920.015747
Relevancy in fault detection analysis,"The author discusses the concept of relevancy of circuits, parts, and part failure to system performance and the impact of relevancy to fault detection analysis results. Relevancy requires that all circuits, parts, and part failure modes in a system be analyzed to determine if, in the event of an equipment malfunction, there is an adverse effect on system performance. Thus, a circuit failure critical to system performance is relevant, whereas a failure not critical to system performance is nonrelevant and therefore does not affect equipment operation. By excluding nonrelevant circuits, parts, and part failure modes form the fault detection analysis, the results of the fault detection predictions for a system's built-in test (BIT) design and/or fault detection test software are more accurate",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=196429,no,no,1487371920.015745
Comparing the Effectiveness of Software Testing Strategies,"This study applies an experimentation methodology to compare three state-of-the-practice software testing techniques: a) code reading by stepwise abstraction, b) functional testing using equivalence partitioning and boundary value analysis, and c) structural testing using 100 percent statement coverage criteria. The study compares the strategies in three aspects of software testing: fault detection effectiveness, fault detection cost, and classes of faults detected. Thirty-two professional programmers and 42 advanced students applied the three techniques to four unit-sized programs in a fractional factorial experimental design. The major results of this study are the following. 1) With the professional programmers, code reading detected more software faults and had a higher fault detection rate than did functional or structural testing, while functional testing detected more faults than did structural testing, but functional and structural testing were not different in fault detection rate. 2) In one advanced student subject group, code reading and functional testing were not different in faults found, but were both superior to structural testing, while in the other advanced student subject group there was no difference among the techniques. 3) With the advanced student subjects, the three techniques were not different in fault detection rate. 4) Number of faults observed, fault detection rate, and total effort in detection depended on the type of software tested. 5) Code reading detected more interface faults than did the other methods. 6) Functional testing detected more control faults than did the other methods.",1987,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1702179,no,no,1487371920.015744
An analysis of the fault correction process in a large-scale SDL production model,"Improvements in the software development process depend on our ability to collect and analyze data drawn from various phases of the development life cycle. Our design metrics research team was presented with a largescale SDL production model plus the accompanying problem reports that began in the requirements phase of development. The goal of this research was to identify and measure the occurrences of faults and the efficiency of their removal by development phase in order to target software development process improvement strategies. The number and severity of problem reports were tracked by development phase and fault class. The efficiency of the fault removal process using a variety of detection methods was measured Through our analysis of the system data, the study confirms that catching faults in the phase of origin is an important goal. The faults that migrated to future phases are on average eight times more costly to repair. The study also confirms that upstream faults are the most critical faults and more importantly it identifies detailed design as the major contributor of faults, including critical faults.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1201239,no,no,1487371920.015742
Fault detectability analysis for requirements validation of fault tolerant systems,"When high assurance applications are concerned, life cycle process control has witnessed steady improvement over the past two decades. As a consequence, the number of software defects introduced in the later phases of the life cycle, such as detailed design and coding, is decreasing. The majority of the remaining defects originate in the early phases of the life cycle. This is understandable, since the early phases deal with the translation from informal requirements into a formalism that will be used by developers. Since the step from informal to formal notation is inevitable, verification and validation of the requirements continue to be the research focus. Discovering potential problems as early as possible provides the potential for significant reduction in development time and cost. In this paper, the focus is on a specific aspect of requirements validation for dynamic fault tolerant control systems: the feasibility assessment of the fault detection task. An analytical formulation of the fault detectability condition is presented. This formulation is applicable to any system whose dynamics can be approximated by a linear model. The fault detectability condition can be used for objective validation of fault detection requirements. In a case study, we analyze an inverted pendulum system and demonstrate that â€œreasonableâ€?requirements for a fault detection system can be infeasible when validated against the fault detectability condition",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=809498,no,no,1487371920.015741
The development and evaluation of three diverse techniques for object-oriented code inspection,"We describe the development and evaluation of a rigorous approach aimed at the effective and efficient inspection of object-oriented (OO) code. Since the time that inspections were developed they have been shown to be powerful defect detection strategies. However, little research has been done to investigate their application to OO systems, which have very different structural and execution models compared to procedural systems. This suggests that inspection techniques may not be currently being deployed to their best effect in the context of large-scale OO systems. Work to date has revealed three significant issues that need to be addressed - the identification of chunks of code to be inspected, the order in which the code is read, and the resolution of frequent nonlocal references. Three techniques are developed with the aim of addressing these issues: one based on a checklist, one focused on constructing abstract specifications, and the last centered on the route that a use case takes through a system. The three approaches are evaluated empirically and, in this instance, it is suggested that the checklist is the most effective approach, but that the other techniques also have potential strengths. For the best results in a practical situation, a combination of techniques is recommended, one of which should focus specifically on the characteristics of OO.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1223643,no,no,1487371920.015738
Applying design metrics to a large-scale software system,"Three metrics were used to extract design information from existing code to identify structural stress points in a software system being analyzed: D<sub>i</sub>, an internal design metric which incorporates factors related to a module's internal structure; D<sub>e </sub>, an external design metric which focuses on a module's external relationships to other modules in the software system; and D(G), a composite design metric which is the sum of D<sub>i</sub> and D<sub>e </sub>. Since stress point modules generally have a high probability for being fault-prone, project managers can use the information to determine where additional testing effort should be spent and assign these modules to more experienced programmers if modifications are needed. To make the analysis more accurate and efficient, a design metrics analyzer (<sub>Ï‡</sub>Metrics) was implemented. We conducted experiments using <sub>Ï‡</sub>Metrics on part of a distributed software system, written in C, with a client-server architecture, and identified a small percentage of its functions as good candidates for fault proneness. Files containing these functions were then validated by the real defect data collected from a recent major release to its next release for their fault proneness. Normalized metrics values were also computed by dividing the D<sub>i</sub>, D<sub>e</sub>, and D(G) values by the corresponding function size determined by non-blank and non-comment lines of code to study the possible impact of function size on these metrics. Results indicate that function size has little impact on the predictive quality of our design metrics in identifying fault-prone functions",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=730891,no,no,1487371919.660341
Complexity measurements of the inter-domain management system design,"Current use of software metrics in the industry focuses on the cost and effort estimation, while some research was carried out in the direction of their use as fault indicators. Empirical studies in software measurement are scarce, especially in the realm of object-oriented metrics, while there is no record of management system assessment using these metrics. We discuss an approach to using established object-oriented software metrics as complexity/coupling and thus risk indicators early in the system development lifecycle. Further, we subject a medium-scale inter-domain network and service management system, developed in UML, to the metric assessment, and present an analysis of these measurements. This system was developed in a European Commission-sponsored ACTS research project - TRUMPET. Results indicate that the highest level of complexity, and thus also risk, is exhibited at major interconnection points between autonomous management domains. Moreover, the results imply a strong ordinal correlation between the metrics.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=962308,no,no,1487371919.66034
Fault analysis of current-controlled PWM-inverter fed induction-motor drives,"In this paper, the fault-tolerance capability of IM-drive is studied. The discussion on the fault-tolerance of IM drives in the literature has mostly been on the conceptual level without any detailed analysis. Most of studies are only achieved experimentally. This paper provides an analytical tool to quickly analyze and predict the performance under fault conditions. Also, most of the presented results were machine specific and not general enough to be applicable as an evaluation tool. So, this paper will present a generalized method for predicting the post-fault performance of IM-drives after identifying the various faults that can occur. The fault analysis for IM in the motoring mode will be presented in this paper. The paper includes an analysis for different classifications of drive faults. The faults in an IM-drive -that will be studied- can be broadly classified as: machine fault, (i.e., one of stator windings is open or short, multiple phase open or short, bearings, and rotor bar is broken) and inverter-converter faults (i.e., phase switch open or short, multiple phase fault, and DC-link voltage drop). Briefly, a general-purpose software package for variety of IM-drive faults -is introduced. This package is very important in IM-fault diagnosis and detection using artificial intelligent techniques, wavelet and signal processing.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1218607,no,no,1487371919.660339
A new metrics set for evaluating testing efforts for object-oriented programs,"Software metrics proposed and used for procedural paradigm have been found inadequate for object oriented software products, mainly because of the distinguishing features of the object oriented paradigm such as inheritance and polymorphism. Several object oriented software metrics have been described in the literature. These metrics are goal driven in the sense that they are targeted towards specific software qualities. We propose a new set of metrics for object oriented programs; this set is targeted towards estimating the testing efforts for these programs. The definitions of these metrics are based on the concepts of object orientation and hence are independent of the object oriented programming languages. The new metrics set has been critically compared with three other metrics sets published in the literature",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=787538,no,no,1487371919.660337
Class point: an approach for the size estimation of object-oriented systems,"In this paper, we present an FP-like approach, named class point, which was conceived to estimate the size of object-oriented products. In particular, two measures are proposed, which are theoretically validated showing that they satisfy well-known properties necessary for size measures. An initial, empirical validation is also performed, meant to assess the usefulness and effectiveness of the proposed measures to predict the development effort of object-oriented systems. Moreover, a comparative analysis is carried out, taking into account several other size measures.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1392720,no,no,1487371919.660335
A tool for automatically gathering object-oriented metrics,"Measuring software and the software development process are essential for organizations attempting to improve their software processes. For organizations using the object-oriented development approach, traditional metrics and metric gathering tools meant for procedural and structured software development are typically not meaningful. Chidamber and Kemerer proposed a suite of metrics for object-oriented design based upon measurement theory and guided by the insights of experienced object-oriented software developers. This paper presents the implementation of a tool to gather a subset of these proposed metrics for an object oriented software development project. Software metrics can be gathered at many levels of granularity; this tool provides measurements at two levels: class-level and system-level. Class-level metrics measure the complexity of individual classes contained in the system. These metrics can be very useful for predicting defect rates and estimating cost and schedule of future development projects. Examples of class-level metrics are depth of inheritance tree and class coupling. System-level metrics deal with a collection of classes that comprise an object-oriented system. Number of class clusters and number of class hierarchies are two examples of system-level metrics. The tool was written in C++ and currently only supports C++ software measurement. The authors plan to add the capability to measure software written in other object-oriented languages in the near future",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=332952,no,no,1487371919.660334
Building UML class diagram maintainability prediction models based on early metrics,"The fact that the usage of metrics in the analysis and design of object oriented (OO) software can help designers make better decisions is gaining relevance in software measurement arena. Moreover, the necessity of having early indicators of external quality attributes, such as maintainability, based on early metrics is growing. In addition to this, the aim is to show how early metrics which measure internal attributes, such as structural complexity and size of UML class diagrams, can be used as early class diagram maintainability indicators. For this purpose, we present a controlled experiment and its replication, which we carried out to gather the empirical data, which in turn is the basis of the current study. From the results obtained, it seems that there is a reasonable chance that useful class diagram maintainability models could be built based on early metrics. Despite this fact, more empirical studies, especially using data taken form real projects performed in industrial settings, are needed in order to obtain a comprehensive body of knowledge and experience.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1232473,no,no,1487371919.660332
A primer on object-oriented measurement,"Many object-oriented metrics have been proposed over the last decade. A few of these metrics have undergone some form of empirical validation, and some are actually being used by a number of organizations as part of an effort to manage software quality. There is evidence that using object-oriented metrics can be beneficial to conducting business and to the bottom line. This is encouraging, as estimates and prediction models using design metrics can be used to allocate maintenance resources and for obtaining assurances about software quality. The purpose of this paper is to present an overview of object-oriented metrics, their use and their (empirical) validation. The focus is on static class-level metrics, since these have been studied the most",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915527,no,no,1487371919.66033
"Comments on ""The confounding effect of class size on the validity of object-oriented metrics""","It has been proposed by El Emam et al. (ibid. vol.27 (7), 2001) that size should be taken into account as a confounding variable when validating object-oriented metrics. We take issue with this perspective since the ability to measure size does not temporally precede the ability to measure many of the object-oriented metrics that have been proposed. Hence, the condition that a confounding variable must occur causally prior to another explanatory variable is not met. In addition, when specifying multivariate models of defects that incorporate object-oriented metrics, entering size as an explanatory variable may result in misspecified models that lack internal consistency. Examples are given where this misspecification occurs.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1214331,no,no,1487371919.660328
Extracting facts from open source software,"Open source software systems are becoming increasingly important these days. Many companies are investing in open source projects and lots of them are also using such software in their own work. But because open source software is often developed without proper management, the quality and reliability of the code may be uncertain. The quality of the code needs to be measured and this can be done only with the help of proper tools. We describe a framework called Columbus with which we calculate the object oriented metrics validated by Basili et al. for illustrating how fault-proneness detection from the open source Web and e-mail suite called Mozilla can be done. We also compare the metrics of several versions of Mozilla to see how the predicted fault-proneness of the software system changed during its development. The Columbus framework has been further developed recently with a compiler wrapping technology that now gives us the possibility of automatically analyzing and extracting information from software systems without modifying any of the source code or makefiles. We also introduce our fact extraction process here to show what logic drives the various tools of the Columbus framework and what steps need to be taken to obtain the desired facts.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357790,no,no,1487371919.660325
Application of neural network for predicting software development faults using object-oriented design metrics,"In this paper, we present the application of neural network for predicting software development faults including object-oriented faults. Object-oriented metrics can be used in quality estimation. In practice, quality estimation means either estimating reliability or maintainability. In the context of object-oriented metrics work, reliability is typically measured as the number of defects. Object-oriented design metrics are used as the independent variables and the number of faults is used as dependent variable in our study. Software metrics used include those concerning inheritance measures, complexity measures, coupling measures and object memory allocation measures. We also test the goodness of fit of neural network model by comparing the prediction result for software faults with multiple regression model. Our study is conducted on three industrial real-time systems that contain a number of natural faults that has been reported for three years (Mei-Huei Tang et al., 1999).",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1201906,no,no,1487371919.316568
Design properties and object-oriented software changeability,"The assessment of the changeability of software systems is of major concern for buyers of the large systems found in fast-moving domains such as telecommunications. One way of approaching this problem is to investigate the dependency between the changeability of the software and its design, with the goal of finding design properties that can be used as changeability indicators. In our research, we defined a model of software changes and change impacts, and implemented it for the C++ language. Furthermore, we identified a set of nine object-oriented (OO) design metrics, four of which are specifically geared towards changeability detection. The model and the metrics were applied to three test systems of industrial size. The experiment showed a high correlation, across systems and across changes, between changeability and the access to a class by other classes through method invocation or variable access. On the other hand, no result could support the hypothesis that the depth of the inheritance tree has some influence on changeability. Furthermore, our results confirm the observation of others that the use of inheritance is rather limited in industrial systems",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=827305,no,no,1487371919.316567
Quality metrics of object oriented design for software development and re-development,"The quality of software has an important bearing on the financial and safety aspects in our daily life. Assessing quality of software at the design level will provide ease and higher accuracy for users. However, there is a great gap between the rapid adoption of Object Oriented (OO) techniques and the slow speed of developing corresponding object oriented metric measures, especially object oriented design measures. To tackle this issue, we look into measuring the quality of Object Oriented designs during both software development and re-development processes. A set of OO design metrics has been derived from the existing work found in literature and been further extended. The paper also presents software tools for assisting software re-engineering using the metric measures developed; this has been illustrated with an example of the experiments conducted during our research, finally concluded with the lessons learned and intended further work",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=883786,no,no,1487371919.316566
Using history information to improve design flaws detection,"As systems evolve and their structure decays, maintainers need accurate and automatic identification of the design problems. Current approaches for automatic detection of design problems are not accurate enough because they analyze only a single version of a system and consequently they miss essential information as design problems appear and evolve over time. Our approach is to use the historical information of the suspected flawed structure to increase the accuracy of the automatic problem detection. Our means is to define measurements which summarize how persistent the problem was and how much maintenance effort was spent on the suspected structure. We apply our approach on a large scale case study and show how it improves the accuracy of the detection of god classes and data classes, and additionally how it adds valuable semantical information about the evolution of flawed design structures.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1281423,no,no,1487371919.316565
An overview of object-oriented design metrics,"In this paper, we examine the current state in the field of object-oriented design metrices. We describe three sets of currently available metrics suites, namely, those of Chidamber and Kemerer (1993), Lorenze and Kidd (1994) and Abreu (1995). We consider the important features of each set, and assess the appropriateness and usefulness of each in evaluating the design of object-oriented systems. As a result, we identify problems common to all three sets of metrices, allowing us to suggest possible improvements in this area",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=615494,no,no,1487371919.316562
Can metrics help to bridge the gap between the improvement of OO design quality and its automation?,"During the evolution of object-oriented (OO) systems, the preservation of a correct design should be a permanent quest. However, for systems involving a large number of classes and that are subject to frequent modifications, the detection and correction of design flaws may be a complex and resource-consuming task. The use of automatic detection and correction tools can be helpful for this task. Various works have proposed transformations that improve the quality of an OO system while preserving its behavior. In this paper, we investigate whether some OO metrics can be used as indicators for automatically detecting situations where a particular transformation can be applied to improve the quality of a system. The detection process is based on analyzing the impact of various transformations on these OO metrics using quality estimation models",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=883034,no,no,1487371919.316559
Predicting class testability using object-oriented metrics,"We investigate factors of the testability of object-oriented software systems. The starting point is given by a study of the literature to obtain both an initial model of testability and existing OO metrics related to testability. Subsequently, these metrics are evaluated by means of two case studies of large Java systems for which JUnit test cases exist. The goal of This work is to define and evaluate a set of metrics that can be used to assess the testability of the classes of a Java system.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1386167,no,no,1487371919.316554
Estimating software fault-proneness for tuning testing activities,"The article investigates whether a correlation exists between the fault-proneness of software and the measurable attributes of the code (i.e. the static metrics) and of the testing (i.e. the dynamic metrics). The article also studies how to use such data for tuning the testing process. The goal is not to find a general solution to the problem (a solution may not even exist), but to investigate the scope of specific solutions, i.e., to what extent homogeneity of the development process, organization, environment and application domain allows data computed on past projects to be projected onto new projects. A suitable variety of case studies is selected to investigate a methodology applicable to classes of homogeneous products, rather than investigating if a specific solution exists for few cases.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=870474,no,no,1487371919.003105
An empirical evaluation of fault-proneness models,"Planning and allocating resources for testing is difficult and it is usually done on an empirical basis, often leading to unsatisfactory results. The possibility of early estimation of the potential faultiness of software could be of great help for planning and executing testing activities. Most research concentrates on the study of different techniques for computing multivariate models and evaluating their statistical validity, but we still lack experimental data about the validity of such models across different software applications. The paper reports on an empirical study of the validity of multivariate models for predicting software fault-proneness across different applications. It shows that suitably selected multivariate models can predict fault-proneness of modules of different software packages.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1007972,no,no,1487371919.003104
Towards a functional size measure for object-oriented systems from requirements specifications,This work describes a measurement protocol to map the concepts used in the OO-method requirements model onto the concepts used by the COSMIC full function points (COSMIC-FFP) functional size measurement method. This protocol describes a set of measurement operations for modeling and sizing object-oriented software systems from requirements specifications obtained in the context of the OO-method. This development method starts from a requirements model that allows the specification of software functional requirements and generates a conceptual model through a requirements analysis process. The main contribution of this work is an extended set of rules that allows estimating the functional size of OO systems at an early stage of the development lifecycle. A case study is introduced to report the obtained results from a practical point of view.,2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357949,no,no,1487371919.003102
Object oriented design function points,"Estimating different characteristics viz., size, cost, etc. of software during different phases of software development is required to manage the resources effectively. Function points measure can be used as an input to estimate these characteristics of software. The Traditional Function Point Counting Procedure (TFPCP) can not be used to measure the functionality of an Object Oriented (OO) system. This paper suggests a counting procedure to measure the functionality of an OO system during the design phase from a designers' perspective. It is adapted from the TFPCP. The main aim of this paper is to use all the available information during the OO design phase to estimate Object Oriented Design Function Points (OODFP). The novel feature of this approach is that it considers all the basic concepts of OO systems such as inheritance, aggregation, association and polymorphism",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=883785,no,no,1487371919.003101
"Measurement, prediction and risk analysis for Web applications","Accurate estimates of development effort play an important role in the successful management of larger Web development projects. By applying measurement principles to measure qualities of the applications and their development processes, feedback can be obtained to help understand, control and improve products and processes. The objective of this paper is to present a Web design and authoring prediction model based on a set of metrics which were collected using a case study evaluation (CSE). The paper is organised into three parts. Part I describes the CSE in which the metrics used in the prediction model were collected. These metrics were organised into five categories: effort metrics, structure metrics, complexity metrics, reuse metrics and size metrics. Part II presents the prediction model proposed, which was generated using a generalised linear model (GLM), and assesses its predictive power. Finally, part III investigates the use of the GLM as a framework for risk management",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915541,no,no,1487371919.003098
Comparing fault-proneness estimation models,"Over the last, years, software quality has become one of the most important requirements in the development of systems. Fault-proneness estimation could play a key role in quality control of software products. In this area, much effort has been spent in defining metrics and identifying models for system assessment. Using this metrics to assess which parts of the system are more fault-proneness is of primary importance. This paper reports a research study begun with the analysis of more than 100 metrics and aimed at producing suitable models for fault-proneness estimation and prediction of software modules/files. The objective has been to find a compromise between the fault-proneness estimation rate and the size of the estimation model in terms of number of metrics used in the model itself. To this end, two different methodologies have been used, compared, and some synergies exploited. The methodologies were the logistic regression and the discriminant analyses. The corresponding models produced for fault-proneness estimation and prediction have been based on metrics addressing different aspects of computer programming. The comparison has produced satisfactory results in terms of fault-proneness prediction. The produced models have been cross validated by using data sets derived from source codes provided by two application scenarios.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1467901,no,no,1487371919.003095
Fault coverage in testing real-time systems,"Real-time systems interact with their environment, through time constrained input/output events. The misbehavior of real-time systems is generally caused by the violation of the specified time constraints. Validation of real-time system software is an important quality control activity in the software lifecycle. Among the validation processes, testing aims at assessing the conformance of an implementation against the reference specification. One of the important aspects in testing real-time software systems is the fault coverage measurement, which consists of studying the potential faults that can be detected by a test suite generated by a given test generation method. This paper addresses the fault coverage of the Tinted Wp-method we have introduced in (En-Nouaary et al., 1998). We present a timed fault model based on the TIOA model for real-time systems specification. We study the fault coverage of the timed Wp-method with respect to our fault model",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=811206,no,no,1487371918.691445
Identifying modules which do not propagate errors,"Our goal is to identify software modules that have some locations which do not propagate errors induced by a suite of test cases. This paper focuses on whether or not data state errors can propagate from a location in the code to the outputs or observable data state during random testing with inputs drawn from an operational distribution. If a code-location's probability of propagation is estimated to be zero, then a fault in that location could escape defection during testing. Because testing is never exhaustive, there is a risk that failures due to such latent faults could occur during operations. Fault injection is a technique for directly measuring the probability of propagation. However, measurement for every location in the code of a full-scale program is often prohibitively computation-intensive. Our objective is a practical, useful alternative to direct measurement. We present empirical evidence that static software product metrics can be useful for identifying software modules where the effects of a fault in that module are not observable. A case study of an intricate computer game program revealed a useful empirical relationship between static software product metrics and propagation of errors. The case study program was an order of magnitude larger than previously reported studies",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=756768,no,no,1487371918.691443
Inheritance graph assessment using metrics,"Presents a new method integrated in Bell Canada's software acquisition process to assess software. This paper is focused on the assessment of the understandability of the inheritance graph of object-oriented software. The method is based on metrics and on the graphical illustration of the inheritance graph. A technique to decompose the inheritance graph of an object-oriented program into sub-graphs using metrics is described. Metrics are also used to identify complex sub-graphs. On the selected graphs, a technique to improve the understandability of the graphical illustration representing the inheritance graph is also described. This technique is based on extracting the main tree on the inheritance graph. A C++ case study containing 1080 classes is presented, on which this assessment method was applied",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=492443,no,no,1487371918.69144
An expression's single fault model and the testing methods,"This paper proposes a single fault model for the faults of the expressions, including operator faults (operator reference fault: an operator is replaced by another, extra or missing operator for single operand), incorrect variable or constant, incorrect parentheses. These types of faults often exist in the software, but some fault classes are hard to detect using traditional testing methods. A general testing method is proposed to detect these types of faults. Furthermore the fault simulation method of the faults is presented which can accelerate the generation of test cases and minimize the testing cost greatly. Our empirical results indicate that our methods require a smaller number of test cases than random testing, while retaining fault-detection capabilities that are as good as, or better than the traditional testing methods.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1250793,no,no,1487371918.691439
Increasing the efficiency of fault detection in modified code,"Many software systems are developed in a number of consecutive releases. Each new release does not only add new code but also modifies already existing one. In this study we have shown that the modified code can be an important source of faults. The faults are widely recognized as one of the major cost drivers in software projects. Therefore we look for methods of improving fault detection in the modified code. We suggest and evaluate a number of prediction models for increasing the efficiency of fault detection. We evaluate them against the theoretical best model, a simple model based on size, as well as against analyzing the code in a random order (not using any model). We find that using our models provides a significant improvement both over not using any model at all and using the simple model based on the class size. The gain offered by the models corresponds to 30% to 60% of the theoretical maximum.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607179,no,no,1487371918.691438
A cohesion measure for classes in object-oriented systems,"Classes are the fundamental concepts in the object-oriented paradigm. They are the basic units of object-oriented programs, and serve as the units of encapsulation, which promotes the modifiability and the reusability of them. In order to take full advantage of the desirable features provided by classes, such as data abstraction and encapsulation, classes should be designed to have good quality. Because object-oriented systems are developed by heavily reusing the existing classes, the classes of poor quality can be a serious obstacle to the development of systems. We define a new cohesion measure for assessing the quality of classes. Our approach is based on the observations on the salient natures of classes which have not been considered in the previous approaches. A Most Cohesive Component (MCC) is introduced as the most cohesive form of a class. We believe that the cohesion of a class depends on the connectivity of itself and its constituent components. We propose the connectivity factor to indicate the degree of the connectivity among the members of a class, and the structure factor to take into account the cohesiveness of its constituent components. Consequently, the cohesion of a class is defined as the product of the connectivity factor and the structure factor. This cohesion measure indicates how closely a class approaches MCC; the closely a class approaches MCC, the greater cohesion the class has",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=731241,no,no,1487371918.691436
The conceptual cohesion of classes,"While often defined in informal ways, software cohesion reflects important properties of modules in a software system. Cohesion measurement has been used for quality assessment, fault proneness prediction, software modularization, etc. Existing approaches to cohesion measurement in object-oriented software are largely based on the structural information of the source code, such as attribute references in methods. These measures reflect particular interpretations of cohesion and try to capture different aspects of cohesion and no single cohesion metric or suite is accepted as standard measurement for cohesion. The paper proposes a new set of measures for the cohesion of individual classes within an OO software system, based on the analysis of the semantic information embedded in the source code, such as comments and identifiers. A case study on open source software is presented, which compares the new measures with an extensive set of existing metrics. The differences and similarities among the approaches and results are discussed and analyzed.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510110,no,no,1487371918.691435
An empirical investigation of an object-oriented software system,"The paper describes an empirical investigation into an industrial object oriented (OO) system comprised of 133000 lines of C++. The system was a subsystem of a telecommunications product and was developed using the Shlaer-Mellor method (S. Shlaer and S.J. Mellor, 1988; 1992). From this study, we found that there was little use of OO constructs such as inheritance, and therefore polymorphism. It was also found that there was a significant difference in the defect densities between those classes that participated in inheritance structures and those that did not, with the former being approximately three times more defect-prone. We were able to construct useful prediction systems for size and number of defects based upon simple counts such as the number of states and events per class. Although these prediction systems are only likely to have local significance, there is a more general principle that software developers can consider building their own local prediction systems. Moreover, we believe this is possible, even in the absence of the suites of metrics that have been advocated by researchers into OO technology. As a consequence, measurement technology may be accessible to a wider group of potential users",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=879814,no,no,1487371918.691431
Criticality prediction models using SDL metrics set,"This paper focuses on the experiences gained from defining design metrics for SDL and comparing three prediction models for identifying the most fault-prone entities using the defined metrics. Three sets of design complexity metrics for SDL are defined according to two design phases and SDL entity types. Two neural net based prediction models and a model using the hybrid metrics are implemented and compared by a simulation. Though the backpropagation model shows the best prediction results, the selection method in hybrid complexity order is expected to have similar performance with some supports. Also two hybrid metric forms (weighted sum and weighted multiplication) are compared and it is shown that two metric forms can be used interchangeably for ordinal purpose",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=640158,no,no,1487371918.403119
A case study on the fault-tolerizability of an archetypical shared memory minisuper multiprocessor,"Issues in assessing fault-tolerance are addressed. A specific archetypical high-performance shared-memory architecture is chosen as a test case (specifically the Encore Multimax, to be used in space defense applications and environments), and a set of target fault-tolerance requirements are specified. The requirements are established to meet realistically the anticipated needs of aerospace applications of the 1990s. A methodology is then presented to systematize the steps and document the alternatives to be evaluated and the issues to be resolved in the process of fault-tolerization. The design of the resultant final target fault-tolerant machine is presented. The advantage to design based on parallel architectures is that resources can be dynamically reallocated to enhance either performance or fault-tolerance",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=26716,no,no,1487371918.403116
Using regression trees to classify fault-prone software modules,"Software faults are defects in software modules that might cause failures. Software developers tend to focus on faults, because they are closely related to the amount of rework necessary to prevent future operational software failures. The goal of this paper is to predict which modules are fault-prone and to do it early enough in the life cycle to be useful to developers. A regression tree is an algorithm represented by an abstract tree, where the response variable is a real quantity. Software modules are classified as fault-prone or not, by comparing the predicted value to a threshold. A classification rule is proposed that allows one to choose a preferred balance between the two types of misclassification rates. A case study of a very large telecommunications systems considered software modules to be fault-prone, if any faults were discovered by customers. Our research shows that classifying fault-prone modules with regression trees and the using the classification rule in this paper, resulted in predictions with satisfactory accuracy and robustness.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1044344,no,no,1487371918.403113
Performing fault simulation in large system design,"This paper presents a methodology and supporting set of tools for performing fault simulation throughout the design process for large systems. Most of the previous work in fault simulation has sought efficient methods for simulating faults at a single level design abstraction. This paper has developed a methodology for performing fault simulation of design models at the architectural, algorithmic, functional-block, and gate levels of design abstraction. As a result, fault simulation is supported throughout the design process from system definition through hardware/software implementation. Furthermore, since the fault simulation utilities are provided in an advanced design environment prototype tool, an iterative design/evaluation process is available for system designers at each stage of design refinement. The two key contributions of this paper are: a fault simulation methodology and supporting tools for performing fault simulation throughout the design process of large systems; and a methodology for performing fault simulation concurrently in hardware and software component designs and a proof-of-concept implementation",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=571707,no,no,1487371918.403112
Identifying fault-prone function blocks using the neural networks an empirical study,"An empirical study was carried out at ETRI to investigate the relationship between several metrics and the number of change requests associated with function blocks. For this, we propose the software metrics of CHILL language, which are used to develop the telecommunication software. We present the identification method of fault-prone function blocks of telecommunication software based on neural-networks. We focus on investigate the relationship between the proposed metrics and function block's faults which are found during development phase. Using the neural network model, we classify function blocks as either fault-prone or not fault prone based on the proposed metrics. Therefore, for newly added function blocks, we can predict whether the function block is fault-prone or not fault prone",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=620378,no,no,1487371918.40311
An evaluation of software structure metrics,"Evaluates some software design metrics, based on the information flow metrics of S. Henry and D. Kafura (1981, 1984), using data from a communications system. The ability of the design metrics to identify change-prone, error-prone, and complex programs was contrasted with that of simple code metrics. It was found that the design metrics were not as good at identifying change-prone, fault-prone, and complex programs as simple code metrics (i.e. lines of code and number of branches). It was also observed that the compound metrics, built up from several different basic counts, can obscure underlying effects, and thus make it more difficult to use metrics constructively",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=17200,no,no,1487371918.403108
Measuring design-level cohesion,"Cohesion was first introduced as a software attribute that, when measured, could be used to predict properties of implementations that would be created from a given design. Unfortunately, cohesion, as originally defined, could not be objectively assessed, while more recently developed objective cohesion measures depend on code-level information. We show that association-based and slice-based approaches can be used to measure cohesion using only design-level information. An analytical and empirical analysis shows that the design-level measures correspond closely with code-level cohesion measures. They can be used as predictors of or surrogates for the code-level measures. The design-level cohesion measures are formally defined, have been implemented, and can support software design, maintenance and restructuring",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=666825,no,no,1487371918.403106
An evaluation of some design metrics,"Some software design metrics are evaluated using data from a communications system. The design metrics investigated were based on the information flow metrics proposed by S. Henry and D. Kafura (1981) and the problems they encountered are discussed. The slightly simpler metrics used in this study are described. The ability of the design metrics to identify change-prone, error-prone and complex programs are contrasted with that of simple code metrics. Although one of the design metrics (informational fan-out)/ was able to identify change-prone, fault-prone and complex programs, code metrics (i.e. lines of code and number of branches) were better. In this context `better' means correctly identifying a larger proportion of change-prone, error-prone and/or complex programs, while maintaining a relatively low false identification rate (i.e. incorrectly identifying a program which did not in fact exhibit any undesirable features)",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=42974,no,no,1487371918.403104
The impact of costs of misclassification on software quality modeling,"A software quality model can make timely predictions of the class of a module, such as not fault prone or fault prone. These enable one to improve software development processes by targeting reliability improvement techniques more effectively and efficiently. Published software quality classification models generally minimize the number of misclassifications. The contribution of the paper is empirical evidence, supported by theoretical considerations, that such models can significantly benefit from minimizing the expected cost of misclassifications, rather than just the number of misclassifications. This is necessary when misclassification costs for not fault prone modules are quite different from those of fault prone modules. We illustrate the principles with a case study using nonparametric discriminant analysis. The case study examined a large subsystem of the Joint Surveillance Target Attack Radar System, JS-TARS, which is an embedded, real time, military application. Measures of the process history of each module were independent variables. Models with equal costs of misclassification were unacceptable, due to high misclassification rates for fault prone modules, but cost weighted models had acceptable, balanced misclassification rates",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=637165,no,no,1487371918.118551
Cyclomatic complexity and the year 2000,"The year 2000 problem is omnipresent, fast approaching, and will present us with something we're not used to: a deadline that can't slip. It will also confront us with two problems, one technical, the other managerial. My cyclomatic complexity measure, implemented using my company's tools, can address both of these concerns directly. The technical problem is that most of the programs using a date or time function have abbreviated the year field to two digits. Thus, as the rest of society progresses into the 21st century, our software will think it's the year 00. The managerial problem is that date references in software are everywhere; every line of code in every program in every system will have to be examined and made date compliant. In this article, I elaborate on an adaptation of the cyclomatic complexity measure to quantify and derive the specific tests for date conversion. I originated the use of cyclomatic complexity as a software metric. The specified data-complexity metric is calculated by first removing all control constructs that do not interact with the referenced data elements in the specified set, and then computing cyclomatic complexity. Specifying all global data elements gives an external coupling measure that determines encapsulation. Specifying all the date elements would quantify the effort for a year-2000 upgrade. This effort will vary depending on the quality of the code that must be changed",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=493032,no,no,1487371918.118549
Validating fault tolerant designs using laser fault injection (LFI),"Fault-tolerant processors and processing architectures traditionally have relied on higher level (module level and above) techniques for fault detection and recovery. The advent of VLSI and VHSIC technology, with increasing numbers of gates on a chip and increasing numbers of I/O pins available, has provided increasing opportunities for the inclusion of on-chip fault and defect tolerance. On-chip fault tolerance, encompassing hardware concurrent error detection and recovery mechanisms, offers the potential for improving system availability and supporting real-time fault tolerance. This paper describes the successful development and demonstration of a Laser Fault Injection (LFI) technique to inject soft, i.e., transient faults into VLSI circuits in a precisely-controlled, non-destructive, non-intrusive manner for the purpose of validating fault tolerant design and performance. This technique, as well as enabling the validation of fault-tolerant VLSI designs, also offers the potential for performing automated testing of board-level and system-level fault tolerant designs including the operating system and application software. It requires no vacuum test chamber, radioactive source, or additional on-chip fault injection circuitry. The laser fault injection technique has the advantage that it supports in situ testing of circuits and systems operating at speed. It can also emulate transient error conditions that standard diagnostic test patterns and techniques may miss",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=628323,no,no,1487371918.118548
A comparison of network level fault injection with code insertion,"This paper describes our research into the application of fault injection to Simple Object Access Protocol (SOAP) based service oriented-architectures (SOA). We show that our previously devised WS-FIT method, when combined with parameter perturbation, gives comparable performance to code insertion techniques with the benefit that it is less invasive. Finally we demonstrate that this technique can be used to compliment certification testing of a production system by strategic instrumentation of selected servers in a system.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510069,no,no,1487371918.118547
Predicting testability of program modules using a neural network,"J.M. Voas (1992) defines testability as the probability that a test case will fail if the program has a fault. It is defined in the context of an oracle for the test, and a distribution of test cases, usually emulating operations. Because testability is a dynamic attribute of software, it is very computation-intensive to measure directly. The paper presents a case study of real time avionics software to predict the testability of each module from static measurements of source code. The static software metrics take much less computation than direct measurement of testability. Thus, a model based on inexpensive measurements could be an economical way to take advantage of testability attributes during software development. We found that neural networks are a promising technique for building such predictive models, because they are able to model nonlinearities in relationships. Our goal is to predict a quantity between zero and one whose distribution is highly skewed toward zero. This is very difficult for standard statistical techniques. In other words, high testability modules present a challenging prediction problem that is appropriate for neural networks",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=888032,no,no,1487371918.118544
What makes inspections work?,"Software inspections are considered a cheap and effective way to detect and remove defects from software. Still, many people believe that the process can be improved and have thus proposed alternative methods. We reviewed many of these proposals and detected a disturbing pattern: too often competing methods are based on conflicting arguments. The existence of these competing views indicates a serious problem: we have yet to identify the fundamental drivers of inspection costs and benefits. We first built a taxonomy of the potential drivers of inspection costs and benefits. Then we conducted a family of experiments to evaluate their effects. We chose effort and interval, measured as calendar time to completion, as our primary measures of cost. Defects discovered per thousand lines of code served as our primary measure of inspection benefits",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=636690,no,no,1487371918.118542
The lines of code metric as a predictor of program faults: a critical analysis,"The relationship between measures of software complexity and programming errors is explored. Four distinct regression models were developed for an experimental set of data to create a predictive model from software complexity metrics to program errors. The lines of code metric, traditionally associated with programming errors in predictive models, was found to be less valuable as a criterion measure in these models than measures of software control complexity. A factor analytic technique used to construct a linear compound of lines of code with control metrics was found to yield models of superior predictive quality",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=139396,no,no,1487371918.118541
When to stop testing for large software systems with changing code,"Developers of large software systems must decide how long software should be tested before releasing it. A common and usually unwarranted assumption is that the code remains frozen during testing. We present a stochastic and economic framework to deal with systems that change as they are tested. The changes can occur because of the delivery of software as it is developed, the way software is tested, the addition of fixes, and so on. Specifically, we report the details of a real time trial of a large software system that had a substantial amount of code added during testing. We describe the methodology, give all of the relevant details, and discuss the results obtained. We pay particular attention to graphical methods that are easy to understand, and that provide effective summaries of the testing process. Some of the plots found useful by the software testers include: the Net Benefit Plot, which gives a running chart of the benefit; the Stopping Plot, which estimates the amount of additional time needed for testing; and diagnostic plots. To encourage other researchers to try out different models, all of the relevant data are provided",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=277579,no,no,1487371918.118539
Time domain analysis and its practical application to the measurement of phase noise and jitter,"The precise determination of phase is necessary for a very large set of measurements. Traditionally, phase measurements have been made using analog phase detectors which suffer from limited accuracy and dynamic range. This paper describes how phase digitizing, which uses time domain techniques, removes these limitations. Phase digitizing is accomplished using a time interval analyzer, which measures the signal zero-crossing times. These zero-crossing times are processed to compute phase deviation, with the reference frequency specified as a numerical value or derived from the times themselves. Phase digitizing can be applied even in the presence of modulation, as the underlying clock can be reconstructed in software to fit the data. Measurements derived from this phase data such as phase noise, jitter analysis, Allan variance (AVAR), Maximum Time Interval Error (MTIE), and Time Deviation (TDEV) are applied to such applications as the characterization of oscillators, computer clocks, chirp radar, token ring networks, and tributaries in communication systems",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=507607,no,no,1487371918.118537
Comparing high-change modules and modules with the highest measurement values in two large-scale open-source products,"Identifying change-prone modules can enable software developers to take focused preventive actions that can reduce maintenance costs and improve quality. Some researchers observed a correlation between change proneness and structural measures, such as size, coupling, cohesion, and inheritance measures. However, the modules with the highest measurement values were not found to be the most troublesome modules by some of our colleagues in industry, which was confirmed by our previous study of six large-scale industrial products. To obtain additional evidence, we identified and compared high-change modules and modules with the highest measurement values in two large-scale open-source products, Mozilla and OpenOffice, and we characterized the relationship between them. Contrary to common intuition, we found through formal hypothesis testing that the top modules in change-count rankings and the modules with the highest measurement values were different. In addition, we observed that high-change modules had fairly high places in measurement rankings, but not the highest places. The accumulated findings from these two open-source products, together with our previous similar findings for six closed-source products, should provide practitioners with additional guidance in identifying the change-prone modules.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1498769,no,no,1487371917.821429
Some effects of fault recovery order on software reliability models,"Since traditional approaches to software reliability modeling allow the user to formulate predictions using data from one realization of the debugging process, it is necessary to understand the influence of the fault recovery order on predictive performance. We introduce an experimental methodology using a data structure called the debugging graph and use it to analyze the effects of various fault recovery orders on the predictive accuracy of four well-known software reliability algorithms. Further we note fault interactions and their potential effects on the predictive process. Based on our experiment, we conjecture that the accuracy of a reliability prediction is affected by the fault recovery order as well as by fault interactions",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=341400,no,no,1487371917.821428
An input-domain based method to estimate software reliability,"A method is presented for software reliability estimation that is input-domain based. It was developed to overcome some of the difficulties in using existing reliability models in critical applications. The method classifies the faults that could be in a software program. Then it accounts for the distribution over the input-domain of input values which could activate each fault-type. The method assumes that these distributions change, by reducing their extent, with the number of test cases correctly executed. Using a simple example, the paper suggests a convenient fault classification and a choice of distributions for each fault-type. The introduction of the distributions permit better use of the information collected during the testing phase",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=488923,no,no,1487371917.821426
An automated approach for identifying potential vulnerabilities in software,"The paper presents results from analyzing the vulnerability of security-critical software applications to malicious threats and anomalous events using an automated fault injection analysis approach. The work is based on the well understood premise that a large proportion of security violations result from errors in software source code and configuration. The methodology employs software fault injection to force anomalous program states during the execution of software and observes their corresponding effects on system security. If insecure behaviour is detected, the perturbed location that resulted in the violation is isolated for further analysis and possibly retrofitting with fault tolerant mechanisms",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=674827,no,no,1487371917.821425
A fault-tolerant distributed sorting algorithm in tree networks,"The paper presents a distributed sorting algorithm for a network with a tree topology. The distributed sorting problem can be informally described as follows: nodes cooperate to reach a global configuration where every node, depending on its identifier, is assigned a specific final value taken from a set of input values distributed across all nodes; the input values may change in time. In our solution, the system reaches its final configuration in a finite time after the input values are stable and the faults cease. The fault tolerance and adaption to changing input are achieved using Dijkstra's paradigm of self stabilization. Our solution is based on continuous broadcast with acknowledgment along the tree network to achieve synchronization among processes in the system; it has O(nh) time complexity and only O(log(n)deg) memory requirement where deg is the degree of the tree and h is the height of the tree",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=659896,no,no,1487371917.821423
Thresholds for object-oriented measures,"A practical application of object oriented measures is to predict which classes are likely to contain a fault. This is contended to be meaningful because object oriented measures are believed to be indicators of psychological complexity, and classes that are more complex are likely to be faulty. Recently, a cognitive theory was proposed suggesting that there are threshold effects for many object oriented measures. This means that object oriented classes are easy to understand as long as their complexity is below a threshold. Above that threshold their understandability decreases rapidly, leading to an increased probability of a fault. This occurs, according to the theory, due to an overflow of short-term human memory. If this theory is confirmed, then it would provide a mechanism that would explain the introduction of faults into object oriented systems, and would also provide some practical guidance on how to design object oriented programs. The authors empirically test this theory on two C++ telecommunications systems. They test for threshold effects in a subset of the Chidamber and Kemerer (CK) suite of measures (S. Chidamber and C. Kemerer, 1994). The dependent variable was the incidence of faults that lead to field failures. The results indicate that there are no threshold effects for any of the measures studied. This means that there is no value for the studied CK measures where the fault-proneness changes from being steady to rapidly increasing. The results are consistent across the two systems. Therefore, we can provide no support to the posited cognitive theory",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885858,no,no,1487371917.821422
Multi-level fault injection experiments based on VHDL descriptions: a case study,"The probability of transient faults increases with the evolution of technologies. There is a corresponding increased demand for an early analysis of erroneous behaviors. This paper reports on results obtained with SEU-like fault injections in VHDL descriptions of digital circuits. Several circuit description levels are considered, as well as several fault modeling levels. These results show that an analysis performed at a very early stage in the design process can actually give a helpful insight into the response of a circuit when a fault occurs.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1030192,no,no,1487371917.821421
A new directional relay based on the measurement of fault generated current transients,"This paper presents a new principle in directional relaying in which the fault direction is determined based on a comparison of the spectral energy of the fault generated high frequency current transient components. In this scheme, a current transient detection unit, which is interfaced to the CTs on each of the output lines connecting the busbar, is employed to capture the fault generated transient current signals. The transient signal captured from each line is first integrated (based on a small window of information) to obtain its spectral energy; the level of the energy entering the busbar is then compared with that leaving it, to ascertain the direction of the fault. It is shown that the detector is also able to make correct discrimination when a fault occurs on the busbar. Simulation studies were carried out using the EMTP software. The faulted responses were examined with respect to various system and fault conditions",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=608187,no,no,1487371917.821419
Estimating Dependability of Parallel FFT Application using Fault Injection,This paper discusses estimation of dependability of a parallel FFT application. The application uses FFTW library. Fault susceptibility is assessed using software implemented fault injection. The fault injection campaign and the experiment results are presented. The response classes to injected faults are analyzed. The accuracy of evaluated data is verified experimentally.,2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1376763,no,no,1487371917.821417
An experimental evaluation of the effectiveness of automatic rule-based transformations for safety-critical applications,"Over the last years, an increasing number of safety-critical tasks have been demanded of computer systems. In particular, safety-critical computer-based applications are hitting markets where costs is a major issue, and thus solutions are required which conjugate fault tolerance with low costs. In this paper, a software-based approach for developing safety-critical applications is analyzed. By exploiting an ad-hoc tool implementing the proposed technique, several benchmark applications have been hardened against transient errors. Fault injection campaigns have been performed to evaluate the fault detection capability of the hardened applications. Moreover, a comparison of the proposed techniques with the Algorithm-Based Fault Tolerance (ABFT) approach is proposed. Experimental results show that the proposed approach is far more effective than ABFT in terms of fault detection capability when injecting transient faults in data and code memory, at a cost of an increased memory overhead. Moreover, the performance penalty introduced by the proposed technique is comparable, and sometimes lower, than that ABFT requires",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=887164,no,no,1487371917.821415
Fault emulation: a new approach to fault grading,"In this paper, we propose a method of using an FPGA-based emulation system for fault grading. The real-time simulation capability of a hardware emulator could significantly improve the run-time of fault grading, which is one of the most resource-intensive tasks in the design process. A serial fault emulation algorithm is employed and enhanced by two speed-up techniques. First, a set of independent faults can be emulated in parallel. Second, simultaneous injection of multiple dependent faults is also possible by adding extra supporting circuitry. Because the reconfiguration time spent on mapping the numerous faulty circuits into the FPGA boards could be the bottleneck of the whole process, using extra logic for injecting a large number of faults per configuration can reduce the number of reconfigurations, and thus, significantly improve the efficiency. Some modeling issues that are unique in the fault emulation environment are also addressed. The performance estimation indicates that this approach could be several orders of magnitude faster than the existing software approaches for large designs.",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=480203,no,no,1487371917.525472
Code optimization for code compression,"With the emergence of software delivery platforms such as Microsoft's .NET, the reduced size of transmitted binaries has become a very important system parameter, strongly affecting system performance. We present two novel pre-processing steps for code compression that explore program binaries' syntax and semantics to achieve superior compression ratios. The first preprocessing step involves heuristic partitioning of a program binary into streams with high auto-correlation. The second preprocessing step uses code optimization via instruction rescheduling in order to improve prediction probabilities for a given compression engine. We have developed three heuristics for instruction rescheduling that explore tradeoffs of the solution quality versus algorithm run-time. The pre-processing steps are integrated with the generic paradigm of prediction by partial matching (PPM) which is the basis of our compression codec. The compression algorithm is implemented for x86 binaries and tested on several large Microsoft applications. Binaries compressed using our compression codec are 18-24% smaller than those compressed using the best available off-the-shelf compressor.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1191555,no,no,1487371917.52547
Analyzing change effort in software during development,"We develop ordinal response models to explain the effort associated with non-defect changes of software during development. The explanatory variables include the extent of the change, the change type, and the internal complexity of the software components undergoing the change. The models are calibrated on the basis of a single software system and are then validated on two additional systems",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=809739,no,no,1487371917.525469
Defining and validating measures for object-based high-level design,"The availability of significant measures in the early phases of the software development life-cycle allows for better management of the later phases, and more effective quality assessment when quality can be more easily affected by preventive or corrective actions. We introduce and compare various high-level design measures for object-based software systems. The measures are derived based on an experimental goal, identifying fault-prone software parts, and several experimental hypotheses arising from the development of Ada systems for Flight Dynamics Software at the NASA Goddard Space Flight Center (NASA/GSFC). Specifically, we define a set of measures for cohesion and coupling, which satisfy a previously published set of mathematical properties that are necessary for any such measures to be valid. We then investigate the measures' relationship to fault-proneness on three large scale projects, to provide empirical support for their practical significance and usefulness",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=815329,no,no,1487371917.525468
EYE: a tool for measuring the defect sensitivity of IC layout,"The development of new aggressively target IC processes brings new challenges to the design and fabrication of high yielding ICs. As much as 50% of yield loss in digital ICs can be attributed to the metallisation stages of fabrication. This can be a particular problem when new process technologies such as multilevel interconnect are being introduced. This paper addresses the problem of design for manufacture of IC layout by presenting a tool that enables the measurement of critical area and hence layout defect sensitivity. The EYE (Edinburgh Yield Estimator) tool has two main uses, yield prediction and the comparison of defect sensitivities of place and routing algorithms. Its use as a tool to measure and optimise the defect sensitivity and hence the manufacturability of the IC layout produced by automated routing algorithms will be explored in this paper",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=478208,no,no,1487371917.525466
CATS-an automated user interface for software development and testing,"CATS has been developed as a user interface to allow users at client sites to access and use the ACT (analysis of complexity tool) of McCabe and Associates easily and automatically. It produces various detailed reports based on the ACT tool, and in addition it establishes and updates a complexity-tracking database at the user site. CATS has been implemented at various pilot sites. It has been used both intensively to increase test coverage for a given module and extensively to identify areas on which to focus test and assessment efforts. By using CATS, developers, testers, and managers can obtain key information on software structure, focus their efforts on those areas where defects are more likely to reside, more quickly understand and incorporate re-engineered or legacy code, and estimate the level of effort required for a project",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=500657,no,no,1487371917.525465
Definition and validation of design metrics for distributed applications,"As distributed technologies become more widely used, the need for assessing the quality of distributed applications correspondingly increases. Despite the rich body of research and practice in developing quality measures for centralised applications, there has been little emphasis on measures for distributed software. The need to understand the complex structure and behaviour of distributed applications suggests a shift in interest from traditional centralised measures to the distributed arena. We tackles the problem of evaluating quality attributes of distributed applications using software measures. Firstly, we present a measures suite to quantify internal attributes of design at an early development phase, embracing structural and behavioural aspects. The proposed measures are obtained from formal models derived from intuitive models of the problem domain. Secondly, since theoretical validation of software measures provides supporting evidence as to whether a measure really captures the internal attributes they purport to measure, we consider this validation as a necessary step before empirical validation takes place. Therefore, these measures are here theoretically validated following a framework proposed in the literature.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1232461,no,no,1487371917.525463
Dynamic coupling measurement for object-oriented software,"The relationships between coupling and external quality factors of object-oriented software have been studied extensively for the past few years. For example, several studies have identified clear empirical relationships between class-level coupling and class fault-proneness. A common way to define and measure coupling is through structural properties and static code analysis. However, because of polymorphism, dynamic binding, and the common presence of unused (""dead"") code in commercial software, the resulting coupling measures are imprecise as they do not perfectly reflect the actual coupling taking place among classes at runtime. For example, when using static analysis to measure coupling, it is difficult and sometimes impossible to determine what actual methods can be invoked from a client class if those methods are overridden in the subclasses of the server classes. Coupling measurement has traditionally been performed using static code analysis, because most of the existing work was done on nonobject oriented code and because dynamic code analysis is more expensive and complex to perform. For modern software systems, however, this focus on static analysis can be problematic because although dynamic binding existed before the advent of object-orientation, its usage has increased significantly in the last decade. We describe how coupling can be defined and precisely measured based on dynamic analysis of systems. We refer to this type of coupling as dynamic coupling. An empirical evaluation of the proposed dynamic coupling measures is reported in which we study the relationship of these measures with the change proneness of classes. Data from maintenance releases of a large Java system are used for this purpose. Preliminary results suggest that some dynamic coupling measures are significant indicators of change proneness and that they complement existing coupling measures based on static analysis.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1316867,no,no,1487371917.525461
Computer-aided fault to defect mapping (CAFDM) for defect diagnosis,"Defect diagnosis in random logic is currently done using the stuck-at fault model, while most defects seen in manufacturing result in bridging faults. In this work we use physical design and test failure information combined with bridging and stuck-at fault models to localize defects in random logic. We term this approach computer-aided fault to defect mapping (CAFDM). We build on top of the existing mature stuck-at diagnosis infrastructure. The performance of the CAFDM software was tested by injecting bridging faults into samples of a Streaming audio controller chip and comparing the predicted defect locations and layers with the actual values. The correct defect location and layer was predicted in all 9 samples for which scan-based diagnosis could be performed. The experiment was repeated on production samples that failed scan test, with promising results",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=894269,no,no,1487371917.525459
Evaluating the effectiveness of a software fault-tolerance technique on RISC- and CISC-based architectures,"This paper deals with a method able to provide a microprocessor-based system with safety capabilities by modifying the source code of the executed application, only. The method exploits a set of transformations which can automatically be applied, thus greatly reducing the cost of designing a safe system, and increasing the confidence in its correctness. Fault Injection experiments have been performed on a sample application using two different systems based on CISC and RISC processors. Results demonstrate that the method effectiveness is rather independent of the adopted platform",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=856606,no,no,1487371917.525457
Using process history to predict software quality,"Many software quality models use only software product metrics to predict module reliability. For evolving systems, however, software process measures are also important. In this case study, the authors use module history data to predict module reliability in a subsystem of JStars, a real time military system",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=666844,no,no,1487371917.272225
Temporal complexity and software faults,"Software developers use complexity metrics to predict development costs before embarking on a project and to estimate the likelihood of faults once the system is built. Traditional measures, however, were designed principally for sequential programs, providing little insight into the added complexity of concurrent systems or increased demands of real-time systems. For the purpose of predicting cost and effort of development, the CoCoMo model considers factors such as real-time and other performance requirements; for fault prediction, however, most complexity metrics are silent on concurrency. An outline for developing a measure of what we term temporal complexity, including significant and encouraging results of preliminary validation, is presented. 13 standard measures of software complexity are shown to define only two distinct domains of variance in module characteristics. Two new domains of variance are uncovered through 6 out of 10 proposed measures of temporal complexity. The new domains are shown to have predictive value in the modeling of software faults",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=341361,no,no,1487371917.272223
A source-to-source compiler for generating dependable software,"Over the last years, an increasing number of safety-critical tasks have been demanded for computer systems. In particular, safety-critical computer-based applications are hitting market areas where cost is a major issue, and thus solutions are required which conjugate fault tolerance with low costs. A source-to-source compiler supporting a software-implemented hardware fault tolerance approach is proposed, based on a set of source code transformation rules. The proposed approach hardens a program against transient memory errors by introducing software redundancy: every computation is performed twice and results are compared, and control flow invariants are checked explicitly. By exploiting the tool's capabilities, several benchmark applications have been hardened against transient errors. Fault injection campaigns have been performed to evaluate the fault detection capability of the hardened applications. In addition, we analyzed the proposed approach in terms of space and time overheads",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=972664,no,no,1487371917.272222
"PROOFS: a fast, memory-efficient sequential circuit fault simulator","The authors describe PROOFS, a fast fault simulator for synchronous sequential circuits, PROOFS achieves high performance by combining all the advantages of differential fault simulation, single fault propagation, and parallel fault simulation, while minimizing their individual disadvantages. The fault simulator minimizes the memory requirements, reduces the number of gate evaluations, and simplifies the complexity of the software implementation. PROOFS requires an average of one fifth the memory required for concurrent fault simulation and runs six to 67 times faster on the ISCAS-89 sequential benchmark circuits",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=124398,no,no,1487371917.272221
Flow analysis to detect blocked statements,"In the context of software quality assessment, the paper proposes two new kinds of data which can be extracted from source code. The first, definitely blocked statements, can never be executed because preceding code prevents the execution of the program. The other data, called possibly blocked statements, may be blocked by blocking code. The paper presents original flow equations to compute definitely and possibly blocked statements in source code. The experimental context is described and results are shown and discussed. Suggestions for further research are also presented",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=972712,no,no,1487371917.272219
The detection of faulty code violating implicit coding rules,"In the field of legacy software maintenance, there unexpectedly arises a large number of implicit coding rules, which we regard as a cancer in software evolution. Since such rules are usually undocumented and each of them is recognized only by a few members in a maintenance team, a person who is not aware of a rule often violates it while doing various maintenance activities such as adding a new functionality or repairing faults. The problem here is not only such a violation introduces a new fault but also the same kind of fault will be generated again and again in the future by different maintainers. This paper proposes a method for detecting code fragments that violate implicit coding rules. In the method, an expert maintainer, firstly, investigates the cause of each failure, described in the past failure reports, and identifies all the implicit coding rules that lie behind the faults. Then, the code patterns violating the rules (which we call ""faulty code patterns"") are described in a pattern description language. Finally, the potential faulty code fragments are automatically detected by a pattern matching technique. The result of a case study with large legacy software showed that 32.7% of the failures, which have been reported during a maintenance process, were due to the violation of implicit coding rules. Moreover, 152 faults existed in 772 code fragments detected by the prototype matching system, while 111 of them were not reported.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166936,no,no,1487371917.272218
Exploring defect data from development and customer usage on software modules over multiple releases,"Traditional defect analyses of software modules have focused on either identifying error prone modules or predicting the number of faults in a module, based on a set of module attributes such as complexity, lines of code, etc. In contrast to these metrics based modeling studies, the paper explores the relationship of the number of faults per module to the prior history of the module. Specifically, we examine the relationship between: (a) the faults discovered during development of a product release and those escaped to the field; and (b) faults in the current release and faults in previous releases. Based on the actual data from four releases of a commercial application product consisting of several thousand modules, we show that: modules with more defects in development have a higher probability of failure in the field; there is a way to assess the relative quality of software releases without detailed information on the exact release content or code size; and it is sufficient to consider just the previous release for predicting the number of defects during development or field. These results can be used to improve the prediction of quality at the module level of future releases based on the past history",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=730896,no,no,1487371917.272217
Assessing quantitatively a programming course,"The focus on assessment and measurement represents the main distinction between programming course and software engineering courses in computer curricula. We introduced testing as an essential asset of a programming course. It allows precise measurement of the achievements of the students and allows an objective assessment of the teaching itself. We measured the size and evolution of the programs developed by the students and correlated these metrics with the grades. We plan to collect progressively a large baseline. We compared the productivity and defect density of the program developed by the students during the exam to industrial data and similar academic experiences. We found that the productivity of our students is very high even compared to industrial settings. Our defect density (before rework) is higher than the industrial, which includes rework.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357918,no,no,1487371917.272215
Polymorphism measures for early risk prediction,"Polymorphism is an essential feature of the object-oriented paradigm. However, polymorphism induces hidden forms of class dependencies, which may impact software quality. In this paper, we define and empirically investigate the quality impact of polymorphism on OO design. We define measures of two main aspects of polymorphic behaviors provided by the C++ language: polymorphism based on compile time linking decisions (overloading functions for example) and polymorphism based on run-time binding decisions (virtual functions for example). Then, we validate our measures by evaluating their impact on class fault-proneness, a software quality attribute. The results show that our measures are capturing different dimensions than LOC a size measure, as well as they are significant predictors of fault proneness. In fact, we show that they constitute a good complement to the existing OO design measures.",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=841024,no,no,1487371917.272213
Some misconceptions about lines of code,"Source lines of code (SLOC) is perhaps the oldest of software metrics, and still a benchmark for evaluating new ones. Despite the extensive experience with the SLOC metric, there are still a number of misconceptions about it. The paper addresses three of them: (1) that the format of SLOC is relevant to how to properly count it (a simple experiment shows that, in fact, it does not matter), (2) that SLOC is most useful as a predictor of software quality (in fact it is most useful as a covariate of other predictors), and (3) that there is an important inverse relationship between defect density and code size (in fact, this is an arithmetic artifact of plotting bugs-per-SLOC against SLOC)",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=637174,no,no,1487371917.272211
Towards the design of a snoopy coprocessor for dynamic software-fault detection,"Dynamic Monitoring with Integrity Constraints (DynaMICs) is a software-fault monitoring approach in which the constraints are maintained separately from the program. Since the constraints are not entwined in the code, the approach facilitates the maintenance of the application and constraint code. Through code analysis during compilation, the points at which constraint checking should occur are determined. DynaMICs minimizes performance degradation, addressing a problem that has limited the use of runtime software-fault monitoring. This paper presents the preliminary design of a DynaMICs snoopy-coprocessor system, i.e., one that employs a coprocessor that utilizes bus-monitoring hardware to facilitate the concurrent execution of the application and constraint-checking code. In this approach, the coprocessor executes the constraint-checking code while the main processor executes the application code",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=749454,no,no,1487371917.055047
An experiment on subjective evolvability evaluation of object-oriented software: explaining factors and interrater agreement,"Recent trends in software development have emphasized the importance of refactoring in preserving software evolvability. We performed two experiments on software evolvability evaluation, i.e. evaluating the existence of certain code problems called code smells and the refactoring decision. We studied the agreement of the evaluators. Interrater agreement was high for simple code smells and low for the refactoring decision. Furthermore, we analyzed evaluators' demographics and source code metrics as factors explaining the evaluations. The code metrics explained over 70% of the variation regarding the simple code smell evaluations, but only about 30% of the refactoring decision. Surprisingly, the demographics were not useful predictors neither for evaluating code smells nor the refactoring decision. The low agreement for the refactoring decisions may indicate difficulty in building tool support simulating real-life subjective refactoring decisions. However, code metrics tools should be effective in highlighting straightforward problems, e.g. simple code smells.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541837,no,no,1487371917.055046
Error propagation analysis studies in a nuclear research code,"Software fault injection has recently started showing great promise as a means for measuring the safety of software programs. This paper shows the results from applying one method for implementing software fault injection, data state error corruption, to the Departure from Nucleate Boiling Ratio (DNBR) code, a research code from the Halden Reactor Project, located in Halden Norway",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=682161,no,no,1487371917.055044
Can we measure software testing effectiveness?,"The paper examines the issues of measuring and comparing the effectiveness of testing criteria. It argues that measurement, in the usual sense, is generally not possible, but that comparison is. In particular, it argues that uniform relations that guarantee that one criterion is better at fault detection than another, according to certain types of well-understood probabilistic measures of effectiveness, are especially valuable",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=263796,no,no,1487371917.055043
The repeatability of code defect classifications,"Counts of defects found during the various defect defection activities in software projects and their classification provide a basis for product quality evaluation and process improvement. However, since defect classifications are subjective, it is necessary to ensure that they are repeatable (i.e., that the classification is not dependent on the individual). We evaluate a slight adaptation of a commonly used defect classification scheme that has been applied in IBM's Orthogonal Defect Classification work, and in the SEI's Personal Software Process. The evaluation utilizes the Kappa statistic. We use defect data from code inspections conducted during a development project. Our results indicate that the classification scheme is in general repeatable. We further evaluate classes of defects to find out if confusion between some categories is more common, and suggest a potential improvement to the scheme",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=730897,no,no,1487371917.055042
Managing change in software development using a process improvement approach,"Describes the DESIRE (DESigned Improvements in Requirements Evolution) project, which is investigating novel ways to better manage change during software development. Our work has identified five key process improvement practices: (1) adopting a change improvement framework; (2) establishing an organisation-wide change process; (3) classifying change; (4) change estimation; and (5) measuring change improvement. In this paper, we describe each practice in detail and propose a change maturity model that reflects an organisation's capability to manage change",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=708102,no,no,1487371917.05504
Orthogonal defect classification-a concept for in-process measurements,"Orthogonal defect classification (ODC), a concept that enables in-process feedback to software developers by extracting signatures on the development process from defects, is described. The ideas are evolved from an earlier finding that demonstrates the use of semantic information from defects to extract cause-effect relationships in the development process. This finding is leveraged to develop a systematic framework for building measurement and analysis methods. The authors define ODC and discuss the necessary and sufficient conditions required to provide feedback to a developer; illustrate the use of the defect type distribution to measure the progress of a product through a process; illustrate the use of the defect trigger distribution to evaluate the effectiveness and eventually the completeness of verification processes such as inspection or testing; provides sample results from pilot projects using ODC; and open the doors to a wide variety of analysis techniques for providing effective and fast feedback based on the concepts of ODC",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=177364,no,no,1487371917.055039
Estimating the number of faults remaining in software code documents inspected with iterative code reviews,"Code review is considered an efficient method for detecting faults in a software code document. The number of faults not detected by the review should be small. Current methods for estimating this number assume reviews with several inspectors, but there are many cases where it is practical to employ only two inspectors. Sufficiently accurate estimates may be obtained by two inspectors employing an iterative code review (ICR) process. This paper introduces a new estimator for the number of undetected faults in an ICR process, so the process may be stopped when a satisfactory result is estimated. This technique employs the Kantorowitz estimator for N-fold inspections, where the N teams are replaced by N reviews. The estimator was tested for three years in an industrial project, where it produced satisfactory results. More experiments are needed in order to fully evaluate the approach.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1421075,no,no,1487371917.055037
"Instruction selection, resource allocation, and scheduling in the AVIV retargetable code generator","The AVIV retargetable code generator produces optimized machine code for target processors with different instruction set architectures. AVIV optimizes for minimum code size. Retargetable code generation requires the development of heuristic algorithms for instruction selection, resource allocation, and scheduling. AVIV addresses these code generation subproblems concurrently, whereas most current code generation systems address them sequentially. It accomplishes this by converting the input application to a graphical (Split-Node DAG) representation that specifies all possible ways of implementing the application on the target processor. The information embedded in this representation is then used to set up a heuristic branch-and-bound step that performs functional unit assignment, operation grouping, register bank allocation, and scheduling concurrently. While detailed register allocation is carried out as a second step, estimates of register requirements are generated during the first step to ensure high quality of the final assembly code. We show that near-optimal code can be generated for basic blocks for different architectures within reasonable amounts of CPU time. Our framework thus allows us to accurately evaluate the performance of different architectures on application code.",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=724525,no,no,1487371917.055035
Measuring and understanding the ageing of kraft insulating paper in power transformers,"In this paper we discuss the application of a number of techniques that are currently being developed for use in life assessment/condition monitoring of transformer insulation, and we illustrate possible applications with some recent data. These include: high performance liquid chromatography (HPLC) of the oil to measure the product concentrations; size exclusion chromatography (SEC) of the paper to measure changes in molecular weight distribution (related to the degree of polymerization, DP) and computer modeling of the degradation process to relate chemical degradation rates to changes in physical properties, such as tensile strength, which is of critical importance in assessing the probability of damage to the paper under fault conditions. We also revisit the use of DP measurement as a means of assessing insulation life and present results that illustrate problems that we have identified in obtaining consistent data. We describe a method, developed from the British and American standards, which enables us to obtain reproducible results with a known variance. The aims of this paper are to present a limited review of techniques that are relatively new to the transformer monitoring field, to provide some recent results from this laboratory to demonstrate their potential application, and to show the need for a range of new tools, if we are to improve our ability to assess the condition of transformers and the remanent life of their insulation.",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=509922,no,no,1487371917.055033
Technology assessment of commercially available critical area extraction tools,"In the 1990s, the semiconductor industry witnessed a philosophical change in the subject of modeling wafer final test yields. The calculation of average faults per chip has been historically calculated as the product of chip area and fault density, and often incorrectly referred to as defect density. For more than 20 years, several pioneering researchers, representing both academia and industry, have advocated a more accurate estimation of average faults per chip by using critical area, a better metric of chip sensitivity to defect mechanisms. The implementation of this concept requires sophisticated software tools that interrogate the physical design data. At the request of the member companies, International SEMATECH launched a study in 1999 of four commercially available critical area extraction (CAE) tools, with a primary objective of providing an independent technical assessment of capability, performance, accuracy, ease of use, features, and other distinguishing characteristics. Each of the tools were run on several product designs from Agilent Technologies and IBM Microelectronics. The CAE tools were all installed and evaluated at a common member company location, and each of the tool suppliers were visited, providing detailed visibility into the supplier's environment. This paper will review the evaluation methodology, and summarize the findings and results of this International SEMATECH sponsored study",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=902562,no,no,1487371916.858549
The 9 quadrant model for code reviews,"Discusses a decision-making model which can be used to determine the efficiency of a code review process. This model is based on statistical techniques such as control charts. The model has nine quadrants, each of which depicts a range of values of the cost and yield of a code review. The efficiency of the code review in detecting defects is determined by taking inputs from past data, in terms of the costs and yields of those code reviews. This estimate also provides an in-process decision-making tool. Other tools can be used effectively, in conjunction with this model, to plan for code reviews and to forecast the number of defects that could be expected in the reviews. This model can be successfully used to decide what the next step of the operational process should be. The decisions taken using this model help to reduce the number of defects present in the software delivered to the customer",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=883792,no,no,1487371916.858547
Process measures for predicting software quality,"Software reliability is essential for tactical military systems, such as the Joint Surveillance Target Attack Radar System (JSTARS). It is an embedded, real-time military application, which performs real-time detection, location, classification and tracking of moving and fixed objects on the ground. A software quality model can make timely predictions of reliability indicators. These enable one to improve software development processes by targeting reliability improvement techniques more effectively and efficiently. This paper presents a case study of a large subsystem of JSTARS to improve integration and testing. The dependent variable of a logistic regression model was the class of a module: either fault-prone or not. Measures of the process history of each module were the independent variables. The case study supports our hypothesis that the likelihood of discovering additional faults during integration and testing can be usefully modeled as a function of the module history prior to integration. This history is readily available by combining data from the project's configuration management system and problem-reporting system",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=648056,no,no,1487371916.858546
"Comments, with reply, on 'Predicting source-code complexity at the design state' by S. Henry and C. Selig","The commenter objects to the claim made by the authors of the above-mentioned article (see ibid., vol.7, no.2, p.36-44, 1990) that Halstead's E (effort) metric has been extensively validated and to a lack of precise definition for units of 'elementary mental discrimination.' With regard to the first point, S. Henry points out that since E seems to give an indication of error-prone software, it must be measuring some aspect of the source code, and that many studies have shown high correlations between effort and lines of code and McCabe's cyclomatic complexity. She agrees with the commenter's second point.<<ETX>>",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=57885,no,no,1487371916.858544
On the validation of relative test complexity for object-oriented code,"In order to help software managers and engineers allocate test resources in object-oriented software development, the Relative Test Complexity (RTC) metric was proposed and applied to an industrial system developed in C++. The initial validation by an engineering quality circle supported the validity of the RTC metric. In this paper, the RTC metric is further validated using actual fault data. It is shown that the RTC metric is a better surrogate of faults than Relative Complexity (RC), Harrison's Macro-Micro Complexity (MMC), McCabe's cyclomatic complexity (V(g)), Halstead's effort (E), and simple measures of size like LOC. Finally, the RTC and RC models are applied to change data with results that indicate that the RTC and RC metrics can be used to predict source code turmoil",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=731252,no,no,1487371916.858543
Performing high efficiency source code static analysis with intelligent extensions,"This paper presents an industry practice for highly efficient source code analysis to promote software quality. As a continuous work of previously reported source code analysis system, we researched and developed a few engineering-oriented intelligent extensions to implement more cost-effective extended code static analysis and engineering processes. These include an integrated empirical scan and filtering tool for highly accurate noise reduction, and a new code checking test tool to detect function call mismatch problems, which may lead to many severe software defects. We also extended the system with an automated defect filing and verification procedure. The results show that, for a huge code base of millions of lines, our intelligent extensions not only contribute to the completeness and effectiveness of static analysis, but also establish significant engineering productivity.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1371937,no,no,1487371916.858542
A framework for understanding conceptual changes in evolving source code,"As systems evolve, they become harder to understand because the implementation of concepts (e.g. business rules) becomes less coherent. To preserve source code comprehensibility, we need to be able to predict how this property will change. This would allow the construction of a tool to suggest what information should be added or clarified (e.g. in comments) to maintain the code's comprehensibility. We propose a framework to characterize types of concept change during evolution. It is derived from an empirical investigation of concept changes in evolving commercial COBOL II files. The framework describes transformations in the geometry and interpretation of regions of source code. We conclude by relating our observations to the types of maintenance performed and suggest how this work could be developed to provide methods for preserving code quality based on comprehensibility.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1235453,no,no,1487371916.85854
Detection of software modules with high debug code churn in a very large legacy system,"Society has become so dependent on reliable telecommunications, that failures can risk loss of emergency service, business disruptions, or isolation from friends. Consequently, telecommunications software is required to have high reliability. Many previous studies define the classification fault prone in terms of fault counts. This study defines fault prone as exceeding a threshold of debug code churn, defined as the number of lines added or changed due to bug fixes. Previous studies have characterized reuse history with simple categories. This study quantified new functionality with lines of code. The paper analyzes two consecutive releases of a large legacy software system for telecommunications. We applied discriminant analysis to identify fault prone modules based on 16 static software product metrics and the amount of code changed during development. Modules from one release were used as a fit data set and modules from the subsequent release were used as a test data set. In contrast, comparable prior studies of legacy systems split the data to simulate two releases. We validated the model with a realistic simulation of utilization of the fitted model with the test data set. Model results could be used to give extra attention to fault prone modules and thus, reduce the risk of unexpected problems",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=558896,no,no,1487371916.858538
Predicting the order of fault-prone modules in legacy software,"A goal of software quality modeling is to recommend modules for reliability enhancement early enough to prevent poor quality. Reliability improvement techniques include more rigorous design and code reviews and more extensive testing. This paper introduces the concept of module-order models for guiding software reliability enhancement and provides an empirical case study that shows how such models can be used. A module-order model predicts the rank-order of modules according to a quantitative quality factor. The case study examined a large legacy telecommunications system. We found that the amount of new and changed code due to the development of a release can be a better predictor of code churn due to subsequent bug fixes, compared to software product metrics alone. In such projects, process-related measures derived from configuration management data may be adequate for software quality modeling, without resorting to software product measurement tools and expertise",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=730899,no,no,1487371916.858537
System acquisition based on software product assessment,"The procurement of complex software product involves many risks. To properly assess and manage those risks, Bell Canada has developed methods and tools that combine process capability assessment with a static analysis based software product assessment. This paper describes the software product assessment process that is part of our risk management approach. The process and the tools used to conduct a product assessment are described. The assessment is in part based on static source code metrics and inspections. A summary of the lessons learned since the initial implementation in 1993 is provided. Over 20 products totalling more than 100 million lines of code have gone through this process",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=493417,no,no,1487371916.858534
Predicting fault detection effectiveness,"Regression methods are used to model software fault detection effectiveness in terms of several product and testing process measures. The relative importance of these product/process measures for predicting fault detection effectiveness is assessed for a specific data set. A substantial family of models is considered, specifically, the family of quadratic response surface models with two way interaction. Model selection is based on â€œleave one out at a timeâ€?cross validation using the predicted residual sum of squares (PRESS) criterion. Prediction intervals for fault detection effectiveness are used to generate prediction intervals for the number of residual faults conditioned on the observed number of discovered faults. High levels of assurance about measures like fault detection effectiveness (residual faults) require more than just high (low) predicted values, they also require that the prediction intervals have high lower (low upper) bounds",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=637168,no,no,1487371916.696589
Fault recovery characteristics of the fault tolerant multiprocessor,"The fault handling performance of the fault tolerant multiprocessor (FTMP) was investigated. Fault handling errors detected during fault injection experiments were characterized. In these fault injection experiments, the FTMP disabled a working unit instead of the faulted unit once every 500 faults, on the average. System design weaknesses allow active faults to exercise a part of the fault management software that handles byzantine or lying faults. It is pointed out that these weak areas in the FTMP's design increase the probability that, for any hardware fault, a good LRU (line replaceable unit) is mistakenly disabled by the fault management software. It is concluded that fault injection can help detect and analyze the behavior of a system in the ultra-reliable regime. Although fault injection testing cannot be exhaustive, it has been demonstrated that it provides a unique capability to unmask problems and to characterize the behavior of a fault-tolerant system",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=111293,no,no,1487371916.696587
A fault locator for radial subtransmission and distribution lines,"This paper presents the design and development of a prototype fault locator that estimates the location of shunt faults on radial subtransmission and distribution lines. The fault location technique is based on the fundamental frequency component of voltages and currents measured at the line terminal. Extensive sensitivity studies of the fault location technique have been done-some of the results are included in the paper. Hardware and software of the fault locator are described. The fault locator was tested in the laboratory using simulated fault data and a real-time playback simulator. Some results from the tests are presented. Results indicate that the fault location technique has acceptable accuracy, is robust and can be implemented with the available technology",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=867627,no,no,1487371916.696586
Assessment of fault-detection processes: an approach based on reliability techniques,"Two major factors influence the number of faults uncovered by a fault-detection process applied to a software artifact (e.g., specification, code): ability of the process to uncover faults, quality of the artifact (number of existing faults). These two factors must be assessed separately, so that one can: switch to a different process if the one being used is not effective enough, or stop the process if the number of remaining faults is acceptable. The fault-detection process assessment model can be applied to all sorts of artifacts produced during software development, and provides measures for both the `effectiveness of a fault-detection process' and the `number of existing faults in the artifact'. The model is valid even when there are zero defects in the artifact or the fault-detection process is intrinsically unable to uncover faults. More specifically, the times between fault discoveries are modeled via reliability-based techniques with an exponential distribution. The hazard rate is the product of `effectiveness of the fault-detection process' and `number of faults in the artifact'. Based on general hypotheses, the number of faults in an artifact follows a Poisson distribution. The unconditional distribution, whose parameters are estimated via maximum likelihood, is obtained",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=556586,no,no,1487371916.696585
Fault simulation with PLDs,"One measure of the effectiveness of fault tolerance in a circuit is fault coverage, the ratio of the number of faults detectable or correctable to the number of possible faults. Fault simulation can provide an estimate of the fault coverage, but is often prohibitively slow if performed in software. Simulating a faulty circuit using programmable logic devices can speed up the process. Two basic approaches to fault simulation using PLDs are discussed. Fault simulation, for ripple carry adders is carried out in both a PLD and wholly in software and the performance compared.",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=680279,no,no,1487371916.696583
Understanding the nature of software evolution,"Over the past several years, we have been developing methods of measuring the change characteristics of evolving software systems. Not all changes to software systems are equal. Some changes to these systems are very small and have low impact on the system as a whole. Other changes are substantial and have a very large impact of the fault proneness of the complete system. In this study we will identify the sources of variation in the set of software metrics used to measure the system. We will then study the change characteristics to the system over a large number of builds. We have begun a new investigation in these areas in collaboration with a flight software technology development effort at the Jet Propulsion Laboratory (JPL) and have progressed in resolving the limitations of the earlier work in two distinct steps. First, we have developed a standard for the enumeration of faults. This new standard permits software faults to be measured precisely and accurately. Second, we have developed a practical framework for automating the measurement of these faults. This new standard and fault measurement process was then applied to a software system's structural evolution during its development. Every change to the software system was measured and every fault was identified and tracked to a specific code module. The measurement process was implemented in a network appliance, minimizing the impact of measurement activities on development efforts and enabling the comparison of measurements across multiple development efforts. In this paper, we analyze the measurements of structural evolution and fault counts obtained from the JPL flight software technology development effort. Our results indicate that the measures of structural attributes of the evolving software system are suitable for forming predictors of the number of faults inserted into software modules during their development, and that some types of change are more likely to result in the insertion of faults than others. The new fault standard also insures that the model so developed has greater predictive validity.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1235409,no,no,1487371916.696582
A change impact dependency measure for predicting the maintainability of source code,"We first articulate the theoretic difficulties with the existing metrics designed for predicting software maintainability. To overcome the difficulties, we propose to measure a purely internal and objective attribute of code, namely change impact dependency, and show how it can be modeled to predict real change impact. The proposed base measure can be further elaborated for evaluating software maintainability.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342659,no,no,1487371916.69658
The effects of fault counting methods on fault model quality,"Over the past few years, we have been developing software fault predictors based on a system's measured structural evolution. We have previously shown there is a significant linear relationship between code chum, a set of synthesized metrics, and the rate at which faults are inserted into the system in terms of number of faults per unit change in code chum. A limiting factor in this and other investigations of a similar nature has been the absence of a quantitative, consistent, and repeatable definition of what constitutes a fault. The rules for fault definition were not sufficiently rigorous to provide unambiguous, repeatable fault counts. Within the framework of a space mission software development effort at the Jet Propulsion Laboratory (JPL) we have developed a standard for the precise enumeration of faults. This new standard permits software faults to be measured directly from configuration control documents. Our results indicate that reasonable predictors of the number of faults inserted into a software system can be developed from measures of the system's structural evolution. We compared the new method of counting faults with two existing techniques to determine whether the fault counting technique has an effect on the quality of the fault models constructed from those counts. The new fault definition provides higher quality fault models than those obtained using the other definitions of fault",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342827,no,no,1487371916.696579
Developing fault predictors for evolving software systems,"Over the past several years, we have been developing methods of predicting the fault content of software systems based on measured characteristics of their structural evolution. In previous work, we have shown there is a significant linear relationship between code churn, a synthesized metric, and the rate at which faults are inserted into the system in terms of number of faults per unit change in code churn. We have begun a new investigation of this relationship with a flight software technology development effort at the jet propulsion laboratory (JPL) and have progressed in resolving the limitations of the earlier work in two distinct steps. First, we have developed a standard for the enumeration of faults. Second, we have developed a practical framework for automating the measurement of these faults. we analyze the measurements of structural evolution and fault counts obtained from the JPL flight software technology development effort. Our results indicate that the measures of structural attributes of the evolving software system are suitable for forming predictors of the number of faults inserted into software modules during their development. The new fault standard also ensures that the model so developed has greater predictive validity.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1232479,no,no,1487371916.696578
Determining fault insertion rates for evolving software systems,"In developing a software system, we would like to be able to estimate the way in which the fault content changes during its development, as well as determining the locations having the highest concentration of faults. In the phases prior to test, however, there may be very little direct information regarding the number and location of faults. This lack of direct information requires the development of a fault surrogate from which the number of faults and their location can be estimated. We develop a fault surrogate based on changes in relative complexity, a synthetic measure which has been successfully used as a fault surrogate in previous work. We show that changes in the relative complexity can be used to estimate the rates at which faults are inserted into a system between successive revisions. These rates can be used to continuously monitor the total number of faults inserted into a system, the residual fault content, and identify those portions of a system requiring the application of additional fault detection and removal resources",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=730895,no,no,1487371916.696575
How to measure the impact of specific development practices on fielded defect density,"This author has mathematically correlated specific development practices to defect density and probability of on time delivery. She summarizes the results of this ongoing study that has evolved into a software prediction modeling and management technique. She has collected data from 45 organizations developing software primarily for equipment or electronic systems. Of these 45 organizations, complete and unbiased delivered defect data and actual schedule delivery data was available for 17 organizations. She presents the mathematical correlation between the practices employed by these organizations and defect density. This correlation can and is used to: predict defect density; and improve software development practices for the best return on investment",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885868,no,no,1487371916.527539
Code generation for WSLAs using AXpect,"WSLAs can be viewed as describing the service aspect of Web services. By their nature, Web services are distributed. Therefore, integrating support code into a Web service application is potentially costly and error prone. Viewed from this AOP perspective, then, we present a method for integrating WSLAs into code generation using the AXpect weaver, the AOP technology for Infopipes. This helps to localize the code physically and therefore increase the eventual maintainability and enhance the reuse of the WSLA code. We then illustrate the weavers capability by using a WSLA document to codify constraints and metrics for a streaming image application that requires CPU resource monitoring.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1314732,no,no,1487371916.527538
Does code decay? Assessing the evidence from change management data,"A central feature of the evolution of large software systems is that change-which is necessary to add new functionality, accommodate new hardware, and repair faults-becomes increasingly difficult over time. We approach this phenomenon, which we term code decay, scientifically and statistically. We define code decay and propose a number of measurements (code decay indices) on software and on the organizations that produce it, that serve as symptoms, risk factors, and predictors of decay. Using an unusually rich data set (the fifteen-plus year change history of the millions of lines of software for a telephone switching system), we find mixed, but on the whole persuasive, statistical evidence of code decay, which is corroborated by developers of the code. Suggestive indications that perfective maintenance can retard code decay are also discussed",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=895984,no,no,1487371916.527537
Software measurement and formal methods: a case study centered on TRIO+ specifications,"Presents a case study where product measures are defined for a formal specification language (TRIO+) and are validated as quality indicators. To this end, defect and effort data were collected during the development of a monitoring and control system for a power plant. We show that some of the underlying hypotheses of these measures are supported bp empirical results and that several measures are significant early indicators of specification change and effort. From a more general perspective, this study exemplifies one important advantage of formal specifications: they are measurable and can thus be better controlled, assessed and managed than informal ones.",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=630438,no,no,1487371916.527535
Gemini: maintenance support environment based on code clone analysis,"Maintaining software systems is becoming a more complex and difficult task, as the scale becomes larger. It is generally said that code cloning is one of the factors that make software maintenance difficult. A code clone is a code portion in source files that is identical or similar to another. If some faults are found in a code clone, it is necessary to correct the faults in its all code clones. However for large scale software, it is very difficult to correct them completely. We develop a maintenance support environment, called Gemini, which visualizes the code clone information from a code clone detection tool, CCFinder. Using Gemini, we can specify a set of distinctive code clones through the GUI (scatter plot and metrics graph about code clones), and refer the fragments of source code corresponding to the clone on the plot or graph.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1011326,no,no,1487371916.527534
How to predict software defect density during proposal phase,"The author has developed a method to predict defect density based on empirical data. The author has evaluated the software development practices of 45 software organizations. Of those, 17 had complete actual observed defect density to correspond to the observed development practices. The author presents the correlation between these practices and defect density in this paper. This correlation can and is used to: (a) predict defect density as early as the proposal phase, (b) evaluate proposals from subcontractors, (c) perform tradeoffs so as to minimize software defect density. It is found that as practices improve, defect density decreases. Contrary to what many software engineers claim, the average probability of a late delivery is less on average for organizations with better practices. Furthermore, the margin of error in the event that a schedule is missed was smaller on average for organizations with better practices. It is also interesting that the average number of corrective action releases required is also smaller for the organizations with the best practices. This means less downtime for customers. It is not surprising that the average SEI CMM level is higher for the organizations with the better practices",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=894894,no,no,1487371916.527532
Performance/Reliability Measures for Fault-Tolerant Computing Systems,Some fault-tolerant computing systems are discussed and existing reliability measures are explained. Some performance/reliability measures are introduced. Several systems are compared by using numerical examples with the new measures.,1984,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5221818,no,no,1487371916.527529
Empirical studies on requirement management measures,The goal of this research is to demonstrate that a subset of a set of 38 requirements management measures are good predictors of stability and volatility of requirements and change requests. At the time of writing we have theoretically validated ten of these 38 measures. We are currently planning and performing an industrial case study where we want to reach the goal described above.,2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1317416,no,no,1487371916.527525
Software testing and sequential sampling,"The paper briefly describes sequential sampling and explains how such sampling procedures can be integrated into a software testing strategy to provide benefit. Using first-pass failure rate, (a simple software testing metric), as a quality indicator, the authors show theoretically and empirically how sequential sampling plans can be used to establish a mechanism with known statistical confidence/risk which allows for decisions to stop testing when product quality either meets the target or is worse than the expectation. Quality assessment, as well as quality certification roles, can be addressed. When sequential sampling is coupled with other simple statistical techniques, information can also be provided to help identify fault prone areas. Data from releases of the 5ESS Switch project are used in several examples to validate the usefulness of the techniques",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=272880,no,no,1487371916.399937
"Code coverage, what does it mean in terms of quality?","Unit code test coverage has long been known to be an important metric for testing software, and many development groups require 85% coverage to achieve quality targets. Assume we have a test, T<sub>1</sub> which has 100% code coverage and it detects a set of defects, D<sub>1</sub>. The question, which is answered here, is ""What percentage of the defects in D<sub>1</sub> will be detected if a random subset of the tests in T<sub>1</sub> are applied to the code, which has code coverage of X% of the code?"" The purpose of this paper is to show the relation between code quality and code coverage. The relationship is derived via a model of code defect levels. A sampling technique is employed and modeled with the hypergeometric distribution while assuming uniform probability and a random distribution of defects in the code, which invokes the binomial distribution. The result of this analysis is a simple relation between defect level and quality of the code delivered after the unit code is tested. This model results in the rethinking of the use of unit code test metrics and the use of support tools",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=902502,no,no,1487371916.399935
Modeling the impact of preflushing on CTE in proton irradiated CCD-based detectors,"A software model is described that performs a ""real world"" simulation of the operation of several types of charge-coupled device (CCD)-based detectors in order to accurately predict the impact that high-energy proton radiation has on image distortion and modulation transfer function (MTF). The model was written primarily to predict the effectiveness of vertical preflushing on the custom full frame CCD-based detectors intended for use on the proposed Kepler Discovery mission, but it is capable of simulating many other types of CCD detectors and operating modes as well. The model keeps track of the occupancy of all phosphorous-silicon (P-V), divacancy (V-V) and oxygen-silicon (O-V) defect centers under every CCD electrode over the entire detector area. The integrated image is read out by simulating every electrode-to-electrode charge transfer in both the vertical and horizontal CCD registers. A signal level dependency on the capture and emission of signal is included and the current state of each electrode (e.g., barrier or storage) is considered when distributing integrated and emitted signal. Options for performing preflushing, preflashing, and including mini-channels are available on both the vertical and horizontal CCD registers. In addition, dark signal generation and image transfer smear can be selectively enabled or disabled. A comparison of the charge transfer efficiency (CTE) data measured on the Hubble space telescope imaging spectrometer (STIS) CCD with the CTE extracted from model simulations of the STIS CCD show good agreement",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1003673,no,no,1487371916.399934
Wrapping windows NT software for robustness,"As Windows NT workstations become more entrenched in enterprise-critical and even mission-critical applications, the dependability of the Windows 32-bit (Win32) platform is becoming critical. To date, studies on the robustness of system software have focused on Unix-based systems. This paper describes an approach to assessing the robustness for Win32 software and providing robustness wrappers for third party commercial off-the-shelf (COTS) software. The robustness of Win32 applications to failing operating system (OS) functions is assessed by using fault injection techniques at the interface between the application and the operating system. Finally, software wrappers are developed to handle OS failures gracefully in order to mitigate catastrophic application failures.",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=781070,no,no,1487371916.399932
"Testability, testing, and critical software assessment","Although the phrases â€œcritical systemâ€?and â€œcritical softwareâ€?encompass different degrees of â€œcriticalityâ€?based on the user and application, I consider critical software to be that which performs a task whose success is necessary to avoid a loss of property or life. Software testability is a software characteristic that refers to the ease with which some formal or informal testing criteria can be satisfied. There are varying metrics that can be applied to this measurement. Software validation generally refers to the process of showing that software is computing an expected function. Software testing is able to judge the quality of the code produced. Software testability, on the other hand, is not able to do so, because it has no information concerning whether the code is producing correct or incorrect results. It is only able to predict the likelihood of incorrect results occurring if a fault or faults exist in the code. Software testability is a validation technique, but in a different definition of the term &Gt;OPEN validationâ€?that the IEEE Standard Glossary of Software Engineering Terminology allows for. Software testability is assessing behavioral characteristics that are not related to whether the code is producing correct output",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=318458,no,no,1487371916.39993
A framework for top-down cost estimation of software development,"The Function Point Method, estimation by analogy, and algorithmic modeling are three of the most commonly applied methods used to estimate the costs and worker hours needed for a software development project. These methods, however, require a deep and wide expertise in particular areas and may still result in unacceptable discrepancies between the estimated costs and the actual costs. The paper presents a framework for a top-down cost estimation method (TCE). The method is based on the assumption that different types of software have different intrinsic complexities. We expect that this method will produce easier, faster, and more accurate estimations in the early stages of a software project",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=812729,no,no,1487371916.399928
Reliable heterogeneous applications,"This paper explores the notion of computational resiliency to provide reliability in heterogeneous distributed applications. This notion provides both software fault-tolerance and the ability to tolerate information-warfare attacks. This technology seeks to strengthen a military mission, rather than to protect its network infrastructure using static defense measures such as network security, intrusion sensors, and firewalls. Even if a failure or attack is successful and never detected, it should be possible to continue information operations and achieve mission objectives. Computational resiliency involves the dynamic use of replicated software structures, guided by mission policy, to achieve reliable operation. However, it goes further to regenerate, automatically, replication in response to a failure or attack, allowing the level of system reliability to be restored and maintained. This paper examines a prototype concurrent programming technology to support computational resiliency in a heterogeneous distributed computing environment. The performance of the technology is explored through two example applications.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1248650,no,no,1487371916.399926
Facilitating adaptation to trouble spots in wireless MANs,"This paper presents schemes that enable high level communications protocols and applications to adapt to connectivity loss and quality degradation in metropolitan area wireless networks. We postulate that the majority of these problem areas or trouble spots, which are intrinsic to wireless networks, are related to location and environmental factors. Based on this premise, we propose a mechanism that gathers semantic data pertaining to trouble spots; prior knowledge of such locations can be used by higher-level communication protocols to preemptively adapt, thereby avoiding undesirable effects at the application level. To facilitate the detection and categorization of trouble spots, we propose a list of metrics to analyze the status of a wireless service. We report on our experiences with using these metrics to identify trouble spots and present initial results from an experimental evaluation of their effectiveness.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1021709,no,no,1487371916.399925
Building effective defect-prediction models in practice,"Defective software modules cause software failures, increase development and maintenance costs, and decrease customer satisfaction. Effective defect prediction models can help developers focus quality assurance activities on defect-prone modules and thus improve software quality by using resources more efficiently. These models often use static measures obtained from source code, mainly size, coupling, cohesion, inheritance, and complexity measures, which have been associated with risk factors, such as defects and changes.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1524911,no,no,1487371916.399922
Review sample shaping through the simultaneous use of multiple classification technologies in IMPACT ADC [IC yield analysis],"Among the various options available for yield improvement tools is automatic defect classification (ADC). ADC reduces the amount of manual inspection required as well as increasing the amount of classified data available. It can serve to entirely remove operator-based manual classification. The classified data can then be used to identify the cause of the yield-limiting problem in the line. To allow this, ADC can sample a limited yet representative number of defects from the total count, but does so under less constraints than those used by an operator who will, depending on experience and knowledge, adjust their chosen sample for each case. As such, the sampling methods employed can have critical effect on the overall results. This paper looks at how the sampling criteria should be established in order to provide the best representation of existing problems in the inspected wafers, and whether this is actually possible using ADC",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=925648,no,no,1487371916.309817
An approach for designing and assessing detectors for dependable component-based systems,"In this paper, we present an approach that helps in the design and assessment of detectors. A detector is a program component that asserts the validity of a predicate in a given program state. We first develop a theory of error detection, and identify two main properties of detectors, namely completeness and accuracy. Given the complexity of designing efficient detectors, we introduce two metrics, namely completeness (C) and inaccuracy (I), that capture the operational effectiveness of detector operations, and each metric captures one efficiency aspect of the detector. Subsequently, we present an approach for experimentally evaluating these metrics, and is based on fault-injection. The metrics developed in our approach also allow a system designer to perform a cost-benefit analysis for resource allocation when designing efficient detectors for fault-tolerant systems. The applicability of our approach is suited for the design of reliable component-based systems.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1281731,no,no,1487371916.309816
Realistic defect coverages of voltage and current tests,This paper presents the realistic defect coverage of voltage and current measurements on a Philips digital CMOS ASIC library obtained by defect simulation. This analysis was made to study the minimisation of overlap between voltage and current based test methods by means of defect detection tables. Results show a poor defect coverage of voltage measurements and the major influence of the defect resistance on the voltage detectability and the possible overlap.,1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=557798,no,no,1487371916.309814
Application-based metrics for strategic placement of detectors,"The goal of this paper is to provide low-latency detection and prevent error propagation due to value errors. This paper introduces metrics to guide the strategic placement of detectors and evaluates (using fault injection) the coverage provided by ideal detectors embedded at program locations selected using the computed metrics. The computation is represented in the form of a dynamic dependence graph (DDG), a directed-acyclic graph that captures the dynamic dependencies among the values produced during the course of program execution. The DDG is employed to model error propagation in the program and to derive metrics (e.g., value fanout or lifetime) for detector placement. The coverage of the detectors placed is evaluated using fault injections in real programs, including two large SPEC95 integer benchmarks fgcc and perl). Results show that a small number of detectors, strategically placed, can achieve a high degree of detection coverage.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607501,no,no,1487371916.309813
Industrial application of criticality predictions in software development,"Cost-effective software project management has the serious need to focus resources on areas where they have biggest impact. Criticality predictions are typically applied for finding out such high-impact areas in terms of most effective defect detection. The paper investigates two related questions in the context of real projects, namely the selection of the best classification technique and the use of its results in directing management decisions. Results from a current large-scale switching project are included to show practical benefits. Fuzzy classification yielded best results and is in a second study enhanced with genetic algorithms to improve overall prediction effectiveness",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=730845,no,no,1487371916.309812
Assessment criteria for safety critical computer,This paper deals with the approach for assessing the safety critical computer-based railway systems. It presents the method used for planning and conducting an assessment. This paper provides a precise and workable assessment criteria of safety critical digital architecture. It lists the generic measures/techniques used to construct a safe architecture. The objective of this paper is to present the activities related to the assessment of safety digital architectures. It gives a set of criteria used for judging the compliance with the quality and the safety requirements. The criteria are drawn from standards requirements and best practices. These criteria have been applied to assess various case studies used in the European project named ACRuDA (Assessment and Certification Rules for Digital Architecture),1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=726502,no,no,1487371916.30981
On the assessment of safety-critical software systems,An introduction to the assessment of safety-critical software systems is presented. Particular attention is given to the role of process models,1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=63626,no,no,1487371916.309809
When can we test less?,"When it is impractical to rigorously assess all parts of complex systems, test engineers use defect detectors to focus their limited resources. We define some properties of an ideal defect detector and assess different methods of generating one. In the case study presented here, traditional methods of generating such detectors (e.g. reusing detectors from the literature, linear regression, model trees) were found to be inferior to those found via a PACE analysis.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1232459,no,no,1487371916.309807
"What are we assessing: compliance, adequacy or function?","The article is based on the authors' experience and primarily on experience of assessing safety related programmable electronic systems (PESs). Therefore the thoughts expressed are directed principally towards safety, or lack of it, and PESs. The authors believe that it is possible to identify at least three important components of assessment, addressing three different aspects of what it is that the authors are trying to assess: assessment of compliance (the objective assessment of a system's compliance with a standard or standards); assessment of adequacy (the subjective assessment a system's fitness for purpose); and assessment of function (the validation of a system to assess whether it fulfils its specified functions)",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=274657,no,no,1487371916.309806
An investigation into formatting and layout errors produced by blind word-processor users and an evaluation of prototype error prevention and correction techniques,"This paper presents the results of an investigation into tools to support blind authors in the creation and checking of word processed documents. Eighty-nine documents produced by 14 blind authors are analyzed to determine and classify common types of layout and formatting errors. Based on the survey result, two prototype tools were developed to assist blind authors in the creation of documents: a letter creation wizard, which is used before the document is produced; and a format/layout checker that detects errors and presents them to the author after the document has been created. The results of a limited evaluation of the tools by 11 blind computer users are presented. A survey of word processor usage by these users is also presented and indicates that: authors have concerns about the appearance of the documents that they produce; many blind authors fail to use word processor tools such as spell checkers, grammar checkers and templates; and a significant number of blind people rely on sighted help for document creation or checking. The paper concludes that document formatting and layout is a problem for blind authors and that tools should be able to assist.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1231235,no,no,1487371916.309804
Eleventh Annual International Phoenix Conference on Computers and Communications (Cat. No.92CH3129-4),The following topics are dealt with: hypercubes; performance analysis and modeling: advanced architectures; operating systems design; database systems design; object-oriented databases; spread spectrum; modulation; communications theory; connection and interconnection; topology and traffic control CAD/CAM and image processing; expert system design; real-time computations; reliability and fault tolerance; reconfiguration; software design; programming languages and compilers: satellite communication; networking systems; open systems and artificial intelligent and applications. Abstracts of individual papers can be found under the relevant classification codes in this or other issues,1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=200506,no,no,1487371916.158212
Safety evaluation using behavioral simulation models,"This paper describes a design environment called ADEPT (advanced design environment prototype tool) which enables designers to assess the dependability of systems early in the design process using behavioral simulation models. ADEPT is an interactive graphical design environment which allows design and analysis of systems throughout the entire design cycle. ADEPT supports functional verification, performance evaluation, and dependability analysis early in the design cycle from a single model in order to dramatically reduce design cycles and deliver products on schedule. In this paper, ADEPT is applied to the design of a distributed computer system used to control trains. Two distinct experiments were run to illustrate dependability evaluation using behavioral simulation models. The first experiment evaluates the effectiveness of using a simple (7,4) Hamming code for protecting information in a distributed system. The second experiment evaluates the effectiveness of a watchdog monitor whose role is to detect hardware and software errors in the distributed system. The experiments illustrate dependability analysis using behavioral simulation models. The first simulation demonstrates estimation of the error coverage of the (7,4) code and the mean time to hazardous event (MTTHE). The second experiment demonstrates functional verification and controllability of behavioral simulation experiments by testing the response of a watchdog monitor design to rare malicious events",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=500646,no,no,1487371916.158211
Predicting the probability of change in object-oriented systems,"Of all merits of the object-oriented paradigm, flexibility is probably the most important in a world of constantly changing requirements and the most striking difference compared to previous approaches. However, it is rather difficult to quantify this aspect of quality: this paper describes a probabilistic approach to estimate the change proneness of an object-oriented design by evaluating the probability that each class of the system will be affected when new functionality is added or when existing functionality is modified. It is obvious that when a system exhibits a large sensitivity to changes, the corresponding design quality is questionable. The extracted probabilities of change can be used to assist maintenance and to observe the evolution of stability through successive generations and identify a possible ""saturation"" level beyond which any attempt to improve the design without major refactoring is impossible. The proposed model has been evaluated on two multiversion open source projects. The process has been fully automated by a Java program, while statistical analysis has proved improved correlation between the extracted probabilities and actual changes in each of the classes in comparison to a prediction model that relies simply on past data.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1492374,no,no,1487371916.158209
Application of hazard analysis to software quality modelling,"Quality is a fundamental concept in software and information system development. It is also a complex and elusive concept. A large number of quality models have been developed for understanding, measuring and predicting quality of software and information systems. It has been recognised that quality models should be constructed in accordance to the specific features of the application domain. This paper proposes a systematic method for constructing quality models of information systems. A diagrammatic notation is devised to represent quality models that enclose application specific features. Techniques of hazard analysis for the development and deployment of safety related systems are adapted for deriving quality models from system architectural designs. The method is illustrated by a part of Web-based information systems.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1044544,no,no,1487371916.158208
Fault tolerance: a verification strategy for switching systems,"At GLOBECOM 87, R. Paterson described the new approach to performing failure mode analysis. This technique insured that each high-level functional failure mode of the detailed design is detected, recovered, and isolated according to the intent of the system-level design. Since that work, Bell-Northern Research has improved the technique and developed a toolset to make the analysis more efficient and effective. The toolset models a system from the high-level architecture to the device level while taking the software maintenance design into consideration. The model calculates the impact of subtle detail design changes at the system level and consequently identifies the effect on the end user and the operating company. The BNR design process and fault-tolerant verification process are described. To illustrate the approach considered, a switching network for a telephone switch is considered. It is concluded that the current fault tolerant verification strategy with the complementing toolset is an effective and efficient way of designing high-quality fault-tolerant systems",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=64047,no,no,1487371916.158206
Error/failure analysis using event logs from fault tolerant systems,"A methodology for the analysis of automatically generated event logs from fault tolerant systems is presented. The methodology is illustrated using event log data from three Tandem systems. Two are experimental systems, with nonstandard hardware and software components causing accelerated stresses and failures. Errors are identified on the basis of knowledge of the architectural and operational characteristics of the measured systems. The methodology takes a raw event log and reduces the data by event filtering and time-domain clustering. Probability distributions to characterize the error detection and recovery processes are obtained, and the corresponding hazards are calculated. Multivariate statistical techniques (factor analysis and cluster analysis) are used to investigate error and failure dependency among different system components. The dependency analysis is illustrated using processor halt data from one of the measured systems. It is found that the number of errors is small, even though the measurement period is relatively long. This reflects the high dependability of the measured systems.<<ETX>>",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=146626,no,no,1487371916.158205
Process diagnosis via electrical-wafer-sorting maps classification,"The commonality analysis is a proven tool for fault detection in semiconductor manufacturing. This methodology extracts subsets of production lots from all the available data. Then, data mining techniques are used only on the selected data. This approach loses part of the available information and does not discriminate among the lots. The new methodology performance the automatic classification of the electrical wafer test maps in order to identify the classes of failure present in the production lots. Subsequently, the proposed procedure uses the process history of each wafer to create a list of the root cause candidates. This methodology is the core of the software tool ACID which is currently used for process diagnosis at the Agrate site of the ST Microelectronics. A real analysis is presented.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1565736,no,no,1487371916.158203
Design-level performance prediction of component-based applications,"Server-side component technologies such as Enterprise JavaBeans (EJBs), .NET, and CORBA are commonly used in enterprise applications that have requirements for high performance and scalability. When designing such applications, architects must select suitable component technology platform and application architecture to provide the required performance. This is challenging as no methods or tools exist to predict application performance without building a significant prototype version for subsequent benchmarking. In this paper, we present an approach to predict the performance of component-based server-side applications during the design phase of software development. The approach constructs a quantitative performance model for a proposed application. The model requires inputs from an application-independent performance profile of the underlying component technology platform, and a design description of the application. The results from the model allow the architect to make early decisions between alternative application architectures in terms of their performance and scalability. We demonstrate the method using an EJB application and validate predictions from the model by implementing two different application architectures and measuring their performance on two different implementations of the EJB platform.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1556552,no,no,1487371916.158201
Globecom '00 - IEEE. Global Telecommunications Conference. Conference Record (Cat. No.00CH37137),The following topics were dealt with: next generation detection and channel estimation; third generation data communication; alternative multiple access techniques; CDMA system design and performance; smart antennas and software defined radio; wireless networks; wideband 3G systems; system adaptation and optimization; multimedia communications; active router queue management; transmission control protocol; Internet applications; multicast communications; Internet routing; network measurement and characterization; network routing; switch architectures; QoS and congestion control; performance analysis in long range dependence; OFDM; turbo codes and iterative decoding; coding; fading and diversity; space time codes; equalization and estimation; multiple antennas; satellite network protocols and IP QoS; LEO network architectures and analysis; optical networks performance; enabling technologies; WDM routing and wavelength assignment; WDM network protocols; multimedia systems; wireless and third generation network advances; future generation wireless data and voice systems; operations and performance of mobility management; advanced techniques for future radio systems; radio resource management; QoS issues in IP networks; modulation and coding; signal processing; wireless Internet; mobile ad hoc networks; congestion control and rate adaptation; performance evaluation; quality management and control of ATM networks; communication networks design and performance evaluation; space time systems; signal processing and coding for data storage,2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=891229,no,no,1487371916.158199
A comprehensive approach for modeling and testing analog and mixed-signal devices,"An approach to optimizing the testing of analog and mixed-signal devices is presented. Once an accurate model has been developed, simple algebraic operations on the model can be used to select an optimum set of test points that will minimize the test effort and maximize the test confidence; estimate the parameters of the model from measurements made at the selected test points; predict the response of the device at all candidate test points as a basis for accepting or rejecting units; calculate the accuracy of the parameter estimates and response predictions on the basis of the random measurement error; and test the validity of the model, online, so that changes in the manufacturing process can be constantly monitored and the model can be updated. The authors show how each of these procedures can be performed using simple calls to routines that are available in both public domain and commercial linear algebra software packages. The approach is quite general and has been experimentally applied to the measurement of the frequency response of an amplifier-attenuator network, to fault diagnosis of a bandpass filter using time-domain measurements, and to efficient linearity tests of A/D and D/A converters",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=114015,no,no,1487371916.158197
"Performance analysis, quality function deployment and structured methods","Quality function deployment, (QFD), an approach to synthesizing several elements of system modeling and design into single unit, is presented. Behavioral, physical, and performance modeling are usually considered as separate aspects of system design without explicit linkages. Structured methodologies have developed linkages between behavioral and physical models before, but have not considered the integration of performance models. QFD integrates performance models with traditional structured models. In this method, performance requirements such as cost, weight, and detection range are partitioned into matrices. Partitioning is done by developing a performance model, preferably quantitative, for each requirement. The parameters of the model become the engineering objectives in a QFD analysis and the models are embedded in a spreadsheet version of the traditional QFD matrices. The performance model and its parameters are used to derive part of the functional model by recognizing that a given performance model implies some structure to the functionality of the system.<<ETX>>",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=255324,no,no,1487371916.007423
Performance properties of vertically partitioned object-oriented systems,"A vertically partitioned structure for the design and implementation of object-oriented systems is proposed, and their performance is demonstrated. It is shown that the application-independent portion of the execution overheads in object-oriented systems can be less than the application-independent overheads in conventionally organized systems built on layered structures. Vertical partitioning implements objects through extended type managers. Two key design concepts result in performance improvement: object semantics can be used in the state management functions of an object type and atomicity is maintained at the type manager boundaries providing efficient recovery points. The performance evaluation is based on a case study of a simple but nontrivial distributed real-time system application",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=31351,no,no,1487371916.007421
"Software reliability: assumptions, realities and data",Software reliability models are an important issue to identify the actual state of the system during the quality assurance process and to predict the customer release reliability. Major assumptions of state of the art models are compared with some realities of a very large system from industry. Data (failure statistics) observed during the quality assurance process and after 7 years of customer release (1990-96) are supplied to validate existing or upcoming research models,1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=792631,no,no,1487371916.00742
Using databases to streamline Test Program Set development,"Databases can be used to improve efficiency and accuracy during Test Program Set (TPS) development. Key elements of the development process such as requirements analysis, establishing a fault universe, failure modes, standardization, Operational Test Program Set (OTPS) groupings, fault detection and isolation requirements, hardware design, and TPS performance, can easily be controlled and monitored using a Rapid Application Development (RAD) environment. This paper describes the guidelines, using RAD, to establish and design a group of Units Under Test (UUTs) resulting in improvements in quality, efficiency, and accuracy that are incorporated into each TPS",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885629,no,no,1487371916.007418
Structure in errors: a case study in fingerprint verification,"Measuring the accuracy of biometrics systems is important. Accuracy estimates depend very much on the quality of the test data that are used: including poor quality data will degrade the accuracy estimates. Factors that determine the good quality data and poor quality data can not be revealed by simple accuracy estimates. We propose a novel methodology to analyze how the overall accuracy estimate of a system relates to the specific quality of biometrics samples. Using a large collection of fingerprint samples, we present an analysis of system accuracy, which suggests that a significant part of the error is due to few fingers.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1047971,no,no,1487371916.007417
Design Phase Analysis of Software Reliability Using Aspect-Oriented Programming,"Software system may have various nonfunctional requirements such as reliability, security, performance and schedulability. If we can predict how well the system will meet such requirements at an early phase of software development, we can significantly save the total development cost and time. Among non-functional requirements, reliability is commonly required as the essential property of the system being developed. Therefore, many analysis methods have been proposed but methods that can be practically performed in the design phase are rare. In this paper we show how design-level aspects can be used to separate reliability concerns from essential functional concerns during software design. The aspect-oriented design technique described in this paper allows one to independently specify fault tolerance and essential functional concerns, and then weave the specifications to produce a design model that reflects both concerns. We illustrate our approach using an example.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1598598,no,no,1487371916.007415
Comparison of conventional approaches and soft-computing approaches for software quality prediction,"Managing software development and maintenance projects requires early knowledge about quality and effort needed for achieving a necessary quality level. Quality prediction models can identify outlying software components that might cause potential quality problems. Quality prediction is based on experience with similar predecessor projects constructing a relationship between the output-usually the number of errors-and some kind of input-here we use complexity metrics-to the quality of a software development project. Two approaches are presented to build quality prediction models: multilinear discriminant analysis as one example for conventional approaches and fuzzy expert-systems generated by genetic algorithms. Using the capability of genetic algorithms, the fuzzy rules can be automatically generated from example data to reduce the cost and improve the accuracy. The generated quality model-with respect to changes-provides both quality of fit (according to past data) and predictive accuracy (according to ongoing projects). The comparison of the approaches gives an answer on the effectiveness and the efficiency of a soft-computing approach",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=638086,no,no,1487371916.007414
Statistical evaluation of data allocation optimization techniques with application to computer integrated manufacturing,"A statistical approach is used to analyze the performance of data allocation optimization techniques in distributed computer systems. This approach is applied to a computer integrated manufacturing (CIM) system with hierarchical control constraints. The performance of some existing general purpose optimization techniques are compared with heuristics specifically developed for allocating data in a CIM system. Data allocation optimization techniques were actually implemented and performance was evaluated for a 250 node apparel CIM factory. The specific issues addressed include: (1) solution quality in terms of the minimum system wide input/output (I/O) task operating cost, (2) the probability of finding a better solution, (3) the performance of optimization heuristics in terms of their figure-of-merit, and (4) the impact of I/O task activation scenarios on the choice of an optimization technique for solving the CIM data allocation problem.<<ETX>>",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=200576,no,no,1487371916.007413
A framework for selecting system design metrics,Specification and design have a decisive influence over the quality of software systems. The article is based on a classification of design aspects. Design decisions that determine the design and specification products are derived by a selection process according to the functional specification and a set of non-functional constraints. The techniques of a CASE tool as a constructive approach and those of software metrics as an analytic approach have been combined to support quality management early in the development process,1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=145347,no,no,1487371916.007411
The role of test in total quality management,"The quality of a delivered product or system is ultimately limited by the quality of its design and manufacturing process. Defects that escape the design, manufacturing, and test processes are delivered to customers. Process Potential Index (Cp) and Process Capability Index (Cpk) are now widely used to express design and manufacturing quality. Design quality may be estimated from product models, if they exist, or measured using parametric test data. The difficulty in using test data is not so much in the calculations, but in the recording, storing, and selection of the data to be used in the calculations, This paper defines the metrics of quality estimation and describes an implementation of a system for estimating defect levels and process capabilities. The implementation includes the definition of a standardized parametric data exchange language for test data and an analytical system to standardize product quality measurement",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=381544,no,no,1487371916.007409
Optimistic message logging for independent checkpointing in message-passing systems,Message-passing systems with a communication protocol transparent to the applications typically require message logging to ensure consistency between checkpoints. A periodic independent checkpointing scheme with optimistic logging to reduce performance degradation during normal execution while keeping the recovery cost acceptable is described. Both time and space overhead for message logging can be reduced by detecting messages that need not be logged. A checkpoint space reclamation algorithm is presented to reclaim all checkpoints which are not useful for any possible future recovery. Communication trace-driven simulation for several hypercube programs is used to evaluate the techniques,1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=235132,no,no,1487371915.856628
Deep submicron: is test up to the challenge?,"ICs fabricated in increasingly deep submicron technologies are fundamentally shifting the problem areas on which to focus when testing these devices. Several major paradigm shifts are required in order for current test technologies to be extended to meet the challenges. In the past, chip sizes and operational speeds were such that numerous assumptions could be employed throughout the manufacturing test process which greatly eased pattern generation and application. Among those assumptions were: high stuck-at fault coverage is sufficient to guarantee high quality testing (other forms of testing can be neglected); the primary problem to be solved by test/DFT is defect detection (as opposed to defect isolation); parasitic effects can be ignored; test is a â€œstand-alone processâ€? Unfortunately, it is being discovered that deep submicron technologies invalidate many of these assumptions. It is suggested that increased emphasis should be placed on design and test generation techniques for delay defects. The industry should formalize an acceptable metric to measure the coverage of such defects. When and where appropriate, test generation software should comprehend parasitic effects and thereby create tests which are more robust with respect to test application. A tester model should be put into place to more easily debug test patterns without the necessity of actual silicon. Finally, test generation cannot be performed in a vacuum. There is a great deal of commonality between the design requirements for the use of static timing analysis and those of testability. There also must be a merging of functionalities, such as between critical path identification and path-based test generation, to ease the use of these technologies",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=529932,no,no,1487371915.856627
Quantization table design for JPEG compression of angiocardiographic images,"The lossy JPEG standard may be used for high performance image compression. As implemented in presently available hard- and software in most cases the so-called luminance quantization table is applied for gray level images, which may be scaled by a quality factor. The questions arise which quality factor is optimal and whether it is possible and worthwhile to specify quantization tables for the particular characteristics of angiocardiograms. Two new quantization tables are derived from the transfer function of the angiocardiographic system, which is a worst-case approach with respect to preserving sharp edges. To assess the performance, evaluations based on Hosaka-plots are developed. These diagrams compare the different errors introduced by lossy JPEG compression objectively.<<ETX>>",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=470199,no,no,1487371915.856626
Development of a portable digital radiographic system based on FOP-coupled CMOS image sensor and its performance evaluation,"As a continuation of our digital X-ray imaging sensor R&D, we have developed a cost-effective, portable, digital radiographic system based on the CMOS image sensor coupled with a fiber optic plate (FOP) and a conventional scintillator. The imaging system mainly consists of a commercially available CMOS image sensor of 48times48 mum<sup>2</sup> pixel size and 49.2times49.3 mm<sup>2</sup> active area (RadEyetrade2 from Rad-icon Imaging Corp.), a FOP bundled with several millions of glass fibers of about 6 mum in diameter, a phosphor screen such as Min-R or Lanex series, a readout IC board and a GUI software we developed, and a battery-operated X-ray generator (20-60 kV<sub>p</sub>; up to 1 mA). Here the FOP was incorporated into the imaging system to reduce the performance degradation of the CMOS sensor and the readout IC board caused by irradiation, and also to improve image qualities. In this paper, we describe each imaging component of the fully-integrated portable digital radiographic system in detail, and also present its performance analysis with experimental measurements and acquired X-ray images in terms of system response with exposure, contrast-to-noise ratio (CNR), modulation transfer function (MTF), noise power spectrum (NPS), and detective quantum efficiency (DQE).",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1462547,no,no,1487371915.856625
"Software engineering project management, estimation and metrics: discussion summary and recommendations","The topics of software engineering project management, estimation and metrics are discussed. The following questions are used to guide the discussion. How should student projects be managed-by staff or the students themselves? Can project management tools be used effectively for student projects? Should metrics be taught as a separate course component? To what degree can we rely on metric data derived from student projects? Much of the discussion focuses on the nature and organisation of the student projects themselves",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=534043,no,no,1487371915.856624
Improvement of sensor accuracy in the case of a variable surface reflectance gradient for active laser range finders,"In active laser range finders, the computation of the (x, y, z) coordinates of each point of a scene can be performed using the detected centroid p~ of the image spot on the sensor. When the reflectance of the scene under analysis is uniform, the intensity profile of the image spot is a Gaussian and its centroid is correctly detected assuming an accurate peak position detector. However, when a change of reflectance occurs on the scene, the intensity profile of the image spot is no longer Gaussian. This change introduces a deviation Î”p on the detected centroid p~, which will lead to erroneous (x, y, z) coordinates. This paper presents two heuristic models to improve the sensor accuracy in the case of a variable surface reflectance gradient. Simulation results are presented to show the quality of the correction and the resulting accuracy.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1246552,no,no,1487371915.856623
Discrimination of software quality in a biomedical data analysis system,"Object-oriented visualization-based software systems for biomedical data analysis must deal with complex and voluminous datasets within a flexible yet intuitive graphical user interface. In a research environment, the development of such systems are difficult to manage due to rapidly changing requirements, incorporation of newly developed algorithms, and the needs imposed by a diverse user base. One issue that research supervisors must contend with is an assessment of the quality of the system's software objects with respect to their extensibility, reusability, clarity, and efficiency. Objects from a biomedical data analysis system were independently analyzed by two software architects and ranked according to their quality. Quantitative software features were also compiled at varying levels of granularity. The discriminatory power of these software metrics is discussed and their effectiveness in assessing and predicting software object quality is described",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=943808,no,no,1487371915.856621
A reliability model combining representative and directed testing,"Directed testing methods, such as functional or structural testing, have been criticized for a lack of quantifiable results. Representative testing permits reliability modeling, which provides the desired quantification. Over time, however, representative testing becomes inherently less effective as a means of improving the actual quality of the software under test. A model is presented which permits representative and directed testing to be used in conjunction. Representative testing can be used early, when the rate of fault revelation is high. Later results from directed testing can be used to update the reliability estimates conventionally associated with representative methods. The key to this combination is shifting the observed random variable from interfailure time to a post-mortem analysis of the debugged faults, using order statistics to combine the observed failure rates of faults no matter how those faults were detected",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=493445,no,no,1487371915.856619
Architecture Optimization of Aerospace Computing Systems,"Simultaneous consideration of both performance and reliability issues is important in the choice of computer architectures for real-time aerospace applications. One of the requirements for such a fault-tolerant computer system is the characteristic of graceful degradation. A shared and replicated resources computing system represents such an architecture. In this paper, a combinatorial model is used for the evaluation of the instruction execution rate of a degradable, replicated resources computing system such as a modular multiprocessor system. Next, a method is presented to evaluate the computation reliability of such a system utilizing a reliability graph model and the instruction execution rate. Finally, this computation reliability measure, which simultaneously describes both performance and reliability, is applied as a constraint in an architecture optimization model for such computing systems.",1983,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1676135,no,no,1487371915.856618
Performance Evaluation for Self-Healing Distributed Services,"Distributed applications, based on internetworked services, provide users with more flexible and varied services and developers with the ability to incorporate a vast array of services into their applications. Such applications are difficult to develop and manage due to their inherent dynamics and heterogeneity. One desirable characteristic of distributed applications is self-healing, or the ability to reconfigure themselves ""on the fly"" to circumvent failure. In this paper, we discuss our middleware for developing self-healing distributed applications. We present the model we adopted for self-healing behaviour and show as case study the reconfiguration of an application that uses networked sorting services and an application for networked home appliances. We discuss the performance benefits of self-healing property by analyzing the elapsed time for automatic reconfiguration without user intervention. Our results show that a distributed application developed with our self-healing middleware is able to perform smoothly by quickly reconfiguring its services upon detection of failure",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1524271,no,no,1487371915.853357
Predicting C++ program quality by using Bayesian belief networks,"There have been many attempts to build models for predicting the software quality. Such models are used to measure the quality of software systems. The key variables in these models are either size or complexity metrics. There are, however, serious statistical and theoretical difficulties with these approaches. By using Bayesian belief network, we can overcome some of the more serious problems by taking more quality factors, which have direct or indirect impact on the software quality. In this paper, we have suggested a model to predicting the computer program quality by using Bayesian belief network. We found that the implementation of all quality factors were not feasible. Therefore, we have selected 14 quality factors to be implemented on an average size of two C++ programs. The selection criteria were based on the reviewer's opinions. Each node on the given Bayesian believe network represents one quality factor. We have drawn the BBN for the two C++ programs considering 14 nodes. The BBN has been constructed. The model has been executed and the results have been discussed.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1307903,no,no,1487371915.853356
A fail safe node using transputers for railway signalling applications,"A new fail-safe node using transputers for application in railway signaling systems is reported. This fail-safe node provides fault tolerance and safe reaction, which are required for railway signaling systems. The basic scheme of the fail-safe node is discussed, together with the concepts of leadership based on rotation, recovery in the fail-safe node, reconfiguration, and safe shutdown",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=271897,no,no,1487371915.853355
The Schneidewind software reliability model revisited,"A software reliability model based on a nonhomogeneous Poisson process (NHPP) was proposed by N.F. Schneidewind (Sigplan Notices, vol.10, p.337, 1975). Since then, many other NHPP models have been suggested and studied by various authors. The authors show that several NHPP models can be derived based on the general assumptions made by Schneidewind. Also, they note that in the original paper, there are several interesting approaches worth further consideration. To mention a few, Schneidewind modelled both the software correction process and the software detection process. Methods of weighted least squares were adopted together with the software reliability forecasting based on previous measurements. In this paper, the Schneidewind model is revisited and some further research result are presented",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=285846,no,no,1487371915.853354
Prediction-based routing for cell-switched networks,"ATM is the target switching and multiplexing technology for implementation of B-ISDN. However, existing ATM software systems are deficient in many respects, such as QoS-based routing. Noting that the route of a virtual circuit remains fixed throughout a session, it is clear that route selection will have long-term effects on network congestion, especially for long sessions. Hence, it is important that routing decisions be based not only on current network state information, but also on predicted future network state information. In this paper we investigate a new prediction-based routing methodology. An ATM LAN software simulator, SimATM, is developed and utilized in this research. SimATM is used to investigate the relative performances of various routing algorithms and link weight assignment strategies. The results clearly indicate that prediction-based routing significantly improves performance with respect to average cell delay, throughput, and jitter. Furthermore, this improvement is shown not to come at the expense of background traffic performance",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=493672,no,no,1487371915.853352
Task allocation algorithms for maximizing reliability of distributed computing systems,"We consider the problem of finding an optimal and suboptimal task allocation (i.e., to which processor should each module of a task or program be assigned) in distributed computing systems with the goal of maximizing the system reliability (i.e., the probability that the system can run the entire task successfully). The problem of finding an optimal task allocation is known to be NP-hard in the strong sense. We present an algorithm for this problem, which uses the idea of branch and bound with underestimates for reducing the computations in finding an optimal task allocation. The algorithm reorders the list of modules to allow a subset of modules that do not communicate with one another to be assigned last, for further reduction in the computations of optimal task allocation for maximizing reliability. We also present a heuristic algorithm which obtains suboptimal task allocations in a reasonable amount of computational time. We study the performance of the algorithms over a wide range of parameters such as the number of modules, the number of processors, the ratio of average execution cost to average communication cost, and the connectivity of modules. We demonstrate the effectiveness of our algorithms by comparing them with recent competing task allocation algorithms for maximizing reliability available in the literature",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=600888,no,no,1487371915.853351
Guest Editor's Introduction: The Promise of Public Software Engineering Data Repositories,"Scientific discovery related to software is based on a centuries-old paradigm common to all fields of science: setting up hypotheses and testing them through experiments. Repeatedly confirmed hypotheses become models that can describe and predict real-world phenomena. The best-known models in software engineering describe relationships between development processes, cost and schedule, defects, and numerous software ""-ilities"" such as reliability, maintainability, and availability. But, compared to other disciplines, the science of software is relatively new. It's not surprising that most software models have proponents and opponents among software engineers. This introduction to the special issue discusses the power of modeling, the promise of data repositories, and the workshop devoted to this topic.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1524910,no,no,1487371915.85335
A process for software architecture evaluation using metrics,"Software systems often undergo changes. Changes are necessary not only to fix defects but also to accommodate new features demanded by users. Most of the time, changes are made under schedule and budget constraints and developers lack time to study the software architecture and select the best way to implement the changes. As a result, the code degenerates, making it differ from the planned design. The time spent on the planned design to create architecture to satisfy certain properties is lost, and the systems may not satisfy those properties any more. We describe an approach to systematically detect and correct deviations from the planned design as soon as possible based on architectural guidelines. We also describe a case study, in which the process was applied.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1199475,no,no,1487371915.853349
Paging strategies in an object oriented environment,"Paging issues which arise in object-oriented programming systems are considered. Unlike conventional virtual memory systems, object-oriented system provide clues about the underlying structure of the executing programs. This fact is used to suggest improved memory management strategies for object-oriented systems. These strategies are modeled using a kind of simple game on graphs. This game allows estimation of the performance needed to ensure object-fault-free computation",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=113668,no,no,1487371915.853348
Development life cycle management: a multiproject experiment,"A variety of life cycle models for software systems development are generally available. However, it is generally difficult to compare and contrast the methods and very little literature is available to guide developers and managers in making choices. Moreover in order to make informed decisions developers require access to real data that compares the different models and the results associated with the adoption of each model. This paper describes an experiment in which fifteen software teams developed comparable software products using four different development approaches (V-model, incremental, evolutionary and extreme programming). Extensive measurements were taken to assess the time, quality, size, and development efficiency of each product. The paper presents the experimental data collected and the conclusions related to the choice of method, its impact on the project and the quality of the results as well as the general implications to the practice of systems engineering project management.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1409928,no,no,1487371915.853346
Analysis of hypergeometric distribution software reliability model,"The article gives detailed mathematical results on the hypergeometric distribution software reliability model (HGDSRM) proposed by Y. Tohma et al. (1989; 1991). In the above papers, Tohma et al. developed the HGDSRM as a discrete-time stochastic model and derived a recursive formula for the mean cumulative number of software faults detected up to the i-th (>0) test instance in testing phase. Since their model is based on only the mean value of the cumulative number of faults, it is impossible to estimate not only the software reliability but also the other probabilistic dependability measures. We introduce the concept of cumulative trial processes, and describe the dynamic behavior of the HGDSRM exactly. In particular, we derive the probability mass function of the number of software faults detected newly at the i-th test instance and its mean as well as the software reliability defined as the probability that no faults are detected up to an arbitrary time. In numerical examples with real software failure data, we compare several HGDSRMs with different model parameters in terms of least squared sum and show that the mathematical results obtained here are very useful to assess the software reliability with the HGDSRM.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=989470,no,no,1487371915.850085
A New Allocation Scheme for Parallel Applications with Deadline and Security Constraints on Clusters,"Parallel applications with deadline and security constraints are emerging in various areas like education, information technology, and business. However, conventional job schedulers for clusters generally do not take security requirements of realtime parallel applications into account when making allocation decisions. In this paper, we address the issue of allocating tasks of parallel applications on clusters subject to timing and security constraints in addition to precedence relationships. A task allocation scheme, or TAPADS (task allocation for parallel applications with deadline and security constraints), is developed to find an optimal allocation that maximizes quality of security and the probability of meeting deadlines for parallel applications. In addition, we proposed mathematical models to describe a system framework, parallel applications with deadline and security constraints, and security overheads. Experimental results show that TAPADS significantly improves the performance of clusters in terms of quality of security and schedulability over three existing allocation schemes",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4154100,no,no,1487371915.850084
A methodology for constructing maintainability model of object-oriented design,"It is obvious that qualities of software design heavily affects on qualities of software ultimately developed. One of claimed advantages of object-oriented paradigm is the ease of maintenance. The main goal of this work is to propose a methodology for constructing maintainability model of object-oriented software design model using three techniques. Two subcharacteristics of maintainability: understandability and modifiability are focused in this work. A controlled experiment is performed in order to construct maintainability models of object-oriented designs using the experimental data. The first maintainability model is constructed using metrics-discriminant technique. This technique analyzes the pattern of correlation between maintainability levels and structural complexity design metrics applying discriminant analysis. The second one is built using weighted-score-level technique. The technique uses a weighted sum method by combining understandability and modifiability levels which are converted from understandability and modifiability scores. The third one is created using weighted-predicted-level technique. Weighted-predicted-level uses a weighted sum method by combining predicted understandability and modifiability level, obtained from applying understandability and modifiability models. This work presents comparison of maintainability models obtained from three techniques.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357962,no,no,1487371915.850082
Adaptive parameter tuning for relevance feedback of information retrieval,"Relevance feedback is an effective way to improve the performance of an information retrieval system. In practice, the parameters for feedback were usually determined manually without the consideration of the quality of the query. We propose a new concept (adaptiveness) to measure the quality of the query. We built two models to predict the adaptiveness of the query. The parameters for feedback were then determined by the quality of the query. Our experiments on TREC data showed that the performance was improved significantly when compared with blind relevance feedback.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1021462,no,no,1487371915.850081
"Complexities in DSP software compilation: performance, code size, power, retargetability","The paper presents a new methodology for software compilation for embedded DSP systems. Although it is well known that conventional compilation techniques do not produce high quality DSP code, few researchers have addressed this area. Performance, estimated power dissipation, and code size are important design constraints in embedded DSP design. New techniques for code generation targeting DSP processors are introduced and employed to show improvements and applicability to different fixed point and floating point DSP popular architectures. Code is generated in fast CPU times and is optimized for minimum code size, energy dissipation, or maximum performance. Code generated for realistic DSP applications provide performance and code size improvements of up to 118% and measured power improvements of up to 49% for popular DSP processors compared to previous research and a commercial compiler. This research is important for industry since DSP software can be efficiently generated, with constraints on code size, performance, and energy dissipation",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=656093,no,no,1487371915.85008
A case study in root cause defect analysis,"There are three interdependent factors that drive our software development processes: interval, quality and cost. As market pressures continue to demand new features ever more rapidly, the challenge is to meet those demands while increasing, or at least not sacrificing, quality. One advantage of defect prevention as an upstream quality improvement practice is the beneficial effect it can have on interval: higher quality early in the process results in fewer defects to be found and repaired in the later parts of the process, thus causing an indirect interval reduction. We report a retrospective root cause defect analysis study of the defect Modification Requests (MRs) discovered while building, testing, and deploying a release of a transmission network element product. We subsequently introduced this analysis methodology into new development projects as an in-process measurement collection requirement for each major defect MR. We present the experimental design of our case study discussing the novel approach we have taken to defect and root cause classification and the mechanisms we have used for randomly selecting the MRs to analyze and collecting the analyses via a Web interface. We then present the results of our analyses of the MRs and describe the defects and root causes that we found, and delineate the countermeasures created to either prevent those defects and their root causes or detect them at the earliest possible point in the development process. We conclude with lessons learned from the case study and resulting ongoing improvement activities.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=870433,no,no,1487371915.850079
Performance of a real time low rate voice codec,"A real time low rate voice coder/decoder has been developed and is being tested using speakers in simulated operational environments. The unit uses the TMS32010 to perform the signal processing functions, and is packaged in a manner suitable for an office or benign field environment. The software implements the following algorithms: <img src=""/images/tex/3140.gif"" alt=""\bullet""> 2400 bps LPC using the United States Government standard LPC-10 algorithm. <img src=""/images/tex/3140.gif"" alt=""\bullet""> 800 bps using vector quantization. <img src=""/images/tex/3140.gif"" alt=""\bullet""> 400 bps using frame repeat vector quantization with trellis coding of gain and pitch. The hardware configuration is described, along with a brief explanation of the available test configurations and execution options. A review of the coding algorithms is presented, with a summary of the operational parameters. Test results include measured execution times, program and data memory sizes. Preliminary listening tests indicate that the voice quality at 800 bps is very close to that achieved at 2400 bps with the standard algorithms.",1986,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1168611,no,no,1487371915.850078
Predicting fault-prone modules with case-based reasoning,"Software quality classification models seek to predict quality factors such as whether a module will be fault prone, or not. Case based reasoning (CBR) is a modeling technique that seeks to answer new questions by identifying similar â€œcasesâ€?from the past. When applied to software reliability, the working hypothesis of our approach is this: a module currently under development is probably fault prone if a module with similar product and process attributes in an earlier release was fault prone. The contribution of the paper is application of case based reasoning to software quality modeling. To the best of our knowledge, this is the first time that case based reasoning has been used to identify fault prone modules. A case study illustrates our approach and provides evidence that case based reasoning can be the basis for useful software quality classification models that are competitive with discriminant models. The case study revisits data from a previously published nonparametric discriminant analysis study. The Type II misclassification rate of the CBR model was substantially better than that of the discriminant model. Although the Type I misclassification rate was slightly greater and the overall misclassification rate was only slightly less, the CBR model was preferred when costs of misclassification were considered",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=630845,no,no,1487371915.850077
A resynchronization method for real-time supervision,"Real-time supervision is one technique used to improve the perceived reliability of software systems. A real-time supervisor observes the inputs and outputs of the system and reports failures that occur. The approach presented in this paper uses the specification of external behavior of the system to detect failures. Failures are reported in real-time. In addition, the approach permits the assessment of the erroneous states of the system. Following a failure, the supervisor makes an assumption of the (system) erroneous state. Consequences of the same fault are not reported repeatedly. The supervisor accommodates the nondeterminism permissible under some specification formalism. The formalism considered in this paper is the CCITT SDL",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=336863,no,no,1487371915.850076
Testing decision systems with classification components,"Many decision tools and complex decision systems require components that use learning technology to improve the quality of the decisions, based on observations (such as sensor data). In order to employ these tools and systems in high- or medium-risk applications, the design, implementation, and deployment process needs to follow principled verification, validation, and testing procedures that assure a reliable operation. This task is far from being trivial because of the very nature of learning - a technology that provides tools for making decisions under uncertainty. Only little research efforts have been dedicated so far to validating and testing learning-based systems. This paper describes a novel tool for the testing and the validation of learning systems and a set of statistical tests that are employed by this tool for the assessment of learned classification decisions. We also describe some aspects of the underlying theoretical and experimental framework for the validation and testing of systems that learn.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1556390,no,no,1487371915.850074
"High performance Chinese OCR based on Gabor features, discriminative feature extraction and model training","We have developed a Chinese OCR engine for machine printed documents. Currently, our OCR engine can support a vocabulary of 6921 characters which include 6707 simplified Chinese characters in GB2312-80, 12 frequently used GBK Chinese characters, 62 alphanumeric characters, 140 punctuation marks and symbols. The supported font styles include Song, Fang Song, Kat, He, Yuan, LiShu, WeiBei, XingKai, etc. The averaged character recognition accuracy is above 99% for newspaper quality documents with a recognition speed of about 250 characters per second on a Pentium III-450 MHz PC yet only consuming less than 2 MB memory. We describe the key technologies we used to construct the above recognizer. Among them, we highlight three key techniques contributing to the high recognition accuracy, namely the use of Gabor features, the use of discriminative feature extraction, and the use of minimum classification error as a criterion for model training",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=941220,no,no,1487371915.846903
Software reliability models with time-dependent hazard function based on Bayesian approach,"In this paper, two models predicting mean time until next failure based on Bayesian approach are presented. Times between failures follow Weibull distributions with stochastically decreasing ordering on the hazard functions of successive failure time intervals, reflecting the tester's intent to improve the software quality with each corrective action. We apply the proposed models to actual software failure data and show they give better results under sum of square errors criteria as compared to previous Bayesian models and other existing times between failures models. Finally, we utilize likelihood ratios criterion to compare new model's predictive performance",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=823478,no,no,1487371915.846902
Building survivable services using redundancy and adaptation,"Survivable systems-that is, systems that can continue to provide service despite failures, intrusions, and other threats-are increasingly needed in a wide variety of civilian and military application areas. As a step toward realizing such systems, this paper advocates the use of redundancy and adaptation to build survivable services that can provide core functionality for implementing survivability in networked environments. An approach to building such services using these techniques is described and a concrete example involving a survivable communication service is given. This service is based on Cactus, a system for building highly configurable network protocols that offers the flexibility needed to easily add redundant and adaptive components. Initial performance results for a prototype implementation of the communication service built using Cactus/C2.1 running on Linux are also given.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1176985,no,no,1487371915.846901
Mutation of Java objects,"Fault insertion based techniques have been used for measuring test adequacy and testability of programs. Mutation analysis inserts faults into a program with the goal of creating mutation-adequate test sets that distinguish the mutant from the original program. Software testability is measured by calculating the probability that a program will fail on the next test input coming from a predefined input distribution, given that the software includes a fault. Inserted faults must represent plausible errors. It is relatively easy to apply standard transformations to mutate scalar values such as integers, floats, and character data, because their semantics are well understood. Mutating objects that are instances of user defined types is more difficult. There is no obvious way to modify such objects in a manner consistent with realistic faults, without writing custom mutation methods for each object class. We propose a new object mutation approach along with a set of mutation operators and support tools for inserting faults into objects that instantiate items from common Java libraries heavily used in commercial software as well as user defined classes. Preliminary evaluation of our technique shows that it should be effective for evaluating real-world software testing suites.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173285,no,no,1487371915.8469
Parametric optimization of measuring systems according to the joint error criterion,"The paper presents a method and some results of the modelling and design of measuring systems. The minimum of designed system errors is achieved using parametric optimization methods of the measuring system model. A â€œstructural methodâ€?of measuring system modelling is used. The used equipment properties, as well as data processing algorithms are taken into account",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=507370,no,no,1487371915.846899
Can a software quality model hit a moving target?,This paper examines factors that make accurate quality modeling of an evolving software system challenging. The context of our discussion is development of a sequence of software releases. Our goal is to predict software quality of a release early enough co make significant quality improvements prior to release,1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=738491,no,no,1487371915.846898
A formal mechanism for assessing polymorphism in object-oriented systems,"Although quality is not easy to evaluate since it is a complex concept compound by different aspects, several properties that make a good object-oriented design have been recognized and widely accepted by the software engineering community. We agree that both the traditional and the new object-oriented properties should be analyzed in assessing the quality of object-oriented design. However, we believe that it is necessary to pay special attention to the polymorphism concept and metric, since they should be considered one of the key concerns in determining the quality of an object-oriented system. In this paper, we have given a rigorous definition of polymorphism. On top of this formalization we propose a metric that provides an objective and precise mechanism to detect and quantify dynamic polymorphism. The metric takes information coming from the first stages of the development process giving developers the opportunity to early evaluate and improve the quality of the software product. Finally, a first approach to the theoretical validation of the metric is presented",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=883778,no,no,1487371915.846896
A multiobjective module-order model for software quality enhancement,"The knowledge, prior to system operations, of which program modules are problematic is valuable to a software quality assurance team, especially when there is a constraint on software quality enhancement resources. A cost-effective approach for allocating such resources is to obtain a prediction in the form of a quality-based ranking of program modules. Subsequently, a module-order model (MOM) is used to gauge the performance of the predicted rankings. From a practical software engineering point of view, multiple software quality objectives may be desired by a MOM for the system under consideration: e.g., the desired rankings may be such that 100% of the faults should be detected if the top 50% of modules with highest number of faults are subjected to quality improvements. Moreover, the management team for the same system may also desire that 80% of the faults should be accounted if the top 20% of the modules are targeted for improvement. Existing work related to MOM(s) use a quantitative prediction model to obtain the predicted rankings of program modules, implying that only the fault prediction error measures such as the average, relative, or mean square errors are minimized. Such an approach does not provide a direct insight into the performance behavior of a MOM. For a given percentage of modules enhanced, the performance of a MOM is gauged by how many faults are accounted for by the predicted ranking as compared with the perfect ranking. We propose an approach for calibrating a multiobjective MOM using genetic programming. Other estimation techniques, e.g., multiple linear regression and neural networks cannot achieve multiobjective optimization for MOM(s). The proposed methodology facilitates the simultaneous optimization of multiple performance objectives for a MOM. Case studies of two industrial software systems are presented, the empirical results of which demonstrate a new promise for goal-oriented software quality modeling.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1369249,no,no,1487371915.846895
Effective values: an approach for characterizing dependability parameters,"The authors discuss a novel direction for evaluating system dependability. The research efforts outlined are part of a special focus concerning the dependability of complex distributed systems at the Department of Computer Engineering of the Federal Armed Forces University in Munich, Germany. Our approach emphasizes the behavior of components after they have been embedded in a system. We use the notion of effective values for the analysis and estimation of key dependability parameters. The concept is targeted to unify performance characterization with the dependability. Besides analysis, we report on some simulation results, which were in favor of our assumptions.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=922418,no,no,1487371915.846894
Understanding the sources of software defects: a filtering approach,"The paper presents a method proposal of how to use product measures and defect data to enable understanding and identification of design and programming constructs that contribute more than expected to the defect statistics. The paper describes a method that can be used to identify the most defect-prone design and programming constructs and the method proposal is illustrated on data collected from a large software project in the telecommunication domain. The example indicates that it is feasible, based on defect data and product measures, to identify the main sources of defects in terms of design and programming constructs. Potential actions to be taken include less usage of particular design and programming constructs, additional resources for verification of the constructs and further education into how to use the constructs",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=852475,no,no,1487371915.846893
Predictability measures for software reliability models,"A two-component predictability measure is presented that characterizes the long-term predictability of a software reliability growth model. The first component, average predictability, measures how well a model predicts throughout the testing phase. The second component, average bias, is a measure of the general tendency to overestimate or underestimate the number of faults. Data sets for both large and small projects from diverse sources have been analyzed. The results seem to support the observation that the logarithmic model appears to have good predictability is most cases. However, at very low fault densities, the exponential model may be slightly better. The delayed S-shaped model which in some cases has been shown to have good fit, generally performed poorly",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=139306,no,no,1487371915.843786
Disk space guarantees as a distributed resource management problem: A case study,"In the single system UNIX, successful completion of a write system call implies a guarantee of adequate disk space for any new pages created by the system call. To support such a guarantee in a distributed file system designers need to solve the problems of accurately estimating the space needed, communication overhead, and fault tolerance. In the Calypso file system, which is a cluster-optimized, distributed UNIX file system, we solve these problems using an advance-reservation scheme. Measurements show that the overhead of this scheme for typical UNIX usage patterns is 1% to 3%",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=530697,no,no,1487371915.843785
Reducing test application time through interleaved scan,This paper proposes a new method for reducing the test length for digital circuits by adopting an architecture derived from the popular scan approach. An evolutionary optimization algorithm is exploited to find the optimal solution. The proposed approach was tested on the ISCAS89 standard benchmarks and the experimental results show its effectiveness.,2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1137642,no,no,1487371915.843784
A high integrity monitor for signalling interference prevention at 50 Hz,"The design of a high integrity monitor for the prevention of signalling interference is described. It belongs to the family known as Interference Current Monitor Units, or ICMUs, and has to be extremely reliable since a fault would otherwise compromise railway safety; a calculated value of Mean Time Between Wrong Side Failure (MTBWSF) of five thousand million hours was specified and achieved. High levels of out of band currents had to be rejected, in particular up to 2 kA DC The basic design was kept as simple as possible with only one frequency band being detected. A hardware only solution, i.e. no software, was adopted and triplication of the circuit was used to achieve the MTBWSF specification",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=396040,no,no,1487371915.843783
Optimal state prediction for feedback-based QoS adaptations,"In heterogeneous network environments with performance variations present, complex distributed applications, such as distributed visual tracking applications, are desired to adapt themselves and to adjust their resource demands dynamically, in response to fluctuations in either end system or network resources. By such adaptations, they are able to preserve the user-perceptible critical QoS parameters, and trade off non-critical ones. However, correct decisions on adaptation timing and scale, such as determining data rate transmitted from the server to clients in an application, depend on accurate observations of system states, such as quantities of data in transit or arrived at the destination. Significant end-to-end delay may obstruct the desired accurate observation. We present an optimal state prediction approach to estimate current states based on available state observations. Once accurate predictions are made, the applications can be adjusted dynamically based on a control-theoretical model. Finally, we show the effectiveness of our approach with experimental results in a client-server based visual tracking application, where application control and state estimations are accomplished by middleware components",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=766476,no,no,1487371915.843782
Assessing Fault Sensitivity in MPI Applications,"Today, clusters built from commodity PCs dominate high-performance computing, with systems containing thousands of processors now being deployed. As node counts for multi-teraflop systems grow to thousands and with proposed petaflop system likely to contain tens of thousands of nodes, the standard assumption that system hardware and software are fully reliable becomes much less credible. Concomitantly, understanding application sensitivity to system failures is critical to establishing confidence in the outputs of large-scale applications. Using software fault injection, we simulated single bit memory errors, register file upsets and MPI message payload corruption and measured the behavioral responses for a suite of MPI applications. These experiments showed that most applications are very sensitive to even single errors. Perhaps most worrisome, the errors were often undetected, yielding erroneous output with no user indicators. Encouragingly, even minimal internal application error checking and program assertions can detect some of the faults we injected.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1392967,no,no,1487371915.84378
Object-oriented information extraction for the monitoring of sensitive aquatic environments,"According to the new European water framework directive, water management plans will become mandatory for all inland water bodies. The development of management plans requires information about the environmental status of the water bodies and their surroundings. Wetland vegetation is a good indicator for water quality monitoring. Remote sensing methods are assumed to be able to provide the specialist with the information needed for the monitoring of wetland vegetation. Applying methods offered by the object oriented eCognition software, an evaluation chain for sensitive aquatic environments is under development which should be used for monitoring purposes. This paper describes two approaches for wetland classification and the evaluation of reed stands. The first one is deriving the information exclusively from Ikonos data analysis, simulating the case of a basic inventory. The second one is taking advantage of existing thematic GIS layers, a situation which often appears in monitoring purposes.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1026876,no,no,1487371915.843779
"A Petri net based method for resource estimation: an approach considering data-dependency, causal and temporal precedences",This work presents a combined reachability-structural methodology for computing the number of functional units in hardware/software co-design context considering timing constraints. The proposed method extends some previous works in the sense that data-dependency has been captured and considered in the functional unit estimation methodology. The proposed hardware/software co-design framework uses the Petri net as common formalism for performing quantitative and qualitative analysis,2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=953007,no,no,1487371915.843778
Applying independent verification and validation to the automatic test equipment life cycle,"Automatic Test Equipment (ATE) is becoming increasingly more sophisticated and complex, with an even greater dependency on software. With the onset of Versa Modular Eurocard (VME) Extensions for Instrumentation (VXI) technology into ATE, which is supposed to bring about the promise of interchangeable instrument components. ATE customers are forced to contend with system integration and software issues. One way the ATE customer can combat these problems is to have a separate Independent Verification and Validation (IV&V) organization employ rigorous methodologies to evaluate the correctness and quality of the ATE product throughout its life cycle. IV&V is a systems engineering process where verification determines if the ATE meets its specifications, and validation determines if the ATE performs to the customer's expectations. IV&V has the highest potential payoff of assuring a safe and reliable system if it is initiated at the beginning of the acquisition life cycle and continued throughout the acceptance of the system. It is illustrated that IV&V effects are more pronounced when IV&V activities begin early in the software development life cycle, but later application of IV&V is still deemed to have a significant impact. IV&V is an effective technique to reducing costs, schedule, and performance risks on the development of complex ATE, and a â€œtoolâ€?to efficiently and effectively manage ATE development risks. The IV&V organization has the ability to perform a focused and systematic technical evaluation of hardware and software processes and products. When performed in parallel with the ATE development life cycle, IV&V provides for early detection",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=690866,no,no,1487371915.843777
Self-calibration of metrics of Java methods,"Self-calibration is a new technique for the study of internal product metrics, sometimes called â€œobservationsâ€?and calibrating these against their frequency, or probability of occurring in common programming practice (CPP). Data gathering and analysis of the distribution of observations is an important prerequisite for predicting external qualities, and in particular software complexity. The main virtue of our technique is that it eliminates the use of absolute values in decision-making, and allows gauging local values in comparison with a scale computed from a standard and global database. Method profiles are introduced as a visual means to compare individual projects or categories of methods against the CPP. Although the techniques are general and could in principle be applied to traditional programming languages, the focus of the paper is on object oriented languages using Java. The techniques are employed in a suite of 17 metrics in a body of circa thirty thousand Java methods",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=891361,no,no,1487371915.843775
Wavelet based rate scalable video compression,"In this paper, we present a new wavelet based rate scalable video compression algorithm. We will refer to this new technique as the scalable adaptive motion compensated wavelet (SAMCoW) algorithm. SAMCoW uses motion compensation to reduce temporal redundancy. The prediction error frames and the intracoded frames are encoded using an approach similar to the embedded zerotree wavelet (EZW) coder. An adaptive motion compensation (AMC) scheme is described to address error propagation problems. We show that, using our AMC scheme, the quality of the decoded video can be maintained at various data rates. We also describe an EZW approach that exploits the interdependency between color components in the luminance/chrominance color space. We show that, in addition to providing a wide range of rate scalability, our encoder achieves comparable performance to the more traditional hybrid video coders, such as MPEG1 and H.263. Furthermore, our coding scheme allows the data rate to be dynamically changed during decoding, which is very appealing for network-oriented applications",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=744279,no,undetermined,0
Measurements for managing software maintenance,"Software maintenance is central to the mission of many organizations. Thus, it is natural for managers to characterize and measure those aspects of products and processes that seem to affect the cost, schedule, quality and functionality of software maintenance delivery. This paper answers basic questions about software maintenance for a single organization and discusses some of the decisions made based on the answers. Attributes of both the software maintenance process and the resulting product were measured to direct management and engineering attention toward improvement areas, track the improvement over time, and help make choices among alternatives",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=565000,no,undetermined,0
Alternate path reasoning in intelligent instrument fault diagnosis for gas chromatography,"Intelligent instrument fault diagnosis is addressed using expert networks, a hybrid technique which blends traditional rule-based expert systems with neural network style training. One of the most difficult aspects of instrument fault diagnosis is developing an appropriate rule base for the expert network. Beginning with an initial set of rules given by experts, a more accurate representation of the reasoning process can be found using example data. A methodology for determining alternate paths of reasoning and incorporating them into the expert network is presented. Our technique presupposes interaction and cooperation with the expert, and is intended to be used with the assistance of the expert to incorporate knowledge discovered from the data into the intelligent diagnosis tool. Tests of this methodology are conducted within the problem domain of fault diagnosis for gas chromatography. Performance statistics indicate the efficacy of automating the introduction of alternate path reasoning into the diagnostic reasoning system",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=565056,no,undetermined,0
Fault exposure ratio estimation and applications,"One of the most important parameters that control reliability growth is the fault exposure ratio (FER) identified by J.D. Musa et al. (1991). It represents the average detectability of the faults in software. Other parameters that control reliability growth are software size and execution speed of the processor which are both easily evaluated. The fault exposure ratio thus presents a key challenge in our quest towards understanding the software testing process and characterizing it analytically. It has been suggested that the fault exposure ratio may depend on the program structure, however the structuredness as measured by decision density may average out and may not vary with program size. In addition FER should be independent of program size. The available data sets suggest that FER varies as testing progresses. This has been attributed partly to the non-randomness of testing. We relate defect density to FER and present a model that can be used to estimate FER. Implications of the model are discussed. This model has three applications. First, it offers the possibility of estimating parameters of reliability growth models even before testing begins. Secondly, it can assist in stabilizing projections during the early phases of testing when the failure intensity may have large short term swings. Finally, since it allows analytical characterization of the testing process, it can be used in expressions describing processes like software test coverage growth",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=558897,no,undetermined,0
Safety as a metric,"Most software metrics measure the syntactic qualities of a program. While measuring such properties may reveal problems in programs, these metrics fail to measure the essence of programs: (partial) correctness and robustness. We therefore propose to base metrics on semantic, instead of syntactic, criteria. To illustrate the idea of semantics-based metrics, we have built static debuggers, which are tools that detect potential run-time failures. More specifically, a static debugger analyses programs written in safe programming languages and pinpoints those program operations that might trigger a run-time error. This paper briefly recalls what safety means for a programming language. It then sketches how a static debugger works and the role it plays in measuring the robustness of a program. The last section discusses the use of static debuggers in the classroom, an NSF Educational Innovation Project",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=755192,no,undetermined,0
Using antenna patterns to improve the quality of SeaSonde HF radar surface current maps,"The SeaSonde coastal HF current mapping radar realizes its very small and convenient antenna size by employing the MUSIC direction finding (DF) algorithm, rather than beam forming to determine the bearing angle to each point on the sea plot. Beam forming requires large phased array antennas spanning up to 100 m of linear coastal extent. The SeaSonde uses two colocated crossed loops and an omnidirectional monopole as the receive antenna system. Unique features of DF polar maps can be angle sectors with sparse coverage or gaps. This occurs when the antenna patterns are distorted by nearby terrain or buildings, a frequent effect observed with all HF radars. In addition, uncorrected distortions can produce angle biases (misplacements) of the radial current vectors, sometimes as much as 10Â°. The actual antenna patterns (with distortions) are now measured with a small battery operated transponder, either from land in front of the antenna or from a boat. These are then inserted into the software to calibrate, i.e., correct for the distortions. A number of algorithms have been evaluated for this correction process, both using simulations (with known input) as well as actual measured sea echo. The authors present the latest findings on gap and bias mitigation, which show that nearly all of these deleterious effects can be reduced to acceptable levels",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=755204,no,undetermined,0
A new study for fault-tolerant real-time dynamic scheduling algorithms,"Many time-critical applications require predictable performance. Tasks corresponding to these applications have deadlines to be met despite the presence of faults. Failures can happen either due to processor faults or due to task errors. To tolerate both processor and task failures, the copies of every task have to be mutually excluded in space and also in time in the schedule. We assume, each task has two versions, namely, primary copy and backup copy. We believe that the position of the backup copy in the task queue with respect to the position of the primary copy (distance) is a crucial parameter which affects the performance of any fault-tolerant dynamic scheduling algorithm. To study the effect of distance parameter, we make fault-tolerant extensions to the well-known myopic scheduling algorithm which is a dynamic scheduling algorithm capable of handling resource constraints among tasks. We have conducted an extensive simulation to study the effect of distance parameter on the schedulability of fault tolerant myopic scheduling algorithm",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=565837,no,undetermined,0
Developing a design complexity measure,"The cost to develop and maintain software is increasing at a rapid rate. The majority of the total cost to develop software is spent in the post-deployment-maintenance phase of the software life-cycle. In order to reduce life-cycle costs, more effort needs to be spent in earlier phases of the software life-cycle. One characteristic that merits investigation is the complexity of a software design. Project performance metrics (i.e. effort, schedule, defect density, etc.) are driven by software complexity, and affect project costs. The Software Design Complexity Measure examines an organization's historical project performance metrics along with the complexity of a project's software design to estimate future project performance metrics. These estimates indicate costs that the evaluated software design will incur in the future. Equipped with future cost estimates, a project manager will be able to make more informed decisions concerning the future of the project",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=559130,no,undetermined,0
Data partition based reliability modeling,The paper presents an approach to software reliability modeling using data partitions derived from tree based models. We use these data sensitive partitions to group data into clusters with similar failure intensities. The series of data clusters associated with different time segments forms a piecewise linear model for the assessment and short term prediction of reliability. Long term prediction can be provided by the dual model that uses these grouped data as input fitted to some failure count variations of the traditional software reliability growth models. These partition based reliability models can be used effectively to measure and predict the reliability of software systems and can be readily integrated into our strategy of reliability assessment and improvement using tree based modeling,1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=558895,no,undetermined,0
Warehouse creation-a potential roadblock to data warehousing,"Data warehousing is gaining in popularity as organizations realize the benefits of being able to perform sophisticated analyses of their data. Recent years have seen the introduction of a number of data-warehousing engines, from both established database vendors as well as new players. The engines themselves are relatively easy to use and come with a good set of end-user tools. However, there is one key stumbling block to the rapid development of data warehouses, namely that of warehouse population. Specifically, problems arise in populating a warehouse with existing data since it has various types of heterogeneity. Given the lack of good tools, this task has generally been performed by various system integrators, e.g., software consulting organizations which have developed in-house tools and processes for the task. The general conclusion is that the task has proven to be labor-intensive, error-prone, and generally frustrating, leading a number of warehousing projects to be abandoned mid-way through development. However, the picture is not as grim as it appears. The problems that are being encountered in warehouse creation are very similar to those encountered in data integration, and they have been studied for about two decades. However, not all problems relevant to warehouse creation have been solved, and a number of research issues remain. The principal goal of this paper is to identify the common issues in data integration and data-warehouse creation",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=755620,no,undetermined,0
Using codewords to protect database data from a class of software errors,"Increasingly, for extensibility and performance, special-purpose application code is being integrated with database system code. Such application code has direct access to database system buffers and, as a result, the danger of data being corrupted due to inadvertent application writes is increased. Previously proposed hardware techniques to protect data from corruption required system calls, and their performance depended on the details of the hardware architecture. We investigate an alternative approach which uses codewords associated with regions of data to detect corruption and to prevent corrupted data from being used by subsequent transactions. We develop several such techniques which vary in the level of protection, space overhead, performance and impact on concurrency. These techniques are implemented in the DaliÂ´ main-memory storage manager, and the performance impact of each on normal processing is evaluated. Novel techniques are developed to recover when a transaction has read corrupted data caused by a bad write, and then gone on to write other data in the database. These techniques use limited and relatively low-cost logging of transaction reads to trace the corruption, and may also prove useful when resolving problems caused by incorrect data entry and other logical errors",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=754943,no,undetermined,0
A framework backbone for software fault tolerance in embedded parallel applications,"The DIR net (detection-isolation-recovery net) is the main module of a software framework for the development of embedded supercomputing applications. This framework provides a set of functional elements, collected in a library, to improve the dependability attributes of the applications (especially the availability). The DIR net enables these functional elements to cooperate and enhances their efficiency by controlling and co-ordinating them. As a supervisor and the main executor of the fault tolerance strategy, it is the backbone of the framework, of which the application developer is the architect. Moreover, it provides an interface to which all detection and recovery tools should conform. Although the DIR net is meant to be used together within this fault tolerance framework, the adopted concepts and design decisions have a more general value, and can be applied in a wide range of parallel systems",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=746666,no,undetermined,0
Directional dilation for the connection of piece-wise objects: a semiconductor manufacturing case study,"A technique for intelligent dilation of objects in an image was developed that, when compared to standard isotropic techniques, can create a better representative resultant image for subsequent morphological analysis. Individual masses in an image are forced to grow in preferential directions with varying degrees of strength according to a set of morphological parameters. These parameters are determined by applying a model based on gravitational force to determine the strength and direction of the â€œattraction forceâ€?between objects. The result of this analysis is a collection of normalized dilation vectors for each object in the image, where each vector represents a potential direction for growth. Results of the application of this algorithm are shown on binary images of defect distributions on semiconductor wafers where a single defect (e.g., a scratch) can be made up of numerous, disconnected blobs",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=560356,no,undetermined,0
Visual coherence and usability: a cohesion metric for assessing the quality of dialogue and screen designs,"Interface design metrics help developers evaluate user interface quality from designs and visual prototypes before implementing working prototypes or systems. Visual coherence, based on the established software engineering concept of cohesion, measures the fit between the layout of user interface features and their semantic content. Visually coherent interfaces group semantically more closely related features together, enhancing comprehension and ease of use. Preliminary research using a scenario-based technique with built in validity checks found professional developers preferred more visually coherent designs and rated them easier to use, even when these departed from familiar dialogue conventions. Implications for design and further research are discussed",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=559996,no,undetermined,0
Development of GIS fault location system using pressure wave sensors,This paper describes a series of development tests carried out when developing a GIS (gas insulated substation) fault location system using pressure wave sensors for a GIS with a main bus in which each gas section per bay is connected by gas piping. This paper also refers to the basic construction of the software of the fault location system actually applied to a GIS with a rated voltage of less than 168 kV,1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=754076,no,undetermined,0
Measuring long-range dependence under changing traffic conditions,"Previous measurements of various types of network traffic have shown evidence consistent with long-range dependence and self-similarity. However, an alternative explanation for these measurements is non-stationarity. Standard estimators of LRD parameters such as the Hurst parameter H assume stationarity and are susceptible to bias when this assumption does not hold. Hence LRD may be indicated by these estimators when none is present, or alternatively LRD taken to be non-stationarity. The Abry-Veitch (see IEEE Trans. on on Info. Theory, vol.44, no.1, p.2-15, 1998) joint estimator has much better properties when a time-series is non-stationary. In particular the effect of polynomial trends in data may be intrinsically eliminated from the estimates of LRD parameters. This paper investigates the behavior of the AV estimator when there are non-stationarities in the form of a level shift in the mean and/or the variance of a process. We examine cases where the change occurs both gradually or as a single jump discontinuity, and also examine the effect of the size of the shift. In particular we show that although a jump discontinuity may cause bins in the estimates of the H, the bias is negligible except when the jump is sharp, and large compared with the standard deviation of the process. We explain these effects and suggest how any introduced errors might be minimized. We define a broad class of non-stationary LRD processes so that LRD remains well defined under time varying mean and variance. The results are tested by applying the estimator to a real data set which contains a clear non-stationary event falling within this class",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=752173,no,undetermined,0
Bandwidth modelling for network-aware applications,"Network-aware applications attempt to adjust their resource demands in response to changes in resource availability, e.g., if a server maintains a connection to a client, the server may want to adjust the amount of data sent to the client based on the effective bandwidth realized for the connection. Information about current and future network performance is therefore crucial for an adaptive application. This paper discusses three aspects of the coupling of applications and networks: (1) a network-aware application needs timely information about the status of the network; (2) a simple bandwidth estimation technique per forms reasonably well for TCP-Reno connections without timeouts; (3) enhancements proposed to TCP-Reno to reduce the number of timeouts (i.e., SACKs and its variants) increase the bandwidth but also improve the accuracy of bandwidth estimators developed by other researchers. The empirical observations reported in this paper are based on an in-vivo experiment in the Internet. Over a 6-month period, we logged the micro dynamics of random connections between a set of selected hosts. These results are encouraging for the developer of a network-aware application since they provide evidence that a simple widening of the interface between applications and network (protocol) may provide the information that allows an application to successfully adapt to changes in resource availability",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=752148,no,undetermined,0
Relating knock-on viscosity to software modifiability,"The notion of â€œcognitive dimensionsâ€?developed by Green provides an analytic framework for assessing usability for a variety of information artifacts. The work here describes formal interpretation of dimensions in order to precisely assess the suitability of interactive systems for particular tasks. The particular dimension considered is viscosity-this concerns the ease with which information structures can be modified and updated within a given environment. A formal interpretation of such a dimension has the benefit of yielding practical measures and guidelines for assessment. This extends a growing body of work concerned with formally characterising interactive properties that are significant to successful use. The context in which we demonstrate our interpretation of dimensions is that of program modification, where a program represents an information structure to be updated. The framework developed provides an interpretation of empirical evidence regarding software quality and modifiability",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=560014,no,undetermined,0
A test of precision GPS clock synchronization,"This paper describes tests of precision GPS time transfer using geodetic-quality TurboRogue receivers. The GPS data are processed with the GIPSY-OASIS II software, which simultaneously estimates the GPS satellite orbits and clocks, receiver locations and clock offsets, as well as other parameters such as Earth orientation. This GPS solution technique, which emphasizes high accuracy GPS orbit determination and observable modeling, has been shown to enable sub-1 ns time transfer at global distance scales. GPS-based monitoring of clock performance has been carried out for several years through JPL's high precision GPS global network processing. The paper discusses measurements of variations in relative clock offsets down to a level of a few tens of picoseconds. GPS-based clock frequency measurements are also presented",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=560314,no,undetermined,0
A transparent light-weight group service,"The virtual synchrony model for group communication has proven to be a powerful paradigm for building distributed applications. Implementations of virtual synchrony usually require the use of failure detectors and failure recovery protocols. In applications that require the use of a large number of groups, significant performance gains can be attained if these groups share the resources required to provide virtual synchrony. A service that maps user groups onto instances of a virtually synchronous implementation is called a light-weight group service. This paper proposes a new design for the light-weight group protocols that enables the usage of this service in a transparent manner as a test case, the new design was implemented in the Horus system, although the underlying principles can be applied to other architectures as well. The paper also presents performance results from this implementation",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=559710,no,undetermined,0
Automatic classification of wafer defects: status and industry needs,"This paper describes the Automatic Defect Classification (ADC) beta site evaluations performed as part of the SEMATECH ADC project. Two optical review microscopes equipped with ADC software were independently evaluated in manufacturing environments. Both microscopes were operated in bright-field mode with white light illumination. ADC performance was measured on three process levels of random logic devices: source/drain, polysilicon gate, and metal. ADC performance metrics included classification accuracy, repeatability, and speed. In particular, ADC software was tested using a protocol that included knowledge base tests, gauge studies, and small passive data collections",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=559702,no,undetermined,0
Multiscale motion estimation for scalable video coding,"Motion estimation is an important component of video coding systems because it enables us to exploit the temporal redundancy in the sequence. The popular block-matching algorithms (BMAs) produce unnatural, piecewise constant motion fields that do not correspond to â€œtrueâ€?motion. In contrast, our focus here is on high-quality motion estimates that produce a video representation that is less dependent on the specific frame-rate or resolution. To this end, we present an iterated registration algorithm that extends previous work on multiscale motion models and gradient-based estimation for coding applications. We obtain improved motion estimates and higher overall coding performance. Promising applications are found in temporally-scalable video coding with motion-compensated frame interpolation at the decoder. We obtain excellent interpolation performance and video quality; in contrast, BMA leads to annoying artifacts near moving image edges",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=559661,no,undetermined,0
Scheduling VBR traffic for achievable QoS in a shared un-buffered environment,"A system of T resources (wireless channels) shared by a number of un-buffered applications is considered. The shared transmission resources are defined to be the slots (packet transmission times) of a TDMA frame (service cycle). Packets which are not allocated a channel within the current service cycle are dropped. The quality of service (QoS) vector is equivalently described in terms of (diverse) packet dropping probabilities (p<sub>i</sub>), or (diverse) packet dropping rates (d<sub>i</sub>). From a precisely defined region of achievable QoS vectors, easy to implement scheduling policies that deliver the achievable QoS vectors are derived. As an example, a policy that delivers a target QoS vector is derived; its performance is verified through simulations",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=562659,no,undetermined,0
The effect of interface complexity on program error density,"The paper explains how to evaluate software maintainability by considering the effects of interfaced complexities between modified and unmodified parts. When software is maintained, designers develop software, considering not only functions of modified parts but also those of unmodified parts by reading specifications or source codes. So, not only does the volume and complexity of the unmodified and modified parts, but also the interfaced complexities between, them greatly affect the occurrence of software errors. The modified part consists of several subsystems, each of which is composed of several functionally related routines. The unmodified part also consists of several routines. A routine corresponds to a function of C-coding. We experimentally show by regression and discriminant analyses that the quality of each routine tends to decrease as the reuse-ratio increases. The optimal threshold of reuse-ratio is selected by applying AIC (Akaihe Information Criterion) procedures to discriminant analysis for software quality classification. We can reasonably separate routines into two parts, one modified in which most software errors occur and the other unmodified in which few errors occur. The interfaced complexities measured by the extended cyclomatic number between each sub system of the modified part and the unmodified part greatly affect the number of errors and error density. By applying regression analysis to medium size software, we have shown that 40% of variance of error density are represented by interfaced complexities among each subsystem of the modified part and the unmodified part",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=564991,no,undetermined,0
TRACKER: a sensor fusion simulator for generalised tracking,"This paper presents a multisensor, multitarget tracking simulator TRACKER that can be used to demonstrate and evaluate airborne early warning and control surveillance systems. TRACKER is a Level 1 fusion surveillance software package that generates a unified object state vector and its measure of uncertainty for each tracked object based on cluttered multitarget augmented (kinematic and discrete type) measurements from multiple dissimilar sensors. It integrates sensors and other sources of information without requiring synchronism and unifies tracking and identification. The resulting track and identity estimates are of a higher quality than those from algorithm in which tracking and identification are carried out separately. The software package is modular in structure and consists of a tracking and data fusion component written in MATLAB, and a graphical user interface written in Java. The package is equipped with a scenario generator and a set of post-mission analysis tools",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=754184,no,undetermined,0
The design and development of a distributed scheduler agent,"The main objective of the distributed scheduler agent is to provide task placement advice either to parallel and distributed applications directly, or to a distributed scheduler which will despatch normal applications transparently to the advised hosts. To accomplish this, the distributed scheduler agent needs to know the global load situation across all machines and be able to pick the host which best suits the specific resource requirements of individual jobs. Issues concerning the collecting and distribution of load information throughout the system are discussed. This load information is then fed to a ranking algorithm which uses a 3-dimensional load space to generate the most suitable host based on weights which indicate the relative importance of resources to a task. Performance tests are carried out to determine the response times and overhead of the distributed scheduler agent. An application, a distributed ray tracer, is also customised to make use of the distributed scheduler agent and the results presented",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=562864,no,undetermined,0
Locating more corruptions in a replicated file,"When a data file is replicated at more than one site, we are interested in detecting corruption by comparing the multiple copies. In order to reduce the amount of messaging for large files, techniques based on page signatures and combined signatures have been explored. However, for 3 or more sites, the known methods assume that the number of corrupted page copies to be at most [M/2]-1, where M is the number of sites. We point out that this assumption is unrealistic and the corresponding methods are unnecessarily pessimistic. In this paper, we replace this assumption by another assumption which we show to be reasonable. Based on this assumption, we derived a distributed algorithm which in general achieves better performance than previously known results. Our system model is also more refined than previous work",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=559716,no,undetermined,0
A two-stage objective model for video quality evaluation,This paper describes a new methodology that might be useful to develop a model for comparing the present operational readiness of a video system with the same system's past performance. It provides the results of a study conducted to determine the feasibility of developing a video performance model using the ANSI committee TlA1.5 measures to allow the monitoring of video service performance across time. Based on the results of this study it is concluded that the performance measures can be used to develop an objective model of video service performance,1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=559643,no,undetermined,0
A toolset for assisted formal verification,"There has been a growing interest in applying formal methods for functional and performance verification of complex and safety critical designs. Model checking is one of the most common formal verification methodologies utilized in verifying sequential logic due to its automated decision procedures and its ability to provide â€œcounter examplesâ€?for debugging. However, model checking hasn't found broad acceptance as a verification methodology due to its complexity. This arises because of the need to specify correctness properties in a temporal logic language and develop an environment around a partitioned model under test in a non deterministic HDL-type language. Generally, engineers are not trained in mathematical logic languages and becoming proficient in such a language requires a steep learning curve. Furthermore, defining a behavioral environment at the complex and undocumented microarchitectural interface level is a time consuming and error prone activity. As such, there is a strong motivation to bring the model checking technology to a level such that the designers may utilize this technology as a part of their design process without being burdened with the details that are generally only within the grasps of computer theoreticians. The paper outlines two tools which greatly assist in this goal: the first, Polly, automates the difficult and error prone task of developing the behavioral environment around the partitioned model under test; the second Oracle, obviates the need for learning temporal logic to enter specification",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=749477,no,undetermined,0
A description of the software element of the NASA EME flight tests,"In support of NASA's Fly-By-Light/Power-By-Wire (FBL/PBW) program, a series of flight tests were conducted by NASA Langley Research Center in February, 1995. The NASA Boeing 757 was flown past known RF transmitters to measure both external and internal radiated fields. The aircraft was instrumented with strategically located sensors for acquiring data on shielding effectiveness and internal coupling. The data are intended to support computational and statistical modeling codes used to predict internal field levels of an electromagnetic environment (EME) on aircraft. The software was an integral part of the flight tests, as well as the data reduction process. The software, which provided flight test instrument control, data acquisition, and a user interface, executes on a Hewlett Packard (HP) 300 series workstation and uses HP VEEtest development software and the C programming language. Software tools were developed for data processing and analysis, and to provide a database organized by frequency bands, test runs, and sensors. This paper describes the data acquisition system on board the aircraft and concentrates on the software portion. Hardware and software interfaces are illustrated and discussed. Particular attention is given to data acquisition and data format. The data reduction process is discussed in detail to provide insight into the characteristics, quality, and limitations of the data. An analysis of obstacles encountered during the data reduction process is presented",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=559200,no,undetermined,0
"An open solution to fault-tolerant Ethernet: design, prototyping, and evaluation","Presented is an open solution based approach to fault tolerant Ethernet for process control networks. This unique approach provides fault tolerance capability that requires no change of vendor hardware (Ethernet physical link and Network Interface Card) and software (Ethernet driver and protocol), yet it is transparent to control applications. The open fault tolerant Ethernet (OFTE) developed based on this approach performs failure detection and recovery for handling single point of network failure and serves regular IP traffic. Our experimentation shows that OFTE performs efficiently, achieving less than 1 ms end to end LAN swapping time and less than 2 sec failover time, and that concurrent application and system loads have little impact on the performance of failure detection and recovery operations",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=749473,no,undetermined,0
A better ATPG algorithm and its design principles,"The traditional goal of an ATPG algorithm is to achieve a high fault coverage by producing a small number of tests. However, since usually a high fault coverage does not imply a high defect coverage, such an objective can mislead us to overtrust an ATPG method which is optimal for faults but inefficient for defects. The paper presents several new principles to design an ATPG algorithm for solve this problem. The new principles direct an ATPG algorithm to improve its efficiency and reliability for detecting the non target defects through the target faults. Theory and experiments are presented to demonstrate the superiority of the new principles and the new test generation algorithm",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=563564,no,undetermined,0
"Identification, classification and correlation of monitored power quality events","Sensitive customers of electricity have a deep interest in the performance, efficiency, power quality, interchangeability, and safety of electrical operating systems. Standards provide the essential tools that support and complement these interests. They provide economic benefits to the high-tech customers in system design, equipment design, electrical safety, operating performance, energy efficiency, power quality and reliability, and standardisation of products and services. For full utilisation of the guidelines and standards, it is important that the industry use accurate instrumentation to monitor the quality of electricity. This instrumentation must provide `information' that can be analysed using software to arrive at causes and reasons for power quality events. This paper presents an overview of the current power quality standards. The paper describes the characteristics of PQ events and the specifications of the system required to accurately analyse these. Details of the required monitoring software and hardware are presented",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=747495,no,undetermined,0
A comparison of functional and structural partitioning,"Incorporating functional partitioning into a synthesis methodology leads to several important advantages. In functional partitioning, we first partition a functional specification into smaller sub-specifications and then synthesize structure for each, in contrast to the current approach of first synthesizing structure for the entire specification and then partitioning that structure. One advantage is that of greatly improved partitioning among a given set of packages with size and I/O constraints, such as FPGAs or ASIC blocks, resulting in better performance and far fewer required packages. A second advantage is that of greatly reduced synthesis runtimes. We present results of experiments demonstrating these important advantages, leading to the conclusion that further research focus on functional partitioning can lead to improved quality and practicality of synthesis environments",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=565893,no,undetermined,0
ICC '98. 1998 IEEE International Conference on Communications. Conference Record. Affiliated with SUPERCOMM'98 (Cat. No.98CH36220),Presents the front cover of the proceedings.,1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=682525,no,undetermined,0
Availability modeling and validation methodology for RS/6000 systems,"In this paper we show the availability modeling and validation methodology for RS/6000 systems. The availability modeling methodology is used both for prediction of availability characteristics of systems under development, and for the prediction of availability in the special bids process. The core of our methodology is the availability modeling tool-System Availability Estimator (SAVE)-currently being used to predict availability measures. Our availability validation methodology has both a conceptual validation and an empirical validation component. Conceptual model validation is done via model walkthroughs and by talking to the designers. The empirical model validation methodology is performed using tracking data. Concluding, our availability modeling and validation methodology allows us to develop and configure higher availability systems which are critical in the server computer market space",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=744136,no,undetermined,0
A virtual instrument for measurement of flicker,"All types of fluctuations of the rms voltage in power systems may be assessed by using a â€œflickermeterâ€?which complies with the specifications given in IEC-EN 60868. In this paper, a digital flickermeter is presented based on the implementation in the frequency domain of the weighting filter block of the eye-brain simulation chain. The virtual instrument, implemented with an acquisition board inserted into a PC and with a software developed with the LabView tools, samples the voltage and furnishes the instantaneous flicker level and other severity coefficients. Results of simulated and actual data are reported to validate the performance of the developed flickermeter",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=746574,no,undetermined,0
FM-QoS: Real-time Communication using Self-synchronizing Schedules,"FM-QoS employs a novel communication architecture based on network feedback to provide predictable communication performance (e.g. deterministic latencies and guaranteed bandwidths) for high speed cluster interconnects. Network feedback is combined with self-synchronizing communication schedules to achieve synchrony in the network interfaces (NIs). Based on this synchrony, the network can be scheduled to provide predictable performance without special network QoS hardware. We describe the key element of the FM-QoS approach, feedback-based synchronization (FBS), which exploits network feedback to synchronize senders. We use Petri nets to characterize the set of self-synchronizing communication schedules for which FBS is effective and to describe the resulting synchronization overhead as a function of the clock drift across the network nodes. Analytic modeling suggests that for clocks of quality 300 ppm (such as found in the Myrinet NI), a synchronization overhead less than 1% of the total communication traffic is achievable -- significantly better than previous software-based schemes and comparable to hardware-intensive approaches such as virtual circuits (e.g. ATM). We have built a prototype of FBS for Myricom s Myrinet network (a 1.28 Gbps cluster network) which demonstrates the viability of the approach by sharing network resources with predictable performance. The prototype, which implements the local node schedule in software, achieves predictable latencies of 23 Âµs for a single-switch, 8-node network and 2 KB packets. In comparison, the best-effort scheme achieves 104 Âµs for the same network without FBS. While this ratio of over four to one already demonstrates the viability of the approach, it includes nearly 10 Âµs of overhead due to the software implementation. For hardware implementations of local node scheduling, and for networks with cascaded switches, these ratios should be much larger factors.",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1592583,no,undetermined,0
Harmonics in high voltage networks,"The paper presents the results of studies in the area of harmonics in HV networks that were obtained at the Siberian Energy Institute. The studies have been performed based on the method of harmonic distortion powers that was developed at the Siberian Energy Institute. It contains the notions of distortion power generation and absorption The method forms the basis of the software package â€œHarmonicsâ€?that is applied for calculation, analysis and study of the harmonic modes in networks of a complicated configuration. The probabilistic mathematical models of loads are suggested. The probabilistic approach to calculation of harmonic modes in the HV network is considered. High levels of harmonics are typical of the nodes with a lower value of the distortion power absorption. The harmonic voltage levels can be normalized in both operating and planned networks of a complicated configuration in a centralised way",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=759898,no,undetermined,0
The effective use of automated application development tools,"In this paper we report on the results of a four-year study of how automated tools are used in application development (AD). Drawing on data collected from over 100 projects at 22 sites in 15 Fortune 500 companies, we focus on understanding the relationship between using such automated AD tools and various measures of AD performanceâ€”including user satisfaction, labor cost per function point, schedule slippage, and stakeholder-rated effectiveness. Using extensive data from numerous surveys, on-site observations, and field interviews, we found that the direct effects of automated tool use on AD performance were mixed, and that the use of such tools by themselves makes little difference in the results. Further analysis of key intervening factors finds that training, structured methods use, project size, design quality, and focusing on the combined use of AD tools adds a great deal of insight into what contributes to the successful use of automated tools in AD. Despite the many grand predictions of the trade press over the past decade, computer-assisted software engineering (CASE) tools failed to emerge as the promised â€œsilver bullet.â€?The mixed effects of CASE tools use on AD performance that we found, coupled with the complex impact of other key factors such as training, methods, and group interaction, suggest that a cautious approach is appropriate for predicting the impact of similar AD tools (e.g., object-oriented, visual environments, etc.) in the future, and highlight the importance of carefully managing the introduction and use of such tools if they are to be used successfully in the modern enterprise.",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5387179,no,undetermined,0
Thermal transient testing of packages without a tester,"Thermal transient testing is an excellent method to measure the thermal properties of packages, to check die attach quality and to detect and localise physical defects in the heat removal path. The measured transient responses are also the basis for compact, dynamic thermal model generation. The measurement set-up for the thermal transient experiments is rather complex, including expensive dedicated equipment. The purpose of this paper is to present an alternative solution for thermal transient testing which eliminates the need for expensive hardware. A method for compact thermal model generation is also provided",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=756008,no,undetermined,0
Modeling manufacturing dependability,"In this paper, an analytical approach for the availability evaluation of cellular manufacturing systems is presented, where a manufacturing system is considered operational as long as its production capacity requirements are satisfied. The advantage of the approach is that constructing a system level Markov chain (a complex task) is not required. A manufacturing system is decomposed into two subsystems, i.e. machining system and material handling system. The machining subsystem is in turn decomposed into machine cells. For each machine cell and material handling subsystem, a Markovian model is derived and solved to find the probability of a subset of working machines in each cell, and a subset of the operating material handling carriers that satisfies the manufacturing capacity requirements. The overall manufacturing system availability is obtained using a procedure presented in the paper. The novelty of the approach is that it incorporates imperfect coverage and imperfect repair factors in the Markovian models. The approach is used to evaluate transient and steady-state performance of three alternative designs based on an industrial example. Detailed discussion of the results and the impact of imperfect coverage and imperfect repair on the availability of the manufacturing system is presented. Possible extensions of the work and software tools available for model analysis are also discussed",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=563639,no,undetermined,0
Requirement metrics-value added,"Summary form only given. Actions in the requirements phase can directly impact the success or failure of a project. It is critical that project management utilize all available tools to identify potential problems and risks as early in the development as possible, especially in the requirements phase. The Software Assurance Technology Center (GSFC) and the Quality Assurance Division at NASA Goddard Space Flight Center are working on developing and applying metrics from the onset of project development, starting in the requirements phase. This talk will discuss the results of a metrics effort on a real, large system development at GSFC and lessons learned from this experience. The development effort for this project uses an automated tool to manage requirements decomposition. This report focuses on the metrics used to assess the requirement decomposition effort and to identify potential risks. The report discusses how metric analysis in the requirements phase can be accomplished on any government or industry project. The use of an automated tool to manage requirement development facilitation of metrics for improved insight into development and risk assessment will also be expanded",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=566853,no,undetermined,0
An efficient computation-constrained block-based motion estimation algorithm for low bit rate video coding,"We present an efficient computation constrained block-based motion vector estimation algorithm for low bit rate video coding that offers good tradeoffs between motion estimation distortion and number of computations. A reliable predictor determines the search origin. An efficient search pattern exploits structural constraints within the motion field. A flexible cost measure used to terminate the search allows simultaneous control of the motion estimation distortion and the computational cost. Experimental results demonstrate the viability of the proposed algorithm in low bit rate video coding applications, achieving essentially the same levels of rate-distortion performance and subjective quality as that of the full search algorithm when used by the UBC H.263+ video coding reference software. However the proposed motion estimation algorithm provides substantially higher encoding speed as well as graceful computational degradation capabilities.",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=750907,no,undetermined,0
Formal methods for V&V of partial specifications: an experience report,"This paper describes our work exploring the suitability of formal specification methods for independent verification and validation (IV&V) of software specifications for large, safety critical systems. An IV&V contractor often has to perform rapid analysis on incomplete specifications, with no control over how those specifications are represented. Lightweight formal methods show significant promise in this context, as they offer a way of uncovering major errors, without the burden of full proofs of correctness. We describe an experiment in the application of the method SCR to testing for consistency properties of a partial model of the requirements for fault detection isolation and recovery on the space station. We conclude that the insights gained from formalizing a specification is valuable, and it is the process of formalization, rather than the end product that is important. It was only necessary to build enough of the formal model to test the properties in which we were interested. Maintenance of fidelity between multiple representations of the same requirements (as they evolve) is still a problem, and deserves further study",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=566865,no,undetermined,0
Selection of equipment to leverage commercial technology (SELECT),"One of the primary effects of acquisition reform has been to inject uncertainty into the procurement process for both the US Department of Defense (DoD) and its contractors. Of particular concern is the quantification of reliability performance and risk assessment for commercial off-the-shelf (COTS) equipment that is targeted far use in environments more severe than that for which it was originally designed. The mismatch between the design and end-use environment is compounded by several COTS-unique issues, including: (1) a potential shift in predominant COTS equipment failure mechanisms versus their military equivalents in identical applications; (2) the effects of commercial quality practices on inherent COTS reliability;, (3) the ability of commercial packaging to withstand severe environments; and (4) the reliability risk mitigation options which can make the COTS equipment suitable for the application. Acquisition personnel currently lack the tools necessary to translate these figures-of-merit to the more severe environmental requirements inherent in military field operations, or to assess and manage the risks that will allow them to make informed decisions in selecting between competing designs. The SELECT tool will address these issues",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=571669,no,undetermined,0
The case for architecture-specific common cause failure rates and how they affect system performance,"In redundant systems composed of identical units, common cause failures can significantly affect system performance. We show that the probabilities of common cause failure are architecture specific. We derive the probabilities for generalized dual and triplex systems. We analyze and compare three common architectures, i.e., 1oo2, 1oo2D and 2oo3, to determine the effects of common cause failure on system performance as measured by the indices MTTF, MTTFSafe, MTTFDangerous, Probability of Failure on Demand, and availability. We conclude that the 2oo3 provides marginally better availability than the 1oo2D. In all other performance indices, the 1oo2D provides superior performance",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=571693,no,undetermined,0
Software reliability-growth test and the software reliability-testing platform,"Software users are interested in the MTTF value of software. The rules of Î» Pareto diagrams are in general quite different from one software to another. We cannot get an analytical expression to describe the pattern of Î» Pareto diagram and be generally suitable to all software. We have constructed a software reliability testing platform (SRTP) which can automatically produce random input points for software following the rule of the f(P). This platform will check the output. If a failure happens, the software will be examined and the defect will be removed, then the reliability of the software will be increased. If the time interval I of software reliability growth test (SRGT) is not quite long, the failure rates of defects appearing within one interval may have no significant differences. If we assume that these Î»<sub>i</sub>s are mutually equal to each other we find that the later failure data are closely in compliance with the Duane model. Some authors point out that in some cases they achieve an unreasonable result: the total number of defects N estimated is much less than n (the number of defects appeared). Dr. Song proves that, N has at most n roots and there is only one root which lies between n-1 and âˆ?in practical cases. The numerical results of SRGT for a software tested by Dr. Gong and Prof. Zhou are taken as an example. Taking the â€œzero-failure testâ€?as the verification test of software reliability, the zero-failure test time T needed is T=Î¸ <sub>1</sub>/C, where Î¸<sub>1</sub> is the minimum acceptable MTTF value, C=-1/ln(1-Î³), and Î³ is the s-confidence level",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=571700,no,undetermined,0
Photoconductive probing and computer simulation of microwave potentials inside a SiGe MMIC,"Electrical potentials inside a SiGe MMIC are measured at frequencies up to 20 GHz using a micro-machined photoconductive sampling probe and compared with values predicted using microwave CAD software. The results illustrate that this combination of simulation and in-circuit measurement technique is a powerful tool for performing diagnostics of the microwave performance of Si-based RF circuits. The methodology can be used for applications such as fault isolation and validation of device models, as well as for investigation of the sensitivity of performance to process variations.",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=750219,no,undetermined,0
Structural information as a quality metric in software systems organization,"Proposes a metric for expressing the entropy of a software system and for assessing the quality of its organization from the perspective of impact analysis. The metric is called &ldquo;structural information&rdquo; and is based on a model dependency descriptor. The metric is characterized by its independence from the method of building the system and the architectural styles which represent it at the various levels of abstraction. It takes into account both the structure of the components at all levels of abstraction and the structure derived from the links between the different levels of abstraction. The use of this metric makes it possible to monitor changes in the quality levels of the system during its life-cycle and to determine which level of documentation is causing the degradation of the system. This enables adopting timely measures to counter any drop in the quality of a system due to poorly-performed maintenance. In addition, the metric can serve to check that perfective maintenance objectives are attained",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5726939,no,undetermined,0
A recording and analyzing system for cutaneous electrogastrography,"The recording of gastric activities by surface electrodes is called electrogastrography (EGG). It should be carefully filtered and amplified because of its ultra-low frequency, tiny and noisy signal. A new system, including a signal acquisition device and Windows-based software, was designed to record and analyze the EGG signal. Many volunteers and patients have been successfully tested by the system",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=746982,no,undetermined,0
Assessing the benefits of incorporating function clone detection in a development process,"The aim of the experiment presented in this paper is to present an insight into the evaluation of the potential benefits of introducing a function clone detection technology in an industrial software development process. To take advantage of function clone detection, two modifications to the software development process are presented. Our experiment consists of evaluating the impact that these proposed changes would have had on a specific software system if they had been applied over a 3 year period (involving 10000 person-months), where 6 subsequent versions of the software under study were released. The software under study is a large telecommunication system. In total 89 million lines of code have been analyzed. A first result showed that, against our expectations, a significant number of clones are being removed from the system over time. However, this effort is insufficient to prevent the growth of the overall number of clones in the system. In this context the first process change would have added value. We have also found that the second process change would have provided programmers with a significant number of opportunities for correcting problems before customers experienced them. This result shows a potential for improving the software system quality and customer satisfaction",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5726968,no,undetermined,0
An image quality prediction model for optimization of X-ray system performance,"Describes a software modeling system for predicting image quality of a radiological X-ray system. As opposed to simulation, this model bases the prediction on several quasi-separable performance measures, called system quantities (SQs). Examples of SQs are modulation transfer function, contrast gain, and noise distribution. Use of the SQs provides results which are more direct than simulation with significantly less computation. The modeling structure is created to be insensitive to the nature of its components or the methods of subsystem modeling. This system is designed primarily to facilitate component selection for new X-ray systems. It can also be used to predict the impact of component changes on existing systems, including incorporation of new technology, and to assist in resolution of discrepancies between expected and measured performance in existing systems",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=651997,no,undetermined,0
Alpha 21164 testability strategy,A custom DFT strategy solved specific testability and manufacturing issues for this high performance microprocessor. Hardware and software assisted self test and self repair features helped meet aggressive schedule and manufacturing quality and cost goals,1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=573357,no,undetermined,0
Assessment of fault-tolerant computing systems at NASA's Langley Research Center,"In the early 1970's while NASA was studying Advanced Technology Transport concepts, researchers at NASA's Langley Research Center (LaRC) recognized that digital computer systems would be controlling civil transport aircraft in the near future and that the technology did not exist to determine if these digital systems would be reliable enough for this role. In addition, although several existing computer system concepts showed promise to meet the civil transport requirements, none had been realized in an operational system. A multi-initiative program was developed to determine how to assess reliability and performance of fault-tolerant digital computer systems for determining if they could meet the requirements of a civil transport. Subsequent research emphasized the application of formal methods, system safety and digital upset. Some results indicated that dissimilar software may not be reliable enough for critical applications, testing alone will not prove the reliability of highly reliable digital systems and formal methods can find design errors missed by other assessment techniques. Future research will center around the application of formal mathematical methods, insuring software safety, and determination of digital system upsets due to electromagnetic radiation. The long term goal is to define methods for producing error-free systems for flight crucial civil transport applications",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=577999,no,undetermined,0
An efficient dynamic parallel approach to automatic test pattern generation,"Automatic test pattern generation yielding high fault coverage for CMOS circuits has received a wide attention in industry and academic institutions for a long time. Since ATPG is an NP complete problem with complexity exponential to the number of circuit elements, the parallelization of ATPG is an attractive of research. In this paper we describe a parallel sequential ATPG approach which is either run on a standard network of UNIX workstations or, without any changing of the source code, on one of the most powerful high performance parallel computers, the IBM SP2. The test pattern generation is performed in three phases, two for easy-to-detect faults, using fault parallelism with an adaptive limit for the number of backtracks and a third phase for hard-to-detect faults, using search tree parallelism. The main advantage over existing approaches is a dynamic solution for partitioning the fault list and the search tree resulting in a very small overhead for communication without the need of any broadcasts and an optimal load balancing without idle times for the test pattern generators. Experimental results are shown in comparison with existing approaches and are promising with respect to small overhead and utilization of resources",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=580503,no,undetermined,0
Decomposition of inheritance hierarchy DAGs for object-oriented software metrics,"Software metrics are widely used to measure software complexity and assure software quality. However, research in the field of software complexity measurement of a class hierarchy has not yet been carefully studied. The authors introduce a novel factor called unit repeated inheritance (URI) and an important method called the inheritance level technique (ILT) to realize and measure the object-oriented software complexity of a class hierarchy. The approach is based on the graph-theoretical model for measuring the hierarchical complexity in inheritance relations. The proposed metrics extraction shows that inheritance is closely related to the object-oriented software measurement and reveals that overuse of the repeated (multiple) inheritance will increase software complexity and be prone to implicit software errors",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=581870,no,undetermined,0
Why we should use function points [software metrics],"Function point analysis helps developers and users quantify the size and complexity of software application functions in a way that is useful to software users. Are function points a perfect metric? No. Are they a useful metric? In the author's experience, yes. Function points are technologically independent, consistent, repeatable, and help normalize data, enable comparisons, and set project scope and client expectations. The author addresses these issues from the perspective of a practitioner who uses the International Function Point Users Group's Counting Practices Manual, Release 4.0 rules for counting function points. Of course, other function point standards exist, including Mark II and Albrecht's original rules",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=582971,no,undetermined,0
Measuring the performance of a software maintenance department,"All studies indicate that over half of an average data processing user staff is committed to maintaining existing applications. However, as opposed to software development where productivity is measured in terms of lines of code, function-points, data-points or object-points per person month and quality is measured in terms of deficiencies and defect rates per test period, there are no established metrics for measuring the productivity and quality of software maintenance. This means that over half of an organization's software budget cannot be accounted for. The costs occur without being able to measure the benefits obtained. A set of metrics is proposed for helping to remedy this situation by measuring the productivity and quality of the maintenance service",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=583020,no,undetermined,0
Introduction to ProcessModel and ProcessModel 9000,"The paper presents an introduction to ProcessModel simulation software. ProcessModel is a discrete event simulation tool that combines the power of simulation with the simplicity of flowcharting to aid managers and business analysts who are looking for ways to quantify the effects of variability, uncertainty and resource interdependencies on the performance of complex business processes. ProcessModel 9000 is a variation of ProcessModel that allows quality managers to easily document, analyze and improve the quality processes which are typically reviewed during the ISO 9000 or QS-9000 certification process. ProcessModel 9000 includes ProcessModel software and more than 35 pre-defined flowcharts of the most common ISO/QS-9000 processes. Both ProcessModel and ProcessModel 9000 aid in the creation of graphical business process models for the purpose of predicting key process performance measures such as cost, throughput, cycle time and resource utilization",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=744913,no,undetermined,0
A PC-based vibration analyzer for condition monitoring of process machinery,"A fast-response PC-based vibration analyzer has been developed for fault detection and preventive maintenance of process machinery. The analyzer acquires multiple vibration signals with high resolution, and computes frequency spectra, root mean square amplitude, and other peak parameters of interest. Fast execution speed has been achieved by performing data acquisition and frequency spectrum computation using C-language. Vibration signals up to 10 kHz can be analyzed by the spectrum analyzer. Special algorithms, such as window smoothing, digital filtering, data archiving and graphic display have also been incorporated. With these features the vibration analyzer can perform most of the functions available in complex, stand-alone machines. The software for the analyzer is menu driven and user-friendly. The personal computer used is a 66 MHz PC-486 compatible machine. The use of a general purpose PC and standard programming language makes the vibration analyzer simple, economical, and adaptable to a variety of problems. The applications of the system in malfunction detection in rotating machinery are also described",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=744177,no,undetermined,0
Can model-based and case-based expert systems operate together?,"There is an ongoing debate as to whether model-based or case-based diagnostic expert systems are superior. Our experience has shown that the two are not mutually exclusive and, to the contrary, complement each other. Current expert system technology is capable of two reasoning mechanisms, in addition to other mechanisms, integrated into one system. Depending on the knowledge available, and time and cost considerations, expert systems allow the user to decide the relative proportion of case-based to model-based reasoning to employ in any given situation. Diagnostic support software should be evaluated by two critical factor groups: (a) cost and time to deployment, and (b) accuracy, completeness and efficiency of the diagnostic process. In this paper we will discuss the role of expert systems in combining model-based and case-based reasoning to effect the most efficient user defined solution to diagnostic performance",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=743263,no,undetermined,0
Generalization and decision tree induction: efficient classification in data mining,"Efficiency and scalability are fundamental issues concerning data mining in large databases. Although classification has been studied extensively, few of the known methods take serious consideration of efficient induction in large databases and the analysis of data at multiple abstraction levels. The paper addresses the efficiency and scalability issues by proposing a data classification method which integrates attribute oriented induction, relevance analysis, and the induction of decision trees. Such an integration leads to efficient, high quality, multiple level classification of large amounts of data, the relaxation of the requirement of perfect training sets, and the elegant handling of continuous and noisy data",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=583715,no,undetermined,0
Simulation of 8 kbps ACELP over GSM platform,"This study engages the 8 kbps ACELP vocoder on a GSM radio interface software simulation platform. A robust channel codec module is developed to protect the coded bits against fading and GAUSS noise and adapt the ACELP vocoder into the GSM TCH/FS platform. In addition, an error-concealment unit (ECU) is introduced into the ACELP decoder for better synthetic quality over a seriously corrupted channel. The performance of the ACELP vocoder over GSM environment is explored, which include the test of the reconstructed speech quality and the bit-error-rate statistics over typical channel models under different SNR. The final analyses show that this vocoder has better performance than GSM full rate speech coder over a deteriorated transmission environment",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=743226,no,undetermined,0
Compressionless routing: a framework for adaptive and fault-tolerant routing,"Compressionless routing (CR) is an adaptive routing framework which provides a unified framework for efficient deadlock free adaptive routing and fault tolerance. CR exploits the tight coupling between wormhole routers for flow control to detect and recover from potential deadlock situations. Fault tolerant compressionless routing (FCR) extends CR to support end to end fault tolerant delivery. Detailed routing algorithms, implementation complexity, and performance simulation results for CR and FCR are presented. These results show that the hardware for CR and FCR networks is modest. Further, CR and FCR networks can achieve superior performance to alternatives such as dimension order routing. Compressionless routing has several key advantages: deadlock free adaptive routing in toroidal networks with no virtual channels, simple router designs, order preserving message transmission, applicability to a wide variety of network topologies, and elimination of the need for buffer allocation messages. Fault tolerant compressionless routing has several additional advantages: data integrity in the presence of transient faults (nonstop fault tolerance), permanent fault tolerance, and elimination of the need for software buffering and retry for reliability. The advantages of CR and FCR not only simplify hardware support for adaptive routing and fault tolerance, they also can simplify software communication layers",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=584089,no,undetermined,0
The Silicon Gaming Odyssey slot machine,"The Odyssey is a multi-game video slot machine using standard PC hardware components under the control of an embedded real-time operating system. A PCI-bus based peripheral memory board and parallel port-based general-purpose I/O board adapt the PC for use in the slot machine application. The system software is designed to address the unique requirements of casino gaming machines, including high reliability and security, fault detection and recovery, and responsive performance.",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=584736,no,undetermined,0
A digital-signal-processor-based measurement system for on-line fault detection,"This paper deals with the design, construction, and setting up of a measurement apparatus, based on an architecture using two parallel digital signal processors (DSP's), for on-line fault detection in electric and electronic devices. In the proposed architecture, the first DSP monitors a device output on-line in order to detect faults, whereas the second DSP estimates and updates the system-model parameters in real-time in order to track their eventual drifts. The problems which arose when the proposed apparatus was applied to a single-phase inverter are discussed, and some of the experimental results obtained in fault and nonfault conditions are reported",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=585442,no,undetermined,0
Performance evaluation of a distributed application performance monitor,"<div style=""font-variant: small-caps; font-size: .9em;"">First Page of the Article</div><img class=""img-abs-container"" style=""width: 95%; border: 1px solid #808080;"" src=""/xploreAssets/images/absImages/00864207.png"" border=""0"">",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=864207,no,undetermined,0
Development of a QRS detector algorithm for ECG's measured during centrifuge training,This paper describes a QRS detector algorithm for ECG's measured during centrifuge training of fighter pilots at the Netherlands Aerospace Medical Centre (NAMC). Due to the low quality of the ECG signal during centrifuge training a very robust QRS detector is necessary. An algorithm has been developed which uses lower and upper thresholds in spatial velocities to decide if a detected peak is a R-top. The performance (S=99.41% and PP-99.77%) of this new QRS detector was substantially higher than a frequently used QRS detector in the literature,1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=647469,no,undetermined,0
Revisiting measurement of software complexity,"Software complexity measures are often proposed as suitable indicators of different software quality attributes. The paper presents a study of some complexity measures and their correlation with the number of failure reports, and a number of problems are identified. The measures are such poor predictors that one may as well use a very simple measure. The proposed measure is supposed to be ironic to stress the need for a more scientific approach to software measurement. The objective is primarily to encourage discussions concerning methods to estimate different quality attributes. It is concluded that either completely new methods are needed to predict software quality attributes or a new view on predictions from complexity measures is needed. This is particularly crucial if the software industry is to use software metrics successfully on a broad basis",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=566738,no,undetermined,0
A new concept of power control in cellular systems reflecting challenges of today's systems,"When the systems evolved from analog to digital, the performance was improved by the use of power control on the one hand and different modulations and coding schemes on the other. Condensing the available information we are able to propose a new concept of power control. The concept is applicable to real systems, since it uses the available measurements for estimating parameters necessary for the power control. It also supports the use of an adequate quality measure together with a quality specification supplied by the operator. We use frequency hopping GSM as an example and the resulting control algorithm is ready for implementation in the software in the base stations where the output powers are computed. No modifications are needed in the GSM standard, the mobile terminals, the radio interfaces or in the base station transmitters. Finally we provide simulation results confirming the benefits of using the new concept for power control",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=775907,no,undetermined,0
Improving the quality of classification trees via restructuring,"The classification-hierarchy table developed by Chen and Poon (1996) provides a systematic approach to construct classification trees from given sets of classifications and their associated classes. The paper enhances their study by defining a metric to measure the â€œqualityâ€?of a classification tree, and providing an algorithm to improve this quality",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=566743,no,undetermined,0
A toolbox for model-based fault detection and isolation,"A toolbox for model-based fault detection and isolation (FDI) has been developed in the MATLAB/SIMULINK environment. It includes methods relying on analytical or qualitative models of the supervised process. A demonstration of each approach can be performed on a simulation of either a three-tank system, a cocurrent or a countercurrent heat exchanger. The results are displayed in a common format, which allows performance comparison. A user manual including guidelines for tuning method specific parameters is available.",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7099983,no,undetermined,0
"Real-time performance monitoring and anomaly detection in the Internet: An adaptive, objective-driven, mix-and-match approach","Algorithms for real-time performance management and adaptive fault/anomaly detection in Internet protocol (IP) networks and services have been developed, and a corresponding real-time distributed software platform has been implemented. These algorithms automatically and adaptively detect â€œsoftâ€?faults or anomalies (performance degradations) in IP networks and services, enabling timely correction of network exceptions before failures occur and services are compromised and thereby achieving proactive network and service management. Further, these algorithms are implemented as a reliable, fully distributed real-time software platform â€?the network/service anomaly detector (NSAD) â€?with the following features. First, it provides a flexible platform on which preconstructed monitoring and detection components can be mixed, matched, and distributed to form a wide range of application-specific NSADs and performance monitors. Second, NSAD and its components can auto-recover in real time, making it a reliable system for persistent monitoring of networks and their services. Third, anomaly detection is performed on raw network observables â€?for example, performance data such as management information base-2 (MIB2) and remote monitor-1/2 (RMON1/2) variables â€?and algebraic functions of these observables (objective functions), thereby enabling objective-driven anomaly detection of wide range and high sensitivity. Fourth, controlled testing (with anomalies injected a priori into a test network) demonstrates that NSAD can detect anomalies reliably in IP networks. Thus, NSAD provides a powerful framework/platform for automatic, proactive fault/anomaly detection and performance management in IP networks and services.",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6769584,no,undetermined,0
The 1998 York and Williamsburg workshops on dependability: the proposed research agenda,"We first provide a set of statements that explain the intent and discussion areas of the workshops. Dependability comprises three aspects: Attributes, which describe or limit dependability: these include availability, reliability, safety, confidentiality, integrity, and maintainability. Means, which refer to the major factors associated with dependability: fault prevention, fault tolerance, fault removal, and fault forecasting. Threats, which reduce or affect the dependability, including errors, faults, and failures. Dependable systems, among others, include: embedded systems, safety-critical systems, critical-information systems, and electronic commerce. The pair of workshops was initiated to discuss the means available and research to be accomplished in order to establish sets of scientific principles and practical engineering techniques for developing, maintaining, and evaluating dependable computer-based systems. Participants started by assuming that this could be initiated by concentrating on the integration of methods and disciplines used in the fields of security, fault-tolerance, and high-assurance, especially by concentrating on those techniques that enable measurement of their attributes. Emphasis was therefore placed on techniques that enable integration, composability, reuse, and cost prediction of dependable systems",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=798353,no,undetermined,0
Design and analysis of video-on-demand servers,"In this paper we deal with the design and analysis of a typical multimedia server, video-on-demand server (VoD). If we define the performance measure of the VoD server as the maximum number of concurrent access processes it can support, the system performance depends on a number of the system characteristics. They are: (i) storage devices contention; (ii) resource allocation, such as buffer size; (iii) QoS control. Thus, in order to achieve the best performance of a server the design trade-off among theses factors should be investigated quantitatively. In this paper we formulate and solve a problem of allocating resources for such a multimedia server in order to maximize concurrent video accesses under QoS guarantees. The solution is theoretically based on economic models and queuing theory. By using our optimization procedure we may estimate the maximum number of concurrent video accesses with optimal buffer allocation and QoS guarantees",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=776840,no,undetermined,0
Analysis of software process improvement experience using the project visibility index,"Based on the capability maturity model (CMM), process improvement at OMRON, a Japanese microprocessor manufacturer, increased project predictability in three ways: accuracy, variability, and performance. The authors use the project visibility index (PVI) and other measurements to quantitatively demonstrate this. Qualitative analysis of how and why OMRON achieved higher project visibility and increases in the QCD (quality, cost, and delivery on time) factors are supported with data on review-effort ratios and productivity. They identify factors directly affected by high project visibility",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=566749,no,undetermined,0
Software specifications metrics: a quantitative approach to assess the quality of documents,"Following the development of system requirements, the software specifications document consists of a definition of the software product. Each of the following phases of design, coding, and integration/testing transforms the initial software specifications into lower levels of machine implementable details until the final machine processable object code is generated. Therefore, the completeness, readability and accuracy of the software specification directly influences the quality of the final software product. A waterfall development model is not assumed. It is assumed however, that software specifications are kept â€œaliveâ€?and relevant. The purpose of the article is to provide a methodology for deriving quantitative measures of the quality of the software specifications document. These measures are designed to complement good engineering judgment that has to be applied in order to judge the quality of a software specifications document. Quantitative measures (i.e., metrics) will help reveal problem areas in various dimensions of quality characteristics. We focus on completeness, readability and accuracy. The methodology is presented with real life examples",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=566920,no,undetermined,0
Estimation of the explicit cell rate for the ABR service based on the measurements,"The ATM Forum has defined two modes of operation for the ABR (available bit rat) congestion control algorithm, i.e. the explicit rate indication (ERI) and the binary congestion indication (BCI). As the standardization process is mainly focused on the UNI interface, the switch mechanisms are left to the vendors. The objective of the ERI algorithm is to determine the actual explicit rate while the BCI algorithm uses queue thresholds to detect congestion and noncongestion state. This information is sent to the source in order to update the allowed cell rate. In this paper we present a method to implement the ERI algorithm based on the measurements of spare link capacity. To illustrate the effectiveness of the proposition, several numerical results are included. Finally, the method is compared with the standard BCI algorithm",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=568599,no,undetermined,0
Lassoo!: an interactive graphical tool for seafloor classification,"Lassoo! is an interactive graphical tool that facilitates the empirical classification of seafloor materials. Lassoo! can: (1) input a multivariate geo-referenced data set (e.g. geophysical properties, acoustic data or acoustic derivative data sets); (2) display data in geo-referenced map space and/or in multivariate data space (e.g. side scan sonar imagery data in geo-referenced space and parameters derived from the imagery in bivariate space); (3) interactively select subsets of data points in either bivariate or geo-referenced spaces; (4) assign â€œclassesâ€?to selected regions in either data space and; (5) automatically identify these â€œclassesâ€?in both data spaces, Lassoo! has been used for the evaluation of data collected with the commercial sediment classification system â€œRoxAnnâ€?in an area of seafloor dredge spoil dumping, The RoxAnn data were evaluated by direct comparison with side scan sonar data obtained with a Simrad EM1000 multibeam system. The purpose of the project was to monitor the dispersal of sediments from two dump sites in the survey area and to classify the various sediments encountered in this region. The results of acoustic classification of the sediments were compared to ground truth data obtained from cores. It was observed that the ability to select classes in one space and the identification of these classes in other spaces is a powerful tool for enhancing the quality of empirical sediment classification",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=569049,no,undetermined,0
Multiagent testbed for measuring multimedia presentation quality disorders,"The capability to deliver continuous media to the workstation and meet its real time processing requirements is recognised as the central element of future distributed multimedia courseware. In this process, user perception of quality of service (QoS) plays an important role. Since people have different expectations we concluded that it is important to ensure that courseware packages meet the user perceived QoS rather than commonly defined QoS. To investigate this hypothesis, we have constructed a multi agent testbed (MAT) for integrated management of resource distribution. We present an overview of the MAT architecture, and discuss a case study in which the MAT is being used to assess the impact of the quality of presentation decline from the perceived QoS on student's learning process",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=570260,no,undetermined,0
Robust event correlation scheme for fault identification in communications network,"The complexity of communications network and the amount of information transferred in these networks have made the management of such networks increasingly difficult. Since faults are inevitable, quick detection, identification, and recovery are crucial to make the systems more robust and their operation more reliable. This paper proposes a novel event correlation scheme for fault identification in communications network. This scheme is based on the algebraic operations of sets. The causality graph model is used to describe the cause-and-effect relationships between network events. For each problem, and each symptom, a unique prime number is assigned. The use of the greatest common devisor (GCD) makes the correlation process simple and fast. A simulation model is developed to verify the effectiveness and efficiency of the proposed scheme. From simulation results, we notice that this scheme not only identifies multiple problems at one time but also is insensitive to noise alarms. The time complexity of the correlation process is close to a function of n, where n is the number of observed symptoms, with order O(n<sup>2</sup>); therefore, the on-line fault identification is easy to achieve",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=776009,no,undetermined,0
An approach of parameter identification for asynchronous machine,"This work deals with the parameter identification of asynchronous machines in cases where the motor is rotating and at standstill. In this work, an optimal exciting input signal is determined by methods of optimization to give a best trajectory of the system. Integration is applied to a differential equation model to avoid the wrong information caused by a derivative operation on noise. Instrumental variable estimation is introduced to improve the quality of identification by least-square estimation. Experimental results are given. The work is achieved with MATLAB Toolbox",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=570674,no,undetermined,0
A PC-based method for the localisation and quantization of faults in passive tree-structured optical networks using the OTDR technique,"The method presented showed that the optical time domain reflectometry can be used to facilitate the maintenance of multistaged tree-structured passive optical networks at a lowcost. This method has been implemented in a user-friendly software, and the results obtained proved its effectiveness. We also showed that it is possible to estimate the loss of a fault only on the basis of OTDR traces acquired at the network's head despite the fact that such traces result in the combination of the reflected and backscattered signals coming from the different optical paths.",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=571581,no,undetermined,0
A conservative theory for long term reliability growth prediction,The paper describes a different approach to software reliability growth modelling which should enable conservative long term predictions to be made. Using relatively standard assumptions it is shown that the expected value of the failure rate after a usage time t is bounded by: Î»Â¯<sub>t</sub>&les;(N/(et)) where N is the initial number of faults and e is the exponential constant. This is conservative since it places a worst case bound on the reliability rather than making a best estimate. We also show that the predictions might be relatively insensitive to assumption violations over the longer term. The theory offers the potential for making long term software reliability growth predictions based solely on prior estimates of the number of residual faults. The predicted bound appears to agree with a wide range of industrial and experimental reliability data. It is shown that less pessimistic results can be obtained if additional assumptions are made about the failure rate distribution of faults,1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=558887,no,undetermined,0
Implementing fault injection and tolerance mechanisms in multiprocessor systems,"The size and complexity of today's multiprocessor systems require the development of new techniques to measure their dependability. An effective technique allowing one to inject faults in message passing multiprocessor systems is presented. Interrupt messages are used to trigger fault injection routines in the targeted processors. Any fault that can be emulated by a modification of the memory content of processors can be injected. That includes faults that could occur within the processors, memories and even in the communication network. The proposed technique allows one to control the time and location of faults as well as other characteristics. It has been used in a prototype multiprocessor system running real applications in order to compare the efficiency of various error detection and correction mechanisms",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=572038,no,undetermined,0
Harmonic monitoring system via synchronized measurements,"The New York Power Authority (NYPA), and the other Empire State Electric Energy Research Corporation (ESEERCO) member utilities, initiated deployment of a high voltage transmission harmonic measurement system (HMS) in 1992. The HMS consists of hardware and software, which determine the harmonic state of a transmission system, in real time, and stores the acquired data in a historical harmonic database. The HMS hardware consists of GPS synchronized digital event recorders linked to on-site computers, and a master station computer. The present installation includes instrumentation for a total of 150 measurements of which 138 are three phase quantities (voltages or currents) resulting in 46 phasors. The system performs synchronized waveform data acquisition every 15 minutes. The captured data are processed at the on-site computers to correct for error from the nonideal characteristics of the instrumentation, and to compute the harmonics. The computed harmonics (magnitude and phase) are transmitted to the master station computer where the system wide harmonic flow is constructed, using a harmonic state estimation technique. This paper describes the HMS hardware and software as well as HMS applications",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=759956,no,undetermined,0
Design of a test station for the CMS HCAL waveshifter/waveguide fiber system,A test station has been designed and is under construction to test the quality of assembled waveguide to waveshifter fiber to be used in the scintillating tile calorimeter for the Compact Moun Solenoid (CMS) Hadron Calorimeter (HCAL). The test station consists of a light tight enclosure 6.8 meters long with the ability to move a light source over almost 6 meters of fiber. Data acquisition hardware and software are under development to analyze the quality of the fiber as well as motor control hardware and software to operate the moveable light source. The design and performance expectations of the test station will be presented,1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=775201,no,undetermined,0
High frequency piezo-composite transducer array designed for ultrasound scanning applications,"A 20 MHz high density linear array transducer is presented in this paper, This array has been developed using an optimized ceramic-polymer composite material. The electro-mechanical behaviour of this composite, especially designed for high frequency applications, is characterised and the results are compared to theoretical predictions. To support this project, a new method of transducer simulation has been implemented. This simulation software takes into account the elementary boundary phenomena and allows prediction of inter-element coupling modes in the array. The model also yields realistic computed impulse responses of transducers, A miniature test device and water tank have been constructed to perform elementary acoustic beam pattern measurements. It is equipped with highly accurate motion controls and a specific needle-shaped target has been developed. The smallest displacement available in the three main axes of this system is 10 microns. The manufacturing of the array transducer has involved high precision dicing and micro interconnection techniques. The flexibility of the material provides us with the possibility of curving and focusing the array transducer. Performance of this experimental array are discussed and compared to the theoretical predictions. The results demonstrate that such array transducers will allow high quality near field imaging. This work presents the efforts to extend the well known advantages of composite piezoelectric transducers to previously unattainable frequencies",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=584147,no,undetermined,0
Availability of real time Internet services,"The Internet has seen dramatic growth over the last few years aided by the increasing popularity of the World Wide Web. Businesses are rushing to provide a presence on the Web and the number of consumers surfing the Web grows daily. This growth in traffic and applications has implications on the expected quality of services. As competition for services increases and customers are asked to pay for these services, performance will become a key differentiator. We consider quality of service issues for real time Internet applications. In particular we define service availability, the probability that a customer will be properly serviced by a given application at a given point in time, as a key measure of service quality. We discuss the network and service factors affecting the performance of new real time applications and then develop a system availability model for the Internet and use it to provide initial estimates of service availability for telephony over the Internet",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=586120,no,undetermined,0
A novel semiconductor pixel device and system for X-ray and gamma ray imaging,"We are presenting clinical images (objects, mammography phantoms, dental phantoms, dead animals) and data from a novel X-ray imaging device and system. The device comprises a pixel semiconductor detector flip-chip joined to an ASIC circuit. CdZnTe and Si pixel detectors with dimensions of the order of 1 cm<sup>2</sup> have been implemented with a pixel pitch of 35 Î¼m. Individual detectors comprise, therefore, tens of thousands of pixels. A novel ASIC accumulates charge created from directly absorbed X-rays impinging on the detector. Each circuit on the ASIC, corresponding to a detector pixel, is capable of accumulating thousands of X-rays in the energy spectrum from a few to hundreds of keV with high efficiency (CdZnTe). Image (X-ray) accumulation times are user controlled and range from just a few to hundreds of ms. Image frame updates are also user controlled and can be provided as fast as every 20 ms, thus offering the possibility of real time imaging. The total thickness of an individual imaging the including the mounting support does not exceed 4 mm. Individual imaging tiles are combined in a mosaic providing an imaging system with any desired shape and useful active area. The mosaic allows for cost effective replacement of individual tiles. A scanning system, allows for elimination, in the final image, of any inactive space between the imaging tiles without use of software interpolation techniques. The Si version of our system has an MTF of 20% at 14 1p/mm and the CdZnTe version an MTF of 15% at 10 1p/mm. Our digital imaging devices and systems are intended for use in X-ray and gamma-ray imaging for medical diagnosis in a variety of applications ranging from conventional projection X-ray imaging and mammography to fluoroscopy and CT scanning. Similarly, the technology is intended for use in non destructive testing, product quality control and real time on-line monitoring. The advantages over existing X-ray digital imaging modalities (such as digital imaging plates, scintillating screens coupled to CCDs etc.) include compactness, direct X-ray conversion to an immediate real time digital display, exquisite image resolution, dose reduction and large continuous imaging areas. Our measurements and images confirm that this new digital imaging system compares favourably to photoluminescence",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=591636,no,undetermined,0
Application of wavelet basis function neural networks to NDE,"This paper presents a novel approach for training a multiresolution, hierarchical wavelet basis function neural network. Such a network can be employed for characterizing defects in gas pipelines which are inspected using the magnetic flux leakage method of nondestructive testing. The results indicate that significant advantages over other neural network based defect characterization schemes could be obtained, in that the accuracy of the predicted defect profile can be controlled by the resolution of the network. The centers of the basis functions are calculated using a dyadic expansion scheme and a hybrid learning method. The performance of the network is demonstrated by predicting defect profiles from experimental magnetic flux leakage signals",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=593230,no,undetermined,0
N-best-based instantaneous speaker adaptation method for speech recognition,"An instantaneous speaker adaptation method is proposed that uses N-best decoding for continuous mixture-density hidden Markov model-based speech recognition systems. An N-best paradigm of multiple-pass search strategies is used that makes this method effective even for speakers whose decodings using speaker-independent models are error-prone. To cope with an insufficient amount of data, our method uses constrained maximum a posteriori estimation, in which the parameter vector space is clustered, and a mixture-mean bias is estimated for each cluster. Moreover, to maintain continuity between clusters, a bias for each mixture-mean is calculated as the weighted sum of the estimated biases. Performance evaluation using connected-digit (four-digit strings) recognition experiments performed over actual telephone lines showed more than a 20% reduction in the error rates, even for speakers whose decodings using speaker-independent models were error-prone",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=607765,no,undetermined,0
Investigating rare-event failure tolerance: reductions in future uncertainty,"At the 1995 Computer Assurance (COMPASS) conference, Voas and Miller (1995) presented a technique for assessing the failure tolerance of a program when the program was executing in unlikely modes (with respect to the expected operational profile). In that paper, several preliminary algorithms were presented for inverting operational profiles to more easily distinguish the unlikely modes of operation from the likely modes. This paper refines the original algorithms. It then demonstrates the new algorithms being used in conjunction with a failure tolerance assessment technique on two small programs",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=618568,no,undetermined,0
Adaptive recovery for mobile environments,"Mobile computing allows ubiquitous and continuous access to computing resources while the users travel or work at a client's site. The flexibility introduced by mobile computing brings new challenges to the area of fault tolerance. Failures that were rare with fixed hosts become common, and host disconnection makes fault detection and message coordination difficult. This paper describes a new checkpoint protocol that is well adapted to mobile environments. The protocol uses time to indirectly coordinate the creation of new global states, avoiding all message exchanges. The protocol uses two different types of checkpoints to adapt to the current network characteristics, and to trade off performance with recovery time",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=618575,no,undetermined,0
Impact of program transformation on software reliability assessment,"The statistical sampling method is a theoretically sound approach for measuring the reliability of safety critical software, such as control systems for nuclear power plants, aircrafts, space vehicles, etc. It has, however some practical drawbacks, two of which are the large number of test cases needed to attain a reasonable confidence in the reliability estimate and the sensitivity of the reliability estimate to variations in the operational profile. One way of dealing with both of these issues is to combine statistical sampling with formal methods and attempt to verify complete program paths. This combination becomes especially effective if high usage paths are verified. However the verification of complete paths is difficult to perform in practice and viable only when there is a high confidence in the correctness of the specification. We identify program transformations and partial proofs which have a measurable impact on the reliability assessment procedure. These methods reduce the effective size of the input space which can facilitate sampling without replacement, thereby increasing the confidence in the reliability estimate. Furthermore, these techniques increase the probability that the program under test is free of errors if testing reveals no failures",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=618599,no,undetermined,0
Reliability prediction method for electronic systems: a comparative reliability assessment method,"The paper describes a proposed research in defining a new reliability prediction methodology that may be used to evaluate the reliability of computer and electronic systems. The proposed methodology will attempt to minimize the deficiencies of the traditional reliability prediction methods. The deficiencies include: the use of generic failure rates for reliability prediction; and the lack of realism of the reliability prediction in various operational environments. The proposed methodology will employ the use of Analytical Hierarchy Process, a decision tool, to incorporate the qualitative and quantitative data that are most prevalent to the reliability performance of the system under study. This methodology will analyze the reliability of the system under study by comparing its performance characteristics against its predecessor system (or a similar system) with known reliability performance. The resultant analysis will yield a reliability ratio between the two systems and the ratio may be used to describe the system's reliability under various operational environments. The key traits of the proposed methodology are its ability to incorporate all relevant failure modes that are prevalent to reliability performance and the use of realistic data that will provide realism of the predicted reliability",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=618603,no,undetermined,0
"Reliability-oriented software engineering: design, testing and evaluation techniques","Software reliability engineering involves techniques for the design, testing and evaluation of software systems, focusing on reliability attributes. Design for reliability is achieved by fault-tolerance techniques that keep the system working in the presence of software faults. Testing for reliability is achieved by fault-removal techniques that detect and correct software faults before the system is deployed. Evaluation for reliability is achieved by fault-prediction techniques that model and measure the reliability of the system during its operation. This paper presents the best current practices in software reliability engineering for design, evaluation purposes. There are descriptions how fault-tolerant components are designed and applied to software systems, how software testing schemes are performed to show improvement of software reliability, and how reliability quantities are obtained for software systems. The tools associated with these techniques are also examined, and some application results are described",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=765677,no,undetermined,0
Modeling of ultrasonic fields and their interaction with defects,"Ultrasonic modeling at the French Atomic Energy Commission (CEA) is based onto continuous developments of two approximate models covering realistic commonly encountered NDT configurations. The first model, based on an extension of the Rayleigh integral, is dedicated to the prediction of ultrasonic fields radiated by arbitrary transducers. Being computed the field may be inputted in a model of the echo formation. This two step modeling yields the prediction of echo-structures detected with transducer scanning. The associated software is implanted in the CIVA system for processing and imaging multiple technique NDT data",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=762263,no,undetermined,0
Measurement of harmonic impedance on an LV system utilising power capacitor switching and consequent predictions of capacitor induced harmonic distortion,"Due to the existence of harmonic voltage distortion in power systems, the connection of power factor correction capacitors can magnify harmonic distortion by resonance with the supply inductance. A common solution to this problem is to install detuning reactance to ensure that the power factor correction unit remains inductive for all the major harmonics present in the system voltage. This paper describes an algorithm to measure system harmonic impedance and harmonic damping, by monitoring the system response to capacitor switching transitions. A prediction of the harmonic voltage distortion due to different levels of capacitance can then be made. The algorithm offers the possibility of an economic software based solution to the problem of harmonic resonance due to power factor correction capacitors",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=760199,no,undetermined,0
Data flow transformations to detect results which are corrupted by hardware faults,"Design diversity, which is generally used to detect software faults, can be used to detect hardware faults without any additional measures. Since design of diverse programs may use hardware parts in the same way, the hardware fault coverage obtained is insufficient. To improve hardware fault coverage, a method is presented that systematically transforms every instruction of a given program into a modified instruction (sequence), keeping the algorithm fixed. This transformation is based on a diverse data representation and accompanying modified instruction sequences, that calculate the original results in the diverse data representation. If original and systematically modified variants of a program are executed sequentially, the results can be compared online to detect hardware faults. For this method, different diverse data representation have been examined. For the most suitable representation, the accompanying modified instruction sequences have been generated at assembler level and at high language level. The theoretically estimated improvement of the fault coverage of design diversity by additionally using systematically generated diversity have been confirmed by practical examinations",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=618609,no,undetermined,0
An empirical evaluation of maximum likelihood voting in failure correlation conditions,"The maximum likelihood voting (MLV) strategy was recently proposed as one of the most reliable voting methods. The strategy determines the most likely correct result based on the reliability history of each software version. However, the theoretical results were obtained under the assumption that inter-version failures are not correlated by common cause faults. We first discuss the issues that arise in practical implementation of MLV, and present an extended version of the MLV algorithm that uses component reliability estimates to break voting ties. We then empirically evaluate the implemented MLV strategy in a situation where the inter-version failures are highly correlated. Our results show that, although in real situations MLV carries no reliability guarantees, it tends to be statistically more reliable, even under high inter-version correlation conditions, than other voting strategies that we have examined. We also compare implemented MLV performance with that of Recovery Block and hybrid Consensus Recovery Block approaches. Our results show that MLV often outperforms Recovery Block and that it can successfully compete with more elaborate Consensus Recovery Block. To the best of our knowledge, this is the first empirical evaluation of the MLV strategy",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=558890,no,undetermined,0
Knowledge-based re-engineering of legacy programs for robustness in automated design,"Systems for automated design optimization of complex real-world objects can, in principle, be constructed by combining domain-independent numerical routines with existing domain-specific analysis and simulation programs. Unfortunately, such â€œlegacyâ€?analysis codes are frequently unsuitable for use in automated design. They may crash for large classes of input, be numerically unstable or locally non-smooth, or be highly sensitive to control parameters. To be useful, analysis programs must be modified to reduce or eliminate only the undesired behaviors, without altering the desired computation. To do this by direct modification of the programs is labor-intensive, and necessitates costly revalidation. We have implemented a high-level language and runtime environment that allow failure-handling strategies to be incorporated into existing Fortran and C analysis programs while preserving their computational integrity. Our approach relies on globally managing the execution of these programs at the level of discretely callable functions so that the computation is only affected when problems are detected. Problem handling procedures are constructed from a knowledge base of generic problem management strategies. We show that our approach is effective in improving analysis program robustness and design optimization performance in the domain of conceptual design of jet engine nozzles",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=552828,no,undetermined,0
Unification of finite failure non-homogeneous Poisson process models through test coverage,"A number of analytical software reliability models have been proposed for estimating the reliability growth of a software product. We present an Enhanced Non-Homogeneous Poisson Process (ENHPP) model and show that previously reported Non-Homogeneous Poisson Process (NHPP) based models, with bounded mean valve functions, are special cases of the ENHPP model. The ENHPP model differs from previous models in that it incorporates explicitly the time varying test coverage function in its analytical formulation, and provides for defective fault detection and test coverage during the testing and operational phases. The ENHPP model is validated using several available failure data sets",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=558886,no,undetermined,0
Optical 3D motion measurement,"This paper presents a CCD-camera based system for high-speed and accurate measurement of the three-dimensional movement of reflective targets. These targets are attached to the moving object under study. The system has been developed at TU Delft and consists of specialized hardware for real-time multi-camera image processing at 100 frames/s and software for data acquisition, 3D reconstruction and target tracking. An easy-to-use and flexible but accurate calibration method is employed to calibrate the camera setup. Applications of the system are found in a wide variety of areas; biomechanical motion analysis for research, rehabilitation, sports and ergonomics, motion capture for computer animation and virtual reality, and motion analysis of technical constructions like wind turbines. After a more detailed discussion of the image processing aspects of the system the paper will focus on image modeling and parameter estimation for the purpose of camera calibration and 3D reconstruction. A new calibration method that has recently been developed specifically for the measurement of wind turbine blade movements is. Test measurements show that with a proper calibration of the system a precision of 1:3000 relative to the field of view dimensions can be achieved. Future developments will further improve this result",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=507408,no,undetermined,0
An extension of goal-question-metric paradigm for software reliability,"The driving need in software reliability is to mature the â€œphysics of failureâ€?and design aspects related to software reliability. This type of focus would then enhance one's ability to effect reliable software in a predictable form. A major challenge is that software reliability, in essence, requires one to measure compliance to customer/user requirements. Customer/user requirements can range over a wide spectrum of software product attributes that relate directly or indirectly to software performance. Identifying and measuring these attributes in a structured way to minimize risk and allow pro-active preventive action during software development is no easy task. The goal-question-metric paradigm, discussed by Dr. Vic Basili (1992), is one popular and effective approach to measurement identification. However, in practice, additional challenges in using this approach have been encountered. Some of these challenges, though, seem to be alleviated with use of a reliability technique called success/fault tree analysis. Experience has shown that the goal-question-metric paradigm is conducive to the building of G-Q-M trees which may be analyzed using reliability success/fault tree logic",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=500656,no,undetermined,0
Software reliability as a function of user execution patterns,"Assessing the reliability of a software system has always been an elusive target. A program may work very well for a number of years and this same program may suddenly become quite unreliable if its mission is changed by the user. This has led to the conclusion that the failure of a software system is dependent only on what the software is currently doing. If a program is always executing a set of fault free modules, it will certainly execute indefinitely without any likelihood of failure. A program may execute a sequence of fault prone modules and still not fail. In this particular case, the faults may lie in a region of the code that is not likely to be expressed during the execution of that module. A failure event can only occur when the software system executes a module that contains faults. If an execution pattern that drives the program into a module that contains faults is ever selected, then the program will never fail. Alternatively, a program may execute successfully a module that contains faults just as long as the faults are in code subsets that are not executed. The reliability of the system then, can only be determined with respect to what the software is currently doing. Future reliability predictions will be bound in their precision by the degree of understanding of future execution patterns. We investigate a model that represents the program sequential execution of nodules as a stochastic process. By analyzing the transitions between modules and their failure counts, we may learn exactly where the system is fragile and under which execution patterns a certain level of reliability can be guaranteed.",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=772984,no,undetermined,0
Group support systems and deceptive communication,"Electronic communication is becoming more pervasive worldwide with the spread of the Internet, especially through the World Wide Web and electronic mail. Yet, as with all human communication, electronic communication is vulnerable to deceit on the part of senders, and to the less than stellar performance of most people at defecting deceit aimed at them. Despite considerable research over the years into both computer-mediated communication and into deception, there has been little if any research at the intersection of these two research streams. In this paper, we review these two streams and suggest a research model for investigating their intersection, focusing on group support systems as an electronic medium. We focus specifically on research questions concerning how successful people are at deceiving others through computer-mediated communication, and how successful people are at detecting such deception. We also suggest three propositions guiding research in this area.",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=772722,no,undetermined,0
FIRE: a fault-independent combinational redundancy identification algorithm,"FIRE is a novel Fault-Independent algorithm for combinational REdundancy identification. The algorithm is based on a simple concept that a fault which requires a conflict as a necessary condition for its detection is undetectable and hence redundant. FIRE does not use the backtracking-based exhaustive search performed by fault-oriented automatic test generation algorithms, and identifies redundant faults without any search. Our results on benchmark and real circuits indicate that we find a large number of redundancies (about 80% of the combinational redundancies in benchmark circuits), much faster than a test-generation-based approach for redundancy identification. However, FIRE is not guaranteed to identify all redundancies in a circuit.",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=502203,no,undetermined,0
Parameters for system effectiveness evaluation of distributed systems,"In a Distributed Computing System (DCS), the failure of one or more system components causes the degradation in its effectiveness to complete a given task as opposed to a complete network breakdown. This paper addresses the issue of degraded system effectiveness evaluation by introducing two static measures, namely, Distributed Program Performance Index (DPPI) and Distributed System Performance Index (DSPI). These metrics can be used to compare networks with different features for application execution. It can be used to determine if the network with high reliability and low capacity, or low reliability and high capacity is better for a given program execution. An algorithm is also developed for computing these indices, and it is shown to be not only efficient, but general enough to compute many other existing measures such as computer networks, distributed systems, transaction based systems, etc",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=506430,no,undetermined,0
Better software through operational dynamics,"Much of the existing theory about software focuses on its static behaviour, based on analysis of the source listing. Explorers of requirements, estimation, design, encapsulation, dataflow, decomposition, structure, and code complexity all study the static nature of software, concentrating on source code. I call this the study of software statics, an activity that has improved software quality and development, and one that we should continue to investigate. However, quality software remains difficult to produce because our understanding has an incomplete theoretical foundation. There is little theory that addresses software's dynamic behaviour in the field, in particular, how software performs under load. I use the term operational dynamics to differentiate this activity from software statics and from system dynamics, which involves process simulation",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=506469,no,undetermined,0
Exploratory analysis tools for tree-based models in software measurement and analysis,"The paper presents our experience with the adaptation and construction of exploratory analysis tools for tree-based models used in quality measurement and improvement. We extended a commercial off-the-shelf analysis tool to support visualization, presentation, and various other exploration functions. The user-oriented, interactive exploration of analysis results is supported by a new tool we constructed for this specific purpose. These tools have been used successfully in supporting software measurement and quality improvement for several large commercial software products developed in the IBM Software Solutions Toronto Laboratory",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=506473,no,undetermined,0
SoftTrak: an industrial case study,"SoftTrak is a project performance management tool based on earned value tracking of a software project's progress. Performance management tools are needed to report problems back to management. SoftTrak looks to the functional work completed as compared to the work estimation and tracks projects by comparing the actual accomplishments against the planned activities. Projected completion date and effort based on the historical data reported for the project are an integral part of the outputs. SoftTrak was developed by Soft Quest Systems in conjunction with input received from leading industries in Israel. The paper describes the process by which a software development project tracking methodology, using SoftTrak, was defined and later evaluated by B.V.R. Technologies",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=506476,no,undetermined,0
Assessing the potentials of CASE-tools in software process improvement: a benchmarking study,"CASE tools have been thought as one of the most important means for implementing the derived quality programs. Two basic questions should be answered to find the right CASE tool: what attributes the CASE tools should exhibit and how the existing tools can be ranked according to their capability to support the quality program goals. We propose our solution based on the concept of software benchmarking. First, we summarise the QR method, and then we report on how it was applied in CASE tools assessment. Next we derive appropriate benchmark sets for CASE tools on the base of the CMM, the standard ISO 9000 and the MBNQA-criteria. We propose and implement a framework for CASE tools evaluation and selection",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=506487,no,undetermined,0
Detection of shorted-turns in the field winding of turbine-generator rotors using novelty detectors-development and field test,"A method for detecting shorted windings in operational turbine-generators is described. The method is based on the traveling wave method described by El-Sharkawi, et. al. (1971). The method is extended in this paper to operational rotors by the application of a neural network feature extraction and novelty detection. The results of successful laboratory experiments are also reported",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=507183,no,undetermined,0
IP-address lookup using LC-tries,"There has been a notable interest in the organization of routing information to enable fast lookup of IP addresses. The interest is primarily motivated by the goal of building multigigabit routers for the Internet, without having to rely on multilayer switching techniques. We address this problem by using an LC-trie, a trie structure with combined path and level compression. This data structure enables us to build efficient, compact, and easily searchable implementations of an IP-routing table. The structure can store both unicast and multicast addresses with the same average search times. The search depth increases as Î˜(log log n) with the number of entries in the table for a large class of distributions, and it is independent of the length of the addresses. A node in the trie can be coded with four bytes. Only the size of the base vector, which contains the search strings, grows linearly with the length of the addresses when extended from 4 to 16 bytes, as mandated by the shift from IP version 4 to IP version 6. We present the basic structure as well as an adaptive version that roughly doubles the number of lookups/s. More general classifications of packets that are needed for link sharing, quality-of-service provisioning, and multicast and multipath routing are also discussed. Our experimental results compare favorably with those reported previously in the research literature",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=772439,no,undetermined,0
Microcontroller-based performance enhancement of an optical fiber level transducer,"The paper proposes a digital microcontrolled transducer for liquid level measurement based on two optical fibers on which the cladding was removed in suitable zones at a known distance one from each other. A 1 m full scale, 6.25 mm resolution prototype was realized and tested. The adopted hardware and software strategies assure a low cost, high static and dynamic performance, good reliability, and the possibility of remote control. These features were confirmed by the experimental tests carried out on fuel liquids",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=507300,no,undetermined,0
Use of artificial neural networks in estimation of Hydrocyclone parameters with unusual input variables,"The accuracy of the estimation of the Hydrocyclone parameter, d50 <sub>c</sub>, can substantially be improved by application of an Artificial Neural Network (ANN). With an ANN, many non-conventional Hydrocyclone variables, such as water and solid split ratios, overflow and underflow densities, apex and spigot flowrates, can easily be incorporated into the prediction of d50<sub>c</sub>. Selection of training parameters is also shown to affect the accuracy",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=507318,no,undetermined,0
The use of genetic algorithms for advanced instrument fault detection and isolation schemes,"An advanced scheme for Instrument Fault Detection and Isolation is proposed. It is mainly based on Artificial Neural Networks (ANNs), organized in layers and handled by knowledge-based analytical redundancy relationships. ANN design and training is performed by genetic algorithms which allow ANN architecture and parameters to be easily optimized. The diagnostic performance of the proposed scheme is evaluated with reference to a measurement station for automatic testing of induction motors",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=507340,no,undetermined,0
Measuring vibration patterns for rotor diagnostics,"The vibration patterns of machinery and industrial plants are effective means for characterising their current dynamics and, thus, detecting symptoms and trends of incipient anomalies; or, assessing the accumulation of previous damages. The availability of `intelligent' instrumentation is a powerful option for the build-up of diagnostic frameworks, based on the measurement of vibration signatures and the acknowledgment of the threshold features for `pro-active' maintenance schedules. This option is considered with attention paid to the metrological requests supporting the standardised restitution of the signatures, in connection with the opportunities offered by `intelligent' data acquisition, processing and restitution, namely availability of extended sets of procedural schemes leading to consistent metrological images, transparent conditioning contexts granting data restitution standardisation, and sophisticated programming aids for acknowledgment of system hypotheses",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=507452,no,undetermined,0
An on-line algorithm for checkpoint placement,"Checkpointing is a common technique for reducing the time to recover from faults in computer systems. By saving intermediate states of programs in a reliable storage device, checkpointing enables one to reduce the processing time loss caused by faults. The length of the intervals between the checkpoints affects the execution time of the programs. Long intervals lead to a long re-processing time, while too-frequent checkpointing leads to a high checkpointing overhead. In this paper, we present an online algorithm for the placement of checkpoints. The algorithm uses online knowledge of the current cost of a checkpoint when it decides whether or not to place a checkpoint. We show how the execution time of a program using this algorithm can be analyzed. The total overhead of the execution time when the proposed algorithm is used is smaller than the overhead when fixed intervals are used. Although the proposed algorithm uses only online knowledge about the cost of checkpointing, its behavior is close to that of the off-line optimal algorithm that uses the complete knowledge of the checkpointing cost",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=558869,no,undetermined,0
Adaptive network/service fault detection in transaction-oriented wide area networks,"Algorithms and online software for automated and adaptive detection of network/service anomalies have been developed and field-tested for transaction-oriented wide area networks (WAN). These transaction networks are integral parts of electronic commerce infrastructures. Our adaptive network/service anomaly detection algorithms are demonstrated in a commercially important production WAN, currently monitored by our recently implemented real-time software system, TRISTAN (transaction instantaneous anomaly notification). TRISTAN adaptively and proactively detects network/service performance degradations and failures in multiple service-class transaction-oriented networks, where performances of service classes are mutually dependent and correlated, and where external or environmental factors can strongly impact network and service performances. In this paper, we present the architecture, summarize the implemented algorithms, and describe the operation of TRISTAN as deployed in the AT&T transaction access services (TAS) network. TAS is a commercially important, high volume, multiple service classes, hybrid telecommunication and data WAN that services transaction traffic in the USA and neighboring countries. It is demonstrated that TRISTAN detects network/service anomalies in TAS effectively. TRISTAN can automatically and dynamically detect network/service faults, which can easily elude detection by the traditional alarm-based network monitoring systems",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=770721,no,undetermined,0
Real-time determination of power system frequency,"The main frequency is an important parameter of an electrical power system. The frequency can change over a small range due to generation-load mismatches. Some power system protection and control applications require accurate and fast estimates of the frequency. Most digital algorithms for measuring frequency have acceptable accuracy if voltage waveforms are not distorted. However, due to non-linear devices the voltage waveforms can include higher harmonics. The paper presents a new method based on digital filtering and Prony's model. Computers simulation results confirm, that the method is more accurate than others",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=507482,no,undetermined,0
"It ain't what you charge, it's the way that you do it: a user perspective of network QoS and pricing","Shared networks, such as the Internet, are fast becoming able to support heterogeneous applications and a diverse user community. In this climate, it becomes increasingly likely that some form of pricing mechanism will be necessary in order to manage the quality of service (QoS) requirements of different applications. So far, research in this area has focused on technical mechanisms for implementing QoS and charging. This paper reports a series of studies in which users' perceptions of QoS, and their attitudes to a range of pricing mechanisms, were investigated. We found that users' knowledge and experience of networks, and the real-world task they perform with applications, determine their evaluation of QoS and attitude to payment. Users' payment behavior is governed by their level of confidence in the performance of salient QoS parameters. User confidence, in turn, depends on a number of other factors. In conclusion, we argue that charging models that undermine user confidence are not only undesirable from the users' point of view, but may also lead to user behavior that may have a negative impact on QoS",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=770713,no,undetermined,0
A low-cost water sensing method for adaptive control of furrow irrigation,A computer-based optimization technique for furrow irrigation depends upon a network of water sensors to detect the water advance in the initial phase of the irrigation. This paper describes a new type of low-cost water sensing network developed specifically for this technique. Each sensing network consists of up to fourteen simple water sensors baked using inexpensive twin-wire. The distance between the computer and the last sensor can be up to 1000 m. Each sensor uses the variation of capacitance to detect the presence of water and communicates its state to the base computer by means of variable-frequency current pulses,1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=507593,no,undetermined,0
Fault diagnosis of electrical machines-a review,"Recently, research has picked up a fervent pace in the area of fault diagnosis of electrical machines. Like adjustable speed drives, fault prognosis has become almost indispensable. The manufacturers of these drives are now keen to include diagnostic features in the software to decrease machine down time and improve salability. Prodigious improvement in signal processing hardware and software has made this possible. Primarily, these techniques depend upon locating specific harmonic components in the line current, also known as motor current signal analysis (MCSA). These harmonic components are usually different for different types of faults. However with multiple faults or different varieties of drive schemes, MCSA can become an onerous task as different types of faults and time harmonics may end up generating similar signatures. Thus other signals such as speed, torque, noise, vibration etc., are also explored for their frequency contents. Sometimes, altogether different techniques such as thermal measurements, chemical analysis, etc., are also employed to find out the nature and the degree of the fault. It is indeed evident that this area is vast in scope. Hence, keeping in mind the need for future research, a review paper describing the different types of fault and the signatures they generate and their diagnostics schemes, will not be entirely out of place. In particular, such a review helps to avoid repetition of past work and gives a bird's eye-view to a new researcher in this area",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=769076,no,undetermined,0
Specializing object-oriented RPC for functionality and performance,"Remote procedure call (RPC) integrates distributed processing with conventional programming languages. However traditional RPC lacks support for forms of communication such as datagrams, multicast, and streams that fall outside the strict request-response model. Emerging applications such as Distributed Interactive Simulation (DIS) and Internet video require scalable, reliable, and efficient communication. Applications are often forced to meet these requirements by resorting to the error-prone ad-hoc message-based programming that characterized applications prior to the introduction of RPC. In this paper we describe an object-oriented RPC system that supports specialization for functionality and performance, allowing applications to modify and tune the RPC system to meet individual requirements. Our experiences with functional extensions to support reliable multicast and specializations to support streaming of performance-critical RPCs indicate that a wide range of communication semantics can be supported without resorting to ad-hoc messaging protocols",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=507915,no,undetermined,0
A software radio architecture for linear multiuser detection,"The integration of multimedia services over wireless channels calls for provision of variable quality of service (QoS) requirements. While radio resource management algorithms (such as power control and call admission control) can provide certain levels of variability in QoS, an alternate approach is to use reconfigurable radio architectures to provide diverse QoS guarantees. We outline a novel reconfigurable architecture for linear multiuser detection, thereby providing a wide range of bit error rate (BER) requirements amongst the constituent receivers of the reconfigurable architecture. Specifically, we focus on achieving this dynamic reconfiguration via a software radio implementation of linear multiuser receivers. Using a unified framework for achieving this reconfiguration, we partition functionality into two core technologies [field programmable gate arrays (FPGA) and digital signal processor (DSP) devices] based on processing speed requirements. We present experimental results on the performance and reconfigurability of the software radio architecture as well as the impact of fixed point arithmetic (due to hardware constraints)",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=768197,no,undetermined,0
Dynamic resource migration for multiparty real-time communication,"With long-lived multi-party connections, resource allocation subsystems in distributed real-time systems or communication networks must be aware of dynamically changing network load in order to reduce call-blocking probabilities. We describe a distributed mechanism to dynamically reallocate (â€œmigrateâ€? resources without adversely affecting the performance that established connections receive. In addition to allowing systems to dynamically adapt to load, this mechanism allows for distributed relaxation of resources (i.e. the adjustment of overallocation of resources due to conservative assumptions at connection establishment time) for multicast connections. We describe how dynamic resource migration is incorporated in the Tenet Scheme 2 protocols for multiparty real-time communication",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=508016,no,undetermined,0
1999 IEEE International Conference on Communications (Cat. No. 99CH36311),"The following topics were dealt with: antennas; multiuser receivers and interference mitigation; turbo codes; ABR traffic control in ATM networks; OFDM and multicarrier radio systems; QoS and performance issues in advanced communications; last mile technologies; communication software; tactical communications; mobile Internet; architecture; traffic modelling and applications; coding techniques; QoS routing techniques for B-ISDN; signal processing for wireless communications; information transport using optical architecture; infrared access; wireless networks management; enterprise network management; Internet; scheduling and resource management; handover; ATM network applications; modulation and coding; multimedia traffic shaping and prediction in high speed networks; multiuser detection and interference cancellation; interactive video service and applications; network management; satellite communication systems design; multi-access systems; radio resource management; multi-access protocols; radio system synchronization and equalization; networked delivery of multimedia applications and services; distributed management; ATM switch architecture; reliable multimedia wireless networks; antenna diversity, beamforming and smart antennas; radio propagation and channel impairments; applications of coding for storage; wireless multimedia; optical system design; mobility management; wireless ATM; CDMA and spread spectrum communication; signal processing and detection for storage; WDM routing and performance management",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=767864,no,undetermined,0
A systematic DFT procedure for library cells,"We present an automated procedure for improving the testability of a product by improving the testability of cells in the cell library. This method was applied to a scan flip-flop from Cyrix's standard cell library. Based on this analysis, some design and layout changes were suggested, which brought down the probability of difficult-to-detect faults by 70%, without compromising the performance or increasing the area of the circuit",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=766704,no,undetermined,0
Miniature microstrip stepped impedance resonator bandpass filters and diplexers for mobile communications,"Miniature microstrip stepped impedance resonator bandpass filters and diplexers for satellite mobile communications have been developed. A very high dielectric constant substrate (/spl epsiv//sub r/=89 and h=2 mm) is used. Experimental results show that an unloaded half wave resonator quality factor as high as 400 at 1.5 GHz, with such substrate, may be possible. The merit of this circuit lies in the simplicity of design procedure, the possibility of developing this filter with quite a variety of high dielectric constant substrate materials and the simplicity of simulation with most commercial software packages. A four resonator bandpass filter with 35 MHz bandwidth at 1.55 GHz was designed and implemented with this substrate. Based on this filter, a diplexer which meets satellite mobile communications performance has been developed. Experimental results are in good agreement with theoretical predictions.",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=511007,no,undetermined,0
A distributable algorithm for optimizing mesh partitions,"Much effort has been directed towards developing mesh partitioning strategies to distribute a finite element mesh among the processors of a distributed memory parallel computer. The distribution of the mesh requires a partitioning strategy. This partitioning strategy should be designed such that the computational load is balanced and the communication is minimized. Unfortunately, many of the existing approaches for the mesh partitioning are sequential and are performed as a sequential pre-processing step on a serial machine. These approaches may not be appropriate if the mesh is very large, that is to say, this pre-processing on a serial machine may not be feasible due to memory or time constraints, or on a parallel machine if the mesh is already distributed. In this paper we propose an algorithm with a fundamentally distributed design for the optimization of mesh partitions. To assess the quality of the partitioning, the size of edge cuts is the taken as the chosen metric",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=517606,no,undetermined,0
Industrial-strength management strategies,"As our industry undertakes ever larger development projects, the number of defects occurring in delivered software increases exponentially. Drawing on his experiences in the defense industry, the author offers nine best practices to improve the management of large software systems: (1) risk management; (2) agreement on interfaces; (3) formal inspections; (4) metrics-based scheduling and management; (5) binary quality gates at inch/pebble level; (6) program-wide visibility of progress vs. plan; (7) defect tracking against quality targets; (8) configuration management; and (9) people-aware management accountability",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=526836,no,undetermined,0
Why software reliability predictions fail,"Software reliability reflects a customer's view of the products we build and test, as it is usually measured in terms of failures experienced during regular system use. But our testing strategy is often based on early product measures, since we cannot measure failures until the software is placed in the field. The author shows us that such measurement is not effective at predicting the likely reliability of the delivered software.",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=526841,no,undetermined,0
The Midcourse Space Experiment (MSX),"The MSX is designed and built around an infrastructure of eight scientific teams; it will be the first and only extended duration, multiwavelength (0.1 to 28 Î¼m) phenomenology measurement program funded and managed by BMDO. During its 16 month cryogen lifetime and five year satellite lifetime, MSX will provide high quality target, Earth, Earthlimb, and celestial multiwavelength phenomenology data. This data is essential to fill critical gaps in phenomenology and discrimination data bases, furthering development of robust models of representative scenes, and assessing optical discrimination algorithms. The MSX organization is comprised of self-directed work teams in six functional areas. Experiments formulated by each of the eight scientific teams will be executed on the satellite in a 903 km near polar orbit (99.23Â° inclination), with an eccentricity of 0.001, argument of perigee of 0, and the right ascension of the ascending node is 250.0025. Two dedicated target missions are planned consisting of one Strategic Target System launch and two Low Cost Launch Vehicles launches. These target missions will deploy various targets, enabling the MSX principal investigator teams to study key issues such as metric discrimination, deployment phase tracking, cluster tracking, fragment bulk filtering, tumbling re-entry vehicle signatures, etc. A data management infrastructure to ensure that the data is processed, analyzed, and archived will be available at launch time. The raw data and its associated calibration files and software will be archived, providing the customer with a cataloged database. This paper describes the MSX program objectives, target missions, data management architecture, and organization",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=495885,no,undetermined,0
Performance benefits of optimism in fossil collection,"Each process in a time-warp parallel simulation requires a time-varying set of state and event histories to be retained for recovering from erroneous computations. Erroneous computation is discovered when straggler messages arrive with time-stamps in the process's past. The traditional method of determining the set of histories to retain has been through the estimation of a global virtual time (GVT) for the distributed simulation. A distributed GVT calculation requires an estimation of the global progress of the simulation during a real-time interval. Optimistic fossil collection (OFC) predicts a bound for the needed histories using local information, or previously collected information, that enables the process to continue. In most cases, OFC requires less communication overhead and less memory usage, and estimates the set of committed events faster. These benefits come at the cost of a possible penalty of having to recover from a state history that was incorrectly fossil-collected (an OFC fault). Sufficiently lightweight checkpointing and recovery techniques compensate for this possibility while yielding good performance. In this paper, the requirements of an OFC-based simulator (algorithm has been implemented in the WARPED time-warp parallel discrete-event simulator) are detailed along with a presentation of results from an OFC simulation. Performance statistics are given comparing the execution time and required memory usage of each logical process for different fossil collection methods.",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=773082,no,undetermined,0
The use of meta-analysis in validating the DeLone and McLean information systems success model,"The measurement of systems success is important to any research on information systems. A research study typically uses several dependent measures as surrogates for systems success. These dependent measures are generally treated as outcome variables which, as a group, vary as a function of certain independent variables. Recently, a process perspective has been advocated to measure system success. This process perspective recognizes that some of the dependent measures, along with independent variables, also have an impact on outcome variables. A comprehensive success model has been developed by DeLone and McLean (1992) to group success measures cited in the literature into three categories: quality, use, and impact. This model further predicts that quality affects use, which in turn affects impact. This paper explores the plausibility of using meta-analysis to validate this success model. Advantages of using meta-analysis over other research methodologies in validating process models are examined. Potential problems with meta-analysis in validating this particular success model are discussed. A research plan that utilizes past empirical studies to validate this success model is described",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=495324,no,undetermined,0
An empirical validation of four different measures to quantify user interface characteristics based on a general descriptive concept for interaction points,"The main problem of standards (e.g. ISO 9241) in the context of usability of software quality is that they cannot measure all relevant product features in a task independent way. We present a new approach to measure user interface quality in a quantitative way. First, we developed a concept to describe user interfaces on a granularity level, that is detailed enough to presence important interface characteristics, and is general enough to cover most of known interface types. We distinguish between different types of â€œinteraction pointsâ€? With these kinds of interaction points we can describe several types of interfaces (CUI: command, menu, form-fill-in; GUI: desktop, direct manipulation, multimedia, etc.). We carried out two different comparative usability studies to validate our quantitative measures. The results of one other published comparative usability study can be predicted. Results of six different interfaces are presented and discussed. One of the most important result is that the dialog flexibility must exceed a threshold of 15-measured with two of our metrics-to increase significantly the usability",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=494571,no,undetermined,0
The average availability of parallel checkpointing systems and its importance in selecting runtime parameters,"Performance prediction of checkpointing systems in the presence of failures is a well-studied research area. While the literature abounds with performance models of checkpointing systems, none address the issue of selecting runtime parameters other than the optimal checkpointing interval. In particular the issue of processor allocation is typically ignored. In this paper we briefly present it performance model for long-running parallel computations that execute with checkpointing enabled. We then discuss how it is relevant to today's parallel computing environments and software, and present case studies of using the model to select runtime parameters.",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=781059,no,undetermined,0
Fault injection spot-checks computer system dependability,"Computer-based systems are expected to be more and more dependable. For that, they have to operate correctly even in the presence of faults, and this fault tolerance of theirs must be thoroughly tested by the injection of faults both real and artificial. Users should start to request reports from manufacturers on the outcomes of such experiments, and on the mechanisms built into systems to handle faults. To inject artificial physical faults, fault injection offers a reasonably mature option today, with Swift tools being preferred for most applications because of their flexibility and low cost. To inject software bugs, although some promising ideas are being researched, no established technique yet exists. In any case, establishing computer system dependability benchmarks would make tests much easier and enable comparison of results across different machines",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=780999,no,undetermined,0
A formal analysis of the subsume relation between software test adequacy criteria,"Software test adequacy criteria are rules to determine whether a software system has been adequately tested. A central question in the study of test adequacy criteria is how they relate to fault detecting ability. We identify two idealized software testing scenarios. In the first scenario, which we call prior testing scenario, software testers are provided with an adequacy criterion in addition to the software under test. The knowledge of the adequacy criterion is used to generate test cases. In the second scenario, which we call posterior testing scenario, software testers are not provided with the knowledge of adequacy criterion. The criterion is only used to decide when to stop the generation of test cases. In 1993, Frankl and Weyuker proved that the subsume relation between software test adequacy criteria does not guarantee better fault detecting ability in the prior testing scenario. We investigate the posterior testing scenario and prove that in this scenario the subsume relation does guarantee a better fault detecting ability. Two measures of fault detecting ability will be used, the probability of detecting faults and the expected number of exposed errors",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=491648,no,undetermined,0
"Quasi-synchronous checkpointing: Models, characterization, and classification","Checkpointing algorithms are classified as synchronous and asynchronous in the literature. In synchronous checkpointing, processes synchronize their checkpointing activities so that a globally consistent set of checkpoints is always maintained in the system. Synchronizing checkpointing activity involves message overhead and process execution may have to be suspended during the checkpointing coordination, resulting in performance degradation. In asynchronous checkpointing, processes take checkpoints without any coordination with others. Asynchronous checkpointing provides maximum autonomy for processes to take checkpoints; however, some of the checkpoints taken may not lie on any consistent global checkpoint, thus making the checkpointing efforts useless. Asynchronous checkpointing algorithms in the literature can reduce the number of useless checkpoints by making processes take communication induced checkpoints besides asynchronous checkpoints. We call such algorithms quasi-synchronous. In this paper, we present a theoretical framework for characterizing and classifying such algorithms. The theory not only helps to classify and characterize the quasi-synchronous checkpointing algorithms, but also helps to analyze the properties and limitations of the algorithms belonging to each class. It also provides guidelines for designing and evaluating such algorithms",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=780865,no,undetermined,0
Admission control and QoS negotiations for soft-real time applications,"Systems are increasingly supporting a new class of multimedia applications that allow degradation of the application quality so that they use reduced amounts of resources. Management of such applications require new mechanisms to be developed for admission control, negotiation, allocation and scheduling. We present a new admission control algorithm that exploits the degradability property of applications to improve the performance of the system. The algorithm is based on setting aside a portion of the resources as reserves and managing it intelligently so that the total utility of the system is maximized. The mixed greedy and predictive strategy leads to an efficient protocol that also improves the performance of the system. We use the constructs of application benefit functions and resource demand functions in the integrated admission control and negotiation protocol. Extensive simulation experiments are presented in the paper to evaluate the performance of the novel mechanisms and compare it to some of the other methods used in the past",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=779138,no,undetermined,0
Trusted components - 2nd Workshop on Trusted Components,"Summary form only given, as follows. Using ??off-the-shelf?? software components in mission-critical applications is not yet as commonplace as in desktop applications. Opposite to electronic components, there is a lack of methods and quality estimates in the software domain which makes it difficult to build trust into software components. While electronic devices have sets of measures characterizing their quality (reliability, performances, use-domain, speed scale), no real consensus exists to measure such quality characteristics for software components. A second problem is the software component expected capability to evolve (addition of new functionality, implementation change) and to be adapted to various environments. Trusted components are reusable software elements equipped with a strong presumption of high quality, based on a combination of convincing arguments of various kinds: technical (e.g. Design by Contract, thorough and meaningful testing, appropriate programming language, formal proofs), managerial (e.g. systematic process based on CMM or similar), social (e.g. reputation of the components?? authors). The idea of trusted components was introduced in an IEEE Computer article of May 1998 and earlier presentations in the same journal (Object Technology column). A first Trusted Components workshop was held at Monash University (Melbourne, Australia) at the end of TOOLS PACIFIC 1998 on November 28, 1998. The purpose of this workshop is to set up a forum where interested people can share their point of view on the idea of trusted components and identify research directions to help the industry move towards this goal.",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=779108,no,undetermined,0
Application-level measurements of performance on the vBNS,"We examine the performance of high-bandwidth multimedia applications over high-speed wide-area networks, such as the vBNS (Internet2). We simulate a teleimmersion application that sends 30 Mbps (2,700 datagrams per second) to various sites around the USA, and measure network- and application-level loss and throughput. We found that over the vBNS, performance is affected more by the packet-rate generated by an application than by its bit-rate. We also assess the amount of error recovery and buffering needed in times of congestion to present an effective multimedia stream to the user. Lastly, we compare our application-level measurements with data taken from a constellation of GPS-synchronized network probes (IPPM Surveyors). The comparison suggests that network-level probes will not provide information that a multimedia application can use to adapt its behavior to improve performance",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=778437,no,undetermined,0
An experiment to assess cost-benefits of inspection meetings and their alternatives: a pilot study,"We hypothesize that inspection meetings are far less effective than many people believe and that meetingless inspections are equally effective. However two of our previous industrial case studies contradict each other on this issue. Therefore, we are conducting a multi-trial, controlled experiment to assess the benefits of inspection meetings and to evaluate alternative procedures. The experiment manipulates four independent variables: the inspection method used (two methods involve meetings, one method does not); the requirements specification to be inspected (there are two); the inspection round (each team participates in two inspections); and the presentation order (either specification can be inspected first). For each experiment we measure 3 dependent variables: the individual fault detection rate; the team fault detection rate; and the percentage of faults originally discovered after the initial inspection phase (during which phase reviewers individually analyze the document). So far we have completed one run of the experiment with 21 graduate students in computer science at the University of Maryland as subjects, but we do not yet have enough data points to draw definite conclusions. Rather than presenting preliminary conclusions, we describe the experiment's design and the provocative hypotheses we are evaluating. We summarize our observations from the experiment's initial run, and discuss how we are using these observations to verify our data collection instruments and to refine future experimental runs",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=492447,no,undetermined,0
Parameter estimation of the manifold growth model using Z-graph,"The manifold growth model that unifies existing software reliability growth models can cover a wide range of accumulated fault data including various types of data which are difficult to treat with existing models. However, there are problems. For example, it sometimes takes a long time to solve transcendental equations to estimate parameters of the model, and the solution is difficult to obtain in some cases. This paper shows that the most important parameters of the differential equation that defines the manifold growth model can be easily estimated for the given data by using a â€œZ-graphâ€?to express the Z-equation derived from the differential equation in a 2-dimensional graph. This paper also shows that the most appropriate type of software reliability growth model, or the most appropriate shape of curve, can be determined simply by observing a part of the data sequence from Z-graph, since Z-graph makes it possible that the variation of the value of parameters at any time can be recognized visually. Finally, the effectiveness of the Z-graph is shown by applying the Z-graph to both ideal and actual data",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=492449,no,undetermined,0
An empirical study of the correlation between code coverage and reliability estimation,"Existing time-domain models for software reliability often result in an overestimation, of such reliability because they do not take the nature of testing techniques into account. Since every testing technique has a limit to its ability to reveal faults in a given system, as a technique approaches its saturation region fewer faults are discovered and reliability growth phenomena are predicted from the models. When the software is turned over to field operation, significant overestimates of reliability are observed. We present a technique to solve this problem by addressing both time and coverage measures for the prediction of software failures. Our technique uses coverage information collected during testing to extract only effective data from a given operational profile. Execution time between test cases which neither increase coverage nor cause a failure as reduced by a parameterized factor. Experiments using this technique were conducted on a program created in a simulation environment with simulated faults and on an industrial automatic flight control project which contained several natural faults. Results from both experiments indicate that overestimation of reliability is reduced significantly using our technique. This new approach not only helps reliability growth models make more accurate predictions, but also reveals the efficiency of a testing profile so that more effective testing techniques can be conducted",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=492450,no,undetermined,0
Reliability and risk analysis for software that must be safe,"Remaining failures, total failures, test time required to attain a given fraction of remaining failures, and time to next failure are useful reliability metrics for: providing confidence that the software has achieved reliability goals; rationalizing how long to test a piece of software; and analyzing the risk of not achieving remaining failure and time to next failure goals. Having predictions of the extent that the software is not fault free (remaining failures) and whether it is likely to survive a mission (time to next failure) provide criteria for assessing the risk of deploying the software. Furthermore, the fraction of remaining failures can be used as both a program quality goal in predicting test time requirements and, conversely as an indicator of program quality as a function of test time expended. We show how these software reliability predictions can increase confidence in the reliability of safety critical software such as the NASA Space Shuttle Primary Avionics Software",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=492451,no,undetermined,0
Tighter timing predictions by automatic detection and exploitation of value-dependent constraints,"Predicting the worst case execution time (WCET) of a real time program is a challenging task. Though much progress has been made in obtaining tighter timing predictions by using techniques that model the architectural features of a machine, significant overestimations of WCET can still occur. Even with perfect architectural modeling, dependencies on data values can constrain the outcome of conditional branches and the corresponding set of paths that can be taken in a program. While value-dependent constraint information has been used in the past by some timing analyzers, it has typically been specified manually, which is both tedious and error prone. The paper describes efficient techniques for automatically detecting value-dependent constraints by a compiler and automatically exploiting these constraints within a timing analyzer. The result is tighter timing analysis predictions without requiring additional interaction with a user",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=777663,no,undetermined,0
Assessing neural networks as guides for testing activities,"As test case automation increases, the volume of tests can become a problem. Further, it may not be immediately obvious whether the test generation tool generates effective test cases. Indeed, it might be useful to have a mechanism that is able to learn, based on past history, which test cases are likely to yield more failures versus those that are not likely to uncover any. We present experimental results on using a neural network for pruning a testcase set while preserving its effectiveness",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=492452,no,undetermined,0
On classes of problems in asynchronous distributed systems with process crashes,"This paper is on classes of problems encountered in asynchronous distributed systems in which processes can crash but links are reliable. The hardness of a problem is defined with respect to the difficulty to solve it despite failures: a problem is easy if it can be solved in presence of failures, otherwise it is hard. Three classes of problems are defined: F, NF and NFC. F is the class of easy problems, namely, those that can be solved in presence of failures (e.g., reliable broadcast). The class NF includes harder problems, namely, the ones that can be solved in a non-faulty system (e.g., consensus). The class NFC (NF-complete) is a subset of NF that includes the problems that are the most difficult to solve in presence of failures. It is shown that the terminating reliable broadcast problem, the non-blocking atomic commitment problem and the construction of a perfect failure detector (problem P) are equivalent problems and belong to NFC. Moreover the consensus problem is not in NFC. The paper presents a general reduction protocol that reduces any problem of NF to P. This shows that P is a problem that lies at the core of distributed fault-tolerance",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=776549,no,undetermined,0
Performance of quasi-exact cone-beam filtered backprojection algorithm for axially truncated helical data,"Concerns the problem of tomographic reconstruction from axially truncated cone-beam projections acquired with a helical vertex path. H. Kudo et al. (1998) developed a quasi-exact filtered backprojection algorithm for this problem. This paper evaluates the performance of the quasi-exact algorithm by comparing it with variants of the approximate Feldkamp (1984) algorithm. The results show that the quasi-exact algorithm possesses the following advantages over the Feldkamp algorithm. The first advantage is the improved image quality when the pitch of the helix is large. The second advantage is the use of a detector area close to the minimum detector area that is compatible with exact reconstruction. This area is shown to be much smaller than that required by the Feldkamp algorithm for typical geometries. One disadvantage of the quasi-exact algorithm is the computation time, which is much longer than for the Feldkamp algorithm",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=775587,no,undetermined,0
Analysis of a telecommuting experience: a case study,"The use of telecommuting by a large, Australian based, organization is discussed. Fifty employees and twelve managers were involved. These participants completed questionnaires evaluating the method of telecommuting from both telecommuters' and their managers' perspectives. The results indicated that since the introduction of telecommuting: (1) telecommuters' productivity had increased; (2) in general there was a positive attitude towards telecommuting and both the telecommuters and their managers were keen to recommend it to others; (3) an important link was revealed between the quality of supervision of telecommuters and the telecommuters' performance and attitudes. The strongest predictors of employee productivity were the quality of supervision that they had received and the number of days per week that they were telecommuting. The results suggested that there were certain problems relating to the effectiveness of communications, feelings of isolation and sense of loss of belonging to the organization. These problems were predominant in the work groups which included some telecommuters who were dissatisfied with the quality of supervision",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=493233,no,undetermined,0
Spatial metadata and GIS for decision support,"The proliferation of GIS technology has greatly increased the access to and the usage of spatial data. Making maps is relatively easy even for those who do not have much cartographic training. Nonetheless, the concerns for spatial data quality among GIS and spatial data users have just began to sprout partly because information about spatial data quality is not readily available or useful. Metadata, which refer to data describing data, include the quality and accuracy information of the data. The Federal Geographic Data Committee has proposed content standards of metadata for spatial databases. However, the standards are not adequate to document the spatial variation in data quality in geographic data. The paper argues that information about the quality of spatial data over a geographical area, which can be regarded as spatial metadata, should be derived and reported to help users of spatial data to make intelligent spatial decisions or policy formulations. While cartographers focus on the representation of spatial data quality, and statisticians emphasize the quantitative measures of data quality, this paper proposes that GIS are logical tools to assess certain types of error in spatial databases because GIS are widely used to gather, manipulate, analyze, and display spatial data. A framework is proposed to derive several types of data quality information using GIS. These types of quality information include positional accuracy, completeness, attribute accuracy, and to some extent logical consistency. However, not every type of spatial metadata can be derived from GIS",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=493251,no,undetermined,0
Experiences of software quality management using metrics through the life-cycle,"Many software quality metrics to objectively grasp software products and process have been proposed in the past decades. In actual projects, quality metrics has been widely applied to manage software quality. However, there are still several problems with providing effective feedback to intermediate software products and the software development process. We have proposed a software quality management using quality metrics which are easily and automatically measured. The purpose of this proposal is to establish a method for building in software quality by regularly measuring and reviewing. The paper outlines a model for building in software quality using quality metrics, and describes examples of its application to actual projects and its results. As the results, it was found that quality metrics can be used to detect and remove problems with process and products in each phase. Regular technical reviews using quality metrics and information on the change of the regularly measured results was also found to have a positive influence on the structure and module size of programs. Further, in the test phase, it was found that with the proposed model, the progress of corrective action could be quickly and accurately grasped",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=493414,no,undetermined,0
A prioritized handoff dynamic channel allocation strategy for PCS,"An analytical method is developed to calculate the blocking probability (p<sub>b</sub>), the probability of handoff failure (p<sub>h </sub>), the forced termination probability (p<sub>ft</sub>), and the probability that a call is not completed (p<sub>nc</sub>) for the no priority (NPS) and reserved channel (RCS) schemes for handoff, using fixed channel allocation (FCA) in a microcellular system. Based only on the knowledge of the new call arrival rate, a method of assessing the handoff arrival rate for any kind of traffic is derived. The analytical method is valid for uniform and nonuniform traffic distributions and is verified by computer simulations. An extension (generalization) to the nonuniform compact pattern allocation algorithm is presented as an application of this analysis. Based on this extended concept, a modified version of a dynamic channel allocation strategy (DCA) called compact pattern with maximized channel borrowing (CPMCB) is presented. With modifications, it is shown that CPMCB is a self-adaptive prioritized handoff DCA strategy with enhanced performance that can be exploited in a personal communications service (PCS) environment leading either to a reduction in infrastructure or to an increase in capacity and grade of service. The effect of user mobility on the grade of service is also considered using CPMCB",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=775369,no,undetermined,0
Design and evaluation of system-level checks for on-line control flow error detection,"This paper evaluates the concurrent error detection capabilities of system-level checks, using fault and error injection. The checks comprise application and system level mechanisms to detect control flow errors. We propose Enhanced Control-Flow Checking Using Assertions (ECCA). In ECCA, branch-free intervals (BFI) in a given high or intermediate level program are identified and the entry and exit points of the intervals are determined. BFls are then grouped into blocks, the size of which is determined through a performance/overhead analysis. The blocks are then fortified with preinserted assertions. For the high level ECCA, we describe an implementation of ECCA through a preprocessor that will automatically insert the necessary assertions into the program. Then, we describe the intermediate implementation possible through modifications made on gee to make it ECCA capable. The fault detection capabilities of the checks are evaluated both analytically and experimentally. Fault injection experiments are conducted using FERRARI to determine the fault coverage of the proposed techniques",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=774911,no,undetermined,0
Understanding and predicting the process of software maintenance releases,"One of the major concerns of any maintenance organization is to understand and estimate the cost of maintenance releases of software systems. Planning the next release so as to maximize the increase in functionality and the improvement in quality are vital to successful maintenance management. The objective of the paper is to present the results of a case study in which an incremental approach was used to better understand the effort distribution of releases and build a predictive effort model for software maintenance releases. The study was conducted in the Flight Dynamics Division (FDD) of NASA Goddard Space Flight Center (GSFC). The paper presents three main results: (1) a predictive effort model developed for the FDD's software maintenance release process, (2) measurement-based lessons learned about the maintenance process in the FDD, (3) a set of lessons learned about the establishment of a measurement-based software maintenance improvement program. In addition, this study provides insights and guidelines for obtaining similar results in other maintenance organizations",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=493441,no,undetermined,0
Reducing and estimating the cost of test coverage criteria,"Test coverage criteria define a set of entities of a program flowgraph and require that every entity is covered by some test. We first identify E<sub>c</sub>, the set of entities to be covered according to a criterion c, for a family of widely used test coverage criteria. We then present a method to derive a minimum set of entities, called a spanning set, such that a set of test paths covering the entities in this set covers every entity in E<sub>c</sub>. We provide a generalised algorithm, which is parametrized by the coverage criterion. We suggest several useful applications of spanning sets of entities to testing. In particular they help to reduce and to estimate the number of tests needed to satisfy test coverage criteria",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=493443,no,undetermined,0
Modeling software testing processes,"The production of a high quality software product requires application of both defect prevention and defect detection techniques. A common defect detection strategy is to subject the product to several phases of testing such as unit, integration, and system. These testing phases consume significant project resources and cycle time. As software companies continue to search for ways for reducing cycle time and development costs while increasing quality, software testing processes emerge as a prime target for investigation. The paper proposes the utilization of system dynamics models for better understanding testing processes. Motivation for modeling testing processes is presented along with a an executable model of the unit test phase. Some sample model runs are described to illustrate the usefulness of the model",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=493647,no,undetermined,0
Validation of ERS differential SAR interferometry for land subsidence mapping: the Bologna case study,"The city of Bologna, Italy, is ideal to assess the potential of ERS differential SAR interferometry for land subsidence mapping in urban areas for a couple of reasons: the subsiding area is large and presents important velocities of the vertical movements; there is a typical spatial gradient of the vertical movements; many ERS SAR frames are available; a large scientific community is involved in the study of subsidence; a large amount of levelling data is available. The authors analyze a time series of ERS-1/2 data from August 1992 to May 1996 and compare the subsidence maps derived from ERS SAR interferometry and levelling surveys. The authors conclude that for the mapping of land subsidence in urban environments ERS differential SAR interferometry is complementary to levelling surveys and GPS with regard to cost effectiveness, resolution and accuracy",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=774555,no,undetermined,0
Test and testability techniques for open defects in RAM address decoders,"It is a prevalent assumption that all RAM address decoder defects can be modelled as RAM array faults influencing one or more RAM cells. Therefore, can be implicitly detected by testing the RAM matrix with the march tests. Recently, we came across some failures in embedded SRAMs which were not detected by the march tests. The carried out analysis demonstrated the presence of open defects in address decoders that cannot be modelled as the conventional coupling faults, therefore, are not detected by the march tests. In this article, we present the test and testability strategies for such hard-to-detect open defects",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=494336,no,undetermined,0
Hardware/software partitioning using integer programming,"One of the key problems in hardware/software codesign is hardware/software partitioning. This paper describes a new approach to hardware/software partitioning using integer programming (IP). The advantage of using IP is that optimal results are calculated respective to the chosen objective function. The partitioning approach works fully automatic and supports multi-processor systems, interfacing and hardware sharing. In contrast to other approaches where special estimators are used, we use compilation and synthesis tools for cost estimation. The increased time for calculating the cost metrics is compensated by an improved quality of the estimations compared to the results of estimators. Therefore, fewer iteration steps of partitioning will be needed. The paper shows that using integer programming to solve the hardware/software partitioning problem is feasible and leads to promising results",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=494343,no,undetermined,0
A high performance image compression technique for multimedia applications,A novel image compression technique is presented for low cost multimedia applications. The technique is based on quadtree segmented two-dimensional predictive coding for exploiting correlation between adjacent image blocks and uniformity in variable block size image blocks. Low complexity visual pattern block truncation coding (VP-BTC) defined with a set of sixteen visual patterns is employed to code the high activity image blocks. Simulation results showed that the new technique achieved high performance with superior subjective quality at low bit rate,1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=494427,no,undetermined,0
"Model-integrated toolset for fault detection, isolation and recovery (FDIR)","Fault detection, isolation and recovery (FDIR) functions are essential components of complex engineering systems. The design, validation, implementation, deployment and maintenance of FDIR systems are extremely information intensive tests requiring in-depth knowledge of the engineering system. The paper gives an overview of a model-integrated toolset supporting FDIR from design, into post deployment. The toolset is used for development of large, complex spacecraft systems during the engineering design phase",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=494561,no,undetermined,0
Categorization of programs using neural networks,"The paper describes some experiments based on the use of neural networks for assistance in the quality assessment of programs, especially in connection with the reengineering of legacy systems. We use Kohonen networks, or self-organizing maps, for the categorization of programs: programs with similar features are grouped together in a two-dimensional neighbourhood, whereas dissimilar programs are located far apart. Backpropagation networks are used for generalization purposes: based on a set of example programs whose relevant aspects have already been assessed, we would like to obtain an extrapolation of these assessments to new programs. The basis for these investigation is an intermediate representation of programs in the form of various dependency graphs, capturing the essentials of the programs. Previously, a set of metrics has been developed to perform an assessment of programs on the basis of this intermediate representation. It is not always clear, however, which parameters of the intermediate representation are relevant for a particular metric. The categorization and generalization capabilities of neural networks are employed to improve or verify the selection of parameters, and might even initiate the development of additional metrics",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=494569,no,undetermined,0
Systems integration via software risk management,"This paper addresses an evolutionary process currently taking place in software engineering: the shift from hardware to software, where the role of software engineering is increasing and is becoming more central in systems integration. This shift also markedly affects the sources of risk that are introduced throughout the life cycle of a system's development-its requirements, specifications, architecture, process, testing, and end product. Risk is commonly defined as a measure of the probability and severity of adverse effects. Software technical risk is defined as a measure of the probability and severity of adverse effects inherent in the development of software. Consequently, risk assessment and management, as a process, will more and more assume the role of an overall cross-functional system integration agent. Evaluating the changes that ought to take place in response to this shift in the overall pattern leads to two challenges. One is the need to reassess the role of a new breed of software systems engineers/systems integrators. The other is the need to develop new and appropriate metrics for measuring software technical risk. Effective systems integration necessitates that all functions, aspects, and components of the system must be accounted for along with an assessment of most risks associated with the system. Furthermore, for software-intensive systems, systems integration is not only the integration of components, but is also; an understanding of the functionality that emerges from the integration. Indeed, when two or more software components are integrated, they often deliver more than the sum of what each was intended to deliver; this integration adds synergy and enhances functionality. In particular, the thesis advanced in this paper is that the process of risk assessment and management is an imperative requirement for successful systems integration; this is especially true for software-intensive systems. In addition, this paper advances the premise that the process of risk assessment and management is also the sine qua non requirement for ensuring against unwarranted time delay in a project's completion schedule, cost overrun, and failure to meet performance criteria. To achieve the aspired goal of systems integration a hierarchical holographic modeling (HHM) framework, which builds on previous works of the authors, has been developed. This HHM framework constitutes seven major considerations, perspectives, venues, or decompositions, each of which identifies the sources of risk in systems integration from a different, albeit with some overlap, viewpoint: software development, temporal perspective, leadership, the environment, the acquisition process, quality, and technology",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=531900,no,undetermined,0
ATM network design and optimization: a multirate loss network framework,"Asynchronous transfer mode (ATM) network design and optimization at the call-level may be formulated in the framework of multirate, circuit-switched, loss networks with effective bandwidth encapsulating cell-level behavior. Each service supported on the (wide area, broadband) ATM network is characterized by a rate or bandwidth requirement. Future networks will be characterized by links with very large capacities in circuits and by many rates. Various asymptotic results are given to reduce the attendant complexity of numerical calculations. A central element is a uniform asymptotic approximation (UAA) for link analyses. Moreover, a unified hybrid approach is given which allows asymptotic and nonasymptotic methods of calculations to be used cooperatively. Network loss probabilities are obtained by solving fixed-point equations. A canonical problem of route and logical network design is considered. An optimization procedure is proposed, which is guided by gradients obtained by solving a system of equations for implied costs. A novel application of the EM algorithm gives an efficient technique for calculating implied costs with changing traffic conditions. Finally, we report numerical results obtained by the software package TALISMAN, which incorporates the theoretical results. The network considered has eight nodes, 20 links, six services, and as many as 160 routes",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=532863,no,undetermined,0
More on the E-measure of subdomain testing strategies,"The expected number of failures detected (the E-measure) has been found to be a useful measure of the effectiveness of testing strategies. This paper takes a fresh perspective on the formulation of the E-measure. From this, we deduce new sufficient conditions for subdomain testing to be better than random testing. These conditions are simpler and more easily applicable than many of those previously found. Moreover, we obtain new characterisations of subdomain testing strategies in terms of the E-measure",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=534133,no,undetermined,0
Thermal imaging is the sole basis for repairing circuit cards in the F-16 flight control panel,"When the card-level tester for the F-16 flight control panel (FLCP) had been dysfunctional for over 18 months, infrared (IR) thermography was investigated as a viable alternative for diagnosing and repairing the 7 cards in the FLCP box. Using thermal imaging alone, over 20 FLCP boxes were made serviceable, thus bringing the FLCP out of Awaiting Parts (AWP) status. In the current environment of downsizing, the problem of returning the card level tester to functionality is remote. Success of the imager points to corresponding inadequacies of the Automatic Test Equipment to detect certain kinds of failure. In particular, we were informed that one particular relay had never been ordered in the life of the F-16 system, whereas some cards became functional when the relay was the sole component replaced. The incorporation of a novel on-the-fly neural network paradigm, the Neural Radiant Energy Detection System (NREDS) now has the capability to make correct assessments from a large history of repair data. By surveying the historical data, the network makes assessments about relevant repair actions and probable component malfunctions. On FLCP A-2 cards, a repair accuracy of 11 out of 12 was achieved during the first repair attempt",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=547734,no,undetermined,0
Accelerated aging of high voltage stator bars using a power electronic converter,"The use of computer-aided partial discharge detectors has become an increasingly useful tool in the application of electric machine insulation condition monitoring. The problem that exists is the lack of field and laboratory data associated with the faults that the condition monitoring aims to find. A stator winding aging bed has been constructed to allow the monitoring of the growth of induced faults, for comparison with field measurements. The use of a low emission, variable frequency power electronic converter, along with a computer-based discharge analyser, has allowed the study of supply frequency dependence of partial discharge activity along with the monitoring of fault initiation and development. The layout of the aging bed is discussed along with the operating performance of the variable frequency converter and partial discharge results gathered",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=549324,no,undetermined,0
Atomic data from plasma based measurements and compilations of transition probabilities,"Summary from only given. High efficiency electrical light sources used in lighting applications are based on electrical discharges in plasmas. The systematic search for improved lighting plasmas increasingly relies on plasma discharge modeling with computers and requires better and more comprehensive knowledge of basic atomic data such as radiative transition probabilities and collision cross sections. NIST has ongoing research programs aimed at the study of thermal equilibrium plasmas such as high pressure electric arcs and non-equilibrium plasmas in radio-frequency discharges and high current hollow cathode lamps. In emission experiments we have measured branching fractions and determined absolute transition probabilities for spectral lines in Ne I, Ne II, F I, O I and O II. In case of the Ne measurements the line radiation emitted by a high current hollow cathode lamp was analyzed with a 2 m monochromator. In addition, the spectra of Ne I and Ne II were measured with a UV Fourier transform spectrometer at very high spectral resolution. The line intensities were subsequently calibrated to absolute radiances. Where complete sets of transitions from an upper level could be observed, recent lifetime data for these levels were used to determine absolute transition probabilities. The accuracy of our branching fraction data is 5% for the stronger lines.",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=550273,no,undetermined,0
A microcomputer-based automatic testing system for digital circuits using signature analysis,"This paper proposes a microcomputer based automatic test equipment for digital circuits using signature analysis. The proposed system consists of a microcomputer equipped with a special interface card and controlled by a special software package. The system hardware is designed to transmit the test pattern generated by the microcomputer to the circuit under test (CUT), receive the response from the CUT, display the signatures and generate all necessary signals for the CUT. The software package is designed to control the system hardware. It provides the signatures of all modes simultaneously in one measurement and displays them on the screen of the microcomputer, compares them with those of the golden unit, and locates source faulty node(s). Examples for checking the capabilities of the proposed system to detect and locate faults in the CUT are illustrated",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=550977,no,undetermined,0
A proposed ATE for digital systems,"An ATE-system that can perform digital system testing to localize the faulty part is developed. The test system strategy is based on system and/or board partitioning and hierarchical testing. Testing takes place automatically by software programs within complete test packages including test models, test simulation, fault detection and fault diagnosis",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=551113,no,undetermined,0
Improving the effectiveness of software prefetching with adaptive executions,"The effectiveness of software prefetching for tolerating latency depends mainly on the ability of programmers and/or compilers to: 1) predict in advance the magnitude of the run-time remote memory latency, and 2) insert prefetches at a distance that minimizes stall time without causing cache pollution. Scalable heterogeneous multiprocessors, such as network of computers (NOWs), present special challenges to static software prefetching because on these systems the network topology and node configuration are not completely determined at compile time. Furthermore, dynamic software prefetching cannot do much better because individual nodes on heterogeneous large NOWs would tend to experience different remote memory delays over time. A fixed prefetch distance, even when computed at run-time, cannot perform well for the whole duration of a software pipeline. Here we present an adaptive scheme for software prefetching that makes it possible for nodes to dynamically change, not only the amount of prefetching, but the prefetch distance as well. Doing this makes it possible to tailor the execution of software pipeline to the prevailing conditions affecting each node. We show how simple performance data collected by hardware monitors can allow programs to observe, evaluate and change their prefetching policies. Our results show that on the benchmarks we simulated adaptive prefetching was capable of improving performance over static and dynamic prefetching by 10% to 60%. More important, future increases in the heterogeneity and size of NOWs will increase the advantages of adaptive prefetching over static and dynamic schemes",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=552556,no,undetermined,0
Editing for quality: how we're doing it,"Editing for quality (EFQ) is a process used by writers and editors at IBM's Santa Teresa Software Development laboratory to ensure information deliverables will meet customers' requirements and expectations. This editing process includes an overall appraisal of the information unit as a numeric score, or â€œEFQ indexâ€? An EFQ edit consists of a thorough technical edit providing detailed comments to the writer, a written report summarizing the strengths and weaknesses of the information unit in each of the eight categories, and the EFQ index, which is a relative score on a scale of 1 to 100. This paper describes the EFQ process, how it improves the quality of information, and how it is expected to predict customer satisfaction. It also discusses the benefits to editors and writers, summarizing interviews conducted with those who have used the EFQ process",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=552609,no,undetermined,0
Reducing system overheads in home-based software DSMs,"Software DSM systems suffer from the high communication and coherence-induced overheads that limit performance. This paper introduces our efforts in reducing system overheads of a home-based software DSM called JIAJIA. Three measures, including eliminating false sharing through avoiding unnecessarily invalidating cached pages, reducing virtual memory page faults with a new write detection scheme, and propagating barrier message in a hierarchical way, are taken to reduce the system overhead of JIAJIA. Evaluation with some well-known DSM benchmarks reveals that, though varying with memory reference patterns of different applications, these measures can reduce system overhead of JIAJIA effectively",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=760454,no,undetermined,0
A 'crystal ball' for software liability,"Software developers are living in a liability grace period, but it won't last. To adequately insure themselves against potential liability, developers need tools to identify worst-case scenarios and help them quantify the risks associated with a piece of software. For assessing such risks associated with software, the authors recommend fault injection, which provides worst-case predictions about how badly a piece of code might behave and how frequently it might behave that way. By contrast, software testing states how good software is. But even correct code can have ""bad days"", when external influences keep it from working as desired. Fault injection is arguably the next best thing to having a crystal ball, and it certainly beats facing the future with no predictions at all. It should be a regular part of risk assessment. The greatest benefit from fault injection occurs when a piece of software does not tolerate injected anomalies. False optimism gives way to the only honest claim-that the software presents risks.",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=587545,no,undetermined,0
A statistical approach to the inspection checklist formal synthesis and improvement,Proposes a statistical approach to the formal synthesis and improvement of inspection checklists. The approach is based on defect causal analysis and defect modeling. The defect model is developed using IBM's Orthogonal Defect Classification. A case study describes the steps required and a tool for the implementation. The advantages and disadvantages of both empirical and statistical methods are discussed and compared. It is suggested that a statistical approach should be used in conjunction with the empirical approach. The main advantage of the proposed technique is that it allows us to tune a checklist according to the most recent project experience and to identify optimal checklist items even when a source document does not exist,1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=553635,no,undetermined,0
Independent recovery in large-scale distributed systems,"In large systems, replication can become important means to improve data access times and availability. Existing recovery protocols, on the other hand, were proposed for small-scale distributed systems. Such protocols typically update stale, newly-recovered sites with replicated data and resolve the commit uncertainty of recovering sites. Thus, given that in large systems failures are more frequent and that data access times are costlier, such protocols can potentially introduce large overheads in large systems and must be avoided, if possible. We call these protocols dependent recovery protocols since they require a recovering site to consult with other sites. Independent recovery has been studied in the context of one-copy systems and has been proven unattainable. This paper offers independent recovery protocols for large-scale systems with replicated data. It shows how the protocols can be incorporated into several well-known replication protocols and proves that these protocols continue to ensure data consistency. The paper then addresses the issue of nonblocking atomic commitment. It presents mechanisms which can reduce the overhead of termination protocols and the probability of blocking. Finally, the performance impact of the proposed recovery protocols is studied through the use of simulation and analytical studies. The results of these studies show that the significant benefits of independent recovery can be enjoyed with a very small loss in data availability and a very small increase in the number of transaction abortions",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=553700,no,undetermined,0
Development and application of composite complexity models and a relative complexity metric in a software maintenance environment,"A great deal of effort is now being devoted to the study, analysis, prediction, and minimization of expected software maintenance cost, long before software is delivered to users or customers. It had been estimated that, on the average, the effort spent on software maintenance is as costly as the effort spent on all other software stages. Ways to alleviate software maintenance complexity and high costs should originate in software design. Two aspects of maintenance deserve attention: protocols for locating and rectifying defects and ensuring that no new defects are introduced in the development phase of the software process, and development of protocols for increasing the quality and reducing the costs associated with modification, enhancement, and upgrading of software. This article focuses on the second aspect and puts forward newly developed parsimonious models and a relative complexity metric for complexity measurement of software that were used to rank the modules in the system relative to each other. Significant success was achieved by use of the models and relative metric to identify maintenance-prone modules",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=554559,no,undetermined,0
New generation of real-time software-based video codec: Popular Video Coder II (PVC-II),"A new generation of real-time software-based video coder, the Popular Video Coder II (PVC-II), is presented.. The transform and the motion estimation parts are removed and the quantizer and the entropy coder are modified in the PVC-II as compared with the traditional video coder. Moreover, the PVC-II improves the coding performance of its previous version, the Popular Video Coder, by introducing several newly developed efficient coding techniques, such as the adaptive quantizer, the adaptive resolution reduction and the fixed-model intraframe DPCM, into the codec. The coding speed, compression ratio and picture quality of the PVC-II are good enough for applying it to various real-time multimedia applications. Since no compression hardware is needed for the PVC-II to encode and decode video data, the cost and complexity of developing multimedia applications, such as video phone and multimedia e-mail systems, can be largely reduced",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=555793,no,undetermined,0
Sensitivity of reliability-growth models to operational profile errors vs. testing accuracy [software testing],"This paper investigates: 1) the sensitivity of reliability-growth models to errors in the estimate of the operational profile (OP); and 2) the relation between this sensitivity and the testing accuracy for computer software. The investigation is based on the results of a case study in which several reliability-growth models are applied during the testing phase of a software system. The faults contained in the system are known in advance; this allows measurement of the software reliability-growth and comparison with the estimates provided by the models. Measurement and comparison are repeated for various OPs, thus giving information about the effect of a possible error in the estimate of the OP. The results show that: 1) the predictive accuracy of the models is not heavily affected by errors in the estimate of the OP; and 2) this relation depends on the accuracy with which the software system has been tested",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=556576,no,undetermined,0
Software radio-an alternative for the future in wireless personal and multimedia communications,"Software radio development aims at wideband RF access and software partitioning for plug-and-play type of use. The development is facilitated by progress in silicon capabilities, signal processing power of new and future processors and reconfiguration methods (software download, smart cards, etc.). However, there are still a lot of problems in implementing software radio. These problems include high quality wideband RF access and high performance analog to digital and digital to analog conversion due to the extreme demands placed upon them. Due to lack of suitable methods for performance evaluation it is also difficult to estimate the signal processing requirement for a software radio when implementing several systems in the same platform. There are also no methods for estimating the processing capacity of a software radio platform consisting of multiple DSP processors, CPUs, memory modules and low speed/high speed buses. On top of this there are no universal design tools that could be used all the way from system level specification to actual implementation but instead different CAD-tools with different kind of options must be used in the design process. Due to finite processing capacity, novel and computationally efficient DSP-architectures must be defined for critical functions to optimize the designs. This includes identification of common functionalities. These topics will addressed in the paper",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=759650,no,undetermined,0
A software defect report and tracking system in an intranet,"Describes a case study where SofTrack (a software defect reporting and tracking system) was implemented using Internet technology in a geographically distributed organization. Four medium- to large-sized information systems with different levels of maturity are being analysed within the scope of this project. They belong to the Portuguese Navy's Information Systems Infrastructure and were developed using a typical legacy systems technology: COBOL with embedded SQL for queries in a relational database environment. This empirical software engineering pilot project has allowed the development of several techniques to help software managers to better understand, control and ultimately improve the software process. Among them are: the introduction of automatic system documentation, module complexity assessment, and effort estimation for software maintenance activities in the organization",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=756700,no,undetermined,0
Analyze-NOW-an environment for collection and analysis of failures in a network of workstations,"This paper describes Analyze-NOW, an environment for the collection and analysis of failures/errors in a network of workstations. Descriptions cover the data collection methodology and the tool implemented to facilitate this process. Software tools used for analysis are described, with emphasis on the details of the implementation of the Analyzer, the primary analysis tool. Application of the tools is demonstrated by using them to collect and analyze failure data (for 32-week period) from a network of 69 SunOS-based workstations. Classification based on the source and effect of faults is used to identify problem areas. Different types of failures encountered on the machines and network are highlighted to develop a proper understanding of failures in a network environment. The results from the analysis tool should be used to pinpoint the problem areas in the network. The results obtained from using Analyze-NOW on failure data from the monitored network reveal some interesting behavior of the network. Nearly 70% of the failures were network-related, whereas disk errors were few. Network-related failures were 75% of all hard-failures (failures that make a workstation unusable). Half of the network-related failures were due to servers not responding to clients, and half were performance-related and others. Problem areas in the network were found using this tool. The authors' approach was compared to the method of using the network architecture to locate problem areas. This comparison showed that locating problem areas using network architecture over-estimates the number of problem areas",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=556579,no,undetermined,0
Application of a usage profile in software quality models,"Faults discovered by customers are an important aspect of software quality. The working hypothesis of this paper is that variables derived from an execution profile can be useful in software quality models. An execution profile of a software system consists of the probability of execution of each module during operations. Execution represents opportunities for customers to discover faults. However, an execution profile over an entire customer-base can be difficult to measure directly. Deployment records of past releases can be a valuable source of data for calculating an approximation to the probability of execution. We analyze a metric derived from deployment records which is a practical surrogate for an execution profile in the context of a software quality model. We define usage as the proportion of systems in the field which have a module deployed. We present a case study of a very large legacy telecommunications system. We developed models using a standard statistical technique to predict whether software modules will have any faults discovered by customers on systems in the field. Static software product metrics and usage were independent variables. The significance levels of variables in logistic regression models were analyzed, and models with and without usage as an independent variable were compared. The case study was empirical evidence that usage can be a significant contributor to a software quality model",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=756692,no,undetermined,0
A universal technique for accelerating simulation of scan test patterns,Scan test patterns are typically generated by ATPG tools which use a zero delay simulation model. These scan test patterns have to be verified using a golden simulator which is approved by the chip foundry with full timing before the patterns are accepted for manufacturing test. This can be very time consuming for many designs because of the size of the test data and the large number of test cycles which have to be simulated. A universal technique for accelerating scan test pattern simulation which can be used for any simulator with any scan cell type from any foundry is proposed. The extensions to test data languages to support universal acceleration of scan pattern simulation are also proposed. Some experiment results are also provided,1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=556955,no,undetermined,0
High resolution I<sub>DDQ</sub> characterization and testing-practical issues,"I<sub>DDQ</sub> testing has become an important contributor to quality improvement of CMOS ICs. This paper describes high resolution I <sub>DDQ</sub> characterization and testing (from the sub-nA to Î¼A level) and outlines test hardware and software issues. The physical basis of I<sub>DDQ</sub> is discussed. Methods for statistical analysis of I<sub>DDQ</sub> data are examined, as interpretation of the data is often as important as the measurement itself. Applications of these methods to set reasonable test limits for detecting defective product are demonstrated",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=556970,no,undetermined,0
Cost effective frequency measurement for production testing: new approaches on PLL testing,The growing market of consumer multimedia products demands cost effective test solutions for built-in video functions in core chipsets. This work describes a new software approach to perform precise frequency measurements on a digital test system,1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=557129,no,undetermined,0
Monitoring changes in system variables due to islanding condition and those due to disturbances at the utilities' network,"The continuous growth in embedded generation (EG) has emphasised the importance of being able to reliably detect loss of mains (LOM) especially when the EG is `islanded' with part of the utility network. Current methods of detection have not proved completely dependable, particularly when the `islanded' load's capacity matches that of the EG. This paper is the result of an investigation, using the Alternative Transients Program (ATP), into new methods of detecting LOM. To determine which system variables can be reliably used to detect LOM, it is necessary to accurately monitor all the relevant signals at the LOM relay location. This requires that the simulation realistically detects all the changes in system variables caused by disturbances on the network. In some cases, the results can vary markedly, depending on such factors as measurement technique and sampling rate. These problems can cause numerical instability leading to errors. The paper discusses these issues and the results obtained from ATP",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=756097,no,undetermined,0
Feasibility analysis of fault-tolerant real-time task sets,"Many safety critical real-time systems, employ fault tolerant strategies in order to provide predictable performance in the presence of failures. One technique commonly employed is time redundancy using retry/re-execution of tasks. This can in turn affect the correctness of the system by causing deadlines to be missed. This paper provides exact schedulability tests for fault tolerant task sets under specified failure hypothesis",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=557785,no,undetermined,0
Multi-domain surety modeling and analysis for high assurance systems,"Engineering systems are becoming increasingly complex as state of the art technologies are incorporated into designs. Surety modeling and analysis is an emerging science that permits an engineer to qualitatively and quantitatively predict and assess the completeness and predictability of a design. Surety is a term often used in the Department of Defense (DoD) and Department of Energy (DOE) communities, which refers to the integration of safety, security, reliability and performance aspects of design. Current risk assessment technologies for analyzing complex systems fail to adequately describe the problem, thus making assessment fragmented and non-integrated. To address this problem, we have developed a methodology and extensible software toolset to address model integration and complexity for high consequence systems. The MultiGraph Architecture (MGA) facilitates multi-domain, model-integrated modeling and analyses of complex, high-assurance systems. The MGA modeling environment allows the engineer to customize the modeling environment to match a design paradigm representative of the actual design. Previous modeling tools have a predefined model space that forces the modeler to work in less than optimal environments. Current approaches force the problem to be bounded and constrained by requirements of the modeling tool and not the actual design problem. In some small cases, this is only marginally adequate. The MGA facilitates the implementation of a surety methodology, which is used to represent high assurance systems with respect to safety and reliability. Formal mathematical models are used to correctly describe design safety and reliability functionality and behavior. The functional and behavioral representations of the design are then analyzed using commercial-off-the-shelf (COTS) tools",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=755889,no,undetermined,0
A hybrid genetic algorithm applied to automatic parallel controller code generation,"High performance real-time digital controllers employ parallel hardware such as transputers and digital signal processors to achieve short response times when this is not achievable with conventional uni-processor systems. Implementing such fine-grained parallel software is error-prone and difficult. We show how a hybrid genetic algorithm can be applied to automate this parallel code generation for a set of regular control problems such that significant speedup is obtained with few constraints on hardware. Genetic algorithms are particularly suited to this problem since the mapping problem is combinatorial in nature. However, one drawback of the genetic algorithm is that it is sensitive to small changes in the problem size. To overcome this problem the presented approach partitions the original problem into sub-problems, called boxes. The scheduling of these boxes is similar to the VLSI placement problem",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=557799,no,undetermined,0
Integrating metrics and models for software risk assessment,"Enhanced Measurement for Early Risk Assessment of Latent Defects (EMERALD) is a decision support system for assessing reliability risk. It is used by software developers and managers to improve telecommunications software service quality as perceived by the customer and the end user. Risk models are based on static characteristics of source code. This paper shows how a system such as EMERALD can enhance software development, testing, and maintenance by integration of: a software quality improvement strategy; measurements and models; and delivery of results to the desktop of developers in a timely manner. This paper also summarizes empirical experiments with EMERALD's models using data from large industrial telecommunications software systems. EMERALD has been applied to a very large system with over 12 million lines of source code within procedures. Experience and lessons learned are also discussed",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=558707,no,undetermined,0
Safety and reliability driven task allocation in distributed systems,"Distributed computer systems are increasingly being employed for critical applications, such as aircraft control, industrial process control, and banking systems. Maximizing performance has been the conventional objective in the allocation of tasks for such systems. Inherently, distributed systems are more complex than centralized systems. The added complexity could increase the potential for system failures. Some work has been done in the past in allocating tasks to distributed systems, considering reliability as the objective function to be maximized. Reliability is defined to be the probability that none of the system components falls while processing. This, however, does not give any guarantees as to the behavior of the system when a failure occurs. A failure, not detected immediately, could lead to a catastrophe. Such systems are unsafe. In this paper, we describe a method to determine an allocation that introduces safety into a heterogeneous distributed system and at the same time attempts to maximize its reliability. First, we devise a new heuristic, based on the concept of clustering, to allocate tasks for maximizing reliability. We show that for task graphs with precedence constraints, our heuristic performs better than previously proposed heuristics. Next, by applying the concept of task-based fault tolerance, which we have previously proposed, we add extra assertion tasks to the system to make it safe. We present a new heuristic that does this in such a way that the decrease in reliability for the added safety is minimized. For the purpose of allocating the extra tasks, this heuristic performs as well as previously known methods and runs an order of magnitude faster. We present a number of simulation results to prove the efficacy of our scheme",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=755824,no,undetermined,0
Software metrics knowledge and databases for project management,"The construction and maintenance of large, high-quality software projects is a complex, error-prone and difficult process. Tools employing software database metrics can play an important role in the efficient execution and management of such large projects. In this paper, we present a generic framework to address this problem. This framework incorporates database and knowledge-base tools, a formal set of software test and evaluation metrics, and a suite of advanced analytic techniques for extracting information and knowledge from available data. The proposed combination of critical metrics and analytic tools can enable highly efficient and cost-effective management of large and complex software projects. The framework has the potential for greatly reducing venture risks and enhancing production quality in the domain of large-scale software project management",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=755633,no,undetermined,0
Rule-induction and case-based reasoning: hybrid architectures appear advantageous,"Researchers have embraced a variety of machine learning (ML) techniques in their efforts to improve the quality of learning programs. The recent evolution of hybrid architectures for machine learning systems has resulted in several approaches that combine rule induction methods with case-based reasoning techniques to engender performance improvements over more traditional single-representation architectures. We briefly survey several major rule-induction and case-based reasoning ML systems. We then examine some interesting hybrid combinations of these systems and explain their strengths and weaknesses as learning systems. We present a balanced approach to constructing a hybrid architecture, along with arguments in favor of this balance and mechanisms for achieving a proper balance. Finally, we present some initial empirical results from testing our ideas and draw some conclusions based on those results",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=755625,no,undetermined,0
Using the genetic algorithm to build optimal neural networks for fault-prone module detection,"The genetic algorithm is applied to developing optimal or near optimal backpropagation neural networks for fault-prone/not-fault-prone classification of software modules. The algorithm considers each network in a population of neural networks as a potential solution to the optimal classification problem. Variables governing the learning and other parameters and network architecture are represented as substrings (genes) in a machine-level bit string (chromosome). When the population undergoes simulated evolution using genetic operators-selection based on a fitness function, crossover, and mutation-the average performance increases in successive generations. We found that, on the same data, compared with the best manually developed networks, evolved networks produced improved classifications in considerably less time, with no human effort, and with greater confidence in their optimality or near optimality. Strategies for devising a fitness function specific to the problem are explored and discussed",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=558759,no,undetermined,0
Neuron and dendrite pruning by synaptic weight shifting in polynomial time,"There is a lot of redundant information in ANNs. Therefore, pruning algorithms are required in order to save the computational cost and reduce the complexity of the system. A weight shifting technique which was proposed for increasing the fault tolerance of the neural networks is applied to prune links and/or neurons of the networks. After the training converges, each hidden neuron that has less effect on the performance is removed and their weights are shifted to other links by weight shifting technique. The weights of removed links are still in the network only with other links of the same neuron. The experimental result shows that 5%-45% of links can be removed and the pruned network still gives the performance at the same level as the unpruned network. This technique does not require the retraining process, modification of the error cost function, or computational overhead. The time complexities of the link and neuron prunings are O(n<sup>2</sup>) and O(m), respectively, where n is the number of links and m is the number of neurons in the network",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=549003,no,undetermined,0
Using neural networks to solve testing problems,"This paper discusses using neural networks for diagnosing circuit faults. As a circuit is tested, the output signals from a Unit Under Test can vary as different functions are invoked by the test. When plotted against time, these signals create a characteristic trace for the test performed. Sensors in the ATS can be used to monitor the output signals during test execution. Using such an approach, defective components can be classified using a neural network according to the pattern of variation from that exhibited by a known good card. This provides a means to develop testing strategies for circuits based upon observed performance rather than domain expertise. Such capability is particularly important with systems whose performance, especially under faulty conditions, is not well documented or where suitable domain knowledge and experience does not exist. Thus, neural network solutions may in some application areas exhibit better performance than either conventional algorithms or knowledge-based systems. They may also be retrained periodically as a background function, resulting with the network gaining accuracy over time",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=547716,no,undetermined,0
Performance evaluation for distributed system components,"The performance evaluation of hardware and software system components is based on statistics that are long views on the behavior of these components. Since system resources may have unexpected behavior, relevant current information becomes useful in the management process of these systems, especially for data gathering, reconfiguration, and fault detection activities. Actually, there are few criteria to property evaluate the current availability of component services within distributed systems. Hence, the management system can not realistically select the most suitable decision for configuration. In this paper, we present a proposal for a continuous evaluation of component behaviour related to state changes. This model is further extended by considering different categories of events concerning the degradation of the operational state or usage state. Our proposals are based on the possibility of computing at the component level, the current availability of this component by continuous evaluation. we introduce a several current availability features and propose formula to compute them. Other events concerning a managed object are classified as warning, critical or outstanding, which leads to a more accurate operational view on a component. Several counter-based events are thresholded to improve predictable reconfiguration decisions concerning the usability of a component. The main goal is to offer to the management system current relevant information which can be used within management policies the flexible polling frequency tuned with respect to the current evaluation, or particular aspects related to dynamic tests within distributed systems. Implementation issues with respect to the standard recommendations within distributed systems are presented. Finally we describe how the reconfiguration management systems can use these features in order to monitor, predict, improve the existing configuration, or accommodate the polling frequency according to several simple criteria",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=534143,no,undetermined,0
The Neural Engineering of ATE,"This paper describes a neural network-based software configuration implemented on a VXI automatic test equipment platform. The purpose of this unique software configuration, incorporating neural network and other artificial intelligence (AI) technologies, is to enhance ATE capability end efficiency by providing an intelligent interface for a variety of functions that are controlled or monitored by the software. This includes automated end user-directed control of the ATE end diagnostic strategy to streamline test sequences through the use of advanced diagnostic strategies. The use of Neural Engineering techniques are stressed which, in this context, foster the integration of diverse sensor technology capable of analyzing units under test (UUT) from different perspectives that provide new insight into static, dynamic, and historical UUT performance. Such methods can achieve greater accuracy in failure diagnosis and fault prediction; reduction in cannot duplicate (CND), retest-OK (RTOK) rates, and ambiguity group size; and improved confidence in performance testing that results in the determination of UUT ready for issue status. The hardware configuration of the ATE consists of an embedded 486 100 MHz PC controller and an instrument suite as follows: Power Supply, DMM, Digitizer, Counter/Timer, Digital I/O, Pulse Generator, Switching Matrix, Relay, Function Generator and Arbitrary Function Generator",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=547712,no,undetermined,0
An approach to selecting metrics for detecting performance problems in information systems,"Early detection of performance problems is essential to limit their scope and impact. Most commonly, performance problems are detected by applying threshold tests to a set of detection metrics. For example, suppose that disk utilization is a detection metric, and its threshold value is 80%. Then, an alarm is raised if disk utilization exceeds 80%. Unfortunately, the ad hoc manner in which detection metrics are selected often results in false alarms and/or failing to detect problems until serious performance degradations result. To address this situation, we construct rules for metric selection based on analytic comparisons of statistical power equations for five widely used metrics: departure counts (D), number in system (L), response times (R), service times (S), and utilizations (U). These rules are assessed in the context of performance problems in the CPU and paging sub-systems of a production computer system",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=534144,no,undetermined,0
Compiler-assisted generation of error-detecting parallel programs,"We have developed an automated a compile time approach to generating error-detecting parallel programs. The compiler is used to identify statements implementing affine transformations within the program and to automatically insert code for computing, manipulating, and comparing checksums in order to detect data errors at runtime. Statements which do not implement affine transformations are checked by duplication. Checksums are reused from one loop to the next if this is possible, rather than recomputing checksums for every statement. A global dataflow analysis is performed in order to determine points at which checksums need to be recomputed. We also use a novel method of specifying the data distributions of the check data using data distribution directives so that the computations on the original data, and the corresponding check computations are performed on different processors. Results on the time overhead and error coverage of the error detecting parallel programs over the original programs are presented on an Intel Paragon distributed memory multicomputer",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=534621,no,undetermined,0
Determination of functional domains for use with functional size measurement-opportunities to classify software from a business perspective,"Functional size measurement (FSM) is the focus of ISO/IEC Project 14143. This project has five parts, ranging from the recently published Part 1 (â€œConcepts of FSMâ€? to Part 5 (â€œDetermination of Functional Domains for Use with FSMâ€?, which is in development as a Technical Report Type 2 (TR2). This paper outlines the basic principles of FSM and why the functional domain concept has been seen as important to the overall project success. It covers the rationale behind this sub-project, its relevance to the software engineering world and how the functional domain topic extends far beyond the realm of simple FSM. The paper profiles the state of the software industry today and how the classifications of software in modern literature are insufficient for the needs of FSM. The current state of this TR2-in-progress is also presented, together with the various opinions of the international community involved in its development",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=766598,no,undetermined,0
Using neural networks to predict software faults during testing,"This paper investigates the application of principal components analysis to neural-network modeling. The goal is to predict the number of faults. Ten software product measures were gathered from a large commercial software system. Principal components were then extracted from these measures. We trained two neural networks, one with the observed (raw) data, and one with principal components. We compare the predictive quality of the two competing models using data collected from two similar systems. These systems were developed by the same organization, and used the same development process. For the environment we studied, applying principal-components analysis to the raw data yields a neural-network model whose predictive quality is statistically better than a neural-network model developed using the raw data alone. The improvement in model predictive quality is appreciable from a practitioner's point of view. We concur with published literature regarding the number of hidden layers needed in a neural-network model. A single hidden layer of neurons yielded a network of sufficient generality to be useful when predicting faults. This is important, because networks with more hidden layers take correspondingly more time to train. The application of alternative network architectures and training algorithms in software engineering should continue to be investigated",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=537016,no,undetermined,0
Generalized linear models in software reliability: parametric and semi-parametric approaches,"The penalized likelihood method is used for a new semi-parametric software reliability model. This new model is a nonparametric generalization of all parametric models where the failure intensity function depends only on the number of observed failures, viz. number-of-failures models (NF). Experimental results show that the semi-parametric model generally fits better and has better 1-step predictive quality than parametric NF. Using generalized linear models, this paper presents new parametric models (polynomial models) that have performances (deviance and predictive-qualities) approaching those of the semi-parametric model. Graphical and statistical techniques are used to choose the appropriate polynomial model for each data-set. The polynomial models are a very good compromise between the nonvalidity of the simple assumptions of classical NF, and the complexity of use and interpretation of the semi-parametric model. The latter represents a reference model that we approach by choosing adequate link and regression functions for the polynomial models",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=537017,no,undetermined,0
Computer-aided 3D tolerance analysis of disk drives,"Disk drives are multicomponent products in which product build variations directly affect quality. Dimensional management, an engineering methodology combined with software tools, was implemented in disk drive development engineering at IBM in San Jose to predict and optimize critical parameters in disk drives. It applies statistical simulation techniques to predict the amount of variation that can occur in the disk drive due to the specified design tolerances, fixturing tolerances, and assembly variations. This paper presents statistics describing the measurement values produced during simulations, a histogram showing the measurement values graphically, and an analysis of the process capability, C<inf>pk</inf> to ensure robust designs. Additionally, it describes how modeling can determine the location(s) of the predicted variation, the contributing factors, and their percent of contribution. Although a complete 2.5-in. disk drive was modeled and all critical variations such as suspension-to-disk gaps, disk-stack envelope, and merge clearances were analyzed, this paper presents for illustration only one critical disk real estate parameter. The example shows the capability of this methodology. VSAÂ®-3D software by Variation Systems Analysis was used.",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5389399,no,undetermined,0
ORCHESTRA: a probing and fault injection environment for testing protocol implementations,"Ensuring that a distributed system meets its prescribed specification is a growing challenge that confronts software developers and system engineers. Meeting this challenge is particularly important for applications with strict dependability and/or timeliness constraints. We have developed a software fault injection tool, called ORCHESTRA, for testing dependability and timing properties of distributed protocols. ORCHESTRA is based on a simple yet powerful framework, called script-driven probing and fault injection. The emphasis of this approach is on experimental techniques intended to identify specific â€œproblemsâ€?in a protocol or its implementation rather than the evaluation of system dependability through statistical metrics such as fault coverage. Hence, the focus is on developing fault injection techniques that can be employed in studying three aspects of a target protocol: i) detecting design or implementation errors, ii) identifying violations of protocol specifications, and iii) obtaining insights into the design decisions made by the implementers",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=540200,no,undetermined,0
SHARPE: a modeler's toolkit,"SHARPE (Symbolic Hierarchical Automated Reliability and Performance Evaluator) is a program that supports the specification and automated solution of reliability and performance models. It contains support for fault trees, reliability block diagrams, reliability graphs, Markov and semi-Markov chains, generalized stochastic Petri nets, product-form queueing networks, and acyclic task graphs. These model types can be used separately or in hierarchical combination. SHARPE allows users complete freedom to choose model types, use results from models of any type as parameters for other models of any type and choose from among alternate algorithms for model solution. We present an example of how SHARPE can be used to analyze a hierarchical performability model",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=540202,no,undetermined,0
On-line recovery for rediscovered software problems,"This paper discusses a method that can allow a system to avoid or recover from the exercise of certain known faults at runtime and thus make certain software upgrades unnecessary. The method uses the knowledge of the characteristic symptoms of a software fault and the appropriate recovery action for the fault to detect and recover from the future exercise of the fault. An analysis of field data shows that the method is applicable to about 25% of faults and there is a potential for this number to go up. Using the method can not only improve the availability of user applications, but can also reduce the number of rediscovered problems and hence reduce the resources required for software service",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=540209,no,undetermined,0
Efficient multi-field packet classification for QoS purposes,"Mechanisms for service differentiation in datagram networks, such as the Internet, rely on packet classification in routers to provide appropriate service. Classification involves matching multiple packet header fields against a possibly large set of filters identifying the different service classes. In this paper, we describe a packet classifier based on tries and binomial trees and we investigate its scaling properties in three QoS scenarios that are likely to occur in the Internet. One scenario is based on integrated services and RSVP and the other two are based on differentiated services. By performing a series of tests, we characterize the processing and memory requirements for a software implementation of our classifier. Evaluation is done using real data sets taken from two existing high-speed networks. Results from the IntServ/RSVP tests on a Pentium 200 MHz show that it takes about 10.5 Î¼s per packet and requires 2000 KBytes of memory to classify among 11000 entries. Classification for a virtual leased line service based on DiffServ with the same number of entries takes about 9 Î¼s per packet and uses less than 250 KBytes of memory. With an average packet size of 2000 bits, our classifier can manage data rates of about 200 Mbit/s on a 200 MHz Pentium. We conclude that multi-field classification is feasible in software and that high-performance classifiers can run on low-cost hardware",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=766484,no,undetermined,0
A non-homogeneous Markov software reliability model with imperfect repair,"This paper reviews existing non-homogeneous Poisson process (NHPP) software reliability models and their limitations, and proposes a more powerful non-homogeneous Markov model for the fault detection/removal problem. In addition, this non-homogeneous Markov model allows for the possibility of a finite time to repair a fault and for imperfections in the repair process. The proposed scheme provides the basis for decision making both during the testing and the operational phase of the software product. Software behavior in the operational phase and the development test phase are related and the release time formulae are derived. Illustrations of the proposed model are provided",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=540227,no,undetermined,0
Evaluation of integrated system-level checks for on-line error detection,"This paper evaluates the capabilities of an integrated system level error detection technique using fault and error injection. This technique is comprised of two software level mechanisms for concurrent error detection, control flow checking using assertions (CCA) and data error checking using application specific data checks. Over 300,000 faults and errors were injected and the analysis of the results reveals that the CCA detects 95% of all the errors while the data checks are able to detect subtle errors that go undetected by the CCA technique. Latency measurements also shelved that the CCA technique is faster than the data checks in detecting the error. When both techniques were incorporated, the system was able to detect over 98% of all injected errors",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=540230,no,undetermined,0
Detecting failed processes using fault signatures,"A strategy is presented for automatically identifying processes that have failed. A determination is made of the types of data that need to be collected, and circumstances under which the approach is likely to be useful. The strategy is applied to generate signatures for three different types of workloads, and several different resources. A typical failure is injected into the process, and the associated signatures are presented for the same workloads and resources. A measure as defined that is used to determine whether or not a signature is likely to be indicative of a faulty process",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=540231,no,undetermined,0
Evaluation of differentiated services using an implementation under Linux,"Current efforts to provide distinct levels of quality-of-service in the Internet are concentrated on the differentiated services (DS) approach. In order to investigate the gain for users of those differentiated services, early experiences with implementations with respect to real applications are needed. Simulation models are often not sufficient if a judgement of the behavior under realistic traffic scenarios is desired. Because implementing new functionality into dedicated router hardware is difficult and time-consuming, we focused on a software implementation for standard PC hardware. In this paper we present an implementation of differentiated services functions for a PC-based router running under the Linux operating system. Two per-hop forwarding behaviors for assured service and premium service were realized. Components for traffic conditioning such as traffic meter, token bucket, leaky bucket and traffic shaper were implemented as well as an efficient traffic classificator and queueing disciplines. We describe the design and implementation issues of these components, which were validated in detail by measurements. Evaluation of these measurements shows that the proposed forwarding behaviors work well for boundary and interior routers. But, it also becomes apparent that standard applications using short-lived TCP connections cannot always exploit the requested service completely whereas rate-controlled sending applications are able to take full advantage of it. Furthermore, it is planned to release the implementation to the public for research purposes",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=766483,no,undetermined,0
Lag-indexed VQ for pitch filter coding,"The presence of a pitch predictor is critical to low-rate performance of CELP coders. Unfortunately the rate required for high-quality encoding of pitch filter parameters is often a large fraction of the available bandwidth. Moreover, the application of analysis-by-synthesis techniques to pitch filter optimization can require excessive computation. We present a vector quantization (VQ) approach for coding pitch filter parameters which maintains a subjective quality equivalent to other coding schemes while requiring lower (variable) rate with less closed-loop computation than other VQ techniques",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=540334,no,undetermined,0
Algorithms for the generation of state-level representations of stochastic activity networks with general reward structures,"Stochastic Petri nets (SPNs) and extensions are a popular method for evaluating a wide variety of systems. In most cases, their numerical solution requires generating a state-level stochastic process, which captures the behavior of the SPN with respect to a set of specified performance measures. These measures are commonly defined at the net level by means of a reward variable. In this paper, we discuss issues regarding the generation of state-level reward models for systems specified as stochastic activity networks (SANs) with â€œstep-based reward structuresâ€? Step-based reward structures are a generalization of previously proposed reward structures for SPNs and can represent all reward variables that can be defined on the marking behavior of a net. While discussing issues related to the generation of the underlying state-level reward model, we provide an algorithm to determine whether a given SAN is â€œwell-specifiedâ€?A SAN is well-specified if choices about which instantaneous activity completes among multiple simultaneously-enabled instantaneous activities do not matter, with respect to the probability of reaching next possible stable markings and the distribution of reward obtained upon completion of a timed activity. The fact that a SAN is well specified is both a necessary and sufficient condition for its behavior to be completely probabilistically specified, and hence is an important property to determine",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=541432,no,undetermined,0
Combining periodic and probabilistic checkpointing in optimistic simulation,"This paper presents a checkpointing scheme for optimistic simulation which is a mixed approach between periodic and probabilistic checkpointing. The latter based on statistical data collected during the simulation, aims at recording as checkpoints states of a logical process that have high probability to be restored due to rollback (this is done in order to make those states immediately available). The periodic part prevents performance degradation due to state reconstruction (coasting forward) cost whenever the collected statistics do not allow to identify states highly likely to be restored. Hence, this scheme can be seen as a highly general solution to tackle the checkpoint problem in optimistic simulation. A performance comparison with previous solutions is carried out through a simulation study of a store-and-forward communication network in a two-dimensional torus topology",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=766167,no,undetermined,0
A multi-rate resource control (MRRC) scheme for wireless/mobile networks,"In this paper, we propose and study a multi-rate resource control (MRRC) scheme for non-real time data applications in mobile cellular networks. The MRRC scheme adapts to dynamically changing system load by adjusting resource assignments for mobiles. The MRRC scheme consists of four major techniques: 1) variable-rate admission control policy; 2) soft handoff procedure; 3) resource assignment prioritization based on mobile's location, non-handoff versus handoff zone; 4) handoff prioritization by queuing handoff requests. We evaluate the blocking probability and the forced termination probability. In addition, we study the effect of handoff to cell area ratio (HTCR) on MRRC performance. A streamlined analytical model is introduced to evaluate the MRRC scheme for the 2-cells case. Numerical results are presented for the 2-cells case; simulation results are presented for the 10-cells case. Our results show that the MRRC system achieves significant gain over the conventional single rate system in terms of the blocking probability and the forced termination probability",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=765479,no,undetermined,0
Software process improvement at Raytheon,"Over the last eight years, Raytheon mounted a sustained effort to improve its software development processes. The author describes the organizational structure and activities that made these processes more productive and predictable while raising software quality",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=542292,no,undetermined,0
Video compression with random neural networks,"We summarize a novel neural network technique for video compression, using a â€œpoint-processâ€?type neural network model we have developed, which is closer to biophysical reality and is mathematically much more tractable than standard models. Our algorithm uses an adaptive approach based upon the users' desired video quality Q, and achieves compression ratios of up to 500:1 for moving gray-scale images, based on a combination of motion detection, compression and temporal subsampling of frames. This leads to a compression ratio of over 1000:1 for full-color video sequences with the addition of the standard 4:1:1 spatial subsampling ratios in the chrominance images. The signal-to-noise-ratio obtained varies with the compression level and ranges from 29 dB to over 34 dB. Our method is computationally fast so that compression and decompression could possibly be performed in real-time software",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=542792,no,undetermined,0
Cooperative congestion control schemes in ATM networks,"One of the basic problems faced in the design of efficient traffic and congestion control schemes is related to the wide variety of services with different traffic characteristics and quality of service (QoS) requirements supported by ATM networks. The authors propose a new way of organizing the control system so that complexity is easier to manage. The multi-agent system approach, which provides the use of adaptative and intelligent agents, is investigated. The authors show, through the two congestion control schemes proposed, how to take advantage of using intelligent agents to increase the efficiency of the control scheme. First, TRAC (threshold based algorithm for control) is proposed, which is based on the use of fixed thresholds which enables the anticipation of congestion. This mechanism is compared with the push-out algorithm and it is shown that the authors' proposal improves the network performance. Also discussed is the necessity of taking into account the network dynamics. In TRAC, adaptative agents with learning capabilities are used to tune the values of the thresholds according to the status of the system. However, in this scheme, when congestion occurs, the actions we perform are independent of the nature of the traffic. Subsequently, we propose PATRAC (predictive agents in a threshold based algorithm for control) in which different actions are achieved according to the QoS requirements and to the prediction of traffic made by the agents. Specifically, re-routing is performed when congestion is heavy or is expected to be heavy and the traffic is cell loss sensitive. This re-routing has to deflect the traffic away from the congestion point. In this scheme, we propose a cooperative and predictive control scheme provided by a multi-agent system that is built in to each node",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=544199,no,undetermined,0
On modeling concurrent heavy-tailed network traffic sources and its impact upon QoS,"Studies of local area network and wide area network traffic have provided much evidence of what has been called â€œself-similarâ€? â€œlong-range dependentâ€? â€œfractalâ€? â€œchaoticâ€? â€œheavy-tailâ€? or â€œpower-tailâ€?(PT) network traffic. These studies have indicated that conventional traffic models (e.g. Poisson) generate overly optimistic performance measurements. In this paper we present an interarrival process which extends previous work in this area regarding a heavy-tailed ON/OFF source model described in Fiorini and Lipsky (1998). Extending their work, we construct a heavy-tailed ON/OFF traffic model which attempts to characterize traffic generated from concurrent sources. The primary application of this process is to model, say, traffic generated by (World Wide Web) WWW servers and/or, in general, Ethernet traffic. Characterizing and modeling the behavior are important for accessing quality of service (QoS) given much empirical evidence of heavy-tailed phenomena in network teletraffic. As a result, we derive an approximation for computing the â€œcritical pointsâ€?(e.g. system utilization beyond which performance rapidly degrades), and a means to compute the cell loss probabilities for, say, WWW gateways and/or routers",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=765368,no,undetermined,0
Implementing fail-silent nodes for distributed systems,"A fail-silent node is a self-checking node that either functions correctly or stops functioning after an internal failure is detected. Such a node can be constructed from a number of conventional processors. In a software-implemented fail-silent node, the nonfaulty processors of the node need to execute message order and comparison protocols to â€œkeep in stepâ€?and check each other, respectively. In this paper, the design and implementation of efficient protocols for a two processor fail-silent node are described in detail. The performance figures obtained indicate that in a wide class of applications requiring a high degree of fault tolerance, software-implemented fail-silent nodes constructed simply by utilizing standard â€œoff-the-shelfâ€?components are an attractive alternative to their hardware-implemented counterparts that do require special-purpose hardware components, such as fault-tolerant clocks, comparator, and bus interface circuits",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=544479,no,undetermined,0
Control charts for random and fixed components of variation in the case of fixed wafer locations and measurement positions,"In such processes as wafer-grinding and LPCVD, the variation within a group of measurements is traceable to various causes, and therefore, needs to be decomposed into relevant components of variation for an effective equipment monitoring and diagnosis. In this article, an LPCVD process is considered as an example, and control charting methods are developed for monitoring various fixed and random components of variation separately. For this purpose, the structure of measurement data (e.g., thickness) is described by a split-unit model in which two different sizes of experimental units (i.e., the wafer location as a whole unit and the measurement position as a split unit) are explicitly recognized. Then, control charts are developed for detecting unusual differences among fixed wafer locations within a batch and fixed measurement positions within a wafer. Control charts for the overall process average and random error variations are also developed, and the proposed method is illustrated with an example",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=762880,no,undetermined,0
On static compaction of test sequences for synchronous sequential circuits,"We propose three static compaction techniques for test sequences of synchronous sequential circuits. We apply the proposed techniques to test sequences generated for benchmark circuits by various test generation procedures. The results show that the test sequences generated by all the test generation procedures considered can be significantly compacted. The compacted sequences thus have shorter test application times and smaller memory requirements. As a by product, the fault coverage is sometimes increased as well. More importantly, the ability to significantly reduce the length of the test sequences indicates that it may be possible to reduce test generation time if superfluous input vectors are not generated",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=545575,no,undetermined,0
N-version programming: a unified modeling approach,"This paper presents an unified approach aimed at modeling the joint behavior of the N version system and its operational environment. Our objective is to develop reliability model that considers both functional and performance requirements which is particularly important for real-time applications. The model is constructed in two steps. First, the Markov model of N version failure and execution behavior is developed. Next, we develop the user-oriented model of the operational environment. In accounting for dependence we use the idea that the influence of the operational environment on versions failures and execution times induces correlation. The model addresses a number of basic issues and yet yields closed-form solutions that provide considerable insight into how reliability is affected by both versions characteristics and the operational environment",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=546459,no,undetermined,0
Measurement of power quality factors in electrical networks,"Summary form only given, as follows.Summary form only given. A microprocessor based system that correctly estimates asymmetry and harmonics rating is presented. The measurement method, the measuring channel structure, the DSP algorithm and software structure are described.",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=547408,no,undetermined,0
Data collection and recording guidelines for achieving intelligent diagnostics,"Whether using human or machine intelligence, the best decisions are made when using all available information from all relevant sources. In contrast to traditional automatic test equipment (ATE) programming techniques, which stop on failures to perform a diagnosis, we would collect from the entire sequence of electronic tests, and integrate these data with circuit topology and external data (including thermal imaging) to obtain a best informed diagnosis. Novel on-the-fly neural network paradigms provide the capability to make correct assessments from a large history of repair data. Applications with these paradigms require a computer with sufficient horsepower (such as a PC) and high-level programming languages. The circuit topology assessment program can identify â€œdeadâ€?nodes through an entropy calculation, making it possible to perform circuit diagnosis in the absence of good/bad historical information. In theory, this technique could be applied to any automatic test equipment platform, provided that the data collection activities were in place",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=547687,no,undetermined,0
CRUSADE: hardware/software co-synthesis of dynamically reconfigurable heterogeneous real-time distributed embedded systems,"Dynamically reconfigurable embedded systems offer potential for higher performance as well as adaptability to changing system requirements at low cost. Such systems employ run-time reconfigurable hardware components such as field programmable gate arrays (FPGAs) and complex programmable logic devices (CPLDs). In this paper, we address the problem of hardware/ software co-synthesis of dynamically reconfigurable embedded systems. Our co-synthesis system, CRUSADE, takes as an input embedded system specifications in terms periodic acyclic task graphs with rate constraints and generates dynamically reconfigurable heterogeneous distributed hardware and software architecture meeting real-time constraints while minimizing the system hardware cost. We identify the group of tasks for dynamic reconfiguration of programmable devices and synthesize an efficient programming interface for reconfiguring reprogrammable devices. Real-time systems require that the execution time for tasks mapped to reprogrammable devices are managed effectively such that real-time deadlines are not exceeded. To address this, we propose a technique to effectively manage delay in reconfigurable devices. Our approach guarantees that the real-time task deadlines are always met. To the best of our knowledge, this is the first co-synthesis algorithm which targets dynamically reconfigurable embedded systems. We also show how our co-synthesis algorithm can be easily extended to consider fault-detection and fault-tolerance. Application of CRUSADE and its fault tolerance extension, CRUSADE-FT to several real-life large examples (up to 7400 tasks) from mobile communication network base station, video distribution router, a multi-media system, and synchronous optical network (SONET) and asynchronous transfer mode (ATM) based telecom systems shows that up to 56% system cost savings can be realized",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=761103,no,undetermined,0
Automated parameter extraction for ultrasonic flaw analysis,"To make a decision on the nature of a defect contained within a weld specimen, it is necessary to reliably detect suspect regions in the ultrasonic inspection images, derive geometrical parameters such as shape, size, position and orientation from each indication and, finally, collate these parameters intelligently by associating each indication with a possible defect type. This procedure is discussed for the case when the segmentation of indications and parameter calculation procedures are performed by the authors' NDT Workbench facility. A real-flaw example, inspected by a series of probes, is used to demonstrate the final defect categorisation decision. The indication parameters derived during this process can be used either to aid the manual interpreter or as part of a knowledge based system (KBS)",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=587042,no,undetermined,0
Off-line diagnosis of parallel systems,"This paper presents an off-line diagnosis strategy for parallel message-passing systems. This strategy, called host-diagnosis, allows an external observer, i.e. the host system, to perform centralized diagnosis of the system state, given results of distributed tests performed among the system processors. Three algorithms that use the host-diagnosis strategy are proposed. The performance of the three algorithms are evaluated and compared to those of a classic distributed self-diagnosis algorithm. The obtained results show an interesting behaviour of the host-diagnosis algorithms in comparison with the self-diagnosis one",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=740524,no,undetermined,0
Concurrent detection of software and hardware data-access faults,"A new approach allows low-cost concurrent detection of two important types of faults, software and hardware data-access faults, using an extension of the existing signature monitoring approach. The proposed approach detects data-access faults using a new type of redundant data structure that contains an embedded signature. Low-cast fault detection is achieved using simple architecture support and compiler support that exploit natural redundancies in the data structures, in the instruction set architecture, and in the data-access mechanism. The software data-access faults that the approach can detect include faults that have been shown to cause a high percentage of system failures. Hardware data-access faults that occur in all levels of the data-memory hierarchy are also detectable, including faults in the register file, the data cache, the data-cache TLB, the memory address and data buses, etc. Benchmark results for the MIPS R300D processor executing code scheduled by a modified GNU C Compiler show that the new approach can concurrently check a high percentage of data accesses, while causing little performance overhead and little memory overhead",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=588046,no,undetermined,0
Time triggered protocol-fault tolerant serial communications for real-time embedded systems,"This article discusses an advanced serial communications protocol which has been developed for applications which require highly dependable, or fault-tolerant operation. A typical such application would be automotive brake-by-wire or steer-by-wire systems where the system must be `fail-operational' as it is safety critical. `By-wire' systems transfer electrical signals down a wire instead of using a medium such as hydraulic fluid to transfer muscular energy. A conventional Antilock Braking System (ABS) is considered `fail-silent'; if a fault in the electronic control system is detected, the control system is switched off, leaving the manual hydraulic back-up still operational. If no such hydraulic back-up is available (as in the case of a `by-wire' system), the system must continue to function in the event of a fault occurring. The automotive industry has identified many good reasons to develop `by-wire' systems; reduction in parts count, removal of hydraulic system, improved maintenance, increased performance and functionality, increased passive safety by removal of mechanical linkages to passenger compartment, fuel economy, etc. Although there are several non-trivial challenges which must be overcome before `by-wire' systems become the mainstream, there are many compelling reasons for the technology to be introduced and so it is expected that they'll be overcome relatively quickly. The Time Triggered Protocol (TTP) overcomes the challenge of fault-tolerant distributed embedded processing",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=716427,no,undetermined,0
The influence of process physics on the MOS device performance the case of the reverse short channel effect,"The goal of the present review is to show how material experiments using simple structures combined with process simulation can give sufficient insight to complex device phenomena that are critical for the deep submicron MOS device performance. Specifically we shall first present experimental and simulation results on the 2-D distribution of silicon interstitial both in Si and Silicon-On-Insulator. The conclusion drawn from these results will then drive our device experiments and simulations. We shall show that as predicted by the above experiments, NMOS SOI devices exhibit a reduction of the Reverse Short Channel effect (RSCE). Coupled process-device simulation reveals the influence of the fundamental point defect properties on MOS device performance",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=651238,no,undetermined,0
Network enabled solvers for scientific computing using the NetSolve system,"Agent-based computing is increasingly regarded as an elegant and efficient way of providing access to computational resources. Several metacomputing research projects are using intelligent agents to manage a resource space and to map user computation to these resources in an optimal fashion. Such a project is NetSolve, developed at the University of Tennessee and Oak Ridge National Laboratory. NetSolve provides the user with a variety of interfaces that afford direct access to preinstalled, freely available numerical libraries. These libraries are embedded in computational servers. New numerical functionalities can be integrated easily into the servers by a specific framework. The NetSolve agent manages the coherency of the computational servers. It also uses predictions about the network and processor performances to assign user requests to the most suitable servers. This article reviews some of the basic concepts in agent-based design, discusses the NetSolve project and how its agent enhances flexibility and performance, and provides examples of other research efforts. Also discussed are future directions in agent-based computing in general and in NetSolve in particular",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=651477,no,undetermined,0
Adaptive connection admission control for mission critical real-time communication networks,"We report our work on adaptive connection admission control in real-time communication networks. Much of the existing work on connection admission control (CAC) specifies the QoS parameters as fixed values and does not exploit the dynamic fluctuations in resource availability. We take an innovative approach. First, we allow an application to specify QoS over a range, rather than fixed values. Second and more importantly, we design, analyze, and implement CAC modules that, based on QoS specified over a range, adaptively allocate system resources to connections. Delay analysis is an integral part of connection admission control. Our adaptive CAC uses an efficient delay analysis method to derive a closed form solution for end-to-end delay of messages in a connection. Our adaptive CAC improves the system's performance in terms of the probability of admitting connections and the QoS offered to the connections",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=722200,no,undetermined,0
An IDDQ sensor for concurrent timing error detection,"Error control is a major concern in many computer systems, particularly those deployed in critical applications. Experience shows that most malfunctions during system operation are caused by transient faults, which often manifest themselves as abnormal signal delays that may result in violations of circuit element timing constraints. We present a novel complementary metal-oxide-semiconductor-based concurrent error-detection circuit that allows a flip-flop (or other timing-sensitive circuit element) to sense and signal when its data has been potentially corrupted by a setup or hold timing violation. Our circuit employs on-chip quiescent supply current evaluation to determine when the input changes in relation to a clock edge. Current through the detection circuit should be negligible while the input is stable. If the input changes too close to the clock time, the resulting switching transient current in the detection circuit exceeds a reference threshold at the time of the clock transition, and an error is flagged. We have designed, fabricated, and evaluated a test chip that shows that such an approach can be used to detect setup and hold time violations effectively in clocked circuit elements",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=720401,no,undetermined,0
Translating process changes from one development environment to another,Summary form only given. Translating the results from one development environment to another is not an easy task. The authors present practical examples of how translating process improvement results might be done and the considerations involved. They discuss how tools may be applied and how the differences between the two environments and their interaction with the process change may be assessed,1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=653539,no,undetermined,0
Early measurement and improvement of software quality,"The paper combines relevant research in software reliability engineering and software measurement to develop an integrated approach for the early measurement and improvement of software quality. Recent research in these areas is extended 1) to select appropriate software measures based on. A formal model, 2) to construct tree-based reliability models for early problem identification and quality improvement, and 3) to develop tools to support industrial applications. Initial results applying this approach to several IBM products demonstrated the applicability and effectiveness of this approach",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=716656,no,undetermined,0
SEMATECH projects in advanced process control,Scatterometer measurements of critical dimensions paralleled those of atomic force microscopy down to 0.14 Î¼m. Application of a run to run controller to chemical mechanical processes demonstrated control to target for patterned wafers and improvements in CpK of 150% for epitaxial processes. Benchmarking of commercial software for fault detection of plasma etchers demonstrated feasibility in identifying faults during operation,1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=664499,no,undetermined,0
Statistical study of lightning induced overvoltages on distribution power lines and risk analysis,"Owing to the increasing sensitivity of low voltage electrical equipment, it is necessary to be able to assess the magnitude of overvoltages caused by lightning. These are the most problematic overvoltages and, for the most part, they travel through power distribution lines. A risk analysis must be conducted to assist the decision to install a protective device on a given installation. In the French approach, the first stage of this analysis is the evaluation of the level of exposure to lightning induced overvoltages. The second stage consists in allowing for a criterion to evaluate the consequences of these electrical disturbances (destruction or unavailability of equipment) for a given level of exposure and in a given situation. This decision aid method is empirical. It is based on the main characteristics of the electric power supply and on the level of lightning strike density in the relevant area. An analysis of the same type is proposed on a complementary basis by two IEC committees. Finally, a calculation tool (Anastasia) was developed in order to allow the accurate quantification of the level of overvoltages on an installation. It is based on modelling of the coupling of the lightning channel and a power distribution line, allowing overvoltages to be calculated with the EMTP program. The Monte Carlo method is then used, in combination with simulations, to obtain a statistical distribution of the stresses at a given point. A comparison was made between the results obtained with Anastasia and the measurements conducted on an installation in the context of a campaign organized for several years by France Telecom.",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=667249,no,undetermined,0
Software products quality improvement with a programming style guide and a measurement process,"Quality requirements of industrial software are very high and the correctness of the source code is of great importance for this purpose. For this reason, the objective of QUALIMET (ESSI Project 23982) is to improve the software development process, introducing a set of style norms to be used when coding in C++, defining a set of metrics to be used on the source code in order to check its quality and testing this approach in a baseline project. The paper presents the work that is being carried out under this project It is expected that at the end of QUALIMET, the incorporation of these quality assurance techniques into the current methodology for developing software, will allow to have a complete methodology that guarantees software product quality, minimising the complexity of the code earlier in the programming process, yielding more maintainable and less error-prone software and improving the quality of the software and the satisfaction of customers",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=716653,no,undetermined,0
Exploiting delayed synchronization arrivals in light-weight data parallelism,"SPMD (single program multiple data) models and other traditional models of data parallelism provide parallelism at the processor-level. Barrier synchronization is defined at the level of processors where, when a processor arrives at the barrier point early and waits for others to arrive, no other useful work is done on that processor. Program restructuring is one way of minimizing such latencies. However, such programs tend to be error-prone and less portable. The author discusses how multithreading can be used in data parallelism to mask delays due to application irregularity or processor load imbalance. The discussion is in the context of Coir, the object-oriented runtime system for parallelism. The discussion concentrates on shared memory systems. The sample application is an LU factorization algorithm for skyline sparse matrices. The author discusses performance results on the IBM PowerPC-based symmetric multiprocessor system",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=667409,no,undetermined,0
Knowledge based technique to enhance the performance of neural network based motor fault detectors,"The monitoring and fault detection of motors is a very important and difficult topic. Neural networks can often be trained to recognize motor faults by examining the performance of certain motor measurements. Unfortunately, several weaknesses exist for neural networks when used in this application. Examples of these shortcomings are that they can take a considerable time to train, often have less than desirable accuracy and generally are very dependent on the choice of training data. Although neural networks can recognize the nonlinear relationships that exist between motor measurements and motor faults, all aspects of the neural network fault detector performance can be improved if appropriate heuristics can be used to preprocess the input-output training relationship. This paper presents a novel approach of applying knowledge based modeling techniques to preprocess the training data and significantly improve the overall performance of the neural network based motor fault detector",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=668441,no,undetermined,0
An intelligence diagnostic system for reciprocating machine,"A vibration diagnostic system for a reciprocating machine has been developed. Its main function is to monitor operation state and to diagnose faults for the reciprocating machine. The software has two parts, one in a diagnostic machine and the other in a PC. It can perform diagnosis on the spot, realize self learning and adapt to many types of machine. It takes main factor autoregressive model as a character extractor and the ANN model as classifier. With the combination of the two technologies, the system can perform data sampling, signal analysis, character extracting and fault recognition automatically. It reduces factors involved by human beings in a diagnosis process, so the accuracy of the diagnosis and the level of intelligence and automation has been raised",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=669280,no,undetermined,0
Dynamic quality of session control of real-time video multicast,"The paper presents a framework for dynamic control of the quality of real time video multicast applications. In such applications, there is a need for introducing the concept of â€œQuality of Session (Qoss)â€? beyond the Qos received by individual receivers. The Qoss can be best determined by the end application, depending on the application semantics and the actual Qos seen by each receiver. The control of Qoss is achieved by employing a Qoss monitoring mechanism at the application level and a sender receiver combination control mechanism to react to bottlenecks in the network or end systems. Each receiver performs local control, measures the stream quality offered to the end user and feedback the measurement to the sender using extended RTP receiver reports. According to the feedbacks and multiviewer synchronization requirement, the sender assesses the overall Qoss and adjusts the encoding and/or sending rate. The mechanism can be further enhanced if layered codecs are adopted. Policies of the Qos measurement and Qoss assessment were defined, algorithms of the source rate control were proposed, and the mechanisms of adaptive video encoding is described",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=669352,no,undetermined,0
Software configuration management for the 21st century,"The increasing complexity of both software systems and the environments in which they are produced is pressuring projects to improve the development process using innovative methods. The role of software configuration management (SCM) systems, policies, and procedures that help control and manage software development environments is being stretched beyond the conceptual boundaries it has had for the last decade. One of the key enablers of producing higher quality software is a better software development process. The SCM system must instantiate a quality process, allow tracking and monitoring of process metrics, and provide mechanisms for tailoring and continual improvement of the software development process. More than a dozen SCM systems are now available, each one having a distinct architecture and set of core functionalities. Currently, no single system provides all the key SCM functions in the best form. Thus, a project must assess its real needs and choose the right SCM system to meet its software development challenges. This paper focuses on the characteristics of SCM systems, the SCM challenges for Lucent Technologies, the principal SCM systems being used within the company, and the issues of choosing and successfully implementing the best SCM systems.",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6772879,no,undetermined,0
Requirements Modeling,"<div style=""font-variant: small-caps; font-size: .9em;"">First Page of the Article</div><img class=""img-abs-container"" style=""width: 95%; border: 1px solid #808080;"" src=""/xploreAssets/images/absImages/00681886.png"" border=""0"">",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=681886,no,undetermined,0
QUEM: an achievement test for knowledge-based systems,"This paper describes the Quality and Experience Metric (QUEM), a method for estimating the skill level of a knowledge based system based on the quality of the solutions it produces. It allows one to assess how many years of experience the system would be judged to have if it were a human by providing a quantitative measure of the system's overall competence. QUEM can be viewed as a type of achievement or job placement test administered to knowledge based systems to help system designers determine how the system should be used and by what level of user. To apply QUEM, a set of subjects, experienced judges, and problems must be identified. The subjects should have a broad range of experience levels. Subjects and the knowledge based system are asked to solve the problems; and judges are asked to rank order all solutions, from worst quality to best. The data from the subjects is used to construct a skill function relating experience to solution quality, and confidence bands showing the variability in performance. The system's quality ranking is then plugged into the skill function to produce an estimate of the system's experience level. QUEM can be used to gauge the experience level of an individual system, to compare two systems, or to compare a system to its intended users. This represents an important advance in providing quantitative measures of overall performance that can be applied to a broad range of systems",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=649311,no,undetermined,0
Validation of the coupling dependency metric as a predictor of run-time failures and maintenance measures,The coupling dependency metric (CDM) is a successful design quality metric. Here we apply it to four case studies: run-time failure data for a COBOL registration system; maintenance data for a C text-processing utility; maintenance data for a C++ patient collaborative care system; and maintenance data for a Java electronic file transfer facility. CDM outperformed a wide variety of competing metrics in predicting run-time failures and a number of different maintenance measures. These results imply that coupling metrics may be good predictors of levels of interaction within a software product,1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=671604,yes,undetermined,0
Downsizing the estimation of software quality: a small object-oriented case study,"It would be beneficial if the quality of a software system could be estimated early in its lifecycle. Although many estimation methods exist, it is questionable whether the experiences gained in large software projects can successfully be transferred to small scale software development. The paper presents results obtained from a number of projects developed in a small company with limited resources. The projects were analyzed in order to find out metrics which would be suitable for early estimation of quality. A number of possible models were evaluated and discussed",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=713614,no,undetermined,0
Can model-based and case-based expert systems operate together?,"In discussing diagnostic expert systems, there is an ongoing debate as to whether model-based systems are superior to case-based systems, or vice versa. Our experience has shown that there is no real need for debate because the two are not mutually exclusive and, to the contrary, complement each other. Current expert system technology is capable of two reasoning mechanisms, in addition to other mechanisms, into one integrated system. Depending on the knowledge available, and time and cost considerations, expert systems allow the user to decide the relative proportion of case-based to modal-based reasoning he/she wishes to employ in any given situation. Diagnostic support software should be evaluated by two critical factor groups, Ben-Bassat, et al., 1992: (a) cost and time to deployment; and (b) accuracy, completeness and efficiency of the diagnostic process. The question, therefore is: for a given budget of time and money, which approach will result in the best diagnostic performance-case-based, model-based, or combination of the two. In this paper we will discuss the role of expert systems in combining model based and case-based reasoning to effect the most efficient user defined solution to diagnostic performance",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=713502,no,undetermined,0
Using infrared thermography to detect age degradation in EPROM chips,"Dozens of circuit cards (about 5% of the total) failed to function after a recent upgrade of the digital Flight Control Computer (DFLCC), even though all of these cards operated correctly before the modifications. The shop called for the use of the infrared camera to assist in diagnosing and repairing these cards. What the Neural Radiant Energy Detection (NREDS) found was faulty and marginal chips. Of particular interest was the presence of degraded EPROM chips on the Program Memory (PM) cards. While it is known that EPROMs have a limited life cycle (in terms of number of total recordings), the failure has been further characterized. Thermography provides a quantification of the degradation in thermal performance as the EPROMs are reused. When the heat rates exceed a given value, the EPROM chips will not accept a program. Some of the failed chips exhibited enormous heat rates. What is clear from these results is that infrared thermography can be used to identify degrading EPROM chips for replacement before failures become immanent",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=713496,no,undetermined,0
A virtual test-bench for analog circuit testability analysis and fault diagnosis,"Are fault simulation techniques feasible and effective for fault diagnosis of analog circuits? In this paper, we investigate these issues via a software tool which can generate testability metrics and diagnostic information for analog circuits represented by SPICE descriptions. This tool, termed the virtual test bench (VTB), incorporates three different simulation-based techniques for fault detection and isolation. The first method is based on the creation of fault-test dependency models, while the other two techniques employ machine learning principles based on the concepts of: (1) Restricted Coloumb Energy (RCE) Neural Networks, and (2) Learning Vector Quantization (LVQ). Whereas the output of the first method can be used for the traditional off-line diagnosis, the RCE and LVQ models render themselves more naturally to on-line monitoring, where measurement data from various sensors is continuously available. Since it is well known that analog faults and test measurements are affected by component parameter variations, we have also addressed the issues of robustness of our fault diagnosis schemes. Specifically, we have attempted to answer the questions regarding fixing of test measurement thresholds, obtaining the minimum number of Monte-Carlo runs required to stabilize the measurements and their deviations, and the effect of different thresholding schemes on the robustness of fault models. Although fault-simulation is a powerful technique for analog circuit testability analysis, its main shortcomings are the long simulation time, large volume of data and the fidelity of simulators in accurately modeling faults. We have plotted the simulation time and volume of data required for a range of circuit sizes to provide guidance on the feasibility and efficacy of this approach",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=713467,no,undetermined,0
A new dependency model based testability analyzer,"This paper describes the development of a testability analysis tool called ADMA. The approach used in ADMA is based on use of dependency models. Dependency models are the basis of several important products for testability analysis. What makes ADMA different from existing testability analysis tools is its capability to use multiple analysis algorithms and its user-friendly report format. The rationale behind the decision to support multiple algorithms is that an algorithm may perform better for certain circuits than other algorithms ADMA allows users to compare the results of different analysis side by side. ADMA provides concise reports of different formats for different types of users, such as executives, system designers, test engineers, and dependency modelers. Thus, users can focus on the aspects that are of interest to them. The subject paper gives an overview of the ADMA, emphasizing the fundamental advances represented by ADMA. The paper also describes how this tool integrates with the Automatic Dependency Model Generator, and with the Automatic Built-in Test generator. The paper does not focus on ADMA as a product, but more on how it generally extends current dependency mode-based tools",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=713442,no,undetermined,0
Software development: Processes and performance,"This paper presents data that describe the effects on software development performance due to both the production methods of software development and the social processes of how software developers work together. Data from 40 software development teams at one site that produces commercial software are used to assess the effects of production methods and social processes on both software product quality and team performance. Findings indicate that production methods, such as the use of software methodologies and automated development tools, provide no explanation for the variance in either software product quality or team performance. Social processes, such as the level of informal coordination and communication, the ability to resolve intragroup conflicts, and the degree of supportiveness among the team members, can account for 25 percent of the variations in software product quality. These findings suggest two paradoxes for practice: (1) that teams of software developers are brought together to create variability and production methods are used to reduce variability, and (2) that team-level social processes may be a better predictor of software development team performance than are production methods. These findings also suggest that factors such as other social actions or individual-level differences must account for the large and unexplained variations in team performance.",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5387128,no,undetermined,0
An extensible system for source code analysis,"Constructing code analyzers may be costly and error prone if inadequate technologies and tools are used. If they are written in a conventional programming language, for instance, several thousand lines of code may be required even for relatively simple analyses. One way of facilitating the development of code analyzers is to define a very high-level domain-oriented language and implement an application generator that creates the analyzers from the specification of the analyses they are intended to perform. This paper presents a system for developing code analyzers that uses a database to store both a no-loss fine-grained intermediate representation and the results of the analyses. The system uses an algebraic representation, called F(p), as the user-visible intermediate representation. Analyzers are specified in a declarative language, called F(p)-l, which enables an analysis to be specified in the form of a traversal of an algebraic expression, with access to, and storage of, the database information the algebraic expression indices. A foreign language interface allows the analyzers to be embedded in C programs. This is useful for implementing the user interface of an analyzer, for example, or to facilitate interoperation of the generated analyzers with pre-existing tools. The paper evaluates the strengths and limitations of the proposed system, and compares it to other related approaches",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=713328,no,undetermined,0
A study on the design of large-scale mobile recording and tracking systems,"A mobile inventory control system involves a large number of highly mobile and dispersed databases in the form of RFID (radio frequency ID) tag devices. Radio tags have limited communication range, bandwidth, computing power and storage. We examine some critical impacts of these characteristics on design of facilities and techniques required for supporting integrated mobile distributed databases and computing environments. We analyze the performance of RF tag protocols and database mechanisms using process oriented discrete event simulation tools. We present the results of experiments on three simulation models for RF tag protocols: slotted ALOHA/TDMA, Id arbitration and CDMA. The performance results show that packet direct sequence (DS) CDMA gives superior performance compared to slotted ALOHA/TDMA and ID Arbitration. The main performance result of experiments on mobile databases shows that optimistic concurrency control performs better for situations where tag transaction probability and write percentage is high. This scenario is found in many active in-transit visibility recording and tracking systems",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=649272,no,undetermined,0
"IMAGE: a low cost, low power video processor for high quality motion estimation in MPEG-2 encoding","A low cost low power architecture dedicated to perform high quality motion estimation on MPEG-2 CCIR 601 sequences is presented. The chip implements a new high performance motion estimation algorithm based on a modified genetic search strategy that can be enhanced by tracking motion vectors through successive frames. When tested on several sequences at different bit rates, the algorithm delivers nearly full search quality (less than -0.1 dB for a Â±75 H/V search range) while decreasing the processing power to 0.5%. The proposed MIMD processor exposes software programmability at different levels allowing the user to define his own searching strategy and combine multiple chips in Master-Salve configuration to meet the required processing power. Moreover post-processing of motion vectors, computation of dual prime vectors for low delay coding and selection of macroblock prediction mode can be programmed on this architecture. A low power strategy was also developed that mainly relies on an adaptive clocking scheme and low power caches implementation. The chip has been integrated in a 0.35 Î¼m 5L CMOS technology on an area of 50 mm<sup>2</sup>",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=713194,no,undetermined,0
Compression based refinement of spatial interpolation of missing pixels in grey scale images,"We present a method for refining interpolated values for pixels in grey scale images that were corrupted during transmission. The method uses an adaptive image model developed in the context of image compression, allowing it to exploit prior knowledge of image characteristics that can either be derived from examples coming from the same class of images or alternatively be determined by the sender and transmitted separately. The proposed method has been tested in a number of experiments and has been shown to improve both objective quality measures as well as subjective quality over a simpler non-adaptive method",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=711933,no,undetermined,0
How faults can be simulated in self-testable VLSI digital circuits?,"Computer based simulation of self testable VLSI digital circuits is a time consuming process. This is why new methods are still being developed to optimise the simulation process and to reduce its duration. The paper presents a new method of fault simulation, intended for self testable digital circuits. In this method, fault masking performed by an in-circuit tester is estimated, based on only the signature itself which is stored in compressor. It is not necessary to carry out a time consuming analysis of the digital circuit's responses and compare them with stored model responses. Based on performed simulations, an observation was made that the developed method brings a substantial reduction of the duration of fault simulation processes performed for self testable digital circuits. It means the research laboratory needs considerably less time to verify the projects carried out on digital circuits",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=711794,no,undetermined,0
Using GSSs to support error detection in software specifications,"Fagan inspections can be used to find errors in documents used during systems development. In the practice of Fagan inspections it has been found that Group Support Systems (GSSs) can significantly enhance error detection. This paper describes our findings on the use of a GSS by Fagan inspection teams in an experimental set-up. In this study, 24 students and 24 managers participated; they looked for defects in a standardized four-page document. In the preparation phase the participants, searching individually without GSS support, found within one hour between 12 and 40 defects. Prior to the second (or `logging') phase of the inspection we formed 16 groups, each consisting of three students or three managers. Eight groups were selected to work with GSS support, and eight groups without GSS but with a facilitator. We found that only 3 to 9 new defects were found in the second phase. The performance of the GSS groups did not differ significantly from the non-GSS groups, but the GSS participants evaluated their sessions significantly lower than the non-GSS participants",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=653142,no,undetermined,0
Process scheduling for performance estimation and synthesis of hardware/software systems,The paper presents an approach to process scheduling for embedded systems. Target architectures consist of several processors and ASICs connected by shared busses. We have developed algorithms for process graph scheduling based on list scheduling and branch and bound strategies. One essential contribution is in the manner in which information on process allocation is used in order to efficiently derive a good quality or optimal schedule. Experiments show the superiority of these algorithms compared to previous approaches like critical path heuristics and ILP based optimal scheduling. An extension of our approach allows the scheduling of conditional process graphs capturing both data and control flow. In this case a schedule table has to be generated so that the worst case delay is minimized,1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=711792,no,undetermined,0
Parametric optimization of measuring systems according to the joint error criterion,"This paper presents a method and exemplary results of the modeling and design of measuring systems. The minimum of designed system errors is achieved using parametric optimization methods of a measuring system model. A â€œstructural methodâ€?of measuring system modeling is used. The properties of the equipment used, as well as data processing algorithms, are taken into account",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=650770,no,undetermined,0
Measurements and feature extraction in high-speed sewing,"This paper presents the development of signal acquisition and analysis equipment for the measurement of sewing parameters on a high-speed overlock sewing machine. The objective of the work was to provide investigators of the textile area with hardware and software to ease the investigation on the dynamical behaviour of the following sewing parameters: force on needle bar; and presser-foot and thread tensions. It should also have enough flexibility to incorporate further signal entries, and provide the user with tools to ease the gathering and analysis of tests on different materials, sewing speeds and machine configurations and settings. Outputs for actuators to implement closed-loop control strategies are also available. The paper presents an overview of the system, which is a development of earlier hardware and software and focuses on the results concerning the measurement of the force on the needle-bar; this parameter is important to investigate needle penetration force in fabrics during sewing. Several signal-processing algorithms, that aim to automate the detection of some characteristics, are described. The purpose of this system, that implements some novel strategies, is to develop an add-on kit to apply to different sewing machines, but presently it has been implemented on a PC as a quality assessment system which will be used by textile technicians to build a quality database",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=648857,no,undetermined,0
Managing the real-time behaviour of a particle beam factory: the CERN Proton Synchrotron complex and its timing system principles,"In the CERN 26 GeV Proton Synchrotron (PS) accelerator network, super-cycles are defined as sequences of different kinds of beams produced repetitively. Each of these beams is characterised by attributes such as particle type, beam energy, its route through the accelerator network, and the final end user. The super-cycle is programmed by means of an editor through which the operational requirements of the physics programme can be described. Each beam in the normal sequence may later be replaced by a set of spare beams automatically depending on software and hardware interlocks and requests presented to the Master Timing Generator. The MTG calculates at run time how each beam is to be manufactured, and sends a telegram message to each accelerator, just before each cycle, describing what it should be doing now and during the next cycle. These messages, together with key machine timing events and clocks are encoded onto a timing distribution drop net where they are distributed around the PS complex to VME-standard timing reception TG8 modules which generate output pulses and VME bus interrupts for task synchronisation. The TG8 modules are able to use accelerator-related clocks such as the incremental/decremental magnetic field trains, or the beam revolution and radio frequencies to produce high precision beam synchronous timing. Timing Surveillance Modules (TSM) monitor these timings, which give high precision interval measurements used for the machine tuning, beam diagnostics, and fault detection systems",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=710979,no,undetermined,0
A new approach for the failure mode analysis of optical character recognition algorithms,"Concerns the imprecise method of OCR algorithm development and improvement. While trial-and-error techniques must be employed, the evaluation of the failed attempts is the most crucial, tedious, and error prone task. The paper presents a precise and efficient solution to this problem. The overlooked cause of the problem stems from the passive nature of the analysis performed. The proposed approach is highly interactive by employing a tight integration between the analyst and the OCR code they are attempting to find fault with or improve upon. This close coupling can be achieved through the use of special tools built around the code which provide a level of control and precision previously unavailable to developers. By establishing a virtual awareness of the operations being performed and additionally providing the means to hypothesize scenarios with each of the OCR algorithms, an improved state of failure mode analysis (FMA) can be reached. This new approach to algorithm advancement circumvents the current inferior solution to a daunting problem and will allow the future of optical character recognition to rise to a new level at an excitingly fast pace",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=727500,no,undetermined,0
Pragmatic study of parametric decomposition models for estimating software reliability growth,"Numerous stochastic models for the software failure phenomenon based on Nonhomogeneous Poisson Process (NHPP) have been proposed in the last three decades (1968-98). Although these models are quite helpful for software developers and have been widely applied at industrial organizations or research centers, we still need to do more work on examining/estimating the parameters of existing software reliability growth models (SRGMs). We investigate and account for three possible trends of software fault detection phenomena during the testing phase: increasing, decreasing and steady state. We present empirical results from quantitative studies on evaluating the fault detection process and develop a valid time-variable fault detection rate model which has the inherent flexibility of capturing a wide range of possible fault detection trends. The applicability of the proposed model and the related methods of parametric decomposition are illustrated through several real data sets from different software projects. Our evaluation results show that the analytic parametric decomposition approach for SRGM have a fairly accurate prediction capability. In addition, the testing effort control problem based on the proposed model is also demonstrated",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=730861,no,undetermined,0
The personal software process: a cautionary case study,"In 1995, Watts Humphrey introduced the Personal Software Process in his book, A Discipline for Software Engineering (Addison Wesley Longman, Reading, Mass.). Programmers who use the PSP gather measurements related to their own work products and the process by which they were developed, then use these measures to drive changes to their development behavior. The PSP focuses on defect reduction and estimation improvement as the two primary goals of personal process improvement. Through individual collection and analysis of personal data, the PSP shows how individuals can implement empirically guided software process improvement. The full PSP curriculum leads practitioners through a sequence of seven personal processes. The first and most simple PSP process, PSPO, requires that practitioners track time and defect data using a Time Recording Log and Defect Recording Log, then fill out a detailed Project Summary Report. Later processes become more complicated, introducing size and time estimation, scheduling, and quality management practices such as defect density prediction and cost-of-quality analyses. After almost three years of teaching and using the PSP, we have experienced its educational benefits. As researchers, however, we have also uncovered evidence of certain limitations. We believe that awareness of these limitations can help improve appropriate adoption and evaluation of the method by industrial and academic practitioners",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=730851,no,undetermined,0
Applying testability to reliability estimation,"The purpose of the article is to implement the idea of using testability to estimate software reliability. The basic steps involve estimating testability, evaluating how well software was written, and assessing the effectiveness of testing. Results from these steps along with operational profiles are used to estimate software reliability. The paper describes an application of this method to evaluate the reliability of a real software system of about 6000 lines of executable code and discusses the results of such an estimation. The results are also compared with those obtained by using two reliability growth models",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=730848,no,undetermined,0
Testability measurements for data flow designs,The paper focuses on data flow designs. It presents a testability measurement based on the controllability/observability pair of attributes. A case study provided by AEROSPATIALE illustrates the testability analysis of an embedded data flow design. Applying such an analysis during the specification stage allows detection of weaknesses and appraisal of improvements in terms of testability,1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=637169,no,undetermined,0
Formalising the software evaluation process,"Software process evaluation is an essential activity for improving software development in an organisation. Actually there is a need for a formalised method of evaluation that encompasses all the factors that affect software production. If a software process evaluation is to faithfully reflect the current status of an organisation, it must also consider the assessment of other factors. A formalised method of evaluation is presented that jointly assesses the three essential factors in software production: processes, technology and human resources. The aim behind this is to provide a solution to the main shortcomings of current software process evaluation methods: partiality of the evaluated factors and non-formalisation of the evaluation processes. Experimentation in various organisations has proven the adequacy of the proposed method for obtaining results that accurately portray the current status of the organisation's software process",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=730778,no,undetermined,0
Assessing feedback of measurement data: relating Schlumberger RPS practice to learning theory,"Schlumberger RPS successfully applies software measurement to support their software development projects. It is proposed that the success of their measurement practices is mainly based on the organization of the interpretation process. This interpretation of the measurement data by the project team members is performed in so-called `feedback sessions'. Many researchers identify the feedback process of measurement data as crucial to the success of a quality improvement program. However, few guidelines exist about the organization of feedback sessions. For instance, with what frequency should feedback sessions be held, how much information should be presented in a single session, and what amount of user involvement is advisable? Within the Schlumberger RPS search to improve feedback sessions, the authors explored learning theories to provide guidelines to these type of questions. After all, what is feedback more than learning?",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=637176,no,undetermined,0
A strategy for improving safety related software engineering standards,"There are many standards which are relevant for building safety- or mission-critical software systems. An effective standard is one that should help developers, assessors and users of such systems. For developers, the standard should help them build the system cost-effectively, and it should be clear what is required in order to conform to the standard. For assessors, it should be possible to objectively determine compliance to the standard. Users, and society at large, should have some assurance that a system developed to the standard has quantified risks and benefits. Unfortunately, the existing standards do not adequately fulfil any of these varied requirements. We explain why standards are the way they are, and then provide a strategy for improving them. Our approach is to evaluate standards on a number of key criteria that enable us to interpret the standard, identify its scope and check the ease with which it can be applied and checked. We also need to demonstrate that the use of a standard is likely either to deliver reliable and safe systems at an acceptable cost or to help predict reliability and safety accurately. Throughout the paper, we examine, by way of example, a specific standard for safety-critical systems (namely IEC 1508) and show how it can be improved by applying our strategy",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=730547,no,undetermined,0
Advanced control methods in rolling applications,"Continuously increasing quality demands in recent years have led to a considerable reduction of the permissible thickness tolerances for hot as well as cold rolled steel. However, conventional control concepts cannot completely compensate for the effects of various disturbances resulting from the rolling process. The utilization of high performance computer systems for the automation allows the application of advanced control techniques to overcome existing dynamic limitations. Particularly if older mechanical equipment is utilized, which is typical for revamping projects, these new concepts allow a significant improvement of the quality. In this paper, different examples of such technologies are presented: a new concept for advanced thickness control in hot strip mills based on a feedforward strategy combined with a Kalman filter, a statistical online identification tool; to enhance existing tension control systems via looper arms in finishing mills; a massflow control concept; and a predictive control concept to compensate reel tension variations. The development of these methods was based on extensive simulations as a development and testing tool. In the meantime, the simulation results have been verified by practical application of the methods. Some results of these practical implementations are added to demonstrate the potential improvements possible with these concepts.",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=730130,no,undetermined,0
Some conservative stopping rules for the operational testing of safety critical software,"Operational testing, which aims to generate sequences of test cases with the same statistical properties as those that would be experienced in real operational use, can be used to obtain quantitative measures of the reliability of software. In the case of safety critical software it is common to demand that all known faults are removed. This means that if there is a failure during the operational testing, the offending fault must be identified and removed. Thus an operational test for safety critical software takes the form of a specified number of test cases (or a specified period of working) that must be executed failure-free. This paper addresses the problem of specifying the numbers of test cases (or time periods) required for a test, when the previous test has terminated as a result of a failure. It has been proposed that, after the obligatory fix of the offending fault, the software should be treated as if it were completely novel, and be required to pass exactly the same test as originally specified. The reasoning here claims to be conservative, in as much as no credit is given for any previous failure-free operation prior to the failure that terminated the test. We show that, in fact, this is not a conservative approach in all cases, and propose instead some new Bayesian stopping rules. We show that the degree of conservatism in stopping rules depends upon the precise way in which the reliability requirement is expressed. We define a particular form of conservatism that seems desirable on intuitive grounds, and show that the stopping rules that exhibit this conservatism are also precisely the ones that seem preferable on other grounds",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=637384,no,undetermined,0
Fault analysis and performance monitoring in prototyping machine vision systems,"This paper introduces an innovative approach in the development process for machine vision systems. We take all system requirements into consideration, not only digital image processing but also process synchronization and process control. Strict implementation conventions for hardware and software design allow integrated performance monitoring and fault analysis. Even in a prototype stage, various algorithms and hardware components can be assessed directly in the industrial environment. After presenting the system concept in general, the implementation process is discussed for a sample application",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=638840,no,undetermined,0
Machine learning algorithms for fault diagnosis in analog circuits,"In this paper, we investigate and systematically evaluate two machine learning algorithms for analog fault detection and isolation: (1) restricted Coloumb energy (RCE) neural network, and (2) learning vector quantization (LVQ). The RCE and LVQ models excel at recognition and classification types of problems. In order to evaluate the efficacy of the two learning algorithms, we have developed a software tool, termed Virtual Test-Bench (VTB), which generates diagnostic information for analog circuits represented by SPICE descriptions. The RCE and LVQ models render themselves more naturally to online monitoring, where measurement data from various sensors is continuously available. The effectiveness of RCE and LVQ is demonstrated on illustrative example circuits",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=728169,no,undetermined,0
On the expected number of failures detected by subdomain testing and random testing,"We investigate the efficacy of subdomain testing and random testing using the expected number of failures detected (the E-measure) as a measure of effectiveness. Simple as it is, the E-measure does provide a great deal of useful information about the fault detecting capability of testing strategies. With the E-measure, we obtain new characterizations of subdomain testing, including several new conditions that determine whether subdomain testing is more or less effective than random testing. Previously, the efficacy of subdomain testing strategies has been analyzed using the probability of detecting at least one failure (the P-measure) for the special case of disjoint subdomains only. On the contrary, our analysis makes use of the E-measure and considers also the general case in which subdomains may or may not overlap. Furthermore, we discover important relations between the two different measures. From these relations, we also derive corresponding characterizations of subdomain testing in terms of the P-measure",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=485221,no,undetermined,0
Application and analysis of IDDQ diagnostic software,"A current disadvantage of IDDq testing is lack of software-based diagnostic tools that enable IC vendors to create a large database of defects uniquely detected with this test method. We present a methodology for performing defect localization based upon IDDq test information (only). Using this technique, fault localization can be completed within minutes (e.g. <5 minutes) after IC testing is complete. This technique supports multiple fault models and has been successfully applied to a large number of samples-including ones that have been verified through failure analysis. Data is presented related to key issues such as diagnostic resolution, hardware-to-fault model correlation, diagnostic current thresholds, and the diagnosability of various defect types",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=639633,no,undetermined,0
Fault handling mechanisms in the RETHER protocol,"RETHER is a software-driven token-passing protocol designed to provide bandwidth guarantee for real-time multimedia applications over off-the-shelf Ethernet hardware. To our knowledge, it is the first all-software and fully-implemented real-time protocol on top of commodity Ethernet hardware. Because token passing is used to regulate network accesses, node crashes and/or packer corruption may lead to token loss, thus potentially shutting down the network completely. This paper describes the fault handling mechanisms built into RETHER to address this problem in both a single-segment and multi-segment Ethernet environment. The emphasis of the paper is on the uniqueness of the target application context and the rationale of chosen solutions. We present the performance tradeoffs of the fault detection/recovery schemes, the implementation experiences of the prototype",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=640141,no,undetermined,0
Congestion control with two level rate control for continuous media traffic in network without QoS guarantee,"Recently, it is required to transfer continuous media over networks without QoS guarantee. In these networks, network congestion will cause transmission delay variance which degrades the quality of continuous media itself. This paper proposes a new protocol using congestion control with two level rate control in the data transfer level and the coding level. It introduces a TCP-like congestion control mechanism to the rate control of the data transfer level, which can detect the QoS change quickly, and adjust the coding rate of continuous media with a time interval long enough for its quality. The performance evaluation through software simulation with multiplexing continuous media traffic and TCP traffic shows that the proposed protocol works effectively in the case of network congestion",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=726420,no,undetermined,0
Biometry: the characterisation of chestnut-tree leaves using computer vision,"The Department of Biology of the University of Tras-os-Montes e Alto Douro analyses every year a large number of chestnut-tree leaves, in order to measure their biometric characteristics, namely the leaf area, dimensions of the enclosing rectangle, number of teeth and secondary veins. Because for a human operator this is a time consuming and error prone task, a computer vision system has been set up to improve the process. The task of measuring the leaf presents no major problems, while counting the number of teeth and secondary veins has proved to be complex at the resolutions used. This paper describes the state of the project, going into some detail on the algorithms. A complete system has been assembled, based on a PC connected to an imaging system. A windows-based application has been developed, which integrates the control of the operations to grab, store and analyse images of different varieties of chestnut-tree leaves in an organised way. Because the accuracy of the computer vision algorithms used is not sufficient for the system to be completely autonomous, a semi-automatic solution has been adopted. The operator validates or corrects the results of the automatic analysis. This solution leads to a significant improvement in the performance of the human operator, both in terms of speed and quality of the results",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=648634,no,undetermined,0
Integrated dynamic scheduling of hard and QoS degradable real-time tasks in multiprocessor systems,"Many time critical applications require predictable performance and tasks in these applications have deadlines to be met. For tasks with hard deadlines, a deadline miss can be catastrophic, while for QoS degradable tasks (soft real time tasks) timely approximate results of poorer quality or occasional deadline misses are acceptable. Imprecise computation and (m,k) firm guarantee are two workload models that quantify the trade off between schedulability and result quality. We propose dynamic scheduling algorithms for integrated scheduling of real time tasks, represented by these workload models, in multiprocessor systems. The algorithms aim at improving the schedulability of tasks by exploiting the properties of these models in QoS degradation. We also show how the proposed algorithms can be adapted for integrated scheduling of multimedia streams and hard real time tasks, and demonstrate their effectiveness in quantifying QoS degradation. Through simulation, we evaluate the performance of these algorithms using the metrics-success ratio (measure of schedulability) and quality. Our simulation results show that one of the proposed algorithms, multilevel degradation algorithm, outperforms the others in terms of both the performance metrics",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=726408,no,undetermined,0
Automatic commissioning of air-conditioning plant,"The paper presents the results of work being undertaken as part of the UK contribution to an International Energy Agency research project (Annex 34) concerned with computer-aided fault detection and diagnosis in real buildings. The aim of the project is to develop and test software tools, which are based on existing methods of fault detection and diagnosis, for analysing the performance of heating, ventilating and air-conditioning systems. The development of a tool, which can automate the commissioning of air-handling units in large-scale air-conditioning systems, is considered in this paper. The presence of faults is detected by a fuzzy model-based fault diagnosis scheme which uses generic reference models to describe the behaviour of the plant when it is operating correctly and when one of a predefined set of faults has occurred. The paper discusses practical issues such as the integration of the commissioning tool and the building energy management system, the automatic reconfiguration of the control strategy to implement open-loop commission tests, the automatic generation of the test sequences, and the presentation of the diagnostic results to the user. Experimental results are presented that demonstrate the use of the tool to remotely commission the cooling coil of an air-handling unit in a commercial office building",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=726174,no,undetermined,0
Quality assurance certification: adoption by Australian software developers and its association with capability maturity,"Many Australian developers have committed resources to achieve certification in response to government purchasing policies favouring standards-certified-suppliers, but cynics suggest the `piece of paper' does little to improve the processes and subsequent product. This study details a research project undertaken to assess the adoption of QA certification by Australian software developers. Primary data for the study were gathered from a survey of 1,000 Australian software developers, and secondary data from the JAS-ANZ register of certified organisations. The survey design was used to determine the extent of adoption of QA certification by Australian developers, their organisational characteristics, capability maturity and perceptions regarding the value of QA certification. The SEI CMM questionnaire was used as the basis of the capability maturity measurement instrument, enabling an international comparison of CMM levels of developers in Australia, Hong Kong and Singapore",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=640157,no,undetermined,0
Performability analysis of an avionics-interface,"This paper reports on a case study in the quantitative analysis of safety-critical systems. Although formal methods are becoming more and more accepted in the development of such systems, usually they are used in the verification of qualitative properties. However, in many cases system safety also depends on the fact that certain quantitative requirements are met. Therefore we are interested in statements about quantitative properties, which can be achieved by a rigorous formal method. Our approach is to create a generalized stochastic Petri net (GSPN) model of the system and use it for the analysis of the system. The object of this case study is a fault-tolerant computer (FTC) constructed by Daimler Benz Aerospace (DASA) for the International Space Station (ISS). One part of the FTC is the Avionics Interface (AVI) which connects the FTC with a bus-system. We want to determine the data throughput that can be reached by the AVI and obtain informations about bus-usage-profiles which can cause the rejection of messages. Although such rejections are allowed according to the specification, they can cause a significant deterioration in the overall bus performance. In this article we describe a GSPN model of the AVI software and its environment. This model is used to make predictions about the AVI performability. Since a complete analytical solution of the model is not possible due to its complexity and the infinite state space, a simulation is used to analyse the crucial AVI behavior for several bus-usage-profiles.",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=725461,no,undetermined,0
Evaluation of fourteen desktop data mining tools,"Fourteen desktop data mining tools (or tool modules) ranging in price from US$75 to $25,000 (median <$1,000) were evaluated by four undergraduates inexperienced at data mining, a relatively experienced graduate student, and a professional data mining consultant. The tools ran under the Microsoft Windows 95, Microsoft Windows NT, or Macintosh System 7.5 operating systems, and employed decision trees, rule induction, neural networks, or polynomial networks to solve two binary classification problems, a multi-class classification problem, and a noiseless estimation problem. Twenty evaluation criteria and a standardized procedure for assessing tool qualities were developed and applied. The traits were collected in five categories: capability, learnability/usability, interoperability, flexibility, and accuracy. Performance in each of these categories was rated on a six-point ordinal scale, to summarize their relative strengths and weaknesses. This paper summarizes a lengthy technical report (Gomolka et al., 1998), which details the evaluation procedure and the scoring of all component criteria. This information should be useful to analysts selecting data mining tools to employ, as well as to developers aiming to produce better data mining products",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=725108,no,undetermined,0
Fault injection for the masses,"The key technology that the author would like to see adopted by the masses is a family of software fault injection algorithms that can predict where to concentrate testing. From a novelty standpoint, these algorithms were (and still are) unique among other methods of performing fault injection. The author concedes that the algorithms are computational, but the results can provide unequaled information about how â€œbad thingsâ€?propagate through systems. Because of that, he thinks fault injection methods are valuable to anyone responsible for software quality, including those working in one-person independent software vendors (ISVs) or even the largest corporations",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=642820,no,undetermined,0
Identification of high frequency transformer equivalent circuit using Matlab from frequency domain data,"High frequency modelling is essential in the design of power transformers, to study impulse voltage distribution, winding integrity and insulation diagnosis. In this paper, a PC based, fully automated technique for identifying parameters of a high frequency transformer equivalent circuit is proposed. At each discrete measurement point, voltage ratio and phase between input and output is calculated to obtain the frequency response. A parametric system identification technique is utilised to determine the coefficients of an appropriate transfer function to model the measured frequency response from 50 Hz to 1 MHz, divided into low, medium and high frequency ranges. The proposed technique is simple to implement, fully computerised and avoids time consuming measurements reported earlier. Test results on several transformers indicate that the method is highly reliable, consistent and is sensitive to transformer winding faults",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=643049,no,undetermined,0
Sequential test generation based on circuit pseudo-transformation,"The test generation problem for a sequential circuit capable of generating tests with combinational test generation complexity can be reduced to that for the combinational circuit formed by replacing each FF in the sequential circuit by a wire. In this paper, we consider an application of this approach to general sequential circuits. We propose a test generation method using circuit pseudo-transformation technique: given a sequential circuit, we extract a subcircuit with balanced structure which is capable of generating tests with combinational test generation complexity, replace each FF in the subcircuit by wire, generate test sequences for the transformed sequential circuit, and finally obtain test sequences for the original sequential circuit. We also estimate the effectiveness of the proposed method by experiment with ISCAS'89 benchmark circuits",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=643919,no,undetermined,0
TEMPLATES: a test generation procedure for synchronous sequential circuits,"We develop the basic definitions and procedures for a test generation concept referred to as templates that magnifies the effectiveness of test generation by taking advantage of the fact that many faults have â€œsimilarâ€?test sequences. Once a template is generated, several test sequences to detect different faults are derived from it at a reduced complexity compared to the complexity of test generation",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=643923,no,undetermined,0
TBRIM: decision support for validation/verification of requirements,"A decision support system has been developed that provides a structured approach to aid the validation/verification of requirement sets and enhance the quality of the resulting design by reducing risk. Additionally, an automated implementation of this approach has been completed utilizing the Advanced Integrated Requirements Engineering System (AIRES) software. Corroboration for the application of this decision support system has been attained from automated and manual testing performed on the system specification of a large-scale software development and integration project. The decision support approach, called the Test-Based Risk Identification Methodology (TBRIM), has been used to detect four major types of requirements risk potential: ambiguity, conflict, complexity, and technical factors. The TBRIM is based on principles of evidence testing and eliminative logic developed by Bacon and Cohen for making decisions under uncertainty. New techniques for detection of complexity and technical risks have been added to existing methods for identification of risk from ambiguous and conflicting requirements. Comparisons of the automated and manual test results on risk category-by-category basis showed good correlation, and category-independent comparisons showed improvements that were consistent with expectations, Benefits from use of this decision support system include higher accuracy, consistency, and efficiency in the validation/verification of requirements. Knowledge of senior personnel can be captured to provide an expert system for less-experienced personnel",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=725031,no,undetermined,0
Estimating the number of faults using simulator based on generalized stochastic Petri-net model,"In order to manage software projects quantitatively, we have presented a new model far software project based on generalized stochastic Petri-net model which can take influence of human factors into account, and we have already developed software project simulator based on GSPN model. This paper proposes methods for calculating model parameters in the new model and estimating the number of faults in the design and debug phases of software process. Then we present experimental evaluation of proposed method using a data of actual software development project on a certain company. As the result of case study, we confirmed its effectiveness with respect to estimating the number of faults in the software process",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=643969,no,undetermined,0
Automated network fault management,"Future military communication networks will have a mixture of backbone terrestrial, satellite and wireless terrestrial networks. The speeds of these networks vary and they are very heterogeneous. As networks become faster, it is not enough to do reactive fault management. Our approach combines proactive and reactive fault management. Proactive fault management is implemented by dynamic and adaptive routing. Reactive fault management is implemented by a combination of a neural network and an expert system. The system has been developed for the X.25 protocol. Several fault scenarios were modeled and included in the study reduced switch capacity, increased packet generation rate of a certain application, disabled switch in the X.25 cloud, and disabled links. We also modeled the occurrence of alarms including the severity of the problem, location of the event and a threshold. To detect and identify faults we use both numerical data associated with the performance objects (attributes) in the MIB as well as SNMP traps (alarms). Simulation experiments have been performed in order to understand the convergence of the algorithms, the timing of the neural network involved and the G2/NeurOn-Line software environment and MIB design",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=644967,no,undetermined,0
Framework of a software reliability engineering tool,"The usage of commercial off-the-shelf (COTS) software modules in a large, complex software development project has well-known advantages (e.g. reduced development time and reduced cost). However, many such designs remain ill-understood in terms of end-to-end, overall reliability and assurance of the software system. Since the COTS components may not have been designed with assurance attributes in mind, a COTS module integrated system may fail to provide high end-to-end assurance. In applications that require timing, reliability and security guarantees, the usage of COTS software components can therefore mandate an analysis of the assurance attributes. The users of such systems may desire to predict the effect of the occurrence of an event on the reliability of other events in the system, and in general carry out an end-to-end analysis of the software system assurance. In this paper, we evaluate the causality, reliability and the overall performance aspects of large-scale software using a reverse engineering approach. The proposed code analysis approach can evaluate whether the COTS software meets the user-specified individual/group reliability constraints or not. In the case of reliability violation, our proposed approach can identify the modules of the software that may require re-design. The underlying idea is to merge event probabilities, event dependencies and fault propagation to calculate the occurrence probabilities of every event in the system",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=648048,no,undetermined,0
Intelligent AUV on-board health monitoring software (INDOS),"Two aspects of situation analysis are of special importance in the operation of autonomous underwater vehicles: maintenance of system integrity; and identification of danger situations and appropriate reactions, or more generally early detection of potential failures and provision of intelligent countermeasures. The efforts with regard to fault avoidance are centred on the two functions: system health and mission management. It is mainly in these domains that fault detection, diagnosis and recovery are necessary. Any degradation in the performance of the system has to be detected as soon as possible, otherwise the vehicle controller would operate on the basis of abnormal and non-realistic measurement data. Consequently, software development for AUVs at STN ATLAS is currently focused on intelligent behaviour. In the case of the STN ATLAS AUV development â€œC-Catâ€? this complex is handled by the intelligent software INDOS",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=724351,no,undetermined,0
Virtual disassembly-a software tool for developing product dismantling and maintenance systems,"A product is typically designed in the assembled configuration. However, CAD systems do not analyze the product design for disassemblability, a significant part of maintenance. This paper presents a new methodology for performing disassembly evaluation of CAD models of designs for maintenance. This methodology involves analyzing the product for disassemblability utilizing several disassembly techniques, such as selective disassembly and destructive disassembly, to assess: (i) the ease of product disassembly for maintenance, (ii) disassembly sequencing to maintain a selected set of components and (iii) disassembly cost for maintenance. The methodology utilizes a metric for quantitative assessment of the product in order to arrive at an acceptable design and to compare alternate designs for maintenance. Moreover, the disassembly methodology presented in this paper can serve as a basis for identifying an efficient product design for maintenance",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=653676,no,undetermined,0
"Sample implementation of the Littlewood holistic model for assessing software quality, safety and reliability","Software quality, safety and reliability metrics should be collected, integrated, and analyzed throughout the development lifecycle so that corrective and preventive action can be taken in a timely and cost effective manner. It is too late to wait until the testing phase to collect and assess software quality information, particularly for mission critical systems. It is inadequate and can be misleading to only use the results obtained from testing to make a software safety or reliability assessment. To remedy this situation a holistic model which captures, integrates and analyzes product, process, and people/resource (P<sup>3</sup>R) metrics, as recommended by B. Littlewood (1993), is needed. This paper defines one such possible implementation",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=653698,no,undetermined,0
Development of expertise in complex domains,"It is necessary to understand how human planners' performance evolves over time with increasing expertise in order to develop effective computer critics, tutors, knowledge acquisition systems, and training strategies. In this paper we present two studies of the development of expertise in complex domains: manufacturing planning and software development management planning. We had experts in each domain rank order the plans created by practitioners at various levels of experience from best to worst quality. We did this to assess whether practitioners really did gain skill with increased experience in both of these fields or whether experts were â€œself proclaimedâ€? Next, we analyzed the spoken statements of the practitioners to identify the knowledge and problem solving strategies they used or lacked. We used these data to model the skill development phases in each domain. These models can be used to develop computer tools and training strategies to help practitioners achieve higher levels of competence",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=635341,no,undetermined,0
A change prediction model for embedded software applications,"A key aspect of effective software development is the ability to quantify and predict software product quality. Software quality is the degree to which software possesses desired attributes, including portability, reliability, testability and maintainability. Insofar as software with high change traffic affects its maintainability and reliability, a model which produces a change-traffic predictor metric may be useful. The information such a model would provide could be used to help estimate the development cost and effort. Resources could be better allocated to those areas where additional attention may be required. Software changes normally occur due to new requirements or errors in the software, and so a change-traffic metric is not necessarily a good proxy for errors. Users should define their thresholds and ranges of acceptability. This paper identifies metrics collected from embedded Ada software that had a correlation with the change traffic of that software. Using multiple linear regression analysis and sample data from up to 287 embedded Ada software modules, change prediction models yielded values for the average absolute difference between predicted and actual changes per module of less than 3, and an adjusted-R<sup>2</sup> value of 0.57 for the full sample",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=673277,no,undetermined,0
VHDL behavioral ATPG and fault simulation of digital systems,"Due to the increasing level of integration achieved by Very Large Scale Integrated (VLSI) technology, traditional gate-level fault simulation is becoming more complex, difficult, and costly. Furthermore, circuit designs are increasingly being developed through the use of powerful VLSI computer-aided design (CAD) synthesis tools which emphasize circuit descriptions using high-level representations of functional behavior, rather than physical architectures and layout. Behavior fault simulation applied to the top functional level models described using a hardware description language offers a very attractive alternative to these problems. A new way to simulate the behavioral fault models using the hardware description language (HDL), such as VHDL, is presented. Tests were generated by carrying out the behavioral fault simulation for a few circuit models. For comparison, a gate-level fault simulation on the equivalent circuit, produced via a synthesis tool, was used. The performance analysis shows that a very small number of test patterns generated by the behavioral automatic test pattern generation (ATPG)/fault simulation system detected around 98 percent of the testable gate-level faults that were detected by random test",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=670325,no,undetermined,0
Performance test case generation for microprocessors,"We describe a systematic methodology for generating performance test cases for current generation microprocessors. Such rest cases are used for: (a) validating the expected pipeline flow behavior and timing; and, (b) detecting and diagnosing performance bugs in the design. We cite examples of application to a real, superscalar processor in pre- and post-silicon stages of development",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=670849,no,undetermined,0
Estimation of error detection probability and latency of checking methods for a given circuit under check,"A technique of sampling of input vectors (SIV) with statistical measurements is used for the estimation of error detection probability and fault latency of different checking methods. Application of the technique for Berger code, mod3 and parity checking for combinational circuits is considered. The experimental results obtained by a Pilot Software System are presented. The technique may be implemented as an overhead to an already existing fault simulator",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=670891,no,undetermined,0
An empirical study of regression test selection techniques,"Regression testing is an expensive maintenance process directed at validating modified software. Regression test selection techniques attempt to reduce the cost of regression testing by selecting tests from a program's existing test suite. Many regression test selection techniques have been proposed. Although there have been some analytical and empirical evaluations of individual techniques, to our knowledge only one comparative study, focusing on one aspect of two of these techniques, has been performed. We conducted an experiment to examine the relative costs and benefits of several regression test selection techniques. The experiment examined five techniques for reusing tests, focusing on their relative abilities to reduce regression testing effort and uncover faults in modified programs. Our results highlight several differences between the techniques, and expose essential tradeoffs that should be considered when choosing a technique for practical application",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=671115,no,undetermined,0
Defect content estimations from review data,Reviews are essential for defect detection and they provide an opportunity to control the software development process. This paper focuses upon methods for estimating the defect content after a review and hence to provide support for process control. Two new estimation methods are introduced as the assumptions of the existing statistical methods are not fulfilled. The new methods are compared with a maximum-likelihood approach. Data from several reviews are used to evaluate the different methods. It is concluded that the new estimation methods provide new opportunities to estimate the defect content,1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=671393,no,undetermined,0
Analyzing effects of cost estimation accuracy on quality and productivity,"This paper discusses the effects of estimation accuracy for software development cost on both the quality of the delivered code and the productivity of the development team. The estimation accuracy is measured by metric RE (relative error). The quality and productivity are measured by metrics FQ (field quality) and TP (team productivity). Using actual project data on thirty-one projects at a certain company, the following are verified by correlation analysis and testing of statistical hypotheses. There is a high correlation between the faithfulness of the development plan to standards and the value of RE (a coefficient of correlation between them is -0.60). Both FQ and TP are significantly different between projects with -10%<RE<+10% and projects with RE&ges;+10% (the level of significance is chosen as 0.05)",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=671596,no,undetermined,0
Fault-tolerant computer for the Automated Transfer Vehicle,"Matra Marconi Space is developing their fourth generation fault-tolerant majority voting computers for use on future spacecraft such as the European Automated Transfer Vehicle, a servicing vehicle for the International Space Station. This paper presents how the computer participates in the spacecraft safety concept with emphasis on the critical on-orbit rendezvous phase. The functional architecture of the computer pool is described addressing fault containment layers, inter-channel synchronisation, communication, failed channel detection and passivation. Lastly, the hardware and software design of the test bench, built up to validate this new computer pool, is presented together with some performance measurements.",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=689493,no,undetermined,0
Software quality analysis and measurement service activity in the company,"It is very important to improve software quality using program analysis and measurement tools and the SQA (Software Quality Assurance) method at the appropriate points during the process of development. In many development departments, there is often not enough time to evaluate and use the tools and the SQA method or to accumulate the know-how for effective use. This paper describes the support activity of a software quality analysis and measurement service which is performed by our laboratory team within the company as a third-party independent staff group. We call this activity HQC (High Quality software creation support virtual Center). The purpose of this activity is as follows. First we improve the software quality engineering process in the development department, for example, we help them to increase efficiency of review or testing. To accomplish this, we use program static analysis tools to detect fault-prone software components. Next we assist in starting their own self-improvement process. In addition, we provide service activity to many development departments concurrently. We have been making progress with these activities, and some development departments have begun to establish improvement processes themselves",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=671598,no,undetermined,0
Assessment of system evolution through characterization,"Owing to the growing diffusion of the object-oriented paradigm and the need to keep the process of software development under control, industries are looking for metrics/indicators capable of evaluating system evolution to control quality, reusability, maintainability, etc. Some new metrics are proposed for monitoring system development and maintenance. These metrics are used with a set of histograms to give a clear characterization of the system under development. Histograms call be profitably used to detect critical conditions during the system life-cycle. The semantics of these histograms has been validated against several projects: an example is also reported",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=671605,no,undetermined,0
Examples of applying software estimate tool,"Although estimating the cost of a software project and the effort involved is very important, improving accuracy and establishing the software as a technology are both difficult. To solve these problems we have tried to use a software estimating tool. This tool has two characteristics. The first is that the tool uses function point analysis (FPA) instead of source lines of code, and the second is that special factors present in software development (usually estimated only from experience) are considered. This report provides some examples of how software project effort estimates can be improved",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=671608,no,undetermined,0
"Design, implementation, and evaluation of highly available distributed call processing systems","This paper presents the design of a highly available distributed call processing system and its implementation on a local area network of commercial, off-the-shelf workstations. A major challenge of using off-the-shelf components is meeting the strict performance and availability requirements in place for existing public telecommunications systems in a cost-effective manner. Traditional checkpointing and message logging schemes for general distributed applications are not directly applicable since call processing applications built using these schemes suffer from high failure-free overhead and long recovery delays. We propose an application-level fault-tolerance scheme that takes advantage of general properties of distributed call processing systems to avoid message logging and to limit checkpointing overhead. The proposed scheme, applied to a call processing system for wireless networks, shows average call setup latencies of 180 ms, failover times of less than three seconds, and recovery times of less than seventeen seconds. System availability is estimated to be 0.99995. The results indicate that using our proposed scheme meets the above challenge.",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=689461,no,undetermined,0
Empirical studies of a safe regression test selection technique,"Regression testing is an expensive testing procedure utilized to validate modified software. Regression test selection techniques attempt to reduce the cost of regression testing by selecting a subset of a program's existing test suite. Safe regression test selection techniques select subsets that, under certain well-defined conditions, exclude no tests (from the original test suite) that if executed would reveal faults in the modified software. Many regression test selection techniques, including several safe techniques, have been proposed, but few have been subjected to empirical validation. This paper reports empirical studies on a particular safe regression test selection technique, in which the technique is compared to the alternative regression testing strategy of running all tests. The results indicate that safe regression test selection can be cost-effective, but that its costs and benefits vary widely based on a number of factors. In particular, test suite design can significantly affect the effectiveness of test selection, and coverage-based test suites may provide test selection results superior to those provided by test suites that are not coverage-based",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=689399,no,undetermined,0
Return on investment of software quality predictions,"Software quality classification models can be used to target reliability enhancement efforts toward high risk modules. We summarize a generalized classification rule which we have proposed. Cost aspects of a software quality classification model are discussed. The contribution of this paper is a demonstration of how to assess the return on investment of model accuracy, in the context of a software quality classification model. An industrial case study of a very large telecommunications system illustrates the method. The dependent variable of the model was the probability that a module will have faults discovered by customers. The independent variables were software product and process metrics. The model is compared to random selection of modules for reliability enhancement. Calculation of return on investment can guide selection of the generalized classification rule's parameter so that the model is well-suited to the project",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=688249,no,undetermined,0
Process assurance audits: lessons learned,"During 1997, a large Information System (IS) Division of a Canadian phone company implemented formal process assurance in its Quality Assurance group. The status report presents a new perspective on the measurement of process assurance and the lessons learned after one year of assessing the individual conformance of software development projects to the corporate software development process (CSDP) of the organization. The status report presents the assurance process overview, goals. Benefits and scope, as well as the 1997 results overview, followed by the lessons learned for the 1998 audit program",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=671611,no,undetermined,0
Is software quality visible in the code,"Finding and fixing software code defects is crucial to product quality. It also, however, often proves difficult and time-consuming, especially late in the development cycle. While some believe that code analysis provides a simple way to detect quality defects, the authors argue otherwise. To prove their point, they analyzed data from error reports, and their results show that code analysis detects only a small percentage of meaningful defects",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=687949,no,undetermined,0
The application of a 3-D empirical ionosphere model to WAAS simulation,"The Wide Area Augmentation System (WAAS) is designed to augment the Global Positioning System (GPS). One of the services provided by WAAS is a broadcast ionospheric correction based upon dual-frequency measurements. These observations are used to construct a grid of expected vertical ionospheric delays. To date, the broadcast single frequency model has been used to project vertical delay observations onto the predefined grid points. The performance of the WAAS ionospheric grid could be improved by incorporating a more realistic description of the ionosphere than this nominal transport model. In this paper we explore the use of IRI90 for this purpose by means of a software-based WAAS simulator. The quality of the broadcast WAAS ionospheric corrections is tested by comparing the ionospheric delays predicted by the WAAS grid algorithm with those derived from a truth source. Results to date indicate that using a transport model derived from IRI90 yields a small improvement over the broadcast model",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=670150,no,undetermined,0
PC based monitoring and fault prediction for small hydroelectric plants,"Financial constraints often mean that small hydroelectric plants run unattended and hence the recording and monitoring of the performance of the plant relies on, sometimes infrequent, visits by appropriate personnel. The LabView package has appeared on the market and this enables the creation of â€œVirtual Instrumentsâ€?which can display, on the screen of a PC, measured quantities such as are required at a typical small hydroelectric power station. The use of a PC based metering scheme gives rise to numerous added benefits; the measured data can be stored in the memory of the PC to allow subsequent downloading and analysis; the data can be analysed by other software to detect and predict fault conditions, i.e. to perform a condition monitoring function; and the data can be interrogated remotely by a suitable communications link to enable analysis and monitoring, effectively from anywhere in the world. During the 1996/97 session, two final year Honours projects were running at Napier University which related to the use of affordable PC based systems, the first performing a metering function, and the second performing a fault prediction function. This paper reviews maintenance strategies and condition monitoring techniques and then describes the development and outcomes of the two projects (a generator monitoring system and a condition monitoring and fault prediction program)",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=674039,no,undetermined,0
Metrics for software architecture: a case study in the telecommunication domain,"Due to the ever increasing size and complexity of industrial software products, the issue of the design and architecture level metrics has received considerable attention. We propose some new metrics, in addition to employing some existing metrics, to understand their effects on four important quality attributes of software architectures: interoperability, reusability, reliability and maintainability. We also propose a simple `concept selection' methodology to assess these quality attributes using the raw metrics. Data from a very large-scale telecommunications software product and its changing software qualities are measured as the software architecture evolves during two releases of the product. Interesting questions arise as to how to determine the overall quality of an evolving product",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=685597,no,undetermined,0
Design of a software quality decision system: a computational intelligence approach,"This paper introduces an approximate reasoning system for assessing software quality and introduces the application of two computational intelligence methods in designing a software quality decision system, namely, granulation from fuzzy sets and rule-derivation from rough sets. This research is part of a computational intelligent systems approach to software quality evaluation, which includes a fuzzy-neural software quality factor-criteria selection model with learning and a rough-fuzzy-neural software quality decision system. Overall, computational intelligence results from a synergy of various combinations of genetic, fuzzy, rough and neural computing in designing engineering systems. Based on observations concerning software quality and the granulations of measurements in an extended form of the McCall software quality measurement framework, an approach to deriving rules about software quality is given. Quality decision rules express relationships between evaluations of software quality criteria measurements. A quality decision table is constructed relative to the degree of membership of each software quality measurement in particular granules. Decision-tables themselves are as a collection of sensors, which â€œsenseâ€?inputs and output conditions for rules. Rosetta is used to generate quality decision rules. The approach described in this paper illustrates the combined application of fuzzy sets and rough sets in developing a software quality decision system",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=685546,no,undetermined,0
Performance and interface buffer size driven behavioral partitioning for embedded systems,"One of the major differences in partitioning for co-design is in the way the communication cost is evaluated. Generally, the size of the edge cut-set is used. When communication between components is through buffered channels, the size of the edge cut-set is not adequate to estimate the buffer size. A second important factor to measure the quality of partitioning is the system delay. Most partitioning approaches use the number of nodes/functions in each partition as constraints and attempt to minimize the communication cost. The data dependencies among nodes/functions and their delays are not considered. In this paper, we present partitioning with two objectives: (1) buffer size, which is estimated by analyzing the data flow patterns of the control data flow graph (CDFG) and solved as a clique partitioning problem, and (2) the system delay that is estimated using list scheduling. We pose the problem as a combinatorial optimization and use an efficient non-deterministic search algorithm, called the problem-space genetic algorithm, to search for the optimum. Experimental results indicate that, according to a proposed quality metric, our approach can attain an average 87% of the optimum for two-way partitioning",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=676679,no,undetermined,0
A framework based approach to the development of network aware applications,"Modern networks provide a QoS (quality of service) model to go beyond best-effort services, but current QoS models are oriented towards low-level network parameters (e.g., bandwidth, latency, jitter). Application developers, on the other hand, are interested in quality models that are meaningful to the end-user and, therefore, struggle to bridge the gap between network and application QoS models. Examples of application quality models are response time, predictability or a budget (for transmission costs). Applications that can deal with changes in the network environment are called network-aware. A network-aware application attempts to adjust its resource demands in response to network performance variations. This paper presents a framework-based approach to the construction of network-aware programs. At the core of the framework is a feedback loop that controls the adjustment of the application to network properties. The framework provides the skeleton to address two fundamental challenges for the construction of network-aware applications: how to find out about dynamic changes in network service quality; and how to map application-centric quality measures (e.g., predictability) to network-centric quality measures (e.g., QoS models that focus on bandwidth or latency). Our preliminary experience with a prototype network-aware image retrieval system demonstrates the feasibility of our approach. The prototype illustrates that there is more to network-awareness than just taking network resources and protocols into account and raises questions that need to be addressed (from a software engineering point of view) to make a general approach to network-aware applications useful",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=685260,no,undetermined,0
Verification of the redundancy management system for space launch vehicle: a case study,"Formal methods have been widely recognized as effective techniques to uncover design errors that could be missed by a conventional software engineering process. The paper describes the authors' experience with using formal methods in analyzing the redundancy management system (RMS) for a space launch vehicle. RMS is developed by AlliedSignal Inc. for the avionics of NASA's new space shuttle, called VentureStar, that meets the expectations for space missions in the 21st century. A process algebraic formalism is used to construct a formal specification based on the actual RMS design specifications. Analysis is performed using PARAGON, a toolset for formal specification and verification of distributed real-time systems. A number of real-time and fault-tolerance properties were verified, allowing some errors in the RMS pseudocode to be detected. The paper discusses the translation of the RMS specification into process algebra formal notation and results of the formal verification",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=683206,no,undetermined,0
Performance evaluation tool for rapid prototyping of hardware-software codesigns,"Performance evaluation is essential for tradeoff analysis during rapid prototyping. Existing performance evaluation strategies based on co-simulation and static analysis are either too slow or error prone. We therefore present an intermediate approach based on profiling and scheduling for rapid prototyping of hardware-software codesigns. Our performance evaluation tool obtains representative task timings by profiling which is done simultaneously with system specification. During design space exploration the tool obtains performance estimates by using well known scheduling and novel retiming heuristics. It is capable of obtaining both nonpipelined and pipelined schedules. The tool includes an area estimator which calculates the amount of hardware area required by the design by taking resource sharing between different hardware tasks into account. The tool also allows the user to evaluate the performance of a particular schedule with different task timings. In contrast to co-simulation and static analysis, the tool is able to provide fast and accurate performance estimates. The effectiveness of the tool in a rapid prototyping environment is demonstrated by a case study",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=676695,no,undetermined,0
A controlled experiment to assess the benefits of procedure argument type checking,"Type checking is considered an important mechanism for detecting programming errors, especially interface errors. This report describes an experiment to assess the defect-detection capabilities of static, intermodule type checking. The experiment uses ANSI C and Kernighan & Ritchie (K&R) C. The relevant difference is that the ANSI C compiler checks module interfaces (i.e., the parameter lists calls to external functions), whereas K&R C does not. The experiment employs a counterbalanced design in which each of the 40 subjects, most of them CS PhD students, writes two nontrivial programs that interface with a complex library (Motif). Each subject writes one program in ANSI C and one in K&R C. The input to each compiler run is saved and manually analyzed for defects. Results indicate that delivered ANSI C programs contain significantly fewer interface defects than delivered K&R C programs. Furthermore, after subjects have gained some familiarity with the interface they are using, ANSI C programmers remove defects faster and are more productive (measured in both delivery time and functionality implemented)",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=677186,no,undetermined,0
Finding a suitable wavelet for image compression applications,"In this paper we assess the relative merits of various types of wavelet functions for use in a wide range of image compression scenarios. We have delineated different algorithmic criteria that can be used for wavelet evaluation. The assessment undertaken includes both algorithmic aspects (fidelity, perceptual quality) as well as suitability for real-time implementation in hardware. The results obtained indicate that of the wavelets studied the biorthogonal 9&7 taps wavelet is the most suitable from a compression perspective and that the Daubechies 8 taps gives best performance when assessed solely in terms of statistical measures",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=678050,no,undetermined,0
An ANN fault detection procedure applied in virtual measurement systems case,"The prompt detection of anomalous conditions of the Virtual Measurement Systems (VMS) elements, as sensors or transducers, involve a specialised implementation of the VMS software part. One solution for this software implementation is based on the neural processing structures. The implemented Artificial Neural Networks (ANN) are supplied with the voltage signals delivered by the conditioning circuits of the VMS sensors. The signal acquisition was performed using a data acquisition board or a programmable voltmeter. For the acquired signals the ANN delivers the values which can be used for fault detection and localisation of faulty elements. Referring to ANN architectures a study concerning the number of layers, the number of processing neurons, the type of neuron activation functions and the possibilities to optimise those parameters is included in this paper. The performance of proposed ANN fault detection solution was experimentally evaluated in the particular case of a VMS based on a data acquisition board (DAQ) and on a GPIB controller",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=679769,no,undetermined,0
Fault-tolerant software voters based on fuzzy equivalence relations,"Redundancy based fault-tolerant software strategies frequently use some form of voting to decide which of the answers their functionally equivalent versions produce is â€œcorrect.â€?When voting involves comparisons of floating-point outputs, it is necessary to use a tolerance value in order to declare two outputs equal. Comparing more than two floating-point outputs for equivalence is potentially problematic since, in general, floating-point comparisons based on fixed tolerance may not form an equivalence relation, i.e., the comparisons may not be transitive. The more versions are involved, the more acute this problem becomes. This paper discusses an approach that in some situations alleviates this problem by forming a fuzzy equivalence relation (that preserves transitivity). We applied the method to different voting techniques such as consensus voting and maximum likelihood voting. Our analyses, based on simulations, show that voting that uses fuzzy equivalence relations performs better than â€œclassicalâ€?voting provided some criteria are met",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=682152,no,undetermined,0
Evaluation of fault-tolerance latency from real-time application's perspectives,"Information on Fault-Tolerance Latency (FTL), which is defined as the total time required by all sequential steps taken to recover from an error, is important to the design and evaluation of fault-tolerant computers used in safety-critical real-time control systems with deadline information. In this paper we evaluate FTL in terms of several random and deterministic variables accounting for fault behaviors and/or the capability and performance of error-handling mechanisms, while considering various fault-tolerance mechanisms based on the tradeoff between temporal and spatial redundancy, and use the evaluated FTL to check if an error-handling policy can meet the Control System Deadline (CSD) for a given real-time application",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=682154,no,undetermined,0
Design and analysis of a fair scheduling algorithm for QoS guarantees in high-speed packet-switched networks,"B-ISDNs are required to support a variety of services such as audio, data, and video, so that the guarantee of quality-of-service (QoS) has become an increasingly important problem. An effective fair scheduling algorithm permits high-speed switches to divide link bandwidth fairly among competing connections. Together with the connection admission control, it can guarantee the QoS of connections. We propose a novel fair scheduling algorithm, called â€œvirtual-time-based round robin (VTRR)â€? Our scheme maps the priorities of packets into classes and provides service to the first non-empty class in each round. Also, it uses an estimation method of the virtual time necessary to this service discipline. To find the first non-empty class, the VTRR adopts a priority queueing system of O(loglog c) which decreases the number of instructions which need to be carried out in one packet transmission time segment. These policies help the VTRR implementation in software, which presents flexibility for upgrades. Our analysis has demonstrated that the VTRR provides bounded unfairness and its performance is close to that of weighted fair queuing. Therefore, the VTRR has a good performance as well as simplicity, so that it is suitable for high-speed B-ISDN",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=683078,no,undetermined,0
An integrated approach to achieving high software reliability,"In this paper we address the development, testing, and evaluation schemes for software reliability, and the integration of these schemes into a unified and consistent paradigm. Specifically, techniques and tools for the three software reliability engineering phases are described. The three phases are (1) modeling and analysis, (2) design and implementation, and (3) testing and measurement. In the modeling and analysis phase we describe Markov modeling and fault-tree analysis techniques. We present system-level reliability models based on these techniques, and provide modeling examples for reliability analysis and study. We describe how reliability block diagrams can be constructed for a real-world system for reliability prediction, and how critical components can be identified. We also apply fault tree models to fault tolerant system architectures, and formulate the resulting reliability quantity. Finally, we describe two software tools, SHARPE and UltraSAN, which are available for reliability modeling and analysis purpose",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=682162,no,undetermined,0
An efficient strategy for developing a simulator for a novel concurrent multithreaded processor architecture,"In developing a simulator for a new processor architecture, it often is not clear whether it is more efficient to write a new simulator or to modify an existing simulator. Writing a new simulator forces the processor architect to develop or adapt all of the related software tools. However, modifying an existing simulator and related tools, which are usually not well-documented, can be time-consuming and error-prone. We describe the SImulator for Multithreaded Computer Architectures (SIMCA) that was developed with the primary goal of obtaining a functional simulator as quickly as possible to begin evaluating the superthreaded architecture. The performance of the simulator itself was important, but secondary. We achieved our goal using a technique called process-pipelining that exploits the unique features of this new architecture to hide the details of the underlying simulator. This approach allowed us to quickly produce a functional simulator whose performance is only 3.8-4.9 times slower than the base simulator",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=693693,no,undetermined,0
Prediction of distribution transformer no-load losses using the learning vector quantization neural network,"This paper presents an artificial neural network (ANN) approach to classification of distribution transformer no-load losses. The learning vector quantization (LVQ) neural network architecture is applied for this purpose. The ANN is trained to learn the relationship among data obtained from previous completed transformer constructions. For the creation of the training and testing set actual industrial measurements are used. Data comprise grain oriented steel electrical characteristics, cores constructional parameters, quality control measurements of cores production line and transformers assembly line measurements. It is shown that ANNs are very suitable for this application since they present classification success rates between 78% and 96% for all the situations examined",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=699420,no,undetermined,0
VHDL-based distributed fault simulation using SAVANT,"There is a need for simulator-independent, VHDL-based fault simulation. Existing techniques for VHDL-based fault simulation are reviewed. Robust, a simulator-independent fault simulator tool, is described. Slow simulation speed is identified as one limitation of the current Robust tool and distributed simulation on a network of workstations is identified as a feasible way to improve its performance. Previous network-of-workstations fault simulation experiments are reviewed. Current efforts to enhance the Robust tool using SAVANT ate described. A system using Robust (with SAVANT extensions) for fault simulation on a network of workstations is proposed, using the TyVIS VHDL simulation kernel and the Legion distributed processing system",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=710203,no,undetermined,0
Globally convergent edge-preserving regularized reconstruction: an application to limited-angle tomography,"We introduce a generalization of a deterministic relaxation algorithm for edge-preserving regularization in linear inverse problems. This algorithm transforms the original (possibly nonconvex) optimization problem into a sequence of quadratic optimization problems, and has been shown to converge under certain conditions when the original cost functional being minimized is strictly convex. We prove that our more general algorithm is globally convergent (i.e., converges to a local minimum from any initialization) under less restrictive conditions, even when the original cost functional is nonconvex. We apply this algorithm to tomographic reconstruction from limited-angle data by formulating the problem as one of regularized least-squares optimization. The results demonstrate that the constraint of piecewise smoothness, applied through the use of edge-preserving regularization, can provide excellent limited-angle tomographic reconstructions. Two edge-preserving regularizers-one convex, the other nonconvex-are used in numerous simulations to demonstrate the effectiveness of the algorithm under various limited-angle scenarios, and to explore how factors, such as the choice of error norm, angular sampling rate and amount of noise, affect the reconstruction quality and algorithm performance. These simulation results show that for this application, the nonconvex regularizer produces consistently superior results",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=660997,no,undetermined,0
Neural-network techniques for software-quality evaluation,"Software quality modeling involves identifying fault-prone modules and predicting the number of errors in the early stages of the software development life cycle. This paper investigates the viability of several neural network techniques for software quality evaluation (SQE). We have implemented a principal component analysis technique (used in SQE) with two different neural network training rules, and have classified software modules as fault-prone or nonfault-prone using software complexity metric data. Our results reveal that neural network techniques provide a good management tool in a software engineering environment",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=653706,no,undetermined,0
MEADEP and its applications in evaluating dependability for air traffic control systems,"MEADEP (measure dependability) is a user-friendly dependability evaluation tool for measurement-based analysis of computing systems including both hardware and software. Features of MEADEP are: a data processor for converting data in various formats (records with a number of fields stored in a commercial database format) to the MEADEP format, a statistical analysis module for graphical data presentation and parameter estimation, a graphical modeling interface for constructing reliability block and Markov diagrams, and a model solution module for availability/reliability calculation with graphical parametric analysis. Use of the tool on failure data from measurements can provide quantitative assessments of dependability for critical systems, while greatly reducing requirements for specialized skills in data processing, analysis, and modeling from the user. MEADEP has been applied to evaluate dependability for several air traffic control systems (ATC) and results produced by MEADEP have provided valuable feedback to the program management of these critical systems",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=653729,no,undetermined,0
High-availability transaction processing: practical experience in availability modeling and analysis,"High availability is driven by two types of factors: customer site factors such as the frequency of software and hardware upgrades, and system factors such as failure and repair rates, most often associated with mathematical models of reliability and availability. In this paper we describe several tools to assess the effects of these factors on the availability of high availability transaction processing (HATP) systems and make the expected level of performance more understandable to customers who purchase such systems, sales people who sell them, and managers who must make decisions based on the system availability. We employ a survey methodology to identify the key customer site factors that drive availability; we illustrate an accurate but greatly simplified technique for modeling system availability based on the internal system factors; and we apply statistical design of experiments and control chart methodologies to better understand the variability inherent in the system performance",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=653785,no,undetermined,0
Diagnosis and monitoring of high-voltage insulation using computerized analysis of partial discharge measurements,"Transmission of electrical power requires various types of apparatus. All the apparatus and transmission-line conductors must be insulated from the high and extra-high voltages arising from the need of transmitting a large quantum of energy. As the need grows, the quality and reliability of the insulator must also grow to handle extra stresses. Hence an attempt is made to evaluate insulation in power apparatus based on the computerized analysis of partial discharge (PD) testing measurements and observations, leading to the identification of PD fault-sources of electrical insulation degradation. The database offers a decision-making aid for product development and supports condition monitoring, assessing the state of insulation. By conducting suitable experiments on two-electrode models, one does partial discharge characterization by considering one PD defect at a time. Using the database extracted from the various PD distributions attained from physical models fabricated, PD sources or defects in simple power apparatus such as surge arrestors, current transformers and artificially created defects such as slot models and rigid terminal connectors are identified successfully and evaluated. Successful recognition rate is achieved with the developed software package even in the presence of two PD sources; say, surface and internal discharges contributing to electrical insulation degradation.",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7101952,no,undetermined,0
Measuring the effectiveness of a structured methodology: a comparative analysis,This study evaluates a vendor supplied structured methodology which was implemented in a large manufacturing organization. Thirty projects using the methodology are measured for efficiency and effectiveness of software maintenance performance. The performance of these projects is evaluated using both objective metrics and subjective measures taken from stakeholders of the software applications. The performance results of these thirty systems are then contrasted to the performance results of thirty five applications in the same organization that do not use this structured methodology and to one hundred and sixteen applications across eleven other organizations. All of these applications had been in operation at least six months when they were studied. The software applications developed and maintained using the structured methodology were found to have some significant cost and quality performance gains over the applications that do not use this methodology,1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=654809,no,undetermined,0
Towards more efficient TMN model development. TMN information modal-quality evaluation,"There has been a dramatic growth in the demand for telecommunications management systems since the advent of a large volume of new technologies for telecommunications service delivery (SDH, ATM, FR, etc.). TMN is an approach favoured by many management systems users and providers. Development of TMNs is a complex issue. Among the task to be performed during TMN development is the modelling of information to be exchanged across TMN interfaces. Quality is an important matter in all system development activities and this is also the case when producing the TMN information models. It is preferable to detect and resolve errors in all software artifacts as soon as possible in the system life-cycle as the costs of making corrections grows as development progresses",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=654898,no,undetermined,0
Analysis of preventive maintenance in transactions based software systems,"Preventive maintenance of operational software systems, a novel technique for software fault tolerance, is used specifically to counteract the phenomenon of software â€œagingâ€? However, it incurs some overhead. The necessity to do preventive maintenance, not only in general purpose software systems of mass use, but also in safety-critical and highly available systems, clearly indicates the need to follow an analysis based approach to determine the optimal times to perform preventive maintenance. In this paper, we present an analytical model of a software system which serves transactions. Due to aging, not only the service rate of the software decreases with time, but also the software itself experiences crash/hang failures which result in its unavailability. Two policies for preventive maintenance are modeled and expressions for resulting steady state availability, probability that an arriving transaction is lost and an upper bound on the expected response time of a transition are derived. Numerical examples are presented to illustrate the applicability of the models",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=656092,no,undetermined,0
A fault detection service for wide area distributed computations,"The potential for faults in distributed computing systems is a significant complicating factor for application developers. While a variety of techniques exist for detecting and correcting faults, the implementation of these techniques in a particular context can be difficult. Hence, we propose a fault detection service designed to be incorporated, in a modular fashion, into distributed computing systems, tools, or applications. This service uses well-known techniques based on unreliable fault detectors to detect and report component failure, while allowing the user to tradeoff timeliness of reporting against false positive rates. We describe the architecture of this service, report on experimental results that quantify its cost and accuracy, and describe its use in two applications, monitoring the status of system components of the GUSTO computational grid testbed and as part of the NetSolve network-enabled numerical solver",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=709981,no,undetermined,0
Enhancing multimedia protocol performance through knowledge based modeling,"Multimedia transmission protocols take advantage of video compression coders. The core of video sequence compression coders is the motion estimation task. Unfortunately video sequence coders cannot be implemented effectively on desktop workstations because of the high computing requirements of the motion estimation task. It is possible to reduce the computing power required penalizing the compression ratio (i.e. increasing output bandwidth) and/or video quality, but in case of standard software coders running on desktop computers the penalty of compression and quality becomes unacceptable. In order to improve both the compression ratio and processing time we propose to enhance multimedia protocol design through the modeling of the specific application domain. We show the technique in a case study, namely the design of a MPEG coding algorithm targeted to compress video sequences generated by highway surveillance cameras. In addition me show the effectiveness of the technique with experimental results",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=659929,no,undetermined,0
A new technique for motion estimation and compensation of the wavelet detail images,"This work proposes a new block based motion estimation and compensation technique applied on the detail images of the wavelet pyramidal decomposition. The algorithm implements two block matching criteria, namely the absolute difference (AD) and the absolute sum (AS). To assess the coding performance of this method, we have implemented a software simulation of a wavelet video encoder based on our square partitioning coder. Coding results indicate a considerable image quality gain, expressed by PSNR values, of up to 3.4 dB compared to intra wavelet coding for the same bit rate. The quality gain, obtained by using two matching criteria (AS and AD) instead of one (AD), varies between 0.3 and 0.5 dB.",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7090011,no,undetermined,0
New network QoS measures for FEC-based audio applications on the Internet,"New network QoS (quality of service) measures for interactive audio applications using FEC (forward error control) are proposed. Applications such as an Internet phone require both low data loss and a short delay for the underlying transport layer. The FEC-based error control has become popular as a way of meeting such requirements; new application data (or highly compressed data) are copied onto successive packets, so that random packet losses in networks can be concealed to some extent. From the viewpoint of FEC-based applications, actual QoS depends not only on the loss and delay of each packet, but also on the loss and delay of successive packets. Conventional network QoS measures such as loss rate and delay distribution, however, only focus on each packet. Therefore, the probability of long successive losses, for example, cannot be monitored, even though they strongly affect FEC-based application QoS. We propose a new concept named â€œloss window sizeâ€?for measuring the QoS of successive packets. Definitions of loss and delay are generalized using this concept. These definitions take the FEC-based error concealment into account. Therefore, these measures enable more precise estimation of FEC-based application-level QoS than conventional measures. In order to show the effectiveness of the proposed measures, we have built an experimental monitoring system on working networks. The actual data show that network QoS may vary from time to time in terms of newly defined measures, even though QoS variation using conventional measures are not so apparent. We also model FEC-based application QoS, and show that application QoS and proposed network measures correspond well",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=660065,no,undetermined,0
A study on the application of wavelet analysis to power quality,"The quality of electric power is a major concern, since poor electric power quality results in malfunctions, instabilities and shorter lifetime of the load. The poor quality of electric power can be attributed to power line disturbances such as waveshape faults, overvoltages, capacitor switching transients, harmonic distortion and impulse transients. For diagnosing power quality problems, the causes of the disturbances should be understood before appropriate actions can be taken. Monitoring the voltages and currents is the first requirement in this process. In order to determine the causes, one should be able to classify the type of disturbances from the monitored data. Several approaches such as point-to-point comparison of adjacent systems, neural networks and expert systems have been proposed and implemented in the past. Wavelet analysis is gaining greater importance for the recognition of disturbances. A study about the performance of wavelets in detecting and classifying the power quality disturbances is reported in this paper",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=660091,no,undetermined,0
Application of a fuzzy classification technique in computer grading of fish products,"This work presents the enhancement and application of a fuzzy classification technique for automated grading of fish products. Common features inherent in grading-type data and their specific requirements in processing for classification are identified. A fuzzy classifier with a four-level hierarchy is developed based on the â€œgeneralized K-nearest neighbor rulesâ€? Both conventional and fuzzy classifiers are examined using a realistic set of herring roe data (collected from the fish processing industry) to compare the classification performance in terms of accuracy and computational cost. The classification results show that the generalized fuzzy classifier provides the best accuracy at 89%. The grading system can be tuned through two parameters-the threshold of fuzziness and the cost weighting of error types-to achieve higher classification accuracy. An optimization scheme is also incorporated into the system for automatic determination of these parameter values with respect to a specific optimization function that is based on process renditions, including the product price and labor cost. Since the primary common features are accommodated in the classification algorithm, the method presented here provides a general capability for both grading and sorting-type problems in food processing",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=660814,no,undetermined,0
Prediction of decoding time for MPEG-4 video,"This paper presents a new technique capable of predicting the processing time of video decoding tasks. Such technique is based on the collection and transmission in the compressed video bit-stream, of a suitable statistic information relative to the decoding process. Simulation results show that very accurate decoding time predictions can be obtained without the need of the actual decoding. These predictions are useful for the fundamental problem of guaranteeing, at the same time, realtime performance and an efficient use of the processing power in processor based video decoders. This new approach gives the possibility of implementing optimal resource allocation strategies and new interaction schemes between the decoder and the real-time OS. Moreover, it is the key for providing a high quality of services when the required decoding resources exceed the available ones. The described method, which is of general application on any processor platform, has been included in the new video MPEG-4 ISO standard.",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7090007,no,undetermined,0
Computational graceful degradation for video decoding,"The implementation of software based video decoders is an advantageous solution over traditional dedicated hardware real-time systems. However, guaranteeing real-time performance, needed when processing video/audio bit-streams, by scheduling processing tasks of variable load and that might exceed the available resources, is an extremely difficult and in many cases an impossible challenge. In this paper we introduce the motivations and ideas of the Computational Graceful Degradation (CGD) approach for the implementation of video decoding. The goal is to provide results of possibly lower visual quality, but respecting the real-time constraints, when the required processing needs exceed the available processing resources. This approach is very attractive since it enables a more efficient usage of the hardware processing power without needing to implement resources for worst case complexity decoding. We show how such technique can be applied to the decoding of compressed video sequences. Simulation results for MPEG-4 and H.263 video compression standards showing some of the interesting achievements and potentialities of CGD approach are also reported. The presented approach can be virtually applied to any video compression standard and processor based platform, although the best performances are achieved when complexity prediction information provided as side information by the video standard is available. This option has been included in the new MPEG-4 standard.",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7089561,no,undetermined,0
Learning visually guided grasping: a test case in sensorimotor learning,"We present a general scheme for learning sensorimotor tasks, which allows rapid online learning and generalization of the learned knowledge to unfamiliar objects. The scheme consists of two modules, the first generating candidate actions and the second estimating their quality. Both modules work in an alternating fashion until an action which is expected to provide satisfactory performance is generated, at which point the system executes the action. We developed a method for off-line selection of heuristic strategies and quality predicting features, based on statistical analysis. The usefulness of the scheme was demonstrated in the context of learning visually guided grasping. We consider a system that coordinates a parallel-jaw gripper and a fixed camera. The system learns to estimate grasp quality by learning a function from relevant visual features to the quality. An experimental setup using an AdeptOne manipulator was developed to test the scheme",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=668958,no,undetermined,0
Analysis of checkpointing schemes with task duplication,"The paper suggests a technique for analyzing the performance of checkpointing schemes with task duplication. We show how this technique can be used to derive the average execution time of a task and other important parameters related to the performance of checkpointing schemes. The analysis results are used to study and compare the performance of four existing checkpointing schemes. Our comparison results show that, in general, the number of processors used, not the complexity of the scheme, has the most effect on the scheme performance",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=663769,no,undetermined,0
Lessons from using Z to specify a software tool,"The authors were recently involved in the development of a COBOL parser (G. Ostrolenk et al., 1994), specified formally in Z. The type of problem tackled was well suited to a formal language. The specification process was part of a life cycle characterized by the front loading of effort in the specification stage and the inclusion of a statistical testing stage. The specification was found to be error dense and difficult to comprehend. Z was used to specify inappropriate procedural rather than declarative detail. Modularity and style problems in the Z specification made it difficult to review. In this sense, the application of formal methods was not successful. Despite these problems the estimated fault density for the product was 1.3 faults per KLOC, before delivery, which compares favorably with IBM's Cleanroom method. This was achieved, despite the low quality of the Z specification, through meticulous and effort intensive reviews. However, because the faults were in critical locations, the reliability of the product was assessed to be unacceptably low. This demonstrates the necessity of assessing reliability as well as â€œcorrectnessâ€?during system testing. Overall, the experiences reported in the paper suggest a range of important lessons for anyone contemplating the practical application of formal methods",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=663995,no,undetermined,0
A product-process dependency definition method,"The success of most software companies depends largely on software product quality. High product quality is usually a result of advanced software development processes. Hence, improvement actions should be selected based on sound knowledge about the dependencies between software product quality attributes and software development processes. The article describes a method for developing product/process dependency models (PPDMs) for product driven software process improvement. The basic idea of the PPDM approach is that there are dependencies between product quality attributes, which are examined according to ISO 9126, and the software processes, which are assessed with the BOOTSTRAP methodology for example. The Goal-Question-Metric approach is used for product/process dependency hypothesis generation, analysis, and validation. We claim that after finding and using these dependencies, it is possible to focus improvement activities precisely and use resources more efficiently. The approach is currently being applied in three industrial applications in the ESPRIT project PROFES",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=708119,no,undetermined,0
Practical aspects on the assessment of a review process,"The article presents results and lessons learned from a case study done to the Fixed Switching Product Line of Nokia Telecommunications. The study was initiated on the fact, that no regular monitoring was done on the efficiency or profitability of the reviews at the time. Two consecutive CMM self assessments showed that Peer Reviews of CMM Level 3 KPA was fully compatible. The results of this case study suggest that the software review process is clearly established, but not optimised or used to the full potential. The metrics used in this case study can be used in other organisations as well, to assess some aspects of the review process",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=708114,no,undetermined,0
Performance and quality aspects of virtual software enterprises,"High performance organization concepts are applicable to software development and necessary to meet the business goals in a globalized software market. Therefore it is necessary for individual software companies to cooperate with other organizations to stay competitive. In this case, these companies interact in a holonic network which is the basic structure of a Virtual Software Enterprise and have to contribute their core competencies. The companies have to be able to trust in the ability of the processes of other companies to meet the predicted performance and quality goals. As a consequence, different quality management processes have to be established. As the individual companies are locally distributed they need good information and communication technologies to work together. Groupware systems can help to implement a Virtual Software Enterprise",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=708108,no,undetermined,0
Software testability measurements derived from data flow analysis,"The purpose of the research is to develop formulations to measure the testability of a program. Testability is a program's property which is introduced with the intention of predicting efforts required for testing the program. A program with a high degree of testability indicates that a selected testing criterion could be achieved with less effort and the existing faults can be revealed more easily during testing. We propose a new program normalization strategy that makes the measurement of testability more precise and reasonable. If the program testability metric derived from data flow analysis could be applied at the beginning of a software testing phase, much more effective testing of resource allocation and prioritizing is possible",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=665760,no,undetermined,0
"Design, testing, and evaluation techniques for software reliability engineering","Software reliability is closely influenced by the creation, manifestation and impact of software faults. Consequently, software reliability can be improved by treating software faults properly, using techniques of fault tolerance, fault removal, and fault prediction. Fault tolerance techniques achieve the design for reliability, fault removal techniques achieve the testing for reliability, and fault prediction techniques achieve the evaluation for reliability. We present best current practices in software reliability engineering for the design, testing, and evaluation purposes. We describe how fault tolerant components can be applied in software applications, how software testing can be conducted to show improvement of software reliability, and how software reliability can be modeled and measured for complex systems. We also examine the associated tools for applying these techniques",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=708059,no,undetermined,0
Modeling maintenance effort by means of dynamic systems,"The dynamic evolution of ecological systems in which predators and prey compete for survival has been investigated by applying suitable mathematical models. Dynamic systems theory provides a useful way to model interspecies competition and thus the evolution of predators and prey populations. This kind of mathematical framework has been shown to be well suited to describe evolution of economical systems as well, where instead of predators and prey there are consumers and resources. This paper suggests how dynamic systems could be usefully applied to the maintenance context, namely to model the dynamic evolution of the maintenance effort. When maintainers start trying to recognize and correct code defects, while the number of residual defects decreases, the effort spent to find out any new defect has an initial increase, followed by a decline, in a similar way to prey and predator populations. The feasibility of this approach is supported by the experimental data about a 67 months maintenance task of a software project and its successive releases",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=665787,no,undetermined,0
Action models: a reliability modeling formalism for fault-tolerant distributed computing systems,"Modern-day computing system design and development is characterized by increasing system complexity and ever shortening time to market. For modeling techniques to be deployed successfully, they must conveniently deal with complex system models, and must be quick and easy to use by non-specialists. In this paper we introduce â€œaction modelsâ€? a modeling formalism that tries to achieve the above goals for reliability evaluation of fault-tolerant distributed computing systems, including both software and hardware in the analysis. The metric of interest in action models is the job success probability, and we will argue why the traditional availability metric is insufficient for the evaluation of fault-tolerant distributed systems. We formally specify action models, and introduce path-based solution algorithms to deal with the potential solution complexity of created models. In addition, we show several examples of action models, and use a preliminary tool implementation to obtain reliability results for a reliable clustered computing platform",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=707715,no,undetermined,0
A preliminary testability model for object-oriented software,"In software quality assurance, two approaches have been used to improve the effectiveness of fault detection. One attempts to find more efficient testing strategies; the other tries to use the design characteristics of the software itself to increase the probability of a fault revealing itself if it does exist. The latter which combines the best feature of fault prevention and fault detection, is also known as the testability approach to software quality assurance. This paper examines the factors that affect software testability in object-oriented software and proposes a preliminary framework for the evaluation of software testability metrics. The ultimate aim is to formulate a set of guidelines in object-oriented design to improve software quality by increasing their testability",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=707667,no,undetermined,0
Teaching black box testing,"Historically, software testing received relatively less attention compared with other activities (e.g. systems analysis and design) of the software life cycle in an undergraduate computer science/information systems curriculum. Nevertheless, it is a common and important technique used to detect errors in software. This paper reports our recent experience of using a new approach to teaching software testing (particularly black box testing) at the University of Melbourne. Through this paper, we aim to increase the profile of software testing as well as to foster discussion of effective teaching methods of software testing",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=707666,no,undetermined,0
Geophysical signatures from precise altimetric height measurements in the Arctic Ocean,"The use of altimeter data in the polar regions has previously been limited by the presence of permanent and seasonal ice cover. Changes in the radar echo shape received by the altimeter over sea ice, as compared with the open ocean, cause problems in the on-board estimates of surface height, making the data unusable. The majority of noise on the signal can be reduced by retracking the full waveform data set (WAP). Careful quality control is applied to ensure that only those return echoes from which accurate height measurements can be obtained are retained. Consideration of possible backscattering mechanisms from ice and water, and comparisons with imagery, suggest that the specular waveforms typically found in altimeter data over sea ice originate from regions of open water or new thin ice exposed within the altimeter footprint. However, diffuse waveforms similar to those found in ice free seas have been observed in areas of consolidated ice, and may be used to measure ice freeboard. Until recently, even retracked heights contained substantial residual errors due to the interaction of the on-board tracking system with the complex return echoes over sea ice. Software simulation of the tracking system has led to the development of new ground processing algorithms, which further reduce the short wavelength (~26 km) noise, from 30-50 cm to around 7 cm. This provides, for the first time in ice covered seas, the capability for accurate mean sea surface generation, measurement of tidal and oceanographic signals and determination of sea ice freeboard. The authors present the results of comparisons of sea surface height variability from ERS-2 radar altimetry in the Arctic with the output from a high resolution Arctic Ocean circulation model",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=703709,no,undetermined,0
Monitoring and fault diagnosis of a polymerization reactor by interfacing knowledge-based and multivariate SPM tools,"An intelligent process monitoring and fault diagnosis environment is developed by interfacing multivariate statistical process monitoring (MSPM) techniques and knowledge-based systems (KBS) for monitoring continuous multivariable process operation. The software is tested by monitoring the performance of a continuous stirred tank reactor for polymerization of vinyl acetate. The real-time KBS G2 and its diagnostic assistant (GDA) tool are integrated with MSPM methods based on canonical variate state space (CVSS) process models. Fault detection is based on T <sup>2</sup> of state variables and squared prediction errors (SPE) charts. Contribution plots in G2 are used for determining the process variables that have contributed to the out-of-control signal indicated by large T<sup>2</sup> and/or SPE values, and GDA is used to diagnose the source cause of the abnormal process behavior. The MSPM modules developed in Matlab are linked with G2 and GDA, permitting the use of MSPM tools for multivariable processes with autocorrelated data. The presentation will focus on the structure and performance of the integrated system. On-line SPM of the multivariable polymerization process is illustrated by simulation studies",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=703350,no,undetermined,0
Adaptive thresholding for proactive network problem detection,"The detection of network fault scenarios has been achieved using the statistical information contained in the Management Information Base (MIB) variables. An appropriate subset of MIB variables was chosen in order to adequately describe the function of the node. The time series data obtained from these variables was analyzed using a sequential generalized likelihood ratio (GLR) test. The GLR test was used to detect the change points in the behavior of the variables. Using a binary hypothesis test, variable level alarms were generated based on the magnitude of the detected changes as compared to the normal situation. These alarms were combined using a duration filter resulting in a set of node level alarms, which correlated with the experimentally observed network faults and performance problems. The algorithm has been tested on real network data. The applicability of our algorithm to a heterogeneous node was confirmed by using the MIB data from a second node. Interestingly, for most of the faults studied, detection occurs in advance of the fault (at least 5 min) and the algorithm is simple enough for potential online implementation: thus allowing the possibility of prediction and recovery in the future",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=668144,no,undetermined,0
Coupled fluid-thermo-structures simulation methodology for MEMS applications,"A coupled fluid-thermo-structures solution methodology is being developed for application to MEMS. State-of-the-art computational methods are used for simulation of a wide variety of MEMS types and configurations. The proposed software is aimed to be powerful and accurate, yet simple and quick enough to be included in the design cycles of microfluidic devices for performance predictions and device optimization. A distinct feature of the present computational methodology is the fully implicit numerical coupling between all disciplines. A description of the various solver modules is presented. The accuracy of the current software is validated on a benchmark quality experiments of a flow in a microchannel and fluid-structure interaction case. The versatility is demonstrated on two practical microfluidic devices",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=635364,no,undetermined,0
Design and implementation of network services that provide end-to-end QoS in ATM-based real-time environments,"There has been an increasing need of highly predictable, timely, and dependable communication services with QoS guarantees on an end-to-end basis for embedded real-time applications and for multimedia-integrated distributed control. Performance objectives used in conventional networks-such as maximizing the throughput, minimizing the response time, or providing fairness to users-are not of the most important concern for both types of applications. Instead, resources should be appropriately reserved and managed to support quality of service (QoS) on an end-to-end basis, as well as application-specific tradeoffs among them. The main intent of this paper is thus to develop and demonstrate an environment an integrated set of networks resource management techniques, middleware layers, and network software-for supporting QoS on an end-to-end basis in ATM-based distributed real-time environments. We present an analytic QoS framework that incorporates a flow/QoS specification, a QoS translation/negotiation policy, an admission control facility, a route construction scheme, a resource reservation manager, a run-time message scheduler, and a traffic regulator and QoS monitor. Then, sue elaborate on the algorithms and mechanisms to implement these network components. To empirically analyze the interplay among different components, and the performance that results from careful coordination of all the components, we care currently implementing them as middleware layers in the Sun Solaris environment, and will comment in the paper our implementation strategies",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=635088,no,undetermined,0
Developing the 777 airplane information management system (AIMS): a view from program start to one year of service,"The 777 Airplane Information Management System (AIMS) has successfully completed its first year of airline revenue service. This in-service success follows a development and integration program that was an exceptional challenge, primarily due to the system size, technical innovation, and aggressive program schedule. Honeywell and Boeing had to work closely together to complete the design and development of AIMS in a time frame that supported the 777 early extended twin operations (ETOPS) goal and the initial 777 airplane certification. With teams from the two companies working as one unit, redundant activities were eliminated, technical and program problems were identified and solved rapidly, and schedule time was saved by both teams helping with tasks that were traditionally considered to be the other team's job. In service, airline maintenance personnel are finding that the AIMS Central Maintenance Computer System is a tremendous asset in maintaining the 777, and AIMS reliability is exceeding predictions.",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=588382,no,undetermined,0
Robust search algorithms for test pattern generation,"In recent years several highly effective algorithms have been proposed for Automatic Test Pattern Generation (ATPG). Nevertheless, most of these algorithms too often rely on different types of heuristics to achieve good empirical performance. Moreover there has not been significant research work on developing algorithms that are robust, in the sense that they can handle most faults with little heuristic guidance. In this paper we describe an algorithm for ATPG that is robust and still very efficient. In contrast with existing algorithms for ATPG, the proposed algorithm reduces heuristic knowledge to a minimum and relies on an optimized search algorithm for effectively pruning the search space. Even though the experimental results are obtained using an ATPG tool built on top of a Propositional Satisfiability (SAT) algorithm, the same concepts can be integrated on application-specific algorithms.",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=614088,no,undetermined,0
A visual search system for video and image databases,"This paper describes a new methodology for image search that is applicable to both image libraries and keyframe search over video libraries. In contrast to previous approaches, which require templates or direct manipulation of low-level image parameters, our search system classifies images into a pre-defined subject lexicon, including terms such as trees and flesh tones. The classification is performed off-line using neural network algorithms. Query satisfaction is performed on-line using only the image tags. Because most of the work is done off-line, this methodology answers queries much more quickly than techniques that require direct manipulation of images to answer the query. We also believe that pre-defined subjects are easier for users to understand when searching programmatic video material. Experiments using keyframes extracted by our video library system show that the methodology gives high-quality query results with fast on-line performance",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=609764,no,undetermined,0
The T experiments: errors in scientific software,"Extensive tests showed that many software codes widely used in science and engineering are not as accurate as we would like to think. It is argued that better software engineering practices would help solve this problem, but realizing that the problem exists is an important first step",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=609829,no,undetermined,0
"Identification of green, yellow and red legacy components","Software systems are often getting older than expected, and it is a challenge to try to make sure that they grow old gracefully. This implies that methods are needed to ensure that system components are possible to maintain. In this paper, the need to investigate, classify and study software components is emphasized. A classification method is proposed. It is based on classifying the software components into green, yellow and red components. The classification scheme is complemented with a discussion of suitable models to identify problematic components. The scheme and the models are illustrated in a minor case study to highlight the opportunities. The long term objective of the work is to define methods, models and metrics which are suitable to use in order to identify software components which have to be taken care of through either tailored processes (e.g. additional focus on verification and validation) or reengineering. The case study indicates that the long term objective is realistic and worthwhile.",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=738484,no,undetermined,0
A Predictive Metric Based on Discriminant Statistical Analysis,"<div style=""font-variant: small-caps; font-size: .9em;"">First Page of the Article</div><img class=""img-abs-container"" style=""width: 95%; border: 1px solid #808080;"" src=""/xploreAssets/images/absImages/00610267.png"" border=""0"">",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=610267,no,undetermined,0
An Investigation into Coupling Measures for C++,"<div style=""font-variant: small-caps; font-size: .9em;"">First Page of the Article</div><img class=""img-abs-container"" style=""width: 95%; border: 1px solid #808080;"" src=""/xploreAssets/images/absImages/00610307.png"" border=""0"">",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=610307,no,undetermined,0
Benchmarking of commercial software for fault detection and classification (FDC) of plasma etchers for semiconductor manufacturing equipment,"In order to evaluate the performance of a large number of commercial FDC software, we conducted a benchmarking study at SEMATECH. The purpose of this paper is to report the tool data and the procedures that we were successfully able to use to benchmark the FDC software. These procedures can be used to compare FDC software based on similar or different principals. The results from this benchmarking have allowed the SEMATECH member companies to choose the software for beta-testing in the production environment. We conducted the benchmarking, by using the tool data obtained from three SEMATECH member companies, Texas Instruments, MOTOROLA and Lucent Technologies. Tool data was put on a secured FTP site with the instructions for solving the benchmarking problems. The software suppliers downloaded the data, analyzed the data and returned the results within the specified time-limits. The best results indicated that it may be possible to predict the tool faults in advance",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=610849,no,undetermined,0
A fault-tolerant dynamic scheduling algorithm for multiprocessor real-time systems and its analysis,"Many time-critical applications require dynamic scheduling with predictable performance. Tasks corresponding to these applications have deadlines to be met despite the presence of faults. In this paper, we propose an algorithm to dynamically schedule arriving real-time tasks with resource and fault-tolerant requirements on to multiprocessor systems. The tasks are assumed to be nonpreemptable and each task has two copies (versions) which are mutually excluded in space, as well as in time in the schedule, to handle permanent processor failures and to obtain better performance, respectively. Our algorithm can tolerate more than one fault at a time, and employs performance improving techniques such as 1) distance concept which decides the relative position of the two copies of a task in the task queue, 2) flexible backup overloading, which introduces a trade-off between degree of fault tolerance and performance, and 3) resource reclaiming, which reclaims resources both from deallocated backups and early completing tasks. We quantify, through simulation studies, the effectiveness of each of these techniques in improving the guarantee ratio, which is defined as the percentage of total tasks, arrived in the system, whose deadlines are met. Also, we compare through simulation studies the performance our algorithm with a best known algorithm for the problem, and show analytically the importance of distance parameter in fault-tolerant dynamic scheduling in multiprocessor real-time systems",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=735960,no,undetermined,0
Performance of software-based MPEG-2 video encoder on parallel and distributed systems,"Video encoding due to its high processing requirements has been traditionally done using special-purpose hardware. Software solutions have been explored but are considered to be feasible only for nonreal-time applications requiring low encoding rates. However, a software solution using a general-purpose computing system has numerous advantages: it is more available and flexible and allows experimenting with and hence improving various components of the encoder. In this paper, we present the performance of a software video encoder with MPEG-2 quality on various parallel and distributed platforms. The platforms include an Intel Paragon XP/S and an Intel iPSC/860 hypercube parallel computer as well as various networked clusters of workstations. Our encoder is portable across these platforms and uses a data-parallel approach in which parallelism is achieved by distributing each frame across the processors. The encoder is useful for both real-time and nonreal-time applications, and its performance scales according to the available number of processors. In addition, the encoder provides control over various parameters such as the size of the motion search window, buffer management, and bit rate. The performance results include comparisons of execution times, speedups, and frame encoding rates on various systems",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=611179,no,undetermined,0
Computation-constrained fast MPEG-2 encoding,"We introduce a computation-constrained predictive motion estimation algorithm for fast Motion Picture Expert Group-2 (MPEG-2) encoding. Motion estimation is restricted to a relatively small search area whose center is a two-dimensional (2D) median predicted motion vector. The number of computations is further reduced by incorporating explicitly a computational constraint during the estimation process. Experimental results show that the median prediction as well as the dynamic nature of the implementation lead to a dramatic increase in motion estimation speed for only a small loss in compression performance. A unique feature of our algorithm is that the reconstruction quality of the resulting MPEG-2 coder is proportional to the computer's processing power, providing much needed flexibility in many MPEG-2 software applications.",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=611283,no,undetermined,0
Experimental control in a visual engineering environment: the use of DT VEE<sup>TM</sup> in building a system for water quality assay,"This paper describes the use of Visual Engineering Environment (VEE) software in the control of a water quality monitoring system. The main purpose of the monitoring project was to assess the effectiveness of a cage culture turbidostat (CCT), relative to other bioassays, as a method for determining marine water quality (Clarkson and Leftley (I)). Bioassaying is a technique whereby living organisms, in this case algae, are used as indicators of toxin concentration",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=612653,no,undetermined,0
Reusing testing of reusable software components,"A software component that is reused in diverse settings can experience diverse operational environments. Unfortunately, a change in the operating environment can also invalidate past experience about the component's quality of performance. Indeed, most statistical methods for estimating software quality assume that the operating environment remains the same. Specifically, the probability density governing the selection of program inputs is assumed to remain constant. However, intuition suggests that such a stringent requirement is unnecessary. If a component has been executed very many times in one environment without experiencing a failure, one would expect it to be relatively failure-free in other similar environments. This paper seeks to quantify that intuition. The question asked is, â€œhow much can be said about a component's probability of failure in one environment after observing its operation in other environments?â€?Specifically, we develop bounds on the component's probability of failure in the new environment based on its past behavior",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=613254,no,undetermined,0
Static and dynamic metrics for effective object clustering,"In client/server and distributed applications, the quality of object clustering plays a key role in determining the overall performance of the system. Therefore, a set of objects with higher coupling should be grouped into a single cluster so that each cluster can have a higher cohesion. As a result, the overall message traffic among objects can be greatly minimized. In addition, it should also be considered in CORBA-based applications that clusters themselves can evolve due to the dynamic object migration feature of CORBA. Hence, dynamic metrics as well as as static metrics should be developed and used in order to measure the dynamic message traffic and to tune up the system performance effectively. Various object-oriented design metrics proposed mainly deal with static coupling and cohesion, and they only consider the basic class relationships such as association, inheritance, and composition. Therefore, these metrics are not appropriate for measuring the traffic load of object messages which is closely related to the system performance. In this paper, we propose a set of metrics which considers the relevant weights on the various class relationships and estimates the static and dynamic message flow among the objects at the detailed level of member functions. By applying these metrics along with OMT or UML, we believe that clusters can be defined more efficiently and systematically, yielding high performance distributed applications",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=733591,no,undetermined,0
A software metric for logical errors and integration testing effort,"Many software metrics are based on analysis of individual source code modules and do not consider the way that modules are interconnected. This presents a special problem for many current software development project environments that utilize a considerable amount of commercial, off-the-shelf or other reusable software components and devote a considerable amount of time to testing and integrating such components. We describe a new metric called the BVA metric that is based on an assessment of the coupling between program subunits. The metric is based on the testing theory technique known as boundary value analysis. For each parameter or global variable in a program module or subunit, we compute the number of test cases necessary for a â€œblack-boxâ€?test of a program subunit based on partitioning that portion of the domain of the subunit that is affected by the parameter. The BVA metric can be computed relatively early in the software life cycle. Experiments in several different languages and both academic and industrial programming environments suggest a close predictive relationship with the density of logical software errors and also with integration and testing effort",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=613302,no,undetermined,0
"Graceful degradation in real-time control applications using (m, k)-firm guarantee","Tasks in a real-time control application are usually periodic and they have deadline constraints by which each instance of a task is expected to complete its computation even in the adverse circumstances caused by component failures. Techniques to recover from processor failures often involve a reconfiguration in which all tasks are assigned to fault-free processors. This reconfiguration may result in processor overload where it is no longer possible to meet the deadlines of all tasks. In this paper, we discuss an overload management technique which discards selected task instances in such a way sheet the performance of the control loops in the system remain satisfactory even after a failure. The technique is based on the rationale that real-time control applications can tolerate occasional misses of the control law updates, especially if the control law is modified to account for these missed updates. The paper devises a scheduling policy which deterministic guarantees when and where the misses will occur and proposes a methodology for modifying the control law to minimize the deterioration in the control system behavior as a result of these missed control law updates.",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=614086,no,undetermined,0
Automated knowledge acquisition and application for software development projects,"The application of empirical knowledge about the environment-dependent software development process is mostly based on heuristics. In this paper, we show how one can express these heuristics by using a tailored fuzzy expert system. Metrics are used as input, enabling a prediction for a related quality factor like correctness, defined as the inverse of criticality or error-proneness. By using genetic algorithms, we are able to extract the complete fuzzy expert system out of the available data of a finished project. We describe its application for the next project executed in the same development environment. As an example, we use complexity metrics which are used to predict the error-proneness of software modules. The feasibility and effectiveness of the approach is demonstrated with results from large switching system software projects. We present a summary of the lessons learned and give our ideas about further applications of the approach",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=732686,no,undetermined,0
Testing of analogue and mixed-signal circuits by using supply current measurements,"A complete (hardware and software) system has been designed and implemented, which can measure the supply current (I<sub>PS</sub>) of analogue and mixed-mode circuits and detect or diagnose faults, based on RMS and spectrum calculations of the I<sub>PS</sub> and the corresponding fault dictionary. The system is applied to testing and diagnosis of various analogue and mixed-signal circuits, showing very encouraging results",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=739674,no,undetermined,0
ASSISTing exit decisions in software inspection,"Software inspection is a valuable technique for detecting defects in the products of software development. One avenue of research within inspection concerns the development of computer support. It is hoped that such support will provide even greater benefits when applying inspection. A number of prototype systems have been developed by researchers, yet these suffer from some fundamental limitations. One of the most serious of these concerns is the lack of facilities to monitor the process, and to provide the moderator with quantitative information on the performance of the process. The paper begins by briefly outlining the measurement component that has been introduced into the system, and especially discusses an approach to estimating the effectiveness of the current process",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=732677,no,undetermined,0
Fail-awareness: an approach to construct fail-safe applications,"We present a framework for building fail-safe hard real-time applications on top of an asynchronous distributed system subject to communication partitions, i.e. using processors and communication facilities whose real-time delays cannot be guaranteed. The basic assumption behind our approach is that each processor has a local hardware clock that proceeds within a linear envelope of real-time. This allows to compute an upper bound on the actual delays incurred by a particular processing sequence or message transmission. Services and applications can use these computed bounds to detect when they cannot guarantee all their properties because of excessive delays. This allows an application to detect when to switch to a fail-safe mode.",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=614101,no,undetermined,0
COFTA: hardware-software co-synthesis of heterogeneous distributed embedded system architectures for low overhead fault tolerance,"Hardware-software co-synthesis is the process of partitioning an embedded system specification into hardware and software modules to meet performance, cost and reliability goals. In this paper, we address the problem of hardware-software co-synthesis of fault-tolerant real-time heterogeneous distributed embedded systems. Fault detection capability is imparted to the embedded system by adding assertion and duplicate-and-compare tasks to the task graph specification prior to cosynthesis. The reliability and availability of the architecture are evaluated during co-synthesis. Our algorithm allows the user to specify multiple types of assertions for each task. It uses the assertion or combination of assertions which achieves the required fault coverage without incurring too much overhead. We propose new methods to: 1) perform fault tolerance based task clustering 2) derive the best error recovery topology using a small number of extra processing elements, 3) exploit multi-dimensional assertions, and 4) share assertions to reduce the fault tolerance overhead. Our algorithm can tackle multirate systems commonly found in multimedia applications. Application of the proposed algorithm to several real-life telecom transport system examples shows its efficacy.",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=614108,no,undetermined,0
An estimated method for software testability measurement,"Software testability is becoming an important factor to be considered during software development and assessment, especially for critical software. The paper gives software testability, previously defined by Voas (1991, 1992), a new model and measurement which is done before random black-box testing with respect to a particular input distribution. The authors also compared their measurement results with the one simulated according with Voas's model. It showed that their rough testability estimate provides enough information and will be used as guidelines for software development",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=615470,no,undetermined,0
A comparison of efficient dot throwing and shape shifting extra material critical area estimation,"The paper reports a comparison of efficient methods of estimating extra material critical area of any angled IC layout using two different techniques, shape shifting and dot throwing. Both techniques are implemented using the same polygon libraries and are optimised to make best use of the library features. This allows an accurate comparison of the techniques with minimal dependence on the specific implementation. The results presented here suggest that for general yield prediction an efficient dot throwing implementation is best suited for layouts of any significant size (designs greater than 1 MB of layout data). However, the shape shifting technique is considerably more efficient in the analysis of smaller circuits but does not scale well to larger designs",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=732150,no,undetermined,0
BOOTSTRAP: five years of assessment experience,"The main characteristics of the BOOTSTRAP method are briefly described, i.e. the reference framework, the assessment procedure, the structure of the questionnaires and the rating and scoring mechanisms used. The major part deals with the results of the BOOTSTRAP assessments performed worldwide over the last 5 years. In this part, major experiences from over 40 sites and 180 projects are analysed in detail and summarised. The empirical data support the basic concepts of the Capability Maturity Model (CMM), but demonstrate that the more detailed profiles the BOOTSTRAP method provides are an excellent instrument to identify strengths and weaknesses of an organisation. BOOTSTRAP proved to be a very efficient and effective means not only to assess the current status of software process quality but also to initiate appropriate improvement actions. The last part deals with future trends in software process assessment approaches, i.e. the development of a common framework for performing assessments",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=615525,no,undetermined,0
Heterojunction model for Focal Plane Array detector devices,"Night vision systems for military and commercial applications usually use an Infrared Focal Plane Array (IRFPA) for its radiation detector. Existing IRFPA models lack simplicity for setting up the detector's architecture/structure and lack continuity between IR detector material, IR detector processes and detector architecture. This paper will discuss the first version a new IRFPA computer model which is to be used to simulate heterojunction IRFPAs with enhanced quantitative and visual information that allow the device engineer to access the impact of material quality, processing procedures and IR detector architecture on IRFPA performance. This new model will be combined with statistical simulation to enable IRFPA design which have high performance and lowest cost",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=616705,no,undetermined,0
Performability and reliability modeling of N version fault tolerant software in real time systems,"The paper presents a hierarchical modeling approach of the N version programming in a real time environment. The model is constructed in three layers. At the first layer we distinguish the NVP structure from its operational environment. The NVP structure submodel considers both failures of functionality and failures of performance. The operational environment submodel is based on the concept of the operational profile. The second layer consists of a per run reliability and performance submodels. The first considers per run failure probabilities, while the second is responsible for modeling the series of successive runs over a mission. The information contributed by the second layer constitutes third layer models which support the evaluation of a performability and reliability over mission. The work presented, generalizes our previous work as it considers general distributions of the versions' time to failure and execution time. Also, in addition to the performability model, the third layer includes a model aimed at reliability assessment over a mission period.",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=617370,no,undetermined,0
EvA: a tool for optimization with evolutionary algorithms,"We describe the EvA software package which consists of parallel (and sequential) implementations of genetic algorithms (GAs) and evolution strategies (ESs) and a common graphical user interface. We concentrate on the descriptions of the two distributed implementations of GAs and ESs which are of most interest for the future. We present comparisons of different kinds of genetic algorithms and evolution strategies that include implementations of distributed algorithms on the Intel Paragon, a large MIMD computer and massively parallel algorithms on a 16384 processor MasPar MP-1, a large SIMD computer. The results show that parallelization of evolution strategies not only achieves a speedup in execution time of the algorithm, but also a higher probability of convergence and an increase of quality of the achieved solutions. In the benchmark functions we tested, the distributed ESs have a better performance than the distributed GAs.",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=617395,no,undetermined,0
Investigation of printed wiring board testing by using planar coil type ECT probe,A new application of eddy current testing techniques for investigating trace defects on printed circuit boards is proposed. A test probe consisting of a meander type exciting coil is used to induce eddy currents. The following three experiments are conducted: measuring the induced signal when a circuit trace is cut; measuring the induced signal for a number of traces placed in parallel and with a cut in the centre trace; measuring the induced signal for two back to back right angle traces. The experimental results reveal that it is possible to clearly detect defects and that the signal response obtained is strongly associated with a particular defect pattern. The signals obtained from a high density patterned board have a complicated signal signature and are therefore difficult to interpret. This complexity can be avoided by comparing the signal signature of a known good board with a defective board. The difference signal gives a clear indication of a trace defect,1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=617949,no,undetermined,0
Barrel shifter-a close approximation to the completely connected network in supporting dynamic tree structured computations,"High performance computing requires high quality load distribution of processes of a parallel application over processors in a parallel computer at runtime such that both maximum load and dilation are minimized. The performance of a simple randomized tree growing algorithm on the barrel shifter and the Illiac networks is studied in this paper. The algorithm spreads tree nodes by letting them to take random walks to neighboring processors. We develop recurrence relations that characterize expected loads on all processors. We find that the performance ratio of probabilistic dilation-1 tree embedding in the barrel shifter network with N processors (a network with node degree O(log N)) is very close to that in the completely connected network of the same size. However, the hypercube network, which also has node degree log N, does not have such a capability. As a matter of factor, even the Illiac network, which is a subnetwork of the barrel shifter, has an optimal asymptotic performance ratio",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=618081,no,undetermined,0
A general equipment diagnostic system and its application on photolithographic sequences,"This paper presents a general diagnostic system that can be applied to semiconductor equipment to assist the operator in finding the causes of decreased machine performance. Based on conventional probability theory, the diagnostic system incorporates both shallow and deep level information. From the observed evidence, and from the conditional probabilities of faults initially supplied by machine experts (and subsequently updated by the system), the unconditional fault probabilities and their bounds are calculated. We have implemented a software version of the diagnostic system, and tested it on real photolithography equipment malfunctions and performance drifts. Initial experimental results are encouraging",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=618207,no,undetermined,0
"A test structure advisor and a coupled, library-based test structure layout and testing environment","A new test chip design environment, based on commercial tools, containing a test structure advisor and a coupled, library-based layout and testing environment has been developed that dramatically increases productivity. The test structure advisor analyzes cross sections of devices to recommend a comprehensive list of diagnostic and parametric test structures. These test structures can then be retrieved from the libraries of parameterized structures, customized, and placed in a design to rapidly generate customized test chips. Coupling of the layout and test environments enables the automatic generation of the vast majority of the parametric test software. This environment, which has been used to design test chips for a low-power/low-voltage CMOS process, a BiCMOS trench process, and a TFT process, results in an estimated tenfold increase in productivity. In addition, the redesign of five modules of Stanford's existing BiCMOS test chip, using parameterized test structures, showed an 8Ã— improvement in layout time alone",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=618210,no,undetermined,0
Empirical evaluation of retrieval in case-based reasoning systems using modified cosine matching function,"Case-based reasoning (CBR) supports ill-structured decision making by retrieving previous cases that are useful toward the solution of a new decision problem. The usefulness of previous cases is determined by assessing the similarity of a new case with the previous cases. In this paper, we present a modified form of the cosine matching function that makes it possible to contrast the two cases being matched and to include differences in the importance of features in the new case and the importance of features in the previous case. Our empirical evaluation of a CBR application to a diagnosis and repair task in an electromechanical domain shows that the proposed modified cosine matching function has a superior retrieval performance when compared to the performance of nearest-neighbor and the Tversky's contrast matching functions",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=618259,no,undetermined,0
Increasing signal accuracy of automotive wheel-speed sensors by online learning,"The wheel speed is an important signal for modern automotive control systems. The performance of these systems is closely related to the quality of the processed wheel speed. However, due to manufacturing tolerances or corrosion respectively of the sensor gear wheel, the quality of the signal is affected negatively. In this paper a software-based method to compensate for the mechanical inaccuracy of the sensor wheel is introduced. The approach increases the signal accuracy of conventional automotive wheel-speed sensors significantly. Moreover, with the presented method, the demand for manufacturing precision of the sensor wheel is reduced, which in turn cuts down the costs for mass production considerably. The correction is obtained by online learning of a correction factor for each sensor impulse using a fuzzy model for reconstructing the actual wheel-speed and recursive parameter identification for estimating the correction factors. A method to determine the angular position of the gear wheel without a reference impulse is proposed. The introduced approach is assessed by applying it to real world data",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=609709,no,undetermined,0
Automatic evaluation of transmission protection using information from digital fault recorders,"Digital fault recorders are increasingly being used by electric utilities throughout the world. The information from these records enables the power system engineer to determine and assess, by analysis, the power system response and the protection performance during fault conditions. Many fault records can be generated following certain incidents, particularly multiple faults caused by adverse weather conditions. In order to substantially reduce the time and effort required for this analysis, there is a need for tools which automate this process. This paper describes software (FR10A and AFRA) that has been developed for this purpose in The National Grid Company plc",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=608237,no,undetermined,0
A retargetable table reader,"We describe the architecture of a system for reading machine-printed documents in known predefined tabular-data layout styles. In these tables, textual data are presented in record lines made up of fixed-width fields. Tables often do not rely on line-art (ruled lines) to delimit fields, and in this way differ crucially from fixed forms. Our system performs these steps: copes with multiple tables per page; identifies records within tables; segments records into fields; and recognizes characters within fields, constrained by field-specific contextual knowledge. Obstacles to good performance on tables include small print, tight line-spacing, poor-quality text (such as photocopies), and line-art or background patterns that touch the text. Precise skew-correction and pitch-estimation, and high-performance OCR using neural nets proved crucial in overcoming these obstacles. The most significant technical advances in this work appear to be algorithms for identifying and segmenting records with known layout, and integration of these algorithms with a graphical user interface (GUI) for defining new layouts. This GUI has been ergonomically designed to make efficient and intuitive use of exemplary images, so that the skill and manual effort required to retarget the system to new table layouts are held to a minimum. The system has been applied in this way to more than 400 distinct tabular layouts. During the last three years the system has read over fifty million records with high accuracy",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=619833,no,undetermined,0
Optimization of embedded DSP programs using post-pass data-flow analysis,"We investigate the problem of code generation for DSP systems on a chip. Such systems devote a limited quantity of silicon to program ROM, so application software must be maximally dense. Additionally, the software must be written so as to meet various high-performance constraints, which may include hard real-time constraints. Unfortunately, current compiler technology is unable to generate dense, high-performance code for DSPs, whose architectures are highly irregular. Consequently, designers often resort to programming application software in assembly-a Time-consuming, error-prone, and non-portable task. Thus, DSP compiler technology must be improved substantially. We describe some optimizations that significantly improve the quality of compiler-generated code. Our optimizations are applied globally and even across procedure calls. Additionally, they are applied to the machine-dependent assembly representation of the source program. Our target architecture is the Texas Instruments' TMS320C25 DSP",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=599863,no,undetermined,0
Communication synthesis for distributed embedded systems,"Designers of distributed embedded systems face many challenges in determining the tradeoffs when defining a system architecture or retargeting an existing design. Communication synthesis, the automatic generation of the necessary software and hardware for system components to exchange data, is required to more effectively explore the design space and automate very error prone tasks. The paper examines the problem of mapping a high level specification to an arbitrary architecture that uses specific, common bus protocols for interprocessor communication. The communication model presented allows for easy retargeting to different bus topologies, protocols, and illustrates that global considerations are required to achieve a correct implementation. An algorithm is presented that partitions multihop communication timing constraints to effectively utilize the bus bandwidth along a message path. The communication synthesis tool is integrated with a system co-simulator to provide performance data for a given mapping.",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=742910,no,undetermined,0
"Estimates, uncertainty, and risk","The authors discuss the sources of uncertainty and risk, their implications for software organizations, and how risk and uncertainty can be managed. Specifically, they assert that uncertainty and risk cannot be managed effectively at the individual project level. These factors must be considered in an organizational context",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=589239,no,undetermined,0
Use of FFT and ANN techniques in monitoring of transformer fault gases,"This paper reports on continuing work undertaken by the authors in developing a portable monitor to detect multigases especially H<sub>2 </sub> and CO dissolved in oil. A significant improvement in reliability of detection has been achieved using FFT and ANN signal processing techniques. The monitor uses the membrane and forced diffusion techniques to extract the dissolved gases in oil, and senses the gases by two thin film semiconducting gas sensors. Stability in sensor detection especially in a practical varying ambient conditions is achieved by a cyclic heating technique. And ac conductance measurement technique. Sensitivity and quantitative selectivity are achieved by signal processing using FFT and ANN techniques. The FFT resolves the frequency components of the sensor response up to 3rd harmonics and the data are used to train a back propagation ANN network to map the fingerprints. Detection up to 20 ppm has been achieved by just using two sensors. Detection for other gases like C<sub>2</sub>H<sub>2</sub> and CO<sub>2</sub> has been done by using different membranes using the same two sensors and the measured response will be reported. A portable microcontroller based monitor was developed by incorporating in software the derived weights of ANN and the measuring techniques. The results are reported in this paper",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=741688,no,undetermined,0
Buffer management for high performance database systems,"This paper presents the development of a buffer algorithm named RESBAL, which exploits parallelism in order to offer high performance for query execution in relational database systems. The algorithm aims to provide both efficient and predictive data buffering by exploiting the use of prior knowledge of query reference behaviour. Designed to offer a high level of flexibility, RESBAL employs a multiple buffering strategy both on page fetch level and page replacement level. The evaluation of RESBAL has been carried out in a parallel database system environment based on a transputer architecture. The results of this evaluation allow comparisons to be made between different buffer algorithms, and demonstrate the feasibility and effectiveness of the RESBAL algorithm. It is hoped that this work will provide some useful input to research on developing high performance database systems. Their new uses such as data mining and data warehousing are presenting the research community with interesting and important challenges",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=592222,no,undetermined,0
Application of neural networks to software quality modeling of a very large telecommunications system,"Society relies on telecommunications to such an extent that telecommunications software must have high reliability. Enhanced measurement for early risk assessment of latent defects (EMERALD) is a joint project of Nortel and Bell Canada for improving the reliability of telecommunications software products. This paper reports a case study of neural-network modeling techniques developed for the EMERALD system. The resulting neural network is currently in the prototype testing phase at Nortel. Neural-network models can be used to identify fault-prone modules for extra attention early in development, and thus reduce the risk of operational problems with those modules. We modeled a subset of modules representing over seven million lines of code from a very large telecommunications software system. The set consisted of those modules reused with changes from the previous release. The dependent variable was membership in the class of fault-prone modules. The independent variables were principal components of nine measures of software design attributes. We compared the neural-network model with a nonparametric discriminant model and found the neural-network model had better predictive accuracy",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=595888,no,undetermined,0
An efficient random-like testing,"This paper introduces the concepts of random-like testing. In a random-like testing sequence, the total distance among all of the test patterns is chosen maximal so that the sets of faults detected by one test pattern are as different as possible from that of faults detected by the tests previously applied. Procedure of constructing a random-like testing sequence (RLTS) is described in detail. Theorems to justify the effectiveness and helpfulness of the procedure presented are developed. Experimental results on benchmark circuits as well as on other circuit are also given to evaluate the performances of our new approach",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=741664,no,undetermined,0
From software metrics to software measurement methods: a process model,"Presents a process model for software measurement methods. The proposed model details the distinct steps from the design of a measurement method, to its application, then to the analysis of its measurement results, and lastly to the exploitation of these results in subsequent models, such as in quality and estimation models. From this model, a validation framework can be designed for analyzing whether or not a software metric could qualify as a measurement method. The model can also be used for analyzing the coverage of the validation methods proposed for software metrics",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=595954,no,undetermined,0
Predicting how badly â€œgoodâ€?software can behave,"Using fault injection and failure-tolerance measurement with ultrarare inputs, the authors create on automated software environment that can supplement traditional testing methods. Applied to four case studies, their methods promise to make software more robust",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=595959,no,undetermined,0
Supporting dynamic space-sharing on clusters of non-dedicated workstations,"Clusters of workstations are increasingly being viewed as a cost effective alternative to parallel supercomputers. However, resource management and scheduling on workstations clusters is complicated by the fact that the number of idle workstations available for executing parallel applications is constantly fluctuating. We present a case for scheduling parallel applications on non dedicated workstation clusters using dynamic space sharing, a policy under which the number of processors allocated to an application can be changed during its execution. We describe an approach that uses application level checkpointing and data repartitioning for supporting dynamic space sharing and for handling the dynamic reconfiguration triggered when failure or owner activity is detected on a workstation being used by a parallel application. The performance advantages of dynamic space sharing are quantified through a simulation study, and experimental results are presented for the overhead of dynamic reconfiguration of a grid oriented data parallel application using our approach",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=597902,no,undetermined,0
Cost impacts of real-time non-intrusive (RTNI) monitoring technology to real-time embedded systems,"The use of RTNI monitoring has had an impact on life cycle costs of existing programs through a reduction in debug time. Other areas in which RTNI monitoring can provide potential benefits to future programs are through the use of increased dynamic testing and the sharing of test time among more engineers. There are a number of areas in which software life cycle costs are impacted by various cost drivers. To determine which areas were affected by the use of RTNI monitoring, a panel of expert users of RTNI monitoring was created using a form of the Nominal Group Technique methodology to achieve group consensus. The group concluded that the areas described above were most important to their programs. However, merely using RTNI monitoring alone may not have an impact on future programs; management commitment to integrate RTNI monitoring into test programs is also necessary for success",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=741443,no,undetermined,0
An experimental investigation of the Internet integrated services model using the resource reservation protocol (RSVP),"The Internet Protocol (IP) in use today was designed to offer network applications a best-effort delivery service for network traffic and no guarantees are made as to when or if the packets will be delivered to their final destination. Many new network applications need the network to meet certain delay and packet loss requirements, but the current best-effort delivery service does not have the ability to provide this level of service. The Internet Engineering Task Force has proposed the Internet integrated services (IIS) model to allocate network resources achieving the desired service and the resource ReSerVation Protocol (RSVP) to deliver requests on behalf of the application for these resources. The basic concepts of the IIS model and RSVP are described. An experiment is presented in which the network performance is evaluated, both with and without network resource allocation, under varying degrees of load with the purpose of assessing the benefits of resource allocation for particular data flows",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=598648,no,undetermined,0
Parallel architecture selection for image tracking applications,"Some application-specific algorithms exhibit very high processing demand due to the real-time requirement. Architectural structure adaptations are necessary to meet the computational requirement of such algorithms. An optimized architecture should consider all the performance criteria and their degree of importance. Among these, the degree of parallelism, memory and communication bandwidth, and I/O rate should be considered. A functional analysis of several image processing algorithms is used as an example to find a parallel architecture to meet the requirements. The architecture selection factors described, certainly can help in improving the I/O and processor bound limitations. Their lack of consideration can degrade the system performance in terms of the real time requirement and quality of presentation and future expansion",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=598702,no,undetermined,0
The multimodal multipulse excitation vocoder,"This paper presents a new high-quality, variable-rate vocoder in which the average bit-rate is parametrically controllable. The new vocoder is intended for use with data-voice simultaneous channel (DVSC) applications, in which the speech data is transmitted simultaneously with video and other types of data. The vocoder presented in this paper achieves state-of-the-art quality at several different bit-rates between 5.5 kbps and 10 kbps. Further, it achieves this performance at acceptable levels of complexity and delay",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=598837,no,undetermined,0
Performance evaluation of an on-line PGNAA cross-belt analyzer [cement plant monitoring],"Arizona Portland Cement (APC), USA, has operated a prompt gamma neutron activation analyzer (PGNAA) on its 1000 TPH overland belt conveyor system since August of 1995. The belt conveyor passes through the analyzer enclosure where a neutron source under the belt causes gamma ray releases identified by the sodium iodide scintillation detector above the belt. Sophisticated analytical software operating on a personal computer decomposes the gamma ray spectrum into its constituent elements. The result is a real-time one minute update of the complete chemical composition of the crushed rock headed down the beltline. The Crossbelt Analyzer project was a 1995 Capital Budget project. APC ordered the equipment from GammaMetrics in February and completed the installation in August 1995. Training and acceptance testing were conducted in late 1995. This paper summarizes the project after one year of operation",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=599413,no,undetermined,0
A quality methodology based on statistical analysis of software structure,"Presents a methodology to assess software quality, based on the structural parameters of the software, and to validate it by factorial and regression analysis. This methodology is able to predict every file risk level, or, in other words, how likely it is that a file contains faults. The influence of structural parameters on the conditions which characterise a system can vary enormously. Once the environment has been identified and a reasonable amount of data accumulated, it is possible to construct a model which identifies potentially dangerous programs; moreover, this model is dynamic and can be continually recalibrated to suit the environment in question. The conclusions we have reached allow us to assert that both the results obtained by the proposed methodology and the basic assumption upon which the theoretical framework depends have been substantially proven",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=599870,no,undetermined,0
An efficient algorithm for causal message logging,"Causal message logging has many good properties such as nonblocking message logging and no rollback propagation. However, it requires a large amount of information to be piggybacked on each message, which may incur severe performance degradation. This paper presents an efficient causal logging algorithm based on the new message log structure, LogOn, which represents the causal interprocess dependency relation with much smaller overhead compared to the existing algorithms. The proposed algorithm is efficient in the sense that it requires no additional information other than LogOn to be carried in each message, while the other algorithms require extra information other than the message log, to eliminate the duplicates in log entries. Moreover, in those algorithms, as more extra information is added into the message, more duplicates can be detected. However, the proposed algorithm achieves the same degree of efficiency using only the message log carried in each message, without any extra information",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=740470,no,undetermined,0
Intelligent ray tracing-a new approach for field strength prediction in microcells,"With micro- and even picocells becoming an ever more important part of modern network design, deterministic wave propagation modeling has become a widely discussed solution. The necessity of acquiring high resolution-high quality data as well as extensive computation time pose severe challenges to practical planning scenarios however. A number of solutions and accelerating techniques are known in literature, all of them performing quite well under certain preconditions, but are unsatisfactory when conditions change. Therefore, the basic principles of accelerating rigorous 3-D ray tracing are presented under the aspect of their impact on prediction accuracy. A software has been developed which is able to make `intelligent' decisions with regard to the application of various accelerating algorithms and by consequence gives optimized performance in all types of propagation scenarios. Comparison with measurements shows the gain in computation efficiency as well as the achieved prediction accuracy",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=600437,no,undetermined,0
Performance-based design of distributed real-time systems,"The paper presents a design method for distributed systems with statistical, end-to-end real-time constraints, and with underlying stochastic resource requirements. A system is modeled as a set of chains, where each chain is a distributed pipeline of tasks, and a task can represent any activity requiring nonzero load from some CPU or network resource. Every chain has two end-to-end performance requirements: its delay constraint denotes the maximum amount time a computation can take to flow through the pipeline, from input to output. A chain's quality constraint mandates a minimum allowable success rate for outputs that meet their delay constraints. The design method solves this problem by deriving (1) a fixed proportion of resource load to give each task; and (2) a deterministic processing rate for every chain, an which the objective is to optimize the output success rate (as determined by an analytical approximation). They demonstrate their technique on an example system, and compare the estimated success rates with those derived via simulated on-line behavior",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=601321,no,undetermined,0
A variable-rate CELP coder for fast remote voicemail retrieval using a notebook computer,"Remote retrieval of compressed voicemail data over a telephone line is one of several emerging applications of speech coding. Using a notebook computer equipped with a modem, a user can remotely access a networked desktop unit located at their office or home to retrieve various types of information such as email, FAX, electronic documents as well as voicemail. By compressing the speech data, we reduce the amount of time needed for the transfer of voice data over the telephone line. We have developed a high-quality variable-rate CELP (code-excited linear prediction) coder which can be used in such an application. The coder operates at an average rate of 3 kb/s assuming 80% speech activity and uses several new techniques including a mode decision mechanism based on the use of a peakiness measure and a novel excitation search method called gain-matched analysis-by-synthesis. The coder gives quality comparable to the Microsoft GSM 6.10 Audio Codec at 13 kb/s",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=602623,no,undetermined,0
Worst case signalling traffic for a multi-service access protocol,"Modern multi-service medium access protocols use a collision based capacity request signalling channel. Such signalling channels may be based on the slotted Aloha multiaccess principle. This paper studies the performance of slotted Aloha subject to extreme inter-station correlation by means of a discrete-time Markov chain analysis. We study conditions whereby the time to collision resolution becomes unacceptably high (defined as deadlock). Three signalling channel management schemes for alleviating the deadlock problem are evaluated. Of these, the cyclic contention mini-slot (CMS) sharing technique employing multiple CMSs per data slot is the one that extends the protocol's useable load region the furthest. We find that implementation of a scheme, which dynamically adjusts the p-persistence parameter towards its optimal value, is desirable. Both error free and error prone conditions are studied. The results highlight the fact that the critical signalling load is largely unaffected by the presence of errors, so that even in extremely error prone environments, the limiting performance factor is still the collision rate",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=683062,no,undetermined,0
Layered analytic performance modelling of a distributed database system,"Very few analytic models have been reported for distributed database systems, perhaps because of complex relationships of the different kinds of resources in them. Layered queueing models seem to be a natural framework for these systems, capable of modelling all the different features which are important for performance (e.g. devices, communications, multithreaded processes, locking). To demonstrate the suitability of the layered framework, a previous queueing study of the CARAT distributed testbed has been recast as a layered model. Whereas the queueing model bears no obvious resemblance to the database system, the layered model directly reflects its architecture. The layered model predictions have about the same accuracy as the queueing model",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=603390,no,undetermined,0
Development of finite element design simulation tool for proximity sensor coils with ferrite cores,"In the development of inductive proximity sensors, a great deal of time and effort is spent in the design of the sensor coil, which is typically used with a ferrite core for increased Q (Quality factor). Computer simulation of the sensor coil could shorten this design process by eliminating the need for extensive building and testing of prototypes. Analytical solutions require extensive information about the material properties of the ferrite core, which is not readily available. While there are software packages commercially available that use the finite element method to simulate magnetic fields, these packages require extensive user training, and they involve lengthy run times. In this work, a linear minimal node finite element model of a proximity sensor coil is developed, serving as a rapid design simulation tool that requires little operator training. Using this minimal node model, a rapidly converging Pascal program is created to calculate the Q of a coil vs. frequency and vs. temperature. Measured and simulated data are presented for coils having a 22 mmÃ—6.8 mm core. Because this model is linear and uses a minimal number of nodes, errors do exist between measured and simulated data. The simulated Q curves are still reasonable representations of the measured Q curves however, indicating that the model can be used as a rapid design tool to reduce the flowtime of new coil designs",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=603931,no,undetermined,0
Two branch predictor schemes for reduction of misprediction rate in conditions of frequent context switches,"Branch misprediction is one of the important causes of performance degradation in superpipelined and superscalar processors. Most of the existing branch predictors, based on the exploiting of branch history, suffer from prediction accuracy decrease caused by frequent context switches. The goal of this research is to reduce misprediction rate (MPR) when the context switches are frequent, and not to increase the MPR when the context switches are relatively rare. We propose two independent, but closely related modifications of global adaptive prediction mechanisms: first, to flush only the branch history register (BHR) at context switch, instead of reinitialization of the whole predictor, and second, to use two separated BHRs, one for user and one for kernel branches, instead of one global history register. We have evaluated the ideas by measurements on real traces from IBS (Instruction Benchmark Set), and have shown that both modifications reduce MPR at negligible hardware cost",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=740523,no,undetermined,0
An automatic measurement system for the characterization of automotive gaskets,"The paper describes an automatic measurement system for the estimation of both geometrical sizes and physical features of automotive gaskets. The system hardware includes a personal computer and a CCD camera connected to a frame grabber. The system software consists of an original image processing procedure. At first, both hardware and software solutions are illustrated. Then, the whole image processing procedure, including original algorithmic strategies, is described in detail. Finally, experimental results on images of different automotive gaskets are reported together with the metrological characterization of the proposed measurement system",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=603987,no,undetermined,0
A VXI signal analyzer based on the wavelet transform,"The paper deals with a signal analyzer for real time detection and analysis of transients. In particular, the analyzer implements a suitable procedure, based on the wavelet transform, for analyzing disturbances affecting the power supply voltage. It mounts a good performance digital signal processor and adopts the VXI standard interface. Hardware and software solutions are described in detail. Experimental results on signals characterized by different disturbances with known characteristics assess the reliability of the proposed analyzer",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=603988,no,undetermined,0
On-line current monitoring to diagnose airgap eccentricity-an industrial case history of a large high-voltage three-phase induction motor,An appraisal of online monitoring techniques to detect airgap eccentricity in three-phase induction motors is presented. Results from an industrial case history show that an improved interpretation of the current spectrum can increase the reliability of diagnosing airgap eccentricity in large HV three-phase induction motors,1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=604066,no,undetermined,0
"The Chameleon infrastructure for adaptive, software implemented fault tolerance","This paper presents Chameleon, an adaptive software infrastructure for supporting different levels of availability requirements in a heterogeneous networked environment. Chameleon provides dependability through the use of ARMORs-Adaptive, Reconfigurable, and Mobile Objects for Reliability. Three broad classes of ARMORs are defined: Managers, Daemons, and Common ARMORs. Key concepts that support adaptive fault tolerance include the construction of fault tolerance execution strategies from a comprehensive set of ARMORs, the creation of ARMORs from a library of reusable basic building blocks, the dynamic adaptation to changing fault tolerance requirements, and the ability to detect and recover from errors in applications and in ARMORs",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=740508,no,undetermined,0
Knowledge based system for faulty components detection in production testing of electronic device,"A knowledge-based system is proposed and described for faulty components detection and identification, in production testing of analog electronic boards. Its main parts are: the guided measuring probe and diagnostic expert system. Results are reported of using inductive machine learning technique for diagnostic rules acquisition",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=605333,no,undetermined,0
System implications of rain and ice depolarisation in Ka-band satellite communications,This paper investigates the impact of rain and ice depolarisation on the performance of satellite communications systems operating in the Ka-band with orthogonal polarisations. Various prediction methods for evaluating the degradation of dual-polarised system availability due to rain and ice are reviewed. Their results are compared using depolarisation data and statistics collected in Belgium at 20 GHz during the ESA's Olympus satellite experiment and exhibit significant discrepancies. A procedure based on the combined use of experimental data and of a simulation software for communications systems confirms the prediction produced by one method. The proposed procedure also yields estimation of system quality during particular depolarisation events,1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=606947,no,undetermined,0
Dependability analysis of a cache-based RAID system via fast distributed simulation,"We propose a new speculation-based, distributed simulation method for dependability analysis of complex systems in which a detailed functional simulation of a system component is essential to obtain an accurate overall result. Our target example is a networked cluster with compute nodes and a single I/O node. Accurate system dependability characterization is achieved via a combination of detailed simulation of the I/O subsystem behavior in the presence of faults and more abstract simulation of the compute nodes and the switching network. Dependability measures like error coverage, error detection latency and performance measures such as delivery time in the presence of faults are obtained. The approach is implemented on a network of workstations, and experimental results show significant improvements over a Time Warp simulator for the same model",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=740507,no,undetermined,0
An analysis of the post-fault behavior of robotic manipulators,"Operations in hazardous or remote environments are invariably performed by robots. The hostile nature of the environments, however, increase the likelihood of failures for robots used in such applications. The difficulty and delay in the detection and consequent correction of these faults makes the post-fault performance of the robots particularly important. This work investigates the behavior of robots experiencing undetected locked-joint failures in a general class of tasks characterized by point-to-point motion. The robot is considered to have â€œconvergedâ€?to a task position and orientation if all its joints come to rest when the end-effector is at that position. It is seen that the post-fault behavior may be classified into three categories: (1) the robot converges to the task position; (2) the robot converges to a position other than the task position; or (3) the robot does not converge, but keeps moving forever. The specific conditions for convergence are identified, and the different behaviors illustrated with examples of simple planar manipulators",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=619350,no,undetermined,0
Enhanced methods for the collection of on-scene information by emergency medical system providers,"Emergency Medical Systems (EMS) assess their cardiac care performance and effectiveness by measuring how quickly they respond to arrest and infarction. Time intervals defined by standard response models such as the Utstein Style for Cardiac Arrest Reporting are recorded and tracked to identify delays in patient treatment which can impact patient outcomes. Unfortunately, most event times, as entered into the responder's run reports, are inaccurate, since they are estimated retrospectively after the incident is over. A data collection device that can accurately gather information included in the run report is proposed. The device must be able to accept information about the incident context, record the time of key events, and capture the narrative summary without interfering with patient care. When the incident is over, it should import information from computer dispatch systems and diagnostic or therapeutic medical devices used on-scene. Information must be exported for remote diagnoses and for inclusion in the computerized patient care record. Current voice recognition systems which are able to recognize and time-stamp short phrases associated with on-scene events, allow a flow sheet to be composed as the incident progresses. Critical data and times can thus be accurately collected without the need for responders to touch a screen or keypad. These systems can also capture the clinical narrative by transcribing verbal reports as they are delivered to emergency department physicians. A Windows application, running on a waistband computer and suitable for use by EMS responders, has been developed based on these principles and technologies, and is now being evaluated in field use",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=731773,no,undetermined,0
Dynamic scheduling of parallelizable tasks and resource reclaiming in real-time multiprocessor systems,"Many time critical applications require predictable performance and tasks in these applications have deadlines to be met despite the presence of faults. We propose a new dynamic non preemptive scheduling algorithm for a relatively new task model called parallelizable task model where real time tasks can be executed concurrently on multiple processors. We use this parallelism in tasks to meet their deadlines and thus obtain better processor utilization compared to nonparallelizable task scheduling algorithms. We assume that tasks are aperiodic. Further, each task is characterized by its deadline, resource requirements, and worst case computation time on p processors, where p is the degree of task parallelization. To study the effectiveness of our algorithm, we have conducted extensive simulation studies and compared its performance with the myopic scheduling algorithm (K. Ramamritham et al., 1990). We found that the success ratio offered by our algorithm is always higher than the myopic algorithm for a wide variety of task parameters. Also, we propose a resource reclaiming algorithm to reclaim resources from parallelizable real time tasks when their actual computation times are less than their worst case computation times. Our parallelizable task scheduling together with its associated reclaiming offers the best guarantee ratio compared to the other algorithmic combinations",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=634494,no,undetermined,0
Incorporating code coverage in the reliability estimation for fault-tolerant software,"Presents a technique that uses coverage measures in reliability estimation for fault-tolerant programs, particularly N-version software. This technique exploits both coverage and time measures collected during testing phases for the individual program versions and the N-version software system for reliability prediction. The application of this technique to single-version software was presented in our previous research (IEEE 3rd Int. Symp. on Software Metrics, Berlin, Germany, March 1996). In this paper, we extend this technique and apply it on the N-version programs. The results obtained from the experiment conducted on an industrial project demonstrate that our technique significantly reduces the hazard of reliability overestimation for both single-version and multi-version fault-tolerant software systems",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=632796,no,undetermined,0
"Building software quality classification trees: approach, experimentation, evaluation","A methodology for generating an optimum software quality classification tree using software complexity metrics to discriminate between high-quality modules and low-quality modules is proposed. The process of tree generation is an application of the AIC (Akaike Information Criterion) procedures to the binomial distribution. AIC procedures are based on maximum likelihood estimation and the least number of complexity metrics. It is an improvement of the software quality classification tree generation method proposed by Porter and Selby (1990) from the viewpoint that the complexity metrics are minimized. The problems of their method are that the software quality prediction model is unstable because it reflects observational errors in real data too much and there is no objective criterion for determining whether the discrimination is appropriate or not at a deep nesting level of the classification tree when the number of sample modules gets smaller. To solve these problems a new metric is introduced and its validity is theoretically and experimentally verified. In our examples, complexity metrics written in C language, such as lines of source code, Halstead's (1977) software science, McCabe's (976) cyclomatic number, Henry and Kafura's (1981) fan-in/out and Howatt and Baker's (1989) scope number, are investigated. Our experiments with a medium-sized piece of software (85 thousand lines of source code; 562 samples) show that the software quality classification tree generated by our new metric identifies the target class of the observed modules more efficiently using the minimum number of complexity metrics without any significant decrease of the correct classification ratio (76%->72%) than the conventional classification tree",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=630869,no,undetermined,0
Confidence-based reliability and statistical coverage estimation,"In confidence-based reliability measurement, we determine that we are at least C confident that the probability of a program failing is less than or equal to a bound B. The basic results of this approach are reviewed and several additional results are introduced, including the adaptive sampling theorem which shows how confidence can be computed when faults are corrected as they appear in the testing process. Another result shows how to carry out testing in parallel. Some of the problems of statistical testing are discussed and an alternative method for establishing reliability called statistical coverage is introduced. At the cost of making reliability estimates that are relative to a fault model, statistical coverage eliminates the need for output validation during reliability estimation and allows the incorporation of non-statistical testing results into the statistical reliability estimation process. Statistical testing and statistical coverage are compared, and their relationship with traditional reliability growth modeling approaches is briefly discussed",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=630877,no,undetermined,0
Analysis of a software reliability growth model with logistic testing-effort function,"We investigate a software reliability growth model (SRGM) based on the Non Homogeneous Poisson Process (NHPP) which incorporates a logistic testing effort function. Software reliability growth models proposed in the literature incorporate the amount of testing effort spent on software testing which can be described by an exponential curve, a Rayleigh curve, or a Weibull curve. However it may not be reasonable to represent the consumption curve for testing effort only by an exponential, a Rayleigh or a Weibull curve in various software development environments. Therefore, we show that a logistic testing effort function can be expressed as a software development/test effort curve and give a reasonable predictive capability for the real failure data. Parameters are estimated and experiments on three actual test/debug data sets are illustrated. The results show that the software reliability growth model with logistic testing effort function can estimate the number of initial faults better than the model with Weibull type consumption curve. In addition, the optimal release policy of this model based on cost reliability criterion is discussed",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=630886,no,undetermined,0
Software metrics model for integrating quality control and prediction,"A model is developed that is used to validate and apply metrics for quality control and quality prediction, with the objective of using metrics as early indicators of software quality problems. Metrics and quality factor data from the Space Shuttle flight software are used as an example. Our approach is to integrate quality control and prediction in a single model and to validate metrics with respect to a quality factor. Boolean discriminant functions (BDFs) were developed for use in the quality control and quality prediction process. BDFs provide good accuracy for classifying low quality software because they include additional information for discriminating quality: critical values. Critical values are threshold values of metrics that are used to either accept or reject modules when the modules are inspected during the quality control process. A series of nonparametric statistical methods is also used in the method presented. It is important to perform a marginal analysis when making a decision about how many metrics to use in the quality control and prediction process. We found that certain metrics are dominant in their effects on classifying quality and that additional metrics are not needed to accurately classify quality. This effect is called dominance. Related to the property of dominance is the property of concordance, which is the degree to which a set of metrics produces the same result in classifying software quality. A high value of concordance implies that additional metrics will not make a significant contribution to accurately classifying quality; hence, these metrics are redundant",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=630888,no,undetermined,0
Reliability modeling of freely-available Internet-distributed software,"A wealth of freely-available software is available on the Internet; however, developers are wary of its reuse because it is assumed to be of poor quality. Reliability is one way that software quality can be measured, but it requires metrics data that are typically not maintained for freely-available software. A technique is presented which allows reliability data to be extracted from available data, and is validated by showing that the data can be used to fit a logarithmic reliability model. By modeling the reliability, estimates of overall quality, remaining faults, and release times can be predicted for the software",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=731233,no,undetermined,0
Metric selection for effort assessment in multimedia systems development,"This paper describes ongoing research directed at formulating a set of appropriate metrics for assessing effort requirements for multimedia systems development. An exploratory investigation of the factors that are considered by industry to be influential in determining development effort is presented. This work incorporates the use of a GQM framework to assist the metric selection process from a literature basis, followed by an industry questionnaire. The results provide some useful insights into contemporary project management practices in relation to multimedia systems",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=731232,no,undetermined,0
On the test allocations for the best lower bound performance of partition testing,"A partition testing strategy divides the program's input domain into subdomains according to a partitioning scheme, and selects test cases from each subdomain according to a test allocation scheme. Previous studies have shown that partition testing strategies can be very effective or very ineffective in detecting faults, depending on both the partitioning scheme and the test allocation scheme. Given a partitioning scheme, the maximin criterion chooses a test allocation scheme that will maximally improve the lower bound performance of the partition testing strategy. In an earlier work, we have proved that the Basic Maximin Algorithm can generate such a test allocation scheme. In this paper, we derive further properties of the Basic Maximin Algorithm and present the Complete Maximin Algorithm that generates all possible test allocation schemes that satisfy the maximin criterion",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=730923,no,undetermined,0
Advances in debugging high performance embedded systems,"The development of application software has quickly become the most expensive and people-intensive area of systems design. Depending on the nature of the software and with the advent of software quality controls, it has been estimated that each line of code can cost upwards of $50 to develop and debug (this rationale appears to hold true for both assembly language and high level language based algorithms). This being the case, it should be no surprise that debugging techniques have been subject to much rethinking recently and that debugging aids and systems are now being implemented in embedded controllers",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=632326,no,undetermined,0
Availability analysis of transaction processing systems based on user-perceived performance,"Transaction processing systems are judged by users to be correctly functioning not only if their transactions are executed correctly, but also if most of them are completed within an acceptable time limit. Therefore, we propose a definition of availability for systems for whom there is a notion of system failure due to frequent violation of response time constraints. We define the system to be available at a certain time if at that time the fraction of transactions meeting a deadline is above a certain user requirement. This definition lends to very different estimates of availability measures such as system downtimes as compared with more traditional measures. We conclude that for transaction processing systems, where the user's perception is important, our definition more correctly quantifies the availability of the system",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=632791,no,undetermined,0
Applying simulation to the design and performance evaluation of fault-tolerant systems,"The paper illustrates how the CESIUM simulation tool can be used for design and performance evaluation of fault tolerant and real time systems, in addition to testing the correctness of protocol implementations. We calibrate three increasingly accurate simulation models of a network of workstations using independently obtained data. For a sample group membership protocol, the predictions of the simulator are very close to the actual performance measured in the real system. We also apply CESIUM to the evaluation of two potential improvements for the protocol, performing experiments that would have been difficult to implement in the real system. The results of the simulations give us valuable insight on how to tune configuration parameters, as well as on the performance gains of the improved versions. Our experience shows that CESIUM can be used to develop best effort services which adapt their quality of service according to the failures that occur during operation",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=632794,no,undetermined,0
A methodology for detection and estimation of software aging,"The phenomenon of software aging refers to the accumulation of errors during the execution of the software which eventually results in it's crash/hang failure. A gradual performance degradation may also accompany software aging. Pro-active fault management techniques such as â€œsoftware rejuvenationâ€?(Y. Huang et al., 1995) may be used to counteract aging if it exists. We propose a methodology for detection and estimation of aging in the UNIX operating system. First, we present the design and implementation of an SNMP based, distributed monitoring tool used to collect operating system resource usage and system activity data at regular intervals, from networked UNIX workstations. Statistical trend detection techniques are applied to this data to detect/validate the existence of aging. For quantifying the effect of aging in operating system resources, we propose a metric: â€œestimated time to exhaustionâ€? which is calculated using well known slope estimation techniques. Although the distributed data collection tool is specific to UNIX, the statistical techniques can be used for detection and estimation of aging in other software as well",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=730892,no,undetermined,0
Software diagnosability,"This paper is concerned with diagnosability, its definition and the axiomatization of its expected behaviour. The intuitive expected behaviour of diagnosability is defined relative to basic operations that are applicable on software designs. A diagnosability measurement is proposed which is consistent with the stated axioms. The diagnosability metric is based on an analysis of the design structure: fault location effort and precision are measured for a given testing context. Compromises between global test difficulty and diagnostic precision are illustrated on part of a data-flow software design. Throughout the paper, we develop a case study",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=730889,no,undetermined,0
Studying the effects of code inspection and structural testing on software quality,"The paper contributes a controlled experiment to characterize the effects of code inspection and structural testing on software quality. Twenty subjects performed sequentially code inspection and structural testing using different coverage values as test criteria on a C-code module. The results of this experiment show that inspection significantly outperforms the defect detection effectiveness of structural testing. Furthermore, the experimental results indicate little evidence to support the hypothesis that structural testing detects different defects, that is, defects of a particular class, that were missed by inspection and vice versa. These findings suggest that inspection and structural testing do not complement each other well. Since 39 percent (on average) of the defects were not detected at all, it might be more valuable to apply inspection, together with other testing techniques, such as boundary value analysis, to achieve a better defect coverage. We are aware that a single experiment has many limitations and often does not provide conclusive evidence. Hence, we consider this experiment a starting point and encourage other researchers to investigate the optimal mix of defect detection techniques",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=730887,no,undetermined,0
Testing the robustness of Windows NT software,"To date, most studies on the robustness of operating system software have focused on Unix based systems. The paper develops a methodology and architecture for performing intelligent black box analysis of software that runs on the Windows NT platform. The goals of the research are three fold: first, to develop intelligent robustness testing techniques for commercial Off-The-Shelf (COTS) software; second, to benchmark the robustness of NT software in handling anomalous events; and finally, to identify robustness gaps to permit fortification for fault tolerance. The random and intelligent data design library environment (RIDDLE) is a tool for analyzing operating system software, system utilities, desktop applications, component based software, and network services. RIDDLE was used to assess the robustness of native Windows NT system utilities as well as Win32 ports of the GNU utilities. Experimental results comparing the relative performance of the ported utilities versus the native utilities are presented",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=730886,no,undetermined,0
Predicting dependability properties on-line,"Consider a system put into operation at time t<sub>0</sub>. During the design phase, i.e. before time t<sub>0</sub>, a model &Mscr; is used to tune up certain system parameters in order to satisfy some dependability constraints. Once in operation, some aspects of the system's behavior are observed during the interval [t<sub>0</sub>,t]. In this paper, we show how to integrate into model &Mscr; the observations made during [t<sub>0</sub>,t] in the operational phase, in order to improve the predictions on the behavior of the system after time t",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=632797,no,undetermined,0
Testing strategies for form-based visual programs,"Form based visual programming languages, which include electronic spreadsheets and a variety of research systems, have had a substantial impact on end user computing. Research shows that form based visual programs often contain faults, and that their creators often have unwarranted confidence in the reliability of their programs. Despite this evidence, we find no discussion in the research literature of techniques for testing or assessing the reliability of form based visual programs. The paper addresses this lack. We describe differences between the form based and imperative programming paradigms, and discuss effects these differences have on strategies for testing form based programs. We then present several test adequacy criteria for form based programs, and illustrate their application. We show that an analogue to the traditional â€œall-usesâ€?dataflow test adequacy criterion is well suited for code based testing of form based visual programs: it provides important error detection ability, and can be applied more easily to form based programs than to imperative programs",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=630851,no,undetermined,0
Software reliability analysis incorporating fault detection and debugging activities,"The software reliability measurement problem can be approached by obtaining the estimates of the residual number of faults in the software. Traditional black box based approaches to software reliability modeling assume that the debugging process is instantaneous and perfect. The estimates of the remaining number of faults, and hence reliability, are based on these oversimplified assumptions and they tend to be optimistic. We propose a framework relying on rate based simulation technique for incorporating explicit debugging activities along with the possibility of imperfect debugging into the black box software reliability models. We present various debugging policies and analyze the effect of these policies on the residual number of faults in the software. In addition, we propose a methodology to compute the reliability of the software, taking into account explicit debugging activities. An economic cost model to determine the optimal software release criteria in the presence of debugging activities is described. Finally, we present the high level architecture of a tool, called SRSIM, for the purpose of automating the simulation techniques presented",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=730883,no,undetermined,0
Fast replicated state machines over partitionable networks,The paper presents an implementation of replicated state machines in asynchronous distributed environments prone to node failures and network partitions. This implementation has several appealing properties: it guarantees that progress will be made whenever a majority of replicas can communicate with each other; it allows minority partitions to continue providing service for idempotent requests; it offers the application the choice between optimistic or safe message delivery. Performance measurements have shown that our implementation incurs low latency and achieves high throughput while providing globally consistent replicated state machine semantics,1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=632808,no,undetermined,0
A supervisor-based semi-centralized network surveillance scheme and the fault detection latency bound,"Network surveillance (NS) schemes facilitate fast learning by each interested fault free node in the system of the faults or repair completion events occurring in other parts of the system. Currently concrete real time NS schemes effective in distributed computer systems based on point to point network architectures are scarce. We present a semi centralized real time NS scheme effective in a variety of point to point networks, called the supervisor based NS (SNS) scheme. This scheme is highly scalable and can be implemented entirely in software using commercial off the shelf (COTS) components without requiring any special purpose hardware support. An efficient execution support for the scheme has been designed as a new extension of the DREAM kernel, a timeliness guaranteed operating system kernel model developed in the authors' laboratory. This design can be viewed as an implementation model which can be easily adapted to various commercial operating system kernels. The paper also presents an analysis of the SNS scheme on the basis of the implementation model to obtain some tight bounds on the fault detection latency",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=632810,no,undetermined,0
A fail-aware membership service,We propose a new protocol that can be used to implement a partitionable membership service for timed asynchronous systems. The protocol is fail-aware in the sense that a process p knows at all times if its approximation of the set of processes in its partition is up-to-date or out-of-date. The protocol minimizes wrong suspicions of processes by giving processes a second chance to stay in the membership before they are removed. Our measurements show that the exclusion of live processes is rare and the crash detection times are good. The protocol guarantees that the memberships of two partitions never overlap,1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=632811,no,undetermined,0
Reliability simulation of component-based software systems,"Prevalent Markovian and semi Markovian methods to predict the reliability and performance of component based heterogeneous systems suffer from several limitations: they are subject to an intractably large state space for more complex scenarios, and they cannot take into account the influence of various parameters such as reliability growth of individual components, dependencies among components, etc., in a single model. Discrete event simulation offers an alternative to analytical models as it can capture a detailed system structure, and can be used to study the influence of different factors separately as well as in a combined fashion on dependability measures. We demonstrate the flexibility offered by discrete event simulation to analyze such complex systems through two case studies, one of a terminating application, and the other of a real time application with feedback control. We simulate the failure behavior of the terminating application with instantaneous as well as explicit repair. We also study the effect of having fault tolerant configurations for some of the components on the failure behavior of the application. In the second case of the real time application, we initially simulate the failure behavior of a single version taking into account its reliability growth. We also study the failure behavior of three fault tolerant systems: DRB, NVP and NSCP which are built from the individual versions of the real time application. Results demonstrate the flexibility offered by simulation to study the influence of various factors on the failure behavior of the applications for single as well as fault tolerant configurations",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=730882,no,undetermined,0
Probabilistic verification of a synchronous round-based consensus protocol,"Consensus protocols are used in a variety of reliable distributed systems, including both safety-critical and business-critical applications. The correctness of a consensus protocol is usually shown, by making assumptions about the environment in which it executes, and then proving properties about the protocol. But proofs about a protocol's behavior are only as good as the assumptions which were made to obtain them, and violation of these assumptions can lead to unpredicted and serious consequences. We present a new approach for the probabilistic verification of synchronous round based consensus protocols. In doing so, we make stochastic assumptions about the environment in which a protocol operates, and derive probabilities of proper and non proper behavior. We thus can account for the violation of assumptions made in traditional proof techniques. To obtain the desired probabilities, the approach enumerates possible states that can be reached during an execution of the protocol, and computes the probability of achieving the desired properties for a given fault and network environment. We illustrate the use of this approach via the evaluation of a simple consensus protocol operating under a realistic environment which includes performance, omission, and crash failures",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=632812,no,undetermined,0
Transaction reordering in replicated databases,"The paper presents a fault tolerant lazy replication protocol that ensures 1-copy serializability at a relatively low cost. Unlike eager replication approaches, our protocol enables local transaction execution and does not lead to any deadlock situation. Compared to previous lazy replication approaches, we significantly reduce the abort rate of transactions and we do not require any reconciliation procedure. Our protocol first executes transactions locally, then broadcasts a transaction certification message to all replica managers, and finally employs a certification procedure to ensure 1-copy serializability. Certification messages are broadcast using a non blocking atomic broadcast primitive, which alleviates the need for a more expensive non blocking atomic commitment algorithm. The certification procedure uses a reordering technique to reduce the probability of transaction aborts",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=632813,no,undetermined,0
Preventing useless checkpoints in distributed computations,"A useless checkpoint is a local checkpoint that cannot be part of a consistent global checkpoint. The paper addresses the following important problem. Given a set of processes that take (basic) local checkpoints in an independent and unknown way, the problem is to design a communication induced checkpointing protocol that directs processes to take additional local (forced) checkpoints to ensure that no local checkpoint is useless. A general and efficient protocol answering this problem is proposed. It is shown that several existing protocols that solve the same problem are particular instances of it. The design of this general protocol is motivated by the use of communication induced checkpointing protocols in â€œconsistent global checkpointâ€?based distributed applications. Detection of stable or unstable properties, rollback recovery and determination of distributed breakpoints are examples of such applications",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=632814,no,undetermined,0
Resource-constrained non-operational testing of software,"In â€œclassicalâ€?testing approaches, â€œlearningâ€?is said to occur if testers dynamically improve the efficiency of their testing as they progress through a testing phase. However, the pressures of modern business and software development practices seem to favor an approach to testing which is very akin to a â€œsampling without replacementâ€?of a relatively limited number of pre-determined structures and functions conducted under significant schedule and resource constraints. The primary driver is often the desire to â€œcoverâ€?ONLY previously â€œuntestedâ€?functions, operations or code constructs, and to meet milestones. We develop and evaluate a model that describes the fault detection and removal process in such an environment. Results indicate that in environments where â€œcoverageâ€?based testing is promoted, but resources and decisions are constrained, very little dynamic â€œlearningâ€?takes place, and that it may be an artifact of program structure or of the test case sequencing policy",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=730874,no,undetermined,0
Modular flow analysis for concurrent software,"Modern software systems are designed and implemented in a modular fashion by composing individual components. The advantages of early validation are widely accepted in this context, i.e., that defects in individual module designs and implementations may be detected and corrected prior to system-level validation. This is particularly true for errors related to interactions between system components. In this paper, we describe how a whole-program automated static analysis technique can be adapted to the validation of individual components, or groups of components, of sequential or concurrent software systems. This work builds off of an existing approach, FLAVERS, that uses program flow analysis to verify explicitly stated correctness properties of software systems. We illustrate our modular analysis approach and some of its benefits by describing part of a case-study with a realistic concurrent multi-component system",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=632847,no,undetermined,0
Designing and verifying embedded microprocessors,"Motorola's ColdFire products are a line of microprocessors targeting embedded-system applications such as computer peripherals (disk drives, laser printers, scanners).They originated from the same design group that produced the 680X0 general purpose microprocessors, whose target market was desktop computing applications. The ColdFire microprocessors, however, target the highly competitive embedded market, whose time-to-market and cost requirements are much more stringent. The ColdFire design team received a set of challenges quite different from those associated with the 680X0 line. They had to minimize test costs since the target selling price was an order of magnitude less than that of a desktop microprocessor. With no room for design errors, they had to put processes in place that detect errors early in the design flow and provide feedback for continuous improvement. A new methodology reduced new product cycle time to less than a year for the ColdFire products and provided improved techniques for integrating cores in new applications. In addition, it increased quality measurement capability and reduced test cost",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=632885,no,undetermined,0
The lognormal distribution of software failure rates: origin and evidence,"An understanding of the distribution of software failure rates and its origin will strengthen the relation of software reliability engineering both to other aspects of software engineering and to the wider field of reliability engineering. The paper proposes that the distribution of failure rates for faults in software systems tends to be lognormal. Many successful analytical models of software behavior share assumptions that suggest that the distribution of software event rates will asymptotically approach lognormal. The lognormal distribution has its origin in the complexity, that is the depth of conditionals, of software systems and the fact that event rates are determined by an essentially multiplicative process. The central limit theorem links these properties to the lognormal: just as the normal distribution arises when summing many random terms, the lognormal distribution arises when the value of a variable is determined by the multiplication of many random factors. Because the distribution of event rates tends to be lognormal and faults are just a random subset or sample of the events, the distribution of the failure rates of the faults also tends to be lognormal. Failure rate distributions observed by other researchers in twelve repetitive-run experiments and nine sets of field failure data are analyzed and demonstrated to support the lognormal hypothesis. The ability of the lognormal to fit these empirical failure rate distributions is superior to that of the gamma distribution (the basis of the Gamma/EOS family of reliability growth models) or a Power-law model",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=730863,no,undetermined,0
Economics of diagnosis,"Detecting the existence of a fault in complex systems is neither sufficient nor economical without diagnostics assisting in fault isolation and cost-effective repairs. This work attempts to put in economical terms the technical decisions involving diagnostics. It looks at the cost factors of poor diagnostics in terms of the accuracy and completeness of fault identification and the time and effort it takes to come to a final (accurate) repair decision. No Problems Found (NPF), Retest OK (RTOK), False Alarms, Cannot Duplicates (CND) and other diagnostic deficiencies can range from 30% to 60% of all repair actions. According to a 1995 survey run by the IEEE Reliability Society, the Air Transport Association (ATA) has determined that 4500 NPF events cost ATE member airlines $100 million annually. A U.S. Army study has shown that maintenance costs can be reduced by 25% if 70-80% of the items if had been repairing were to be discarded. Many of these situations can be overcome by investing in emerging technologies, such as Built-in (Self) Test (BIST) and expert diagnostic tools. The role of these tools is to minimize dependence on the skills, knowledge and experience of individuals, and thus overcome costs of inaccurate, inefficient, and incomplete diagnostics. Use of BIST can also directly reduce costs. This paper presents technical solutions and economic analyses showing to what extent such solutions provide a sufficient return on investment",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=633657,no,undetermined,0
The impact of test definition standards on TPS quality,"One of the chronic problems associated with Test Program Set (TPS) development has been the lack of unambiguous standards for test definition. Elements of the definition include, but are not necessarily limited to, the questions of what constitutes a test, what aspect or failure mode of the UUT is being addressed by the test, why the test is being performed at that particular point in the overall test flow, and how the test relates to other tests within the main performance verification path or any of the diagnostic branches. Moreover, test definition standards are very closely related to the way in which unambiguous fault defection and fault isolation metrics are implemented, as well as the way in which overall (TPS) test strategy is documented. Definitive standards for test definition will support not only demonstrable TPS quality but also TPS documentation that is both descriptive and indicative of the levels of quality that are being achieved",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=633663,no,undetermined,0
Using reversible computing to achieve fail-safety,"This paper describes a fail-safe design approach that can be used to achieve a high level of fail-safety with conventional computing equipment which may contain design flaws. The method is based on the well-established concept of reversible computing. Conventional programs destroy information and hence cannot be reversed. However it is easy to define a virtual machine that preserves sufficient intermediate information to permit reversal. Any program implemented on this virtual machine is inherently reversible. The integrity of a calculation can therefore be checked by reversing back from the output values and checking for the equivalence of intermediate values and original input values. By using different machine instructions on the forward and reverse paths, errors in any single instruction execution can be revealed. Random corruptions in data values are also detected. An assessment of the performance of the reversible computer design for a simple reactor trip application indicates that it runs about ten times slower than a conventional software implementation and requires about 20 kilobytes of additional storage. The trials also show a fail-safe bias of better than 99.998% for random data corruptions, and it is argued that failures due to systematic flaws could achieve similar levels of fail-safe bias. Potential extensions and applications of the technique are discussed",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=630863,no,undetermined,0
Hierarchical supervisors for automatic detection of software failures,"Software supervision is an approach to the automatic detection of software failures. A supervisor observes the inputs and outputs of a target system. It uses a model of correct behavior, derived from the target system's requirement specification. Discrepancies between specified and observed behaviors are reported as failures. Applications of the supervisor include online failure detection in real time reactive systems, fault localization and automatic collection of failure data. The paper describes a hierarchical approach to supervision. The approach differs from previous approaches in that supervision is split into two sub problems: tracking the behavior of the target system and detailed behavior checking. The architecture of the hierarchical supervisor has two layers: the path detection layer and the base supervisor layer. The hierarchical approach results in a significant reduction in computational cost arising from specification nondeterminism. The approach was evaluated by supervising the call processing software of a small telephone exchange, executed under random telephone traffic at different loads. Several thousand failures were individually seeded into the output generated by the exchange. The supervisor was able to detect the presence of all seeded failures. Reductions in computational cost of several orders of magnitude were measured in comparison with the direct, single layer supervisor",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=630847,no,undetermined,0
An on-line algorithm for checkpoint placement,"Checkpointing enables us to reduce the time to recover from a fault by saving intermediate states of the program in a reliable storage. The length of the intervals between checkpoints affects the execution time of programs. On one hand, long intervals lead to long reprocessing time, while, on the other hand, too frequent checkpointing leads to high checkpointing overhead. In this paper, we present an on-line algorithm for placement of checkpoints. The algorithm uses knowledge of the current cost of a checkpoint when it decides whether or not to place a checkpoint. The total overhead of the execution time when the proposed algorithm is used is smaller than the overhead when fixed intervals are used. Although the proposed algorithm uses only on-line knowledge about the cost of checkpointing, its behavior is close to the off-line optimal algorithm that uses a complete knowledge of checkpointing cost",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=620479,no,undetermined,0
The predictive validity criterion for evaluating binary classifiers,"The development of binary classifiers to identify highly error-prone or high maintenance cost components is increasing in the software engineering quality modeling literature and in practice. One approach for evaluating these classifiers is to determine their ability to predict the classes of unseen cases, i.e., predictive validity. A chi-square statistical test has been frequently used to evaluate predictive validity. We illustrate that this test has a number of disadvantages. The disadvantages include a difficulty in using the results of the test to determine whether a classifier is a good predictor, demonstrated through a number of examples, and a rather conservative Type I error rate, demonstrated through a Monte Carlo simulation. We present an alternative test that has been used in the social sciences for evaluating agreement with a â€œgold standardâ€? The use of this alternative test is illustrated in practice by developing a classification model to predict maintenance effort for an object oriented system, and evaluating its predictive validity on data from a second object-oriented system in the same environment",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=731250,no,undetermined,0
A Chinese bank check recognition system based on the fault tolerant technique,"The contradiction between the high recognition accuracy and the low rejection rate in automatic bank check recognition has not been solved successfully. In this paper, a fault-tolerant Chinese bank check recognition system is presented to solve the contradiction between the need for low-error-recognition probability and the need for low-refused-recognition probability. The main idea is to use a dynamic cipher code (which is to be widely applied in China) to lower both of them. This system achieves a high recognition rate and a high reliability simultaneously when automatically processing Chinese bank checks with dynamic cipher codes. A practical scheme of fault-tolerant recognition of bank checks is given in this paper, and experiments show the performance of our fault-tolerant technique",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=620667,no,undetermined,0
Estimating the number of residual defects [in software],"The number of residual defects is one of the most important factors that allows one to decide if a piece of software is ready to be released. In theory, one can find all the defects and count them. However, it is impossible to find all the defects within a reasonable amount of time. Estimating the defect density can become difficult for high-reliability software, since the remaining defects can be extremely hard to test for. One possible way is to apply the exponential software reliablility growth model (SRGM), and thus estimate the total number of defects present at the beginning of testing. In this paper, we show the problems with this approach and present a new approach based on software test coverage. Test coverage directly measures the thoroughness of testing, avoiding the problem of variations of test effectiveness. We apply this model to actual test data in order to project the residual number of defects. This method results in estimates that are more stable than the existing methods. The method is also easier to understand, and the convergence to the estimate can be observed visually",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=731600,no,undetermined,0
Viscosity as a metaphor for measuring modifiability,"An analytic framework termed `cognitive dimensions' is introduced and developed to provide formal definitions of dimensions for assessing the suitability of interactive systems for particular tasks. Cognitive dimensions is a psychological framework that provides broadbrush characterisations of interactions that are relevant to ease of use, and an effective terminology to support a wide range of assessments, including the resistance of languages and notations to modification. It is proposed that software design can benefit from the use of cognitive dimensions as tools for assessing software characteristics such as modifiability. To enable this, formal definitions of specific dimensions are developed. This enables the interpretation of otherwise informal dimensions in a precise and generic way. The authors develop and examine two dimensions associated with the notion of `viscosity' (resistance to local change) and demonstrate their relevance in the context of program modification. Two case studies exploring modifications in alternative programming languages and differing styles of solution are used to illustrate the utility of cognitive dimensions. The authors continue by identifying similarities between the novel notion of cognitive dimensions and conventional notions of program quality, such as coupling and cohesion",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=621611,no,undetermined,0
Neural networks for engine fault diagnostics,"A dynamic neural network is developed to detect soft failures of sensors and actuators in automobile engines. The network, currently implemented off-line in software, can process multi-dimensional input data in real time. The network is trained to predict one of the variables using others. It learns to use redundant information in the variables such as higher order statistics and temporal relations. The difference between the prediction and the measurement is used to distinguish a normal engine from a faulty one. Using the network, we are able to detect errors in the manifold air pressure sensor and the exhaust gas recirculation valve with a high degree of accuracy",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=622446,no,undetermined,0
A framework for parallel tree-based scientific simulations,"This paper describes an implementation of a platform-independent parallel C++ N-body framework that can support various scientific simulations that involve tree structures, such as astrophysics, semiconductor device simulation, molecular dynamics, plasma physics, and fluid mechanics. Within the framework the users will be able to concentrate on the computation kernels that differentiate different N-body problems, and let the framework take care of the tedious and error-prone details that care common among N-body applications. This framework was developed based on the techniques we learned from previous CM-5 C implementations, which have been rigorously justified both experimentally and mathematically. This gives us confidence that our framework will allow fast prototyping of different N-body applications, to run on different parallel platforms, and to deliver good performance as well",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=622577,no,undetermined,0
The application of fuzzy enhanced case-based reasoning for identifying fault-prone modules,"As highly reliable software is becoming an essential ingredient in many systems, the process of assuring reliability can be a time-consuming, costly process. One way to improve the efficiency of the quality assurance process is to target reliability enhancement activities to those modules that are likely to have the most problems. Within the field of software engineering, much research has been performed to allow developers to identify fault-prone modules within a project. Software quality classification models can select the modules that are the most likely to contain faults so that reliability enhancement activities can be performed to lower the occurrences of software faults and errors. This paper introduces fuzzy logic combined with case-based reasoning (CBR) to determine fault-prone modules given a set of software metrics. Combining these two techniques promises more robust, flexible and accurate models. In this paper, we describe this approach, apply it in a real-world case study and discuss the results. The case study applied this approach to software quality modeling using data from a military command, control and communications (C<sup>3</sup>) system. The fuzzy CBR model had an overall classification accuracy of more than 85%. This paper also discusses possible improvements and enhancements to the initial model that can be explored in the future",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=731599,no,undetermined,0
Using classification trees for software quality models: lessons learned,"High software reliability is an important attribute of high-assurance systems. Software quality models yield timely predictions of reliability indicators on a module-by-module basis, enabling one to focus on finding faults early in development. This paper introduces the CART (Classification And Regression Trees) algorithm to practitioners in high-assurance systems engineering. This paper presents practical lessons learned in building classification trees for software quality modeling, including an innovative way to control the balance between misclassification rates. A case study of a very large telecommunications system used CART to build software quality models. The models predicted whether or not modules would have faults discovered by customers, based on various sets of software product and process metrics as independent variables. We found that a model based on two software product metrics had an accuracy that was comparable to a model based on 40 product and process metrics",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=731598,no,undetermined,0
Automatic classification of wafer defects: status and industry needs,"This paper describes the automatic defect classification (ADC) beta site evaluations performed as part of the SEMATECH ADC project. Two optical review microscopes equipped with ADC software were independently evaluated in manufacturing environments. Both microscopes were operated in bright-field mode with white light illumination, ADC performance was measured on three process levels of random logic devices: source/drain, polysilicon gate, and metal. ADC performance metrics included classification accuracy, repeatability, and speed. In particular, ADC software was tested using a protocol that included knowledge base tests, gauge studies, and small passive data collections",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=622886,no,undetermined,0
Wavelet image extension for analysis and classification of infarcted myocardial tissue,"Some computer applications for tissue characterization in medicine and biology, such as analysis of the myocardium or cancer recognition, operate with tissue samples taken from very small areas of interest. In order to perform texture characterization in such an application, only a few texture operators can be employed: the operators should be insensitive to noise and image distortion and yet be reliable in order to estimate texture quality from the small number of image points available. In order to describe the quality of infarcted myocardial tissue, the authors propose a new wavelet-based approach for analysis and classification of texture samples with small dimensions. The main idea of this method is to decompose the given image with a filter bank derived from an orthonormal wavelet basis and to form an image approximation with higher resolution. Texture energy measures calculated at each output of the filter bank as well as energies of synthesized images are used as texture features in a classification procedure. The authors propose an unsupervised classification technique based on a modified statistical t-test. The method is tested with clinical data, and the classification results obtained are very promising. The performance of the new method is compared with the performance of several other transform-based methods. The new algorithm has advantages in classification of small and noisy input samples, and it represents a step toward structural analysis of weak textures.",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=623055,no,undetermined,0
Validating the defect detection performance advantage of group designs for software reviews: report of a replicated experiment,"It is widely accepted that software development technical reviews (SDTRs) are a useful technique for finding defects in software products. The normative SDTR literature assumes that group reviews are better than individual reviews. Recent debates centre around the need for review meetings. This paper presents the findings of a replicated experiment that was conducted to investigate whether group review meetings are needed and why. We found that an interacting group is the preferred choice over the average individual and artificial (nominal) groups. The source of performance advantage of interacting groups is not synergy as was previously thought, but rather in discriminating between true defects and false positives identified by individual reviewers. As a practical implication, nominal groups may be an alternative review design in situations where individuals exhibit a low level of false positives",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=623751,no,undetermined,0
Supporting hardware trade analysis and cost estimation using design complexity,"Defines and illustrates a hardware design complexity measure (HDCM) and describe its potential applications to trade-off analysis and cost estimation. Specifically, we define a VHDL complexity measure. We have derived the HDCM from an avionics software design complexity measure (ASDCM) that we have shown to be effective in estimation and optimization of overall software costs. Similar to the ASDCM, we believe that the proposed HDCM could enable more optimal hardware design, implementation and maintenance",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=623941,no,undetermined,0
A reverse engineering approach to evaluate function point rules,"Function points are generally used for measuring software functional size from a user perspective. The paper is concerned with the problem of counting function points from source code using the function point analysis proposed by the International Function Point User Group (IFPUG) 1994 standards. The paper presents the Automated FP counting scope and objective, the presentation of an existing semi-formal model and the required extensions for the definition of four IFPUG rules. Then the authors propose reverse engineering techniques to address those four rules",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=624594,no,undetermined,0
Implementation and performance assessment of multilevel data structures,The authors discuss the implementation and performance assessment of a multilevel data structure system prototype. They compare the multilevel data structure approach with corresponding fault-intolerant implementations to assess the overhead imposed for incorporating fault tolerance. The simulation shows promising results in using multilevel data structures,1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=624779,no,undetermined,0
A formal approach to software components classification and retrieval,"We propose an approach to reuse-based software development using a formal method. In our approach, each software component is annotated with a set of predicates to formally describe the component and is classified using a faceted scheme. A user may retrieve components from the library using either keywords or predicates. When a component is retrieved, it is checked to determine if it matches the requirements. Then, the component is integrated with the designed system, along its required functionalities. The integrated component/system as transformed into a Predicate/Transition net (PrT net) to perform consistency checking. If there is no inconsistency, the component may be adapted or incorporated directly. Otherwise, the conditions that cause the inconsistency will be revealed. The user may decide to search the library again by modifying the query specification and restart the whole process, or to terminate the search",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=624812,no,undetermined,0
An integrated process and product model,"The relationship between product quality and process capability and maturity has been recognized as a major issue in software engineering based on the premise that improvements in process will lead to higher quality products. To this end, we have been investigating an important facet of process capability, stability, as defined and evaluated by trend, change, and shape metrics, across releases and within a release. Our integration of product and process measurement and evaluation serves the dual purpose of using metrics to assess and predict reliability and risk and concurrently using these metrics for process stability evaluation. We use the NASA Space Shuttle flight software to illustrate our approach",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=731249,no,undetermined,0
Applications of measurement in product-focused process improvement: a comparative industrial case study,"In ESPRIT project PROFES, measurement according to the Goal/Question/Metric (GQM) approach is conducted in industrial software projects at Drager Medical Technology, Ericsson Finland, and Schlumberger Retail Petroleum Systems. A comparative case study investigates three different ways of applying GQM in product-focused process improvement: long-term GQM measurement programmes at the application sites to better understand and improve software products and processes; GQM-based construction and validation of product/process dependency models, which describe the process impact on software quality; and cost/benefit investigation of the PROFES improvement methodology using GQM for (meta-) analysis of improvement programmes. This paper outlines how GQM is applied for these three purposes",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=731234,no,undetermined,0
Communication middleware and software for QoS control in distributed real-time environments,"There has been an increasing need of highly predictable, timely, and dependable communication services with QoS guarantees on an end-to-end basis either for embedded real-time applications or for multimedia-integrated distributed control. Performance objectives used in conventional networks-such as maximizing the throughput, minimizing the response time, or providing fairness to users-are not of the most important concern for both types of applications. Instead, resources should be appropriately reserved and managed to support multidimensional quality of service (QoS) on an end-to-end basis, as well as application-specific tradeoffs among them. The main intent of the paper is thus to address and demonstrate an environment-an integrated set of network resource management techniques, middleware layers, and network software-for supporting multi-dimensional QoS on an end-to-end basis in distributed real-time environments. The authors first provide an analytic QoS framework Then, they elaborate on the algorithms and mechanisms to provide such network services. To empirically analyze the behavior of the proposed components, they are currently implementing them as software layers on top of the Sun Solaris operating system, and comment on the implementation status",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=625068,no,undetermined,0
The Internal Revenue Service function point analysis program: a brief,"The Internal Revenue Service (IRS) Information Systems has been using function point analysis since 1993 as the foundation of its software metrics program. Using function point analysis, the IRS has significantly improved the quality of its software project management as measured by the SEI Capability Maturity Model. The IRS can determine the size of its software about four times faster than industry averages can estimate software size and resource requirements for new development projects as soon as the central data model is known, and can accurately size small software work orders and estimate corresponding resources needed without examining the software or meeting with the project team. Each of these methodologies qualify at least at CMM Level 2",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=625077,no,undetermined,0
Applying software metrics to formal specifications: a cognitive approach,"It is generally accepted that failure to reason correctly during the early stages of software development causes developers to make incorrect decisions which can lead to the introduction of faults or anomalies in systems. Most key development decisions are usually made at the early system specification stage of a software project and developers do not receive feedback on their accuracy until near its completion. Software metrics are generally aimed at the coding or testing stages of development, however, when the repercussions of erroneous work have already been incurred. This paper presents a tentative model for predicting those parts of formal specifications which are most likely to admit erroneous inferences, in order that potential sources of human error may be reduced. The empirical data populating the model was generated during a series of cognitive experiments aimed at identifying linguistic properties of the Z notation which are prone to admit non-logical reasoning errors and biases in trained users",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=731248,no,undetermined,0
Test order for inter-class integration testing of object-oriented software,"One major problem in inter class integration testing of object oriented software is to determine the order in which classes are tested. This test order, referred to as inter class test order, is important since it affects the order in which classes are developed, the use of test stubs and drivers for classes, and the preparation of test cases. The paper first proposes a number of desirable properties for inter class test order and then presents a new inter class test order strategy. In this new strategy, classes are integrated according to their major and minor level numbers. Major level numbers of classes are determined according to inheritance and aggregation relations between classes, where an aggregation relation refers to a class' inclusion of objects of another class. For classes with the same major level number, their minor level numbers are determined according to association relations between these classes, where an association relation refers to a class' dependency (other than inheritance and aggregation relations) on another class",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=625079,no,undetermined,0
A DSS for ISO 9000 certification in the health service: a case study,"The European Community market points out the need of overtaking the technical obstacle arising from national rules and laws which prevent the free trading of products and services and the cooperation among foreign companies. To make such a process easier and to harmonize the trades, a quality certification was found to assure the technical specifications of the products and the quality of the services offered. These rules has been collected in an E.C. Law called ISO 9000. The conformity to ISO 9000 rules has now become a fundamental requirement and a key element for the competition of the manufacturing enterprises. At present the need to extend the quality parameters is spreading also in the Services field where both rules and their directives are rather indefinite. ISO 8402 rules, in fact, define the idea of quality as a whole of performance and features of a product or of a service that must satisfy declared or implicit needs. Our work has been carried out together with Dental Clinic of the University of Genoa which can be considered as a case study for similar structures. This paper focuses the importance to define clearly both the requirements of a customer and the characteristics of the Service in order to satisfy such requirements. This paper gives a practical solution to outline the policy and the objectives of a quality system for the particular health structure. The proposed software structure, written in Java language, is a part of a decision support system which can be used in order to achieve the goal of an ISO 9000 certification in such environment. The proposed software structure also allows to define and to organize all data to be maintained according to ISO 9000 rules and can detect any deficiency which may compromise the quality standard",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=625814,no,undetermined,0
Forecasting network performance to support dynamic scheduling using the network weather service,"The Network Weather Service is a generalizable and extensible facility designed to provide dynamic resource performance forecasts in metacomputing environments. In this paper, we outline its design and detail the predictive performance of the forecasts it generates. While the forecasting methods are general, we focus on their ability to predict the TCP/IP end-to-end throughput and latency that is attainable by an application using systems located at different sites. Such network forecasts are needed both to support scheduling, and by the metacomputing software infrastructure to develop quality-of-service guarantees",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=626437,no,undetermined,0
Architectural composition in the electronic design studio: conceptual design using CAD visualisation and virtual reality modelling,"The paper evaluates the possibilities for the use of computer aided design and desktop virtual reality technologies as tools for architectural composition. An experimental teaching programme involving undergraduate architectural students at the University of Luton, in which aspects of compositional theory are explored through the direct creation of architectural form and space in digital formats is described. In the programme principles of architectural composition, based upon the ordering and organisation of typological architectural elements according to established rules of composition are introduced to the students through the study of recognised works of design theory. CAD and desktop virtual reality are then used to define and manipulate architectural elements, and to make formal and spatial evaluations of the environments created. The paper describes the theoretical context of the work, assesses the suitability of the software used for performing compositional manipulations, and evaluates the qualities of immersion and intuitive feedback which virtual reality based modelling can offer in the design visualisation process. The teaching programme utilises standard software packages, including AutoCAD, and 3D Studio, as well as Superscape VRT, a PC based desktop VR package",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=626511,no,undetermined,0
Getting a handle on the fault injection process: validation of measurement tools,"In any manufacturing environment, the fault injection rate might be considered one of the most meaningful criterion to evaluate the goodness of the development process. In our field, the estimates of such a rate are often oversimplified or misunderstood generating unrealistic expectations on their prediction power. The computation of fault injection rates in software development requires accurate and consistent measurement, which translates into demanding parallel efforts for the development organization. This paper presents the techniques and mechanisms that can be implemented in a software development organization to provide a consistent method of anticipating fault content and structural evolution across multiple projects over time. The initial estimates of fault insertion rates can serve as a baseline against which future projects can be compared to determine whether progress is being made in reducing the fault insertion rate, and to identify those development techniques that seem to provide the greatest reduction",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=731238,no,undetermined,0
An IDDQ sensor for concurrent timing error detection,"Error control is a major concern in many computer systems, particularly those deployed in critical applications. Experience shows that most malfunctions are caused by transient faults which often manifest themselves as signal delays or other timing violations. We present a novel CMOS based concurrent error detection circuit that allows a flip flop (or other timing sensitive circuit element) to signal when its data has been potentially corrupted by a timing violation. Our circuit employs on-chip IDDQ evaluation to determine when the input changes in relation to a clock edge. If the input changes too close to clock time, the resulting switching transient current exceeds a reference threshold, and an error is flagged. We have designed, fabricated and evaluated a test chip that shows that such an approach can be used to effectively detect setup and hold time violations in clocked circuit elements",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=628335,no,undetermined,0
QoS allocation of multicast connections in ATM,"We study the resource allocation problem and call admission problem for multicast applications in ATM networks. Two approaches are proposed for dividing the end-to-end QoS requirement into the local QoS constraint on each link of the multicast tree. The first approach is to apply the greedy method to divide the end-to-end QoS based on the residual capacity on the links. In the second approach we show how to use a genetic algorithm to solve the QoS allocation problem. The effective bandwidth concept is used for estimating the amount of bandwidth required to guarantee a certain level of QoS on each link. If the end-to-end QoS cannot be guaranteed due to insufficient network resources, the multicast request is rejected (as a result of call admission). A new performance metric, fractional reward loss, is adopted for evaluating the solutions generated by the proposed two approaches. Our experiment results show that our approaches yield lower fractional reward loss and utilize the network resources more efficiently",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=629226,no,undetermined,0
User-safe devices for true end-to-end QoS,"User-safe devices are a little smarter than your average device. They implement some of the functionality traditionally found in device drivers within the device itself. This provides a number of performance benefits, particularly for devices attached to I/O networks. User-safe devices are serious about quality of service. They can provide the guarantees that applications require in order to behave predictably on a loaded system. This paper describes user-safe devices, and the role they play in the University of Cambridge's experimental multimedia workstation",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=629371,no,undetermined,0
Experimenting with error abstraction in requirements documents,"In previous experiments we showed that the Perspective-Based Reading (PBR) family of defect detection techniques was effective at detecting faults in requirements documents in some contexts. Experiences from these studies indicate that requirements faults are very difficult to define, classify and quantify. In order to address these difficulties, we present an empirical study whose main purpose is to investigate whether defect detection in requirements documents can be improved by focusing on the errors (i.e., underlying human misconceptions) in a document rather than the individual faults that they cause. In the context of a controlled experiment, we assess both benefits and costs of the process of abstracting errors from faults in requirements documents",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=731236,no,undetermined,0
Experiences with program static analysis,"Conventionally, software quality has been measured mainly by the number of test items, the test coverage, and the number of faults in the test phase. This approach of relying heavily on testing is not satisfactory from a quality assurance viewpoint. Since software is becoming larger and more complex, quality must be assured from the early phases, such as requirements analysis, design and coding. Code reviews are effective to build in software quality from the coding phase. However, for a large-scale software development, there are limitations in covering all the programs. The advantage of using static analysis tools is the capability to detect fault-prone programs easily and automatically. We describe the effective use of a static analysis tool, and show the effectiveness of the static analysis technique",1998,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=731235,no,undetermined,0
Detection of response time failures of real-time software,"Classical software reliability research has tended to focus on behavioral type failures which typically manifest themselves as incorrect or missing outputs. In real time software, a correct output which is not produced within a specified response time interval may also constitute a failure. As a result, response time failures must also be taken into account when real time software reliability is assessed. The paper considers the case where the detection of response times failures is done by a separate unit which observes the inputs and outputs of the target software. Automatic detection of such failures is complicated by state dependencies which require the unit to track a target's state as well as the elapsed times between specified stimulus and response pairs. A novel black box approach is described for detecting response time failures and quality of service degradations of session oriented, real time software. The behavior of the target software is assumed to be specified in a formalism based on the notion of communicating extended finite state machines. The response time failure detection unit implemented independently of the target software, interprets a formal model derived directly from the target's requirement specifications. The model is used both to track the state of the target and to determine when to start and stop time interval timing measurements. Measurements of multiple response time intervals may occur simultaneously. The approach was evaluated on the call processing program of a small telephone exchange. Some results of the evaluation are presented and discussed",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=630846,no,undetermined,0
RF induction and analog junction techniques for finding opens,"In response to the need for fast and simple methods for detecting and diagnosing open pins on SMT devices, board test manufactures have introduced power-off, vectorless test techniques that provide reasonable fault coverage with little programming effort. This paper describes two techniques: the RF induction and the analog junction technique. The RF technique applies a 200-500kHz signal to a spiral loop antenna located over the device. This antenna or â€œinducerâ€?produces an AC signals at the device pin under test which the in-circuit tester measures. The technique requires one inducer for each device to be tested. The test system software automatically learns the characteristics of the device being tested. The users need only describe the connections between the device and the tester, identify the device's power and ground pins, and which inducer is mounted over the part. The analog junction technique uses the device protection diodes. It requires no fixture hardware, relying only on the bed-of-nails contact with the device pins. The analog junction technique applies voltage to one pin of the device, causing current to flow between the pin and the device ground lead. Voltage is then applied another pin on the device, causing current to flow between this pin and the ground lead. Both the analog junction and RF induction techniques are effective means for detecting opens on digital devices",1997,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=639624,no,undetermined,0
Architectural optimizations for software-based MPEG4 video encoder,"This paper presents a set of architectural optimizations for improving the performance of an MPEG4 video encoder. The techniques presented here focus on optimizing the encoder architecture rather than module level algorithmic modifications. The optimizations contribute to the development of a fast and memory efficient encoder without affecting video quality. An interface driven methodology has been developed to identify and solve performance bottlenecks for the encoder. Appropriate data flow between components has been developed so that memory intensive operations, such as memory access and copying, are minimized. These optimizations have been applied on MPEG4 simple profile encoder. Results demonstrate orders of magnitude computational improvements without any algorithmic modifications.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7077915,no,undetermined,0
On the use of testability measures for dependability assessment,"Program â€œtestabilityâ€?is informally, the probability that a program will fail under test if it contains at least one fault. When a dependability assessment has to be derived from the observation of a series of failure free test executions (a common need for software subject to â€œultra high reliabilityâ€?requirements), measures of testability can-in theory-be used to draw inferences on program correctness. We rigorously investigate the concept of testability and its use in dependability assessment, criticizing, and improving on, previously published results. We give a general descriptive model of program execution and testing, on which the different measures of interest can be defined. We propose a more precise definition of program testability than that given by other authors, and discuss how to increase testing effectiveness without impairing program reliability in operation. We then study the mathematics of using testability to estimate, from test results: the probability of program correctness and the probability of failures. To derive the probability of program correctness, we use a Bayesian inference procedure and argue that this is more useful than deriving a classical â€œconfidence levelâ€? We also show that a high testability is not an unconditionally desirable property for a program. In particular, for programs complex enough that they are unlikely to be completely fault free, increasing testability may produce a program which will be less trustworthy, even after successful testing",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=485220,no,undetermined,0
Designing a controller that works: using formal techniques in robotic systems,"The size and complexity of robot controllers is such that it is impossible to predict their performance by conventional means. Debugging and system maintenance are major problems which have only partial solutions. In conjunction with careful structuring, the authors have used formal mathematical techniques (often known generically as formal methods) in designing the software architecture of a mobile robot controller to gain greater understanding of the system and to validate its expected performance. The impetus for this work is a major mobile robot project to provide sensory control in a factory application. In this article, the authors assess the place of formal methods in the cycle of system development.<<ETX>>",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=109030,no,undetermined,0
Reliability models for very large software systems in industry,"BNR, the R&D subsidiary of Northern Telecom and Bell Canada, has one of the largest software systems in the world, with code libraries exceeding 8 million source lines of a high level language. This software is used in the high-end digital switching systems that Northern Telecom markets. Software reliability methods are applied to a major subset of this software to determine if the total number of customer-perceived failures and actual software faults can be predicted before or soon after a new release of such a system. These predictions are based on pre-customer testing (Î± and Î²) and small field trials. Many of the existing reliability models and methods of parameter estimation currently demonstrated in the literature are compared",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=145351,no,undetermined,0
Parameter estimation of the hyper-geometric distribution model for real test/debug data,"The hyper-geometric distribution model (HGDM) has been proposed for estimating the number of faults initially resident in a program at the beginning of the test/debug process. However, the parameters of the hyper-geometric distribution necessary for making the estimation were previously determined by the 3-dimensional exhaustive search and therefore, much time was needed to get the numerical result. The authors demonstrate, using real test/debug data of programs, that the least square sum method can be well applied to the estimation of such parameters of the hyper-geometric distribution model. Thus, the time needed for calculating the estimates can be reduced greatly",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=145350,no,undetermined,0
A notion of rule-based software quality engineering,"Engineering of software quality is defined by rules for methods, tools, evaluation, measurement and assessment. The product model used requires at least three layers: application concept or requirements specification, data processing concept or system specification, and realization or programs. For a computer evaluation, assessment and certification of a software system one has to examine and assess all its components described by these layers. Quality is decomposed into evaluation factors and quality factors. Evaluation factors define quality factors; metrics for functional and non-functional requirements are the terms used to define evaluation factors. All these are dependent upon quality objectives and goals. Methods and tools for evaluation, measurement and assessment are also imposed by objectives as they are connected to the factors, methods and tools. Each problem domain is represented by a set of rules. Problem domain dependencies are also expressed by rule sets. This might provide the basis to define a framework for learner controlled instruction on software quality engineering",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=143891,no,undetermined,0
Monitoring of distributed systems,"Distributed systems offer opportunities for attaining high performance, fault-tolerance, information sharing, resource sharing, etc. But we cannot benefit from these potential advantages without suitable management functions such as performance management, fault management, security management, etc. Underlying all these management functions is the monitoring of the distributed system. Monitoring consists of collecting information from the system and detecting particular events and states using the collected information. These events and states can be symptoms for performance degradations, erroneous functions, suspicious activities, etc. and are subject to further analysis. Detecting events and states requires a specification language and an efficient detection algorithm. The authors introduce an event/state specification language based on classical temporal logic and a detection algorithm which is a modification of the RETE algorithm for OPS5 rule-based language. They also compare their language with other specification languages",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=143885,no,undetermined,0
The use of a distributed vibration monitoring system for on-line mechanical fault diagnosis,"Modern day paper making machines are high speed, high volume manufacturing units that incorporate advanced computer monitoring and control systems. New Thames Paper company manufactures high quality fine paper on a single machine. Emphasis has been placed on the importance of predictive maintenance within such continuous process industries. The company has installed condition monitoring which requires a measurement to be taken from a machine and used to indicate the current working condition of that machine. The system uses vibration analysis as a tool that can indicate faults at an early stage and thus reduce downtime. The authors outline the condition monitoring system installed at New Thames Mill and the reasons for its selection",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=140086,no,undetermined,0
Parameter value computation by least square method and evaluation of software availability and reliability at service-operation by the hyper-geometric distribution software reliability growth model (HGDM),"The authors explain precisely the idea of the capture-recapture process for software faults in the context of a proposed testing environment and introduce the least square method into the model to estimate the parameter values of the HDGM. For real observed data collected during the service-operational phase, the authors show the applicability of the HGDM in estimating the degree of unavailability of a software system in operation (service). Furthermore, the estimated probability of discovering zero faults as service-operation proceeds can be taken as a reliability measure",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=130647,no,undetermined,0
Toward new techniques to assess the software implementation process,"The author presents the concept of software boundaries and their automated detection. He describes an objective, high-level assessment technology to support process control of software development. The technique is largely independent of underlying design and development methods. An example illustrates an automated system partitioning application. The technique analyzes the end product and the delivered software code. By suitable adjustment of the analysis goals, measures, and criteria, the technique can help evaluate the effectiveness of many software implementation methods. The author discusses the technique's potential for increasing confidence in the fault-tolerating properties of a system",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=130625,no,undetermined,0
A high performance processor switch based architecture for fault tolerant computing,"A novel fault tolerant switch concept in the context of fault tolerant network architecture is discussed. This intelligent switch provides a non-blocking transmission path for all of its resources of the network architecture such as processor buses, disk subsystems, and its peripherals. In addition, it incorporates concurrent error detection, time-out mechanisms and a sophisticated error detecting protocol between the switch matrix and the resource management units to detect the hardware errors. Upon detection of the error, several recovery techniques are provided to ensure continuous operation of the system. Additional features of the network architecture for supporting high performance applications include concurrent channel access and priority channel service. The studies show that the fully connected network which supports both graceful degradation and standby sparing techniques exhibits excellent reliability performance and throughput",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=113796,no,undetermined,0
Fault-tolerant round-robin A/D converter system,"A robust A/D converter system that requires much less hardware overhead than traditional modular redundancy approaches is described. A modest amount of oversampling generates information that is exploited to achieve fault tolerance. A generalized likelihood ratio test is used to detect the most likely failure and also to estimate the optimum signal reconstruction. The error detection and correction algorithm reduces to a simple form and requires only a slight amount of hardware overhead. A derivation of the algorithm is presented, and modifications that lead to a realizable system are discussed. The authors then evaluate overall performance through software simulations",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=108496,no,undetermined,0
EDF: a formalism for describing and reusing software experience,"One approach to achieve high levels of reuse of software experience is to create a database with information on software products, processes, and measurements. The authors present the Extensible Description Formalism (EDF), a language to describe these databases. EDF is a generalization of the faceted index approach to classification. Objects in EDF can be described in terms of different sets of facets and other object descriptions. Classification schemes are easy to extend by adding new attributes or refining existing ones. EDF has been implemented and used to classify a large software library. The authors present the EDF approach to classification; the development of a classification of data structure operations and packages; and a software defect classification scheme used to describe, explain, and predict defects in data structure packages",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=145363,no,undetermined,0
Cyclomatic complexity density and software maintenance productivity,"A study of the relationship between the cyclomatic complexity metric (T. McCabe, 1976) and software maintenance productivity, given that a metric that measures complexity should prove to be a useful predictor of maintenance costs, is reported. The cyclomatic complexity metric is a measure of the maximum number of linearly independent circuits in a program control graph. The current research validates previously raised concerns about the metric on a new data set. However, a simple transformation of the metric is investigated whereby the cyclomatic complexity is divided by the size of the system in source statements. thereby determining a complexity density ratio. This complexity density ratio is demonstrated to be a useful predictor of software maintenance productivity on a small pilot sample of maintenance projects",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=106988,no,undetermined,0
A comparison of voting strategies for fault-tolerant distributed systems,"The problem of voting is studied for both the exact and inexact cases. Optimal solutions based on explicit computation of condition probabilities are given. The most commonly used strategies, i.e. majority, median, and plurality are compared quantitatively. The results show that plurality voting is the most powerful of these techniques and is, in fact, optimal for a certain class of probability distributions. An efficient method of implementing a generalized plurality voter when nonfaulty processes can produce differing answers is also given",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=93959,no,undetermined,0
Adjudicators for diverse-redundant components,"The authors define the adjudication problem, summarize the existing literature on the topic, and investigate the use of probabilistic knowledge about error/faults in the subcomponents of a fault-tolerant component to obtain good adjudication functions. They prove the existence of an optimal adjudication function, which is useful both as an upper bound on the probability of correctly adjudged obtainable output and as a guide for design decisions",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=93957,no,undetermined,0
On the performance of software testing using multiple versions,"The authors present analytic models of the performance of comparison checking (also called back-to-back testing and automatic testing), and they use these models to investigate its effectiveness. A Markov model is used to analyze the observation time required for a test system to uncover a fault using comparison checking. A basis for evaluation is provided by developing a similar Markov model for the analysis of ideal checking, i.e. using a perfect (through unrealizable) oracle. Also presented is a model of the effect of comparison checking on a version's failure probability as testing proceeds. Again, comparison checking is evaluated against ideal checking. The analyses show that comparison checking is a powerful and effective technique.<<ETX>>",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=89395,no,undetermined,0
Practical application and implementation of distributed system-level diagnosis theory,A DSD (distributed self-diagnosing) project that consists of the implementation of a distributed self-diagnosis algorithm and its application to distributed computer networks is presented. The EVENT-SELF algorithm presented combines the rigor associated with theoretical results with the resource limitations associated with actual systems. Resource limitations identified in real systems include available message capacity for the communication network and limited processor execution speed. The EVENT-SELF algorithm differs from previously published algorithms by adopting an event-driven approach to self-diagnosability. Algorithm messages are reduced to those messages required to indicate changes in system those messages required to indicate changes in system state. Practical issues regarding the CMU-ECE DSD implementation are considered. These issues include the reconfiguration of the testing subnetwork for environments in which processors can be added and removed. One of the goals of this work is to utilize the developed CMU-ECE DSD system as an experimental test-bed environment for distributed applications.<<ETX>>,1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=89380,no,undetermined,0
Failure analysis and modeling of a VAXcluster system,"The authors discuss the results of a measurement-based analysis of real error data collected from a DEC VAXcluster multicomputer system. In addition to evaluating basic system dependability characteristics, such as error and failure distributions and hazard rates for both individual machines and the VAXcluster, they develop reward models to analyze the impact of failures on the system as a whole. The results show that more than 46% of all failures were due to errors in shared resources. This is despite the fact that these errors have a recovery probability greater than 0.99. The hazard rate calculations show that not only errors but also failures occur in bursts. Approximately 40% of all failures occur in bursts and involve multiple machines. This result indicates that correlated failures are significant. Analysis of rewards shows that software errors have the lowest reward (0.05 versus 0.74 for disk errors). The expected reward rate (reliability measure) of the VAXcluster drops to 0.5 in 18 hours for the 7-out-of-7 model and in 80 days for the 3-out-of-7 model. The VAXcluster system availability is evaluated to be 0.993 250 days of operation.<<ETX>>",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=89372,no,undetermined,0
Availability evaluation of MIN-connected multiprocessors using decomposition technique,"An analytical technique for the availability evaluation of multiprocessors using a multistage interconnection network (MIN) is presented. The MIN represents a Butterfly-type connection with a 4*4-switching element (SE). The novelty of this approach is that the complexity of constructing a single-level exact Markov chain (MC) is not required. By use of structural decomposition, the system is divided into three subsystems-processors, memories, and MIN. Two simple MCs are solved by using a software package, called HARP, to find the probability of i working processing elements (PEs) and j working memory modules (MMs) at time t. A second level of decomposition is then used to find the approximate number of SEs (x) required for connecting the i PEs and j MMs. A third MC is then solved to find the probability that the MIN will provide the necessary communication. The model has been validated through simulation for up to a 256-node configuration, the maximum size available for a commercial MIN-connected multiprocessor.<<ETX>>",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=89353,no,undetermined,0
Hypertool: a programming aid for message-passing systems,"Programming assistance, automation concepts, and their application to a message-passing system program development tool called Hypertool are discussed. Hypertool performs scheduling and handles the communication primitive insertion automatically, thereby increasing productivity and eliminating synchronization errors. Two algorithms, based on the critical-path method, are presented for scheduling processes statically. Hypertool also generates the performance estimates and other program quality measures to help programmers improve their algorithms and programs",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=80160,no,undetermined,0
Realistic assumptions for software reliability models,A definition of reliability appropriate for systems containing significant software that includes trustworthiness and is independent of requirements is stated and argued for. The systems addressed encompass the entire product development process as well as both product and its documentation. Cost incurred as a result of faults are shown to be appropriate as a performance measurement for this definition. This and more realistic assumptions are shown to lead to the use of auto-regressive integrated moving average (ARIMA) mathematical models for the modeling of reliability growth,1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=145356,no,undetermined,0
Metrics evaluation of software reliability growth models,"Numerous software reliability growth models have been proposed. The authors present three metrics which have been helpful in assessing the applicability and predictive validity of these models. These metrics are the relative fitting error metric, the short term predictive validity metric and the long term predictive validity metric. The application of these three metrics is illustrated on estimation of field reliability of telecommunication switching systems",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=145373,no,undetermined,0
Error recovery in shared memory multiprocessors using private caches,"The problem of recovering from processor transient faults in shared memory multiprocessor systems is examined. A user-transparent checkpointing and recovery scheme using private caches is presented. Processes can recover from errors due to faulty processors by restarting from the checkpointed computation state. Implementation techniques using checkpoint identifiers and recovery stacks are examined as a means of reducing performance degradation in processor utilization during normal execution. This cache-based checkpointing technique prevents rollback propagation, provides rapid recovery, and can be integrated into standard cache coherence protocols. An analytical model is used to estimate the relative performance of the scheme during normal execution. Extensions to take error latency into account are presented",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=80134,no,undetermined,0
A neural network approach to machine vision systems for automated industrial inspection,"A knowledge-based machine vision system for industrial web inspection that is applicable to a broad spectrum of different inspection tasks is proposed. The main function of the machine vision system is to detect undesirable defects that can appear on the surface of the material being inspected. A neural network is used within the blackboard framework for a labeling verification step of the high-level recognition module of this system. As an application, this system has been applied to a lumber inspection problem. It has been successfully tested on a number of boards from several different species. A performance comparison of the neural network with the <e1>k</e1>-nearest neighbor classifier is also presented",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=155177,no,undetermined,0
Experimental evaluation of the cost effectiveness of software reviews,"A new metric for evaluating the cost effectiveness of technical reviews is described. The proposed metric is based on the degree to which testing costs are reduced by technical reviews. The metric can be interpreted as combining two conventional metrics. Using an experimental evaluation of the conventional metrics and the proposed metric for data collected in an industrial environment, the authors show the validity and usefulness of the proposed metric. In particular, they present a method to estimate a value of the proposed metric by using only the values obtained at review phase",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=170216,no,undetermined,0
Performance and degradation of the (p)a-SiC:H/(i)a-Si:H/(n)a-Si:H solar cell: a computer modeling study,"The computer code AMPS (analysis of microelectronic and photonic structures) has been used to study the performance of the (p)a-SiC:H/(i)a-Si:H/(n)a-Si:H heterojunction solar cell. The authors found initially that AMPS predicts for an ideal (p)a-Si:H/(i)a-Si:H/(n)a-Si:H heterojunction an open-circuit voltage <e1>V</e1><sub>oc</sub> value of about 1 V. However, experimental values of <e1>V</e1><sub>oc</sub> are around 0.8 V without (i)a-SiC:H buffer layers at the p/i interface and around 0.85 V with these buffer layers. The authors studied the possible origins of these lower experimentally observed values and have found that although higher concentrations of defect states anywhere in a cell can reduce <e1>V</e1><sub>oc</sub>, the impact of these defect states on <e1>V</e1><sub>oc</sub> and fill factor (FF) is different according to their location",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=169474,no,undetermined,0
Characterization of TFT/LCD arrays,"Thin-film-transistor (TFT) array characterization is important to liquid-crystal display (LCD) design, development, failure analysis, and sorting on a manufacturing line. The authors present characterization highlights obtained using a TFT array tester that can detect, accurately locate, and in many cases identify both line faults and pixel faults in a TFT array. It can provide useful performance information on normally operating pixels.<<ETX>>",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=167477,no,undetermined,0
Implementing an initial software metrics program,"It is noted that software metrics is one of the tools that software practitioners have available to provide software measurement and management of software projects. Using software metrics for measurement allows an organization to assess software products to ensure they adhere to original performance requirements continuously throughout a development effort. Using software metrics to assist in the software management process allows the managers to assess the development process for adherence to development plans and schedules. Ultimately, the inherent quality of a software product and the efficiency of a development effort can then be identified, quantified, and improved. The author discusses the importance of using metrics, explains how to implement an initial software measurement program within a software development organization, and introduces a software quality metrics framework for further growth in the area of software measurement",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=165928,no,undetermined,0
Applying advanced digital simulation techniques in designing fault tolerant systems,"Protocol, a services organization that is providing simulation solutions for the development of digital systems, is discussed. By leveraging the latest in design simulation technology, Protocol has addressed the problems associated with integrating today's most complex systems. The author describes the capabilities developed by Protocol, and how they can be applied to the development of complex fault-tolerant systems. It is noted that the methodologies and techniques developed and applied by Protocol have proven that anomalies in system designs can be located and corrected early in the development process through the use of gate level system simulation (GLSS). Protocol's concurrent hardware and software enabler (CHASE) provides the ability to debug software running on gate-level models of system hardware, thereby providing a software/hardware integration environment prior to fabrication. GLSS and CHASE together provide powerful capabilities needed by fault tolerant system designer",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=165799,no,undetermined,0
A technique for improving software robustness to failure,"Software failure modes analysis is a technique designed to minimize the impact of software failures by providing controlled, recovery paths that affect service as little as possible. Currently, the approach throughout the software industry is an ad hoc one in an attempt to ensure good recovery from software design errors. The authors have taken a rigorous approach to ensure high recovery coverage from software design errors as well as providing reliability predictions as input to software architecture decisions. A description is presented of the author's experiences with these techniques",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=162535,no,undetermined,0
Standards for navigable databases: A progress report,"SAE's Database Standards Task Group is addressing digital street map databases to support vehicle navigation. The initial work of the Task Group is to develop a descriptive ""truth-in-labeling"" standard to permit database vendors to consistently and accurately describe their products and to permit application developers to match available databases to their needs. The truth-in-labeling standard consists of definitions to identify database entities, metrics to provide scales against which entity quality can measured, and tests to score the entities along metrics. The general classes of entities being considered are nodes (e.g., roadway intersections), links (e.g., roadway segments), and regions (e.g., cities, parks, shopping malls, etc.). Among the issues being considered are: how to facilitate broadcasts of traffic information to vehicles using navigable databases; approaches to dealing with roads with multiple names; roadway classifications; and coordination with similar efforts in other parts of the world.",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1623628,no,undetermined,0
Reliability evaluation of subtransmission systems,"A brief description of failure modes encountered in the analysis of subtransmission systems is presented. The concepts discussed are illustrated by application to a system which is sufficiently large that practical factors can be realistically modeled and assessed and also sufficiently small that the effect of sensitivity studies can be easily identified. The computer program SUBTREL, which can be used to consider all the factors which affect the reliability of a subtransmission system, is also discussed. The computer program for the reliability evaluation of subtransmission systems is suitable for analyzing a wide range of networks of sizes normally encountered in actual systems. A wide range of sensitivity studies, including adverse weather effects and high-order outages, can be conducted and the results can be used to make objective decisions",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=160558,no,undetermined,0
Cost effective software quality,"Software quality is decomposed into two complementary aspects. The first aspect is software metrics to show how well the code is currently performing, and at what rate it is improving through test or deployment. The second aspect deals with the roots of software quality in the underlying development process. Specific quality initiatives that have been found to have significant payback are discussed. The beneficial coupling with metrics is demonstrated",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=154475,no,undetermined,0
On the determination of optimum software release time,One of the important applications of software reliability models is the determination of software release time. The author presents some software release policies and discusses the problem of determination of optimum test time. Both reliability requirements and cost models are considered in obtaining specific release policies. It is noted that acceptable failure intensity should be used as a reliability goal and optimum release policy should be based on sequential approach. Some other interesting software release policies are also reviewed,1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=145384,no,undetermined,0
An automated environment for optimizing fault-tolerant systems designs,"A description is given of the computer-aided Markov evaluation (CAME) program, a computer-aided engineering (CAE) tool that automatically generates Markov models directly from a system description, providing an environment for quickly evaluating probabilistic measures of a fault tolerant system's performance. The CAME program provides an automated environment that aids a design in systematically optimizing a fault tolerant system design and, in the process, gaining insight and intuition about why the design behaves as it does. The overall processes incorporated in the CAME program are discussed with particular emphasis on its automated model reduction capabilities. An example is shown where the CAME program is used to optimize the design of a fault-tolerant, integrated navigation system",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=154462,no,undetermined,0
The theory and algorithm of detecting and identifying measurement biases in power system state estimation,"The theory and algorithm of the detection and identification of measurement biases are developed. A fast and efficient recursive measurement biases estimation identification method is proposed. A set of linearized recursive formulae are developed to recursively calculate measurement residual averages and their variance. Using this algorithm, the long list of suspicious data can be avoided and the computational speed can be increased greatly. This adds a new function of monitoring on the operation of measurement system to the real network state analysis application software in power system control center and can improve the quality of the state estimation and the ability to detect and identify the bad data",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=154133,no,undetermined,0
11th International Conference on Distributed Computing Systems (Cat. No.91CH2996-7),Presents the title page of the 11th International Conference on Distributed Computing Systems proceedings record.,1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=148740,no,undetermined,0
The Stealth distributed scheduler,"The justification, design, and performance of the Stealth distributed scheduler is discussed. The goal of Stealth is to exploit the unused computing capacity of a workstation-based distributed system (WDS) without undermining the predictability in quality of service that a WDS provides to workstation owners. It is shown that the liberal approach taken by the Stealth distributed scheduler is a promising method of exploiting the vast quantity of unused computing capacity typically present in a WDS, while preserving predictability of service for workstation owners",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=148686,no,undetermined,0
Yield analysis for a large-area analog X-ray sensor array,"A small-scale CMOS-based radiographic X-ray image sensor array has been developed for nondestructive test and medical imaging. The authors present an analysis of tradeoffs between yield and area of a scaled up large-area X-ray sensor design. The X-ray sensor array can tolerate a low level of faults in the individual pixel cells and these faults can be corrected by imaging software. However, global signal line faults cause X-ray sensor failures. This work models X-ray sensor yield in a 12-layer analog CMOS process for three possible overall defect densities, 1.5 defects/cm, 1.0 defects/cm, and 0.75 defects/cm. It is shown that the X-ray sensor is more manufacturable than a charge coupled device (CCD) array of the same area",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=148140,no,undetermined,0
A new approach to control flow checking without program modification,"An approach to concurrent control flow checking that avoids performance and software compatibility problems while preserving a high error coverage and a low detection latency is proposed. The approach is called watchdog direct processing. Extensions of the basic method, taking into account the characteristics of complex processors, are also considered. The architecture of a watchdog processor based on the proposed method is described. Implementation results are reported for a watchdog designed for the Intel 80386sx microprocessor.<<ETX>>",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=146682,no,undetermined,0
An adaptive distributed system-level diagnosis algorithm and its implementation,"An adaptive distributed system-level diagnosis algorithm, called Adaptive DSD, suitable for local area networks, is presented. Adaptive DSD assumes a distributed network in which nodes perform tests of other nodes and determine them to be faulty or fault-free. Test results conform to the PMC model of system-level diagnosis. Tests are issued from each node adaptively and depend on the fault situation of the network. Adaptive DSD is proved correct in that each fault-free node reaches an accurate independent diagnosis of the fault conditions of the remaining nodes. Furthermore, no restriction is placed on the number of faulty nodes. The algorithm can diagnose any fault situation with any number of faulty nodes. Adaptive DSD is shown to be a considerable improvement over previous efforts including being optimal in terms of the total number of tests and messages required. The use of the algorithm in an actual distributed network environment and the experimentation within that environment are described.<<ETX>>",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=146665,no,undetermined,0
Performability evaluation of CSMA/CD and CSMA/DCR protocols under transient fault conditions,"The authors present the results of an evaluation for the CSMA/CD (carrier sense multiple access with collision detection) protocol and a deterministic protocol under workloads anticipated in an industrial environment. Stochastic activity networks are used as the model type, and simulation is used as the solution method. The results show that the preferred resolution scheme depends on the level of workload anticipated and whether transient faults occur. It is seen that stochastic activity networks permit the representation of a relatively complex fault model as well as normal protocol operations. It is shown that, when transient faults are considered, the deterministic collision resolution scheme performs better than the nondeterministic scheme",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=145420,no,undetermined,0
Depth-first search approach for fault-tolerant routing in hypercube multicomputers,"Using depth-first search, the authors develop and analyze the performance of a routing scheme for hypercube multicomputers in the presence of an arbitrary number of faulty components. They derive an exact expression for the probability of routing messages by way of optimal paths (of length equal to the Hamming distance between the corresponding pair of nodes) from the source node to an obstructed node. The obstructed node is defined as the first node encountered by the message that finds no optimal path to the destination node. It is noted that the probability of routing messages over an optimal path between any two nodes is a special case of the present results and can be obtained by replacing the obstructed node with the destination node. Numerical examples are given to illustrate the results, and they show that, in the presence of component failures, depth-first search routing can route a message to its destination by means of an optimal path with a very high probability",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=80143,no,undetermined,0
PAALS: precision accelerometer alignment and leveling system,"A description is given of the precision accelerometer alignment and leveling system (PAALS), a high-performance microcontroller-corrected accelerometer triad that outputs acceleration (ft/s<sup>2</sup>) via MIL-STD-1553BN communications. Combined tight-packaging, high-reliability, and high-performance requirements were the main design drivers. The key system elements are QA-700 Accelerometers, an Intel 80C 196 microcontroller, custom digital ASIC (application-specific integrated circuit), custom hybrid, and dual redundant MIL-STD-1533B communication components. For high performance, the QA-700 accelerometers are thermally modeled (third-order) for bias, scale factor, and alignment. The hybrid (three digitizers) is thermally modeled (third-order) for scale factor and offset, as well as input linearity. Extensive FDIRR (fault detection, indication, reporting, and recording) has been incorporated for supportability and maintainability",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=66229,no,undetermined,0
Control-flow based testing of Prolog programs,"Presents test selection criteria for Prolog programs which are based on control flow. The control flow in Prolog programs is not obvious because of the declarative nature of Prolog. The authors present two types of control flow graphs to represent the hidden control flow of Prolog programs explicitly. A fault model is developed for Prolog programs for guidance on test selection. Test selection criteria are given in terms of the coverage on these control flow graphs. Under the given fault model, the effectiveness of these criteria is analyzed in terms of fault detection capability of the test cases produced with these criteria",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=285853,no,undetermined,0
Using CSP to develop trustworthy hardware,"An overview of a method for formalizing critical system requirements and decomposing them into requirements of the system components and a minimal, possibly empty, set of synchronization requirements is presented. The trace model of communicating sequential processes (CSPs) is the basis for the formal method, and the EHDM verification system is the basis for mechanizing proofs. The results of the application of this method to the top-level implementation of an error-detecting character repeater are discussed. The critical requirements of the repeater are decomposed into the requirements of its components. Provided that the components meet their derived requirements, the repeater has been proven to meet its critical requirements.<<ETX>>",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=175409,no,undetermined,0
Supporting service development for intelligent networks,"The author examines the development support required to assure the quality of intelligent network service specifications. He then assesses the current state of enabling technologies, which include the technologies for formal specification, software reuse, rapid prototyping, performance evaluation, behavioral property verification, and feature interaction analysis and arbitration. It is concluded that much existing work in software specification is relevant to the support of intelligent network service specification. There are, however, two major classes of technical difficulties that must be addressed: limited experience with intelligent network services and immaturity of support technologies",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=46871,no,undetermined,0
Achieving dependability throughout the development process: a distributed software experiment,"Distributed software engineering techniques and methods for improving the specification and testing phases are considered. To examine these issues, an experiment was performed using the design diversity approach in the specification, design, implementation, and testing of distributed software. In the experiment, three diverse formal specifications were used to produce multiple independent implementations of a distributed communication protocol in Ada. The problems encountered in building complex concurrent processing systems in Ada were also studied. Many pitfalls were discovered in mapping the formal specifications into Ada implementations",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=44379,no,undetermined,0
Software diversity metrics quantifying dissimilarity in the input partition,"Considerations are presented which suggest measuring the diversity degree of fault-tolerant software systems by quantifying the dissimilarity in the input partition of alternative versions. Being closely related to the common failure behaviour of diverse programs and easily estimated by means of dynamic analysis, this method should provide suitable diversity metrics to support the testing and the assessment of ultra-high software reliability",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=42975,no,undetermined,0
Constraint-driven synthesis from architectures described in ELLA,"Demonstrates how the new synthesis tool LOCAM, provided by Praxis Electronic Design, gives system designers, for the first time, an automatic route to silicon from behavioural descriptions of their intended architectures. LOCAM avoids the need for time-consuming, error-prone logic design and raises the design reference level from boolean logic equations to abstract descriptions of architectures. The paper illustrates how the performance of a chosen architecture can be pre-determined both by the nature of the ELLA description and by user-defined constraints imposed on LOCAM",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=207977,no,undetermined,0
Systems analysts performance using CASE versus manual methods,"A laboratory experiment was conducted to determine if systems analysts who use computer-aided software engineering (CASE) tools perform work of higher quality than those who do not use CASE. A group of subjects used a CASE tool to prepare data-flow diagrams and data dictionary entries to represent a system. A second group used traditional pencil-and-paper methods. The outputs of the two groups were compared using three attributes of quality, namely correctness, completeness, and communicability. Correctness is the only attribute of quality in which a significant difference between the two groups of subjects was detected, the CASE tool users being more correct. This result is attributed to the nature of CASE tools. The lack of association between the use of a CASE tool and completeness and communicability has led to the conclusion that the CASE tool does not lead the user to represent the problem more accurately nor does it help the user to understand the problem better. Developing complete and easily understood data flow diagrams and data dictionaries are perhaps quality attributes of the systems analysts themselves, not of the tool they use",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=205296,no,undetermined,0
Application of quantitative feedback theory (QFT) to flight control problems,"QFT has been applied to numerous analog and digital flight control design problems. These include linear time invariant (LTI) fixed compensation <e1>m</e1> output, <e1>n</e1> input (<e1>n</e1>&ges;<e1>m </e1>) over a wide variety of flight conditions (FCs) with effector failures. Quantitative performance specifications are satisfied despite the failures, with no failure detection and identification. Rigorous nonlinear QFT design theory has been applied to large Î± problems for which LTI modeling is incorrect. QFT has been successfully applied to an X29 problem with varying numbers of right half plane poles and zeros (due to different FCs) close together",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=203455,no,undetermined,0
Propagation modelling for TV broadcasting at microwave frequencies,"Summary form only given. A receive system, mounted in a survey vehicle with a telescopic mast, has been used to measure field strength and picture quality throughout the Winchester area. Particular studies have been made of: comparison of the two modulation schemes, FM and VSB-FM; bit-error-ratios for both modulation schemes; cross-polar discrimination; comparison between winter and summer; multi-path effects, using the swept-frequency source; and propagation just beyond the horizon, using four remote data-loggers. These measurements have been compared with the predictions of the IBA transmitter coverage software, which uses a 50 m terrain data base. A clutter data base has been added, with the same horizontal resolution, and various techniques have been assessed for using this information in the prediction software",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=190848,no,undetermined,0
IEE Colloquium on `Control and Optimisation Techniques for the Water Industry: II' (Digest No.135),The following topics were dealt with: electric tariff optimisation for water treatment works; pipe network design; water distribution networks joint state and parameter estimation under bounded measurement error; knowledge-based techniques in pump scheduling; optimising control of water supply/distribution networks; set point control of water treatment; quality control; and Sample Collection Manager software,1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=190674,no,undetermined,0
Using symbolic execution to aid automatic test data generation,"It is shown how symbolic execution is used to solve the internal variable problem in the Godzilla test data generator. In constraint-based testing, which is used by the system, the internal variable problem appears when the constraints that specify the test cases contain internal variables. The necessary background is developed by describing the constraint systems used by Godzilla and by discussing symbolic execution in general terms. The application of symbolic execution to the internal variable problem is presented. The discussion focuses on the software used, including algorithmic details. Following this, a practical example of using this system to detect a fault in a small program is presented.<<ETX>>",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=175398,no,undetermined,0
Software fault content and reliability estimations for telecommunication systems,"The problem of software fault content and reliability estimations is considered. Estimations that can be used to improve the control of a software projects are emphasized. A model of how to estimate the number of software faults is presented, which takes into account both the development process and the developed software product. A model of how to predict, before the testing has started, the occurrences of faults during the testing stage is also presented. These models, together with software reliability growth models, have been evaluated on a number of software projects",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=46880,no,undetermined,0
Testing of the rapidly developed prototypes,"A conflict occurs between rapid prototyping and the adequate testing of these prototypes. The author shows how much of the software testing process could be automated to produce faster and better quality prototypes. The application for which the automated testing tools were developed is a prototyped negotiation support system (NSS). The system's primary function is to assist in the negotiation of the networking services offered by Bellcore Client Companies (BCCs) to their customers. The Negotiation is preceded by an initialization process (called an NSS Maintenance), which creates a customer-specific Data Base (DB). The System for Performance Testing of NSS (SPT) was developed to automate NSS testing. SPT's major function is to simulate the NSS execution of several different classes of negotiation scenarios. The SPT system has been in use for almost six months, and the preliminary estimates show that testing productivity has significantly improved. In the remainder of this paper, the methodology used to build SPT as a general automated testing tool is described",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=144047,no,undetermined,0
A software approach to fault detection on programmable systolic arrays,"Current approaches to fault detection on processor arrays can be classified as hardware-based or algorithmic. The former are particularly rigid while the latter are restricted in their application. The paper presents two versions of general-purpose software fault detection for programmable systolic arrays. The methods automatically transform cell programs to provide fault detection, allowing the user more flexibility than hardware methods and more generality than algorithmic methods. Additionally, since they are based in software, the fault detection can easily be eliminated when ether methods are more appropriate",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=143596,no,undetermined,0
A quantum modification to the Jelinski-Moranda software reliability model,"Recent studies show that the Jelinski-Moranda model always gives an optimistic prediction for software reliability. The major reason for this shortcoming is that each fault occurring in the model is assumed to have exactly the same contribution to the failure rate. In this study the authors assume that different faults may have different contributions to the failure rate. In addition, the structure of software is also incorporated in their approach",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=140722,no,undetermined,0
Extending software complexity metrics to concurrent programs,"A metric for concurrent software is proposed based on an abstract model (Petri nets) as an extension of T.J. McCabe's (1976) cyclomatic number. As such, its focus is on the complexity of control flow. This metric is applied to the assessment of Ada programs, and an automatic method for its direct computation based on the inspection of Ada code is provided. It is pointed out, however, that wider experimentation is needed in order to better assess its effectiveness",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=139397,no,undetermined,0
Measuring software size by distinct lines,"The relationship between DLC (distinct line count) and NCSL (noncomment source lines) is studied on a number of programs, and it is found that, as a simple rule of thumb, the NCSL count can be estimated by twice the DLC. A more accurate model is derived by predicting NCSL from DLC and the number of lines that occur exactly once. It is also shown that, for unrelated programs, the proportion of common lines is very small; hence, DLC is approximately additive. It is concluded that, overall, the DLC is a very attractive measure of size that has two basic advantages over NCSL: it is an intuitively more appealing measure of effort than NCSL, and the problems of measuring size of subsequent releases disappear when using DLC",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=139394,no,undetermined,0
An integrated expert system framework for software quality assurance,"A software quality assurance framework using knowledge-based engineering technology is described. The knowledge-engineering technology uses an object-oriented database to store the knowledge (the software quality information), and rules and meta-rules are its inferential knowledge. A dependency-based truth maintenance system based on hypothetical reasoning is used for design evaluation of the software quality. This framework can provide knowledge-based assistance for quality assurance throughout the entire software development cycle. To ensure high quality software and achieve cost-effective software development and maintenance, software metrics are used during the entire software development cycle to measure and predict the quality of software products. Various metrics for software attributes for all phases of the software development cycle will be collected and stored in the object-oriented database. The integration of the knowledge base with the software quality framework provides a wide range of support to the development of large-scale software systems",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=139345,no,undetermined,0
Monitoring and control of distributed systems,"Describes the Distributed System Manager (DSM) being developed on Sun workstations connected by an Ethernet at the University of California. Effective utilization of distributed systems requires many management facilities such as resource management, performance analysis, behavior analysis, fault management, and security management. Each of the management facilities becomes a client to the DSM and registers to the DSM a request of the form âŒ©assertion, actionâŒ?if it wants the action to be taken when the assertion is detected. The DSM monitors the distributed system to collect information using sampling probes and tracing probes and stores the collected information in a history database. The DSM also detects the moment when the assertion in a registered request becomes satisfied and then takes the corresponding action. An assertion can be a system property holding or an event occurring at a time point or during a time interval. An action can be answering clients' queries to system status stored in the history database, notifying the detection of assertions, or exercising direct control over the system. The overall structure of the managed system and the DSM is given. Data modeling of the managed system, the mechanisms to collect information, and the representation scheme of the information in the history database are described. The assertion specification language based on the classical temporal logic is presented",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=138733,no,undetermined,0
A study on the effect of reengineering upon software maintainability,"The effect of reengineering on software maintainability was investigated in a laboratory experiment conducted within the METKIT research project of the European ESPRIT program for the study and promotion of the use of metrics in software engineering. The experiment was conducted as a case study in measuring software complexity and maintainability. However, the results also serve to assess the benefits of reengineering old programs. Maintainability is defined as the effort required to perform maintenance tasks, the impact domain of the maintenance actions, and the error rate caused by those actions. Complexity is defined as a combination of code, data, data flow, structure, and control flow metrics. The data collected demonstrate that reengineering can decrease complexity and increase maintainability, but that restructuring has only a minor effect on maintainability",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=131330,no,undetermined,0
A framework for software quality measurement,"The authors propose a quality model as a framework which should facilitate the evolution of theoretically based systems of measurement for the processes and products of the software development lifecycle. They start by defining measurement and quality measurement, and they present a model for software quality measurement. This model is used to explore several fundamental concepts and as a framework for the development of a set of software quality metrics. The authors discuss how the model can be developed through experimental work and by developing current and potential applications of the model in its immature state. These include process optimization, quality specification, end product quality control, intermediate product quality control, and the prediction of software quality. The authors conclude with a discussion of current issues and future trends in this area. Topics covered include education, the evolution of standards for data definition, and tool support",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=46876,no,undetermined,0
A modeling approach to software cost estimation,"The author describes a novel software estimation modeling process as well as the important productivity factors and the productivity measurement metrics used in the 5ESS project, one of the largest telecommunication projects at AT&T Bell Laboratories. The 5ESS switch is a modern digital electronic switching system with a distributed hardware and software architecture. The model estimation approach has greatly improved the quality of estimates. The study of productivity factors has resulted in some significant productivity and quality improvement processes in the 5ESS development community",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=46886,no,undetermined,0
"Failure management in spatio-temporal redundant, integrated navigation and flight control reference-systems","Failure management techniques for highly reliable, fault-tolerant inertial reference systems are described. Cost, weight, and power considerations imply the use of a minimum number of inertial sensors in a skewed geometry. Fault-tolerant hardware performance is obtained by spatially separated channels with a preceptron-type information flow. Data diversity in temporally separated software channels yields software fault tolerance. Advanced vector space procedures for fault detection, localization, masking, and dynamic system reconfiguration permit safe and quick response, yielding minimal data and recovery latency",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=66196,no,undetermined,0
A case study of Ethernet anomalies in a distributed computing environment,"Fault detection and diagnosis depend critically on good fault definitions, but the dynamic, noisy, and nonstationary character of networks makes it hard to define what a fault is in a network environment. The authors take the position that a fault or failure is a violation of expectations. In accordance with empirically based expectations, operating behaviors of networks (and other devices) can be classified as being either normal or anomalous. Because network failures most frequently manifest themselves as performance degradations or deviations from expected behavior, periods of anomalous performance can be attributed to causes assignable as network faults. The half-year case study presented used a system in which observations of distributed-computing network behavior were automatically and systematically classified as normal or anomalous. Anomalous behaviors were traced to faulty conditions. In a preliminary effort to understand and catalog how networks behave under various conditions, two cases of anomalous behavior are analyzed in detail. Examples are taken from the distributed file-system network at Carnegie Mellon University",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=58721,no,undetermined,0
Flight evaluation of the Integrated Inertial Sensor Assembly (IISA) on a helicopter,"After successful flight test evaluation of the integrated inertial sensor assembly (IISA) on an F-15 aircraft, the system was installed onboard a Blackhawk helicopter at Wilmington Airport in Delaware. An overview of the flight test evaluation conducted on the Blackhawk is presented. It is concluded that all program objectives were successfully demonstrated and proved that the IISA system can be used for helicopter applications. The IISA Helicopter Demonstration Program consisted of two phases. During the first phase, vibration data were obtained to quantify the helicopter's vibration environment. Based on these data, IISA's digital flight control filters were modified for flight control signal quality evaluations during the second phase. During the flight tests, no evidence of body bending modes or local structural vibration degrading IISA's performance was found. The flight test data indicate that IISA's flight control signals have the necessary dynamic range to satisfy future helicopter fly-by-wire flight control systems. IISA's redundancy management software worked flawlessly during the insertion of hardover failures. From the standpoint of flight safety IISA exhibits full QUAD redundancy",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=66195,no,undetermined,0
Test program for Honeywell/DND helicopter integrated navigation system (HINS),"The advanced development model (ADM) for the helicopter integrated navigation system (HINS) is built for the Canadian Department of National Defence and tested at the Honeywell Advanced Technology Centre (ATC). The system blends the complementary strengths of its components sensors, resulting in fast alignment and optimum navigation accuracy. A failure detection isolation and reconfiguration functionality monitors sensor health, identifies failed components, and automatically reconfigures the system to optimally integrate the remaining components, thus providing a graceful degradation of performance in the event of a sensor failure. Both the test program approach and the test results are discussed. The testing has given the HINS team a high degree of confidence in the navigation system design and in the software that implements the sensor blending algorithms in the real-time system",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=66179,no,undetermined,0
An algorithm for diagnostics with signature analyzer,"An algorithm for fault diagnosis that makes use of the information from a faulty signature is presented. The idea is to search the likely error locations before the tests are performed. The method reduces the number of tests required to diagnose the errors with the probability of aliasing. Such probability is always smaller than that of error detection in signature analysis. When matching tests are difficult or impossible, the method provides an estimate of where errors that caused the incorrect signature might have occurred. Also, the case of `don't cares' at the input sequence of signature analysis is discussed. The algorithm can be readily implemented in software with the faulty signature as the only input variable. The proposed fault diagnostic scheme has an advantage over exhaustive testing in that it leads to fault isolation in a homing-in manner",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=65963,no,undetermined,0
Practice of quality modeling and measurement on software life-cycle,"The authors introduce quality metrics into the quantitative software quality estimation technique, embracing the quality estimate of design, as well as of the source code, in studying a quality quantification support system. They outline a quality quantification technique for this system, describe examples of both its application to actual projects and its evaluation, and consider its relationship conventional techniques for estimate indexing of T.J. McCabe (IEEE Trans. Softw. Eng., vol.SE-2, no.4, 1976) and M.H. Halstead (Elements of Software Science, North Holland, NY, 1977)",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=63607,no,undetermined,0
Microprocessor-based relay for protecting power transformers,"The authors describe the design, implementation and testing of a microprocessor-based relay for protecting single-phase and three-phase transformers. The relay implements algorithms that use nonlinear models of transformers to detect winding faults. The algorithms, hardware and software are also briefly described. The performance of the relay is checked in the laboratory. The testing procedure and some test results are also presented",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=62480,no,undetermined,0
Software-reliability engineering: technology for the 1990s,"It argued that software engineering is about to reach a new stage, the reliability stage, that stresses customers' operational needs and that software-reliability engineering will make this stage possible. Software-reliability engineering is defined as the applied science of predicting, measuring, and managing the reliability of software-based systems to maximize customer satisfaction. The basic concepts of software reliability-engineering and the reasons why it is important are examined. The application of software-reliability engineering at each stage of the life cycle is described.<<ETX>>",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=60588,no,undetermined,0
Algorithm-based fault detection for signal processing applications,"The increasing demands for high-performance signal processing along with the availability of inexpensive high-performance processors have results in numerous proposals for special-purpose array processors for signal processing applications. A functional-level concurrent error-detection scheme is presented for such VLSI signal processing architectures as those proposed for the FFT and <e1>QR</e1> factorization. Some basic properties involved in such computations are used to check the correctness of the computed output values. This fault-detection scheme is shown to be applicable to a class of problems rather than a particular problem, unlike the earlier algorithm-based error-detection techniques. The effects of roundoff/truncation errors due to finite-precision arithmetic are evaluated. It is shown that the error coverage is high with large word sizes",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=59860,no,undetermined,0
Experimental evaluation of the fault tolerance of an atomic multicast system,"The authors present a study of the validation of a dependable local area network providing multipoint communication services based on an atomic multicast protocol. This protocol is implemented in specialized communication servers, that exhibit the fail-silent property, i.e. a kind of halt-on-failure behavior enforced by self-checking hardware. The tests that have been carried out utilize physical fault injection and have two objectives: (1) to estimate the coverage of the self-checking mechanisms of the communication servers, and (2) to test the properties that characterize the service provided by the atomic multicast protocol in the presence of faults. The testbed that has been developed to carry out the fault-injection experiments is described, and the major results are presented and analyzed. It is concluded that the fault-injection test sequence has evidenced the limited performance of the self-checking mechanisms implemented on the tested NAC (network attachment controller) and justified (especially for the main board) the need for the improved self-checking mechanisms implemented in an enhanced NAC architecture using duplicated circuitry",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=58723,no,undetermined,0
Bayes predictive analysis of a fundamental software reliability model,"The concepts of Bayes prediction analysis are used to obtain predictive distributions of the next time to failure of software when its past failure behavior is known. The technique is applied to the Jelinski-Moranda software-reliability model, which in turn can show an improved predictive performance for some data sets even when compared with some more sophisticated software-reliability models. A Bayes software-reliability model is presented which can be applied to obtain the next time to failure PDF (probability distribution function) and CDF (cumulative distribution function) for all testing protocols. The number of initial faults and the per-fault failure rate are assumed to be <e1>s </e1>-independent and Poisson and gamma distributed respectively. For certain data sets, the technique yields better predictions than some alternative methods if the frequential likelihood and U-plot criteria are adopted",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=55879,no,undetermined,0
Modeling of correlated failures and community error recovery in multiversion software,"Three aspects of the modeling of multiversion software are considered. First, the beta-binomial distribution is proposed for modeling correlated failures in multiversion software. Second, a combinatorial model for predicting the reliability of a multiversion software configuration is presented. This model can take as inputs failure distributions either from measurements or from a selected distribution (e.g. beta-binomial). Various recovery methods can be incorporated in this model. Third, the effectiveness of the community error recovery method based on checkpointing is investigated. This method appears to be effective only when the failure behaviors of program versions are lightly correlated. Two different types of checkpoint failure are also considered: an omission failure where the correct output is recognized at a checkpoint but the checkpoint fails to correct the wrong outputs and a destructive failure where the good versions get corrupted at a checkpoint",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=48942,no,undetermined,0
Performance analysis of real-time software supporting fault-tolerant operation,"Analyzing the performance of real-time control systems featuring mechanisms for online recovery from software faults is discussed. The application is assumed to consist of a number of interacting cyclic processes. The underlying hardware is assumed to be a multiprocessor, possibly with a separate control processor. The software structure is assumed to use design diversity along with forward and/or backward recovery. A detailed but efficiently solvable model for predicting various performance and reliability characteristics is developed. One of the key ideas used in modeling is hierarchical decomposition, which enables computation of level-oriented performance parameters in an efficient manner. The model is general, and adaptable for a number of useful special cases",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=55692,no,undetermined,0
Measures of testability as a basis for quality assurance,"Program testing is the most used technique for analytical quality assurance. A lot of time and effort is devoted to this task during the software lifecycle, and it would be useful to have a means for estimating this testing effort. Such estimates could be used, on one hand, for guiding construction and, on the other, to help organise the development process and testing. Thus the effort needed for testing is an important quality attribute of a program; they call it its testability. They argue that a relevant program characteristic contributing to testability is the number of test cases needed for satisfying a given test strategy. They show how this can be measured for glass (white) box testing strategies based on control flow. In this case, one can use structural measures defined on control flowgraphs which can be derived from the source code. In doing so, two well researched areas of software engineering testing strategies and structural metrication are brought together.<<ETX>>",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=54392,no,undetermined,0
The use of self checks and voting in software error detection: an empirical study,"The results of an empirical study of software error detection using self checks and <e1>N</e1>-version voting are presented. Working independently, each of 24 programmers first prepared a set of self checks using just the requirements specification of an aerospace application, and then each added self checks to an existing implementation of that specification. The modified programs were executed to measure the error-detection performance of the checks and to compare this with error detection using simple voting among multiple versions. The analysis of the checks revealed that there are great differences in the ability of individual programmers to design effective checks. It was found that some checks that might have been effective failed to detect an error because they were badly placed, and there were numerous instances of checks signaling nonexistent errors. In general, specification-based checks alone were not as effective as specification-based checks combined with code-based checks. Self checks made it possible to identify faults that had not been detected previously by voting 28 versions of the program over a million randomly generated inputs. This appeared to result from the fact that the self checks could examine the internal state of the executing program, whereas voting examines only final results of computations. If internal states had to be identical in <e1>N</e1>-version voting systems, then there would be no reason to write multiple versions",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=54295,no,undetermined,0
Realizing the V80 and its system support functions,"An overview is given of the architecture of an overall design considerations for the 11-unit, 32-b V80 microprocessor, which includes two 1-kB cache memories and a branch prediction mechanism that is a new feature for microprocessors. The V80's pipeline processing and system support functions for multiprocessor and high-reliability systems are discussed. Using V80 support functions, multiprocessor and high-reliability systems were realized without any performance drop. Cache memories and a branch prediction mechanism were used to improve pipeline processing. Various hardware facilities replaced the usual microprogram to ensure high performance.<<ETX>>",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=52947,no,undetermined,0
Empirically guided software development using metric-based classification trees,"The identification of high-risk components early in the life cycle is addressed. A solution that casts this as a classification problem is examined. The proposed approach derives models of problematic components, based on their measurable attributes and those of their development processes. The models provide a basis for forecasting which components are likely to share the same high-risk properties, such as being error-prone or having a high development cost. Developers can use these classification techniques to localize the troublesome 20% of the system. The method for generating the models, called automatic generation of metric-based classification trees, uses metrics from previous releases or projects to identify components that are historically high-risk.<<ETX>>",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=50773,no,undetermined,0
Predicting source-code complexity at the design stage,"It is shown how metrics can be used to gauge the quality of source code by evaluating its design specifications before coding, thus shortening the development life cycle. The predictive abilities of several types of metrics, all quantitative, are evaluated, and statistics that can be used to adapt them to a particular project are provided. The study data consist of programs written by students, but, because the programs are large, the professor did not provide the specifications, and the test data were not provided, it is believed that the programs are sufficiently realistic for the approach presented to be generally valid.<<ETX>>",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=50772,no,undetermined,0
Superfluid helium tanker instrumentation,"An instrumentation system for a 1992 Space Shuttle flight demonstration of a superfluid helium (SFHe) tanker and transfer technology is presented. The system provides measurement of helium temperatures, pressures, flow rates, and mass, and it detects the presence of liquid or vapor. The instrumentation system described consists of analog and digital portions which comprise a fault tolerant, compact, and relatively lightweight space-quality electronics system. The data processing hardware and software are ground-commandable, perform measurements asynchronously, and format telemetry for transmission to the ground. A novel heat pulse mass gaging technique is described and a new liquid/vapor sensor is presented. Flowmeters for SFHe are discussed, and a SFHe fountain-effect pump is described. Results of tests to date are presented",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=50464,no,undetermined,0
A microprocessor-based piezoelectric quartz microbalance system for compound-specific detection,"The authors describe the implementation of a portable, microprocessor-controlled, piezoelectric quartz microbalance (PQM) system suitable for remote and autonomous monitoring of air quality around hazardous waste sites or chemical spills. The system has been designed to detect nanogram-level mass change or sorbed compounds sensitive to the specific coating materials used. Miniaturized modular hybrid crystal clock oscillators were used to construct a four-sensor array. The quartz crystals of the four modules were exposed and coated with chemically specific compounds so that they would act as sorption detectors. These sensor elements were enclosed in a small flow-through gas chamber along with a fifth module which was left sealed and was used as a reference oscillator. The frequency of each sensor was separately mixed with that of the reference oscillator and multiplexed into the microprocessor. The difference frequencies were measured by the microprocessor through a programmable timer/counter chip. The temperature within the gas chamber was maintained constant by means of a PID (proportional-integral-derivative) controller implemented in software. A Peltier device was used as a heating/cooling element of the gas chamber",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=50462,no,undetermined,0
Quality-time tradeoffs in simulated annealing for VLSI placement,A model is presented to characterize the relationship between the best solution (incumbent) found by an iterative algorithm (simulated annealing) and the time spent in achieving it. The target application has been chosen to be the placement of cells on a VLSI chip. The model is used to achieve a tradeoff between solution quality and time spent. This gives an idea of the time at which the iterative algorithm should be terminated when the marginal gain in solution quality is smaller than the marginal increase in cost (or time) spent. Nonlinear regression analysis is used to predict the decrease in time with respect to improvement in solution quality. Experimental results on benchmark circuits are presented to show the errors of run-time prediction compared to a static prediction,1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=170217,no,undetermined,0
"Performance, effectiveness, and reliability issues in software testing",The author has identified two problems that need to be overcome in order that some of the powerful testing techniques be used in practice: performance and effectiveness. The testing methods referred to are dataflow and mutation testing,1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=170248,no,undetermined,0
Fault-tolerant parallel matrix multiplication with one iteration fault detection latency,"A new algorithm, the ID algorithm, is presented which minimizes the fault-detection latency. In the ID algorithm, a fault is detected as soon as the fault occurs instead of at problem termination. For <e1>n </e1><sup>2</sup> processors, the fault-latency time of the ID algorithm is 1/<e1>n</e1> of that of the checksum algorithm with a run-time penalty of <e1>O</e1>(<e1>n</e1> log<sub>2</sub> <e1>n</e1>) in an <e1>n </e1>Ã—<e1>n</e1> matrix operation. This algorithm has better performance in terms of error coverage and expected run time in large-scale matrix multiplications such as signal and image processing, weather prediction, and finite-element analysis",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=170258,no,undetermined,0
Projecting software defects from analyzing Ada designs,"Models for projecting software defects from analyses of Ada designs are described. The research is motivated by the need for technology to analyze designs for their likely effect on software quality. The models predict defect density based on product and process characteristics. Product characteristics are extracted from a static analysis of Ada subsystems, focusing on context coupling, visibility, and the import-export of declarations. Process characteristics provide for effects of reuse level and extent of changes. Multivariate regression analyses were conducted with empirical data from industry/government-developed projects: 16 Ada subsystems totaling 149000 source lines of code. The resulting models explain 63-74% of the variation in defect density of the subsystems. Context coupling emerged as a consistently significant variable in the models",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=177368,no,undetermined,0
Assessment of reverse engineering tools: A MECCA approach,"It is a general requirement in the software engineering field that quality and productivity should be taken into consideration. Software development tools can have significant impacts in assuring the quality of a software system and productivity of the development process. In a rapidly evolving engineering field such as software engineering, it is therefore important to select appropriate development tools. This paper discusses the reverse engineering tool as a quality software development tool. An introduction to the reverse engineering tools, their impact on the development environment and their functionality in general are cited before a MECCA-model for assessing specific tools is proposed. The main objective is to introduce the reader to how a reverse engineering tool can be assessed in terms of quality and productivity",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=205845,no,undetermined,0
Evaluating and selecting testing tools,"The authors consider how a data collection system that leads a company to successful tool selections must be carefully devised. They discuss the analysis of user needs and the establishment of tool selection criteria. They suggest a number of standards, articles and surveys which will help in the search for tools and provide a procedure for rating them",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=205836,no,undetermined,0
Automatic visual inspection of wood surfaces,"A prototype software system for visual inspection of wood defects has been developed. The system uses a hierarchical vector connected components (HVCC) segmentation which can be described as a multistage region-growing type of segmentation. The HVCC version used in experiments uses RGB color vector differences and Euclidean metrics. The HVCC segmentation seems to be very suitable for wood surface image segmentation. Geometrical, color and structural features are used in classification. Possible defects are classified using combined tree-kNN classifier and pure kNN-classifier. The system has been tested using plywood boards. Preliminary classification accuracy is 85-90% depending on the type of defect",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=201578,no,undetermined,0
Reliability growth for typed defects [software development],"The authors present a reliability growth model for defects that have been categorized into defect types associated with specific stages in the software development process. Modeling the reliability growth of defects for each type separately allows identification of problems in the development process which may otherwise be masked when defects of all types are modeled together. The authors incorporate dependencies, from a failure detection point of view, between defects of two different types into a model for reliability growth. They use typed defect data from three different software projects to validate the model. They find that the defect detection rates are not equal for defects of different types within the project. Since each defect type can be associated with a software development stage, comparing the estimated defect detection rates and the dependency between types provides a basis for feedback on the process",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=187845,no,undetermined,0
Submarine pipeline automatic-repair system-an approach to the mission-availability assessment,"The authors present the methodology followed to evaluate the mission availability of a remotely controlled submarine system designed to repair damaged pipelines in deep waters. The goal of the analysis has been to identify the technical and operational solutions with respect mainly to the logistic support system design, to guarantee operational availability and thereby minimize delays. To this purpose, the FMEA and the fault tree analysis technique for reliability assessment of each subsystem have been supplemented by an event tree analysis performed through a computer software package (ADMIRA). All sequences of failure and/or success for the whole system have been analyzed during all the operation steps. This analysis has allowed an estimate of the delay for each possible system state. The analysis has shown that the event tree approach is an adequate tool to handle situations where failure occurrences and dependencies among different events have to be taken into account. The analysis has led to improved design, operator skills, and training as well as support system definition",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=187810,no,undetermined,0
Assessing the risk of software failure in a funds transfer application,"Software failure risk, the expected loss resulting from software faults, is a useful measure of software quality that can guide development, maintenance, testing, and operational use of software. The author describes the measurement of software failure risk in a funds transfer system and demonstrates how the external environment and the structure of the software minimize failure risk even when the potential financial exposure is extremely large",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=183359,no,undetermined,0
An entropy-based measure of software complexity,"It is proposed that the complexity of a program is inversely proportional to the average information content of its operators. An empirical probability distribution of the operators occurring in a program is constructed, and the classical entropy calculation is applied. The performance of the resulting metric is assessed in the analysis of two commercial applications totaling well over 130000 lines of code. The results indicate that the new metric does a good job of associating modules with their error spans (averaging number of tokens between error occurrences)",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=177371,no,undetermined,0
Improving the reliability of function point measurement: an empirical study,"One measure of the size and complexity of information systems that is growing in acceptance and adoption is function points, a user-oriented, nonsource line of code metric of the systems development product. Previous research has documented the degree of reliability of function points as a metric. This research extends that work by (a) identifying the major sources of variation through a survey of current practice, and (b) estimating the magnitude of the effect of these sources of variation using detailed case study data from commercial systems. The results of this research show that a relatively small number of factors has the greatest potential for affecting reliability, and recommendations are made for using these results to improve the reliability of function point counting in organizations",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=177370,no,undetermined,0
Predictive modeling techniques of software quality from software measures,"The objective in the construction of models of software quality is to use measures that may be obtained relatively early in the software development life cycle to provide reasonable initial estimates of the quality of an evolving software system. Measures of software quality and software complexity to be used in this modeling process exhibit systematic departures of the normality assumptions of regression modeling. Two new estimation procedures are introduced, and their performances in the modeling of software quality from software complexity in terms of the predictive quality and the quality of fit are compared with those of the more traditional least squares and least absolute value estimation techniques. The two new estimation techniques did produce regression models with better quality of fit and predictive quality when applied to data obtained from two software development projects",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=177367,no,undetermined,0
Experimental evaluation of certification trails using abstract data type validation,"The authors report on an attempt to assess the performance of algorithms utilizing certification trails on abstract data types. Specifically, they have applied this method to the following problems: heapsort, Huffman tree, shortest path, and skyline. Previous results used certification trails specific to a particular problem and implementation. The approach allows certification trails to be localized to data structure modules making the use of this technique transparent to the user of such modules",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=217550,no,undetermined,0
Traffic routing for multicomputer networks with virtual cut-through capability,"The problem of selecting routes for interprocess communication in a network with virtual cut-through capability while balancing the network load and minimizing the number of times that a message gets buffered is addressed. The approach taken is to formulate the route selection problem as a minimization problem, with a link cost function that depends upon the traffic through the link. The form of this cost function is derived based on the probability of establishing a virtual cut-through route. It is shown that this route selection problem is NP-hard, and an approximate algorithm is developed which tries to incrementally reduce the cost by rerouting traffic. The performance of this algorithm is evaluated for the hypercube and the C-wrapped hexagonal mesh, example networks for which virtual cut-through switching support has been developed",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=166603,no,undetermined,0
Efficient diagnosis of multiprocessor systems under probabilistic models,"The problem of fault diagnosis in multiprocessor systems is considered under a probabilistic fault model. The focus is on minimizing the number of tests that must be conducted to correctly diagnose the state of every processor in the system with high probability. A diagnosis algorithm that can correctly diagnose these states with probability approaching one in a class of systems performing slightly greater than a linear number of tests is presented. A nearly matching lower bound on the number of tests required to achieve correct diagnosis in arbitrary systems is proved. Lower and upper bounds on the number of tests required for regular systems are presented. A class of regular systems which includes hypercubes is shown to be correctly diagnosable with high probability. In all cases, the number of tests required under this probabilistic model is shown to be significantly less than under a bounded-size fault set model. These results represent a very great improvement in the performance of system-level diagnosis techniques",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=165394,no,undetermined,0
Producing a very high quality still image from HDMAC transmission,"An experimental system for grabbing high definition television pictures from HDMAC transmission is presented. The hardware part of the system is based on a video sequencer which is used for digitizing, storing, and displaying the HDMAC signal. The HDMAC decoder itself has been implemented using computer software. The system is able to receive and display approximately a 3-second-long sequence of HDTV pictures. The software part of the system employs image enhancement algorithms to produce high quality still images. The interlaced transmission format is converted to progressive and both temporal and spatial noise filtering is applied to improve the image quality in the presence of transmission noise. The algorithms depend on simple motion detection and nonlinear filtering. Examples of the received images and of the performance of the image enhancement algorithms are shown",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=160478,no,undetermined,0
"Predicting software errors, during development, using nonlinear regression models: a comparative study","Accurately predicting the number of faults in program modules is a major problem in quality control of a large software system. The authors' technique is to fit a nonlinear regression model to the number of faults in a program module (dependent variable) in terms of appropriate software metrics. This model is to be used at the beginning of the test phase of software development. The aim is not to build a definitive model, but to investigate and evaluate the performance of four estimation techniques used to determine the model parameters. Two empirical examples are presented. Results from average relative error (ARE) values suggest that relative least squares (RLS) and minimum relative error (MRE) procedures possess good properties from the standpoint of predictive capability. Moreover, sufficient conditions are given to ensure that these estimation procedures demonstrate strong consistency in parameter estimation for nonlinear models. Whenever the data are approximately normally distributed, least squares may possess superior predictive quality. However. in most practical applications there are important departures from normality: thus RLS and MRE appear to be more robust",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=159804,no,undetermined,0
Compound-Poisson software reliability model,"The probability density estimation of the number of software failures in the event of clustering or clumping of the software failures is considered. A discrete compound Poisson (CP) prediction model is proposed for the random variable <e1>X</e1><sub>rem</sub>, which is the remaining number of software failures. The compounding distributions, which are assumed to govern the failure sizes at Poisson arrivals, are respectively taken to be geometric when failures are forgetful and logarithmic-series when failures are contagious. The expected value (Î¼) of <e1>X</e1><sub>rem</sub> is calculated as a function of the time-dependent Poisson and compounding distribution based on the failures experienced. Also, the variance/mean parameter for the remaining number of failures, <e1>q</e1><sub>rem</sub>, is best estimated by <e1>q</e1><sub>past</sub> from the failures already experienced. Then, one obtains the PDF of the remaining number of failures estimated by CP(Î¼,<e1>q</e1>). CP is found to be superior to Poisson where clumping of failures exists. Its predictive validity is comparable to the Musa-Okumoto log-Poisson model in certain cases",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=148480,no,undetermined,0
Measuring software reliability,"The component concepts of the term 'reliability' are examined to clarify why its measurement is difficult. Two approaches to measuring the reliability of finished code are described. The first, a developer-based view, focuses on software faults; if the developer has grounds for believing that the system is relatively fault-free, then the system is assumed to be reliable. The second, a user-based view more in keeping with the standard IEEE/ANSI definition, emphasizes the functions of the system and how often they fail. The advantages of the failure-based approach are discussed, and various techniques are described. The issues that require further exploration and definition before reliability measurement becomes a straightforward for software as for hardware are identified.<<ETX>>",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=144538,no,undetermined,0
Virtual checkpoints: architecture and performance,"Checkpoint and rollback recovery is a technique that allows a system to tolerate a failure by periodically saving the entire state and, if an error is detected, rolling back to the prior checkpoint. A technique that embeds the support for checkpoint and rollback recovery directly into the virtual memory translation hardware is presented. The scheme is general enough to be implemented on various scopes of data such as a portion of an address space, a single address space, or multiple address spaces. The technique can provide a high-performance scheme for implementing checkpoint and rollback recovery. The performance. of the scheme is analyzed using a trace-driven simulation. The overhead is a function of the interval between checkpoints and becomes very small for intervals greater than 10<sup>6</sup> references. However, the scheme is shown to be feasible for intervals as small as 1000 references under certain conditions",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=142677,no,undetermined,0
Evaluation and comparison of triple and quadruple flight control architectures,"Fault tolerances of a triple flight control architecture are defined and compared to that of a quadrupole type, and the impact of worst case failures on the transient response of an aircraft is investigated. Insights in computing fault coverage are discussed. Two coverage models have been used to compute the probability of loss of control (PLOC) and the probability of mission abort (PMA) for candidate architectures. Results indicate that both triple and quadruple architectures can meet the fault tolerance requirements with an acceptable level of transients upon first and second failures. Triple architectures will require a higher level of fault detection, isolation, and accommodation coverage than quadruple architectures and produce substantially larger transients upon second failure.<<ETX>>",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=141898,no,undetermined,0
Quorum-oriented multicast protocols for data replication,"A family of communication protocols, called quorum multicasts, is presented that provides efficient communication services for widely replicated data. Quorum multicasts are similar to ordinary multicasts, which deliver a message to a set of destinations. The protocols extend this model by allowing delivery to a subset of the destinations, selected according to distance or expected data currency. These protocols provide well-defined failure semantics, and can distinguish between communication failure and replica failure with high probability. The authors have evaluated their performance, taking measurements of communication latency and failure in the Internet. A simulation study of quorum multicasts showed that they provide low latency and require few messages. A second study that measured a test application running at several sites confirmed these results",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=213160,no,undetermined,0
Fault-tolerant concurrent branch and bound algorithms derived from program verification,"One approach for providing fault tolerance is through examining the behavior and properties of the application and deriving executable assertions that detect faults. This paper focuses on transforming the assertions of a verification proof of a program to executable assertions. These executable assertions may be embedded in the program to create a fault-tolerant program. It is also shown how the natural redundancy of the program variables can be used to reduce the number of executable assertions needed. While this approach has been applied to the sequential programming environment, the distributed programming environment presents special challenges. The authors discuss the application of concurrent programming axiomatic proof systems to generate executable assertions in a distributed environment using distributed branch and bound as a model problem",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=217571,no,undetermined,0
Application of syndrome pattern-matching approach to fault isolation in avionic systems,"The authors examine several attributes of a fault detection, isolation, and reconfiguration (FDIR) approach that is categorized as syndrome pattern matching. This approach is based on the premise that effective fault isolation algorithms must possess the capability of identifying patterns in the results of many fault detection tests, i.e., the fault syndrome. Examples of tests that constitute the syndrome are comparisons of redundant signals, hardware built-in test results and status indication from other systems or equipment. The syndrome pattern-matching method outlined presents a simple approach to implementing efficient and effective FDIR logic. Performance issues discussed include intermittent fault, multiple sequential failures, and unanticipated syndromes",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=177144,no,undetermined,0
Automated reliability prediction?,"With currently available software, it is unrealistic to expect a design engineer to perform an automated reliability prediction during the early design stage. Two major road blocks are: lack of a set of software tools for electrical and thermal stress analyses of an early design, and lack of an industry-wide parts library identifying part type, complexity, and quality levels. Without these tools, a part-count prediction is more appropriate during the early stages of a design. The Naval Avionics Center (NAC) has used a reliability prediction program to generate predictions for the projects listed. The Naval Avionics Center's (NAC) mission is to conduct research, development, engineering, material acquisition, pilot and limited manufacturing, technical evaluation, depot maintenance, and integrated logistics support on assigned airborne electronics (avionics), missile, spaceborne, undersea and surface weapon systems and related equipment. In striving to accomplish this mission, NAC has made a commitment to develop all new designs on their integrated computer-aided engineering environment",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=245507,no,undetermined,0
Test coverage dependent software reliability estimation by the HGD model,"The hyper-geometric distribution model (HGDM) has been presented as a software reliability growth model with the capability to make estimations for various kinds of real observed test-and-debug data. With the HGDM, the discovery and rediscovery of faults during testing and debugging has been discussed. One of the parameters, the `ease of test' function w(i) represents the total number of faults discovered and rediscovered at a test instance. In this paper, the authors firstly show that the ease of test function w(i) can be expressed as a function of the test coverage for the software under test. Test coverage represents a measure of how much of the software has been tested during a test run. Furthermore, the ease of test function w(i) can integrate a user-experience based parameter c that represents the strength of test cases to discover faults. This parameter c allows the integration of information on the goodness of test cases into the estimation process. The application of the HGDM to a set of real observed data clearly shows that the test coverage measure can be integrated directly into the ease of test function w(i) of the model",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=285845,no,undetermined,0
An alternative approach to avionics diagnostics algorithms,A methodology for designing avionics system diagnostics has been developed that uses an automated failure syndrome approach coupled with a partial-pattern matching algorithm to simplify diagnostics design and to provide improved diagnostic performance. The method was validated by performing experiments using a flight control failure simulation and a simulation of the proposed diagnostics. Results demonstrate significantly improved failure isolation performance using diagnostic algorithms of modest size and complexity. The approach described is particularly applicable to emerging highly integrated systems such as modular avionics,1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=282180,no,undetermined,0
Integrated detection and protection schemes for high-impedance faults on distribution systems,"The authors describe two integrated microcomputer-based detection and protection schemes for distribution system faults including high-impedance faults. The main objective of the two schemes is to deal with single-line-to-ground faults of feeders on a distribution substation using delta-delta connection. The first scheme, based on an innovative fault-detection algorithm, identifies a faulty feeder by measuring varied phase angles between phase currents and bus voltages. The second scheme, including several existing detection algorithms, incorporates an expert system for selecting an algorithm suitable for a given application based on their advantages. A simulation software package is built to simulate the implementation of the two schemes and their combination. Using the simulation package and feeder fault data, simulation tests are made to verify the feasibility of the schemes, the algorithms, and the system structures suggested",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=271642,no,undetermined,0
Applying diagnostic techniques to improve software quality,"The author describes the use of diagnostic techniques in the design, testing, and maintenance of software. The goal in applying diagnostic techniques to software is twofold. First, design-for-test techniques can be applied to help ensure that software failures can be diagnosed when they occur. Second, testing software using techniques which are normally applied to the testing of diagnostics can help detect and eliminate software failures before the product is delivered. This means that the diagnostic design techniques are applied to make the software more testable, and that the software is tested more thoroughly using methods which are applied to testing of diagnostic capabilities.<<ETX>>",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=270116,no,undetermined,0
Onboard diagnostics of avionics for fault tolerance and maintenance,"Summary form only given. Emerging modular avionics systems will require excellent onboard fault diagnosis, both to support flight critical functions and to provide for cost effective maintenance. The approach to fault isolation discussed makes use of multiple indications of failures, known as failure signatures or syndromes. A computer-aided design tool that can analyze the architecture and generate syndrome data for inclusion in embedded software algorithms has been developed. This tool performs automatic failure dependency analysis using as input a graphical schematic of the system to be analyzed. A further complication arises with the syndrome approach when detection tests fail to operate correctly. A method of mitigating this problem using partial pattern matching techniques on the pattern of test failure indication has also been developed. Other benefits of this approach are outlined.<<ETX>>",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=270095,no,undetermined,0
Integrated diagnostic for new French fighter,"An important logistic support function of airborne weapon systems is automatic failure detection and localization. On the new French fighter aircraft this function is being developed as a whole, taking into account the needs for all maintenance levels. O-level maintenance uses integrated monitoring and testing, which is described. The intermediate level maintenance is performed by a unique test system used for production equipment acceptance and maintenance in the user's operational intermediate workshops. Appropriate data are exchanged between maintenance levels by means of digital media. The main feature has been to apply, to all maintenance levels and as far as possible, the integrated maintenance concept which has been used for automatic diagnostics in Mirage 2000 aircraft taking into account the good results experienced since 1982.<<ETX>>",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=270079,no,undetermined,0
A system for fault diagnosis in electronic circuits using thermal imaging,"In many instances, the electronic state of an electronic circuit card contains insufficient information for correct fault diagnosis. Infrared thermal images, captured during the initial warmup of an electronic circuit card, are employed to enhance the available information and to assist the technician in performing the fault diagnosis. This system has proven beneficial for radar and microwave technologies where probing changes the electronic response of the circuit and signal levels can be extremely small. The development of this system has required the integration of technologies from a diversity of fields including image capture, image compression, image enhancement, neural networks, genetic algorithms, thermodynamics, and automatic test equipment (ATE) software. These technologies and preliminary system performance are discussed.<<ETX>>",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=270076,no,undetermined,0
A methodology for comparing fault tolerant computers,"A general methodology for comparing fault tolerant computers using a hierarchical taxonomy of pertinent issues is described. The uppermost level taxonomy consists of reliability, specialized fault detection, performance, flexibility, and communication issues. Each class is further decomposed into more specific topics. The methodology consists of studying each issue in turn to determine which candidate architecture(s) best support that issue. The results are then combined using a user defined function to rank the architectures. A design space technique is described to aid in drawing the comparisons. The technique is illustrated by considering the fault tolerant capabilities of the JPL hypercube and the Encore Multimax",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=269070,no,undetermined,0
An instrument for concurrent program flow monitoring in microprocessor based systems,"The activity of a microprocessor system can be altered by random failures due to dynamic hardware faults or unforeseen software errors that cannot be detected by common automatic test (ATE) or built-in self-test (BIST) offline systems. The authors describe a method for the detection of a particular class of such faults. This method is based on a monitoring strategy that surveys online the correctness of the program flow in an actual microprocessor system. It is determined whether the microprocessor program counter is correctly updated by following one of the allowable paths of the program flow. In program flow, two situations can be considered: sequential flow and deviation form sequentiality. It is possible to subdivide the program into segments in which the instructions are sequentially executed. Such segments are linked to each other by those instructions that cause the deviation from sequentiality. Examples of linking between program segments are given, and the logical structure for the main control activity of the implemented checking system is described",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=245156,no,undetermined,0
A chip solution to hierarchical and boundary-scan compatible board level BIST,"To achieve the full benefit of self test approaches, current self test techniques aimed at chip level must be extended to whole boards and systems. The self test must be hierarchical and compatible to the standardized boundary-scan architecture. A hierarchical boundary-scan architecture is presented together with the necessary controller chip and the synthesis software which make a hierarchical self test of arbitrary depth possible and provide sophisticated diagnosis features in case of failure detection",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=218370,no,undetermined,0
A replicated monitoring tool,Modeling the reliability of distributed systems requires a good understanding of the reliability of the components. Careful modeling allows highly fault-tolerant distributed applications to be constructed at the least cost. Realistic estimates can be found by measuring the performance of actual systems. An enormous amount of information about system performance can be acquired with no special privileges via the Internet. A distributed monitoring tool called a tattler is described. The system is composed of a group of tattler processes that monitor a set of selected hosts. The tattlers cooperate to provide a fault-tolerant distributed data base of information about the hosts they monitor. They use weak-consistency replication techniques to ensure their own fault-tolerance and the eventual consistency of the data base that they maintain,1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=242608,no,undetermined,0
A framework for process maintenance [software],"The authors present a framework, called the process cycle, which can assist in supporting and controlling process maintenance. The process cycle incorporates engineering management, performance, and improvement of processes by human agents subjected to desirable goals and policy constraints. Process maintenance is supported by incorporating feedback cycles so that processes, goals, and policies can be assessed and improved. In particular the authors address the identification of the reasons why processes change, the overall process change process, and the issue of policy improvement. Furthermore, they assess the applicability of the process cycle framework by relating it to current process maintenance practices. It is pointed out that an implication of using the process cycle for process maintenance is that there is a clear logical separation of concern in the various roles played by people, tools used, activities carried out, goals, and policies specified",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=242537,no,undetermined,0
Improving the performance of message-passing applications by multithreading,"Achieving maximum performance in message-passing programs requires that calculation and communication be overlapped. However, the program transformations required to achieve this overlap are error-prone and add significant complexity to the application program. The authors argue that calculation/communication overlap can be achieved easily and consistently by executing multiple threads of control on each processor, and that this approach is practical on message-passing architectures without any special hardware support. They present timing data for a typical message-passing application, to demonstrate the advantages of the scheme",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=232684,no,undetermined,0
Design of an image processing integrated circuit for real time edge detection,"Presents the design of a real time image processing micro-system to detect defects on manufacturing products. The analysis method is based on an edge detection algorithm (differential operators) to select the information related to the structure of the objects present in the image. The edge calculation function has been integrated in a standard cell circuit using a CMOS 1.5 Î¼m process. The ASIC has been implemented and tested in an image processing microsystem with a CCD camera. Results show an improvement of performances (speed, noise, size reduction system characterization, etc. . .) in comparison with the first prototypes (software implementation and printed board with standard components) allowing the use of this system in an industrial environment for the real time detection of defects",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=228040,no,undetermined,0
Arithmetic codes for concurrent error detection in artificial neural networks: the case of AN+B codes,"A number of digital implementations of neural networks have been presented in recent literature. Moreover, several authors have dealt with the problem of fault tolerance; whether such aim is achieved by techniques typical of the neural computation (e.g. by repeated learning) or by architecture-specific solutions, the first basic step consists clearly in diagnosing the faulty elements. The present paper suggests adoption of concurrent error detection; the granularity chosen to identify faults is that of the neuron. An approach based on a class of arithmetic codes is suggested; various different solutions are discussed, and their relative performances and costs are evaluated. To check the validity of the approach, its application is examined with reference to multi-layered feed-forward networks",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=224376,no,undetermined,0
Fault detection and isolation in RF subsystems,The authors report on the development of an approach for performing fault detection and isolation in RF subsystems using a highly integrated system-level operation. By integrating diagnostics into system-level operation RF subsystem faults are detected and isolated to the line replaceable module (LRM). This study is conducted in the context of the communication navigation identification (CNI) subsystem and the electronic warfare (EW) subsystem developed by TRW avionics,1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=220617,no,undetermined,0
Neural networks for sensor management and diagnostics,"The application of neural network technology to control system input signal management and diagnostics is explored. Control systems for critical plants, where operation must not be interrupted for safety reasons, are often configured with redundant sensing, computing, and actuating elements to provide fault tolerance and ensure the required degree of safety. In one such test case, the validation, selection and diagnosis of redundant sensor signals required 40%-50% of the control system hardware and software, while the control algorithms required less than 20% of these resources. Neural networks are investigated to determine if they can reduce the computational requirement and improve the performance of control system input signal management and diagnostics. Four neural networks are investigated to determine if they can reduce the computational requirement and improve the performance of control system input signal management and diagnostics. Four neural networks were trained to perform signal validation and selection of redundant sensors, sensor estimation from the data redundancy among dissimilar sensors, diagnostics, and estimation of unobservable control parameters. The performance of the neural networks are compared against plant models, and the results are discussed",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=220484,no,undetermined,0
Computer-aided design of avionic diagnostics algorithms,"Describes the application of a computer-aided design tool developed to complement an alternative approach to diagnostics design using the failure signatures or syndromes. The computer-aided design tool can analyze fault-tolerant architectural networks, provide network design guidance, and generate syndrome data for inclusion in the embedded software diagnostic algorithms. The tool accepts as input a graphical description of the system to be analyzed and produces as output a complete failure dependency analysis and the syndrome vectors needed for a table look-up implementation of the diagnostics. The failure syndrome also provides analysis of the theoretical performance potential of the system. Additional benefits are that transient and intermittent failures can be conveniently handled, and the method can be extended to diagnose multiple sequential failure conditions",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=220480,no,undetermined,0
Assessing testing tools in research and education,"An evaluation of three software engineering tools based on their use in research and educational environments is presented. The three testing tools are Mothra, a mutation-testing tool, Asset, a dataflow testing tool, and ATAC, a dataflow testing tool. Asset, ATAC, and Mothra were used in research projects that examined relative and general fault-detection effectiveness of testing methods, how good a test set is after functional testing based on program specification, how reliability estimates from existing models vary with the testing method used, and how improved coverage affects reliability. Students used ATAC and Mothra by treating the tools as artifacts and studying them from the point of view of documentation, coding style, and possible enhancements, solving simple problems given during testing lectures, and conducting experiments that supported ongoing research in software testing and reliability. The strengths, weaknesses, and performances of Asset, Mothra, and ATAC are discussed.<<ETX>>",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=136170,no,undetermined,0
Evaluating and selecting testing tools,"A system using process and forms to give managers and tool evaluators a reliable way to identify the tool that best fits their organization's needs is discussed. It is shown that a tool evaluator must analyze user needs, establish selection criteria (including general criteria, environment-dependent criteria, tool-dependent functional criteria, tool-dependent nonfunctional criteria, and weighting), search for tools, select the tools, and then reevaluate the tools. Forms for recording tool selection criteria, classifying testing tools, profiling tool-to-organization interconnections, creating hardware and software profiles, and creating tool-interconnection profiles are presented. It is argued that, with these forms, evaluators have a reasonably accurate and consistent system for identifying and quantifying user needs, establishing tool-selection criteria, finding available tools, and selecting tools and estimating return on investment.<<ETX>>",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=136165,no,undetermined,0
How to assess tools efficiently and quantitatively,"A generic, yet tailorable, five-step method to select CASE tools is presented. The guide, developed by the Software Engineering Institute and Westinghouse, includes a tool taxonomy that captures information like the tool name and vendor, release date, what life-cycle phases and methods it supports, key features, and the objects it produces, and includes six categories of questions designed to determine how well a tool does what it was intended to do. The questionnaire comprises 140 questions divided into the categories: ease of use, power, robustness, functionality, ease of insertion, and quality of support.<<ETX>>",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=136163,no,undetermined,0
Pseudoduplication of floating-point addition. A method of compiler generated checking of permanent hardware faults,A method of compiler generated checkpoints for the detection of permanent hardware faults is offered. Some of the floating point addition instructions of a program are used as checkpoints. The compiler automatically generates a diverse execution on different data paths (pseudo-duplication). Rounding-modes and fault coverage are investigated. The method can be implemented without additional hardware and with a tolerable reduction in performance.<<ETX>>,1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=208152,no,undetermined,0
Fault-tolerance in parallel architectures with crosspoint switches,"Based on an existing IBM/370 switch connected parallel processor (SCPP) prototype implementation, conceptual enhancements in the design for continuous availability are described. These enhancements include the user-specifiable two- to three-fold concurrent execution of processes on nonsynchronized processing units (PUs). The output produced by these processes is compared by software or hardware means. The asynchronism of the execution allows the detection of context-dependent software failures in addition to the detection of hardware errors. These design advances encompass novel hardware and software features aimed at achieving continuous system operation as well as superior information integrity at an improved cost/performance ratio, facilitating a proven crosspoint switch intercommunication mechanism of the SCPP",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=257433,no,undetermined,0
Dependability prediction of a fault-tolerant microcomputer for advanced control,"The architecture of a reliable microcomputer for advanced control is presented. The microcomputer provides fault-tolerance features by means of spare modules and graceful degradation of the performance level. To quantitatively analyze its main features it is necessary to numerically evaluate three dependability measures: reliability, steady-state availability, and performability. The influence of permanent, intermittent, and transient faults and the effect of local and global repair actions, are emphasized. Finally, the main capabilities and limitations of the microcomputer are discussed",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=257425,no,undetermined,0
Software reliability modelling: achievements and limitations,"An examination is made of the direct evaluation of the reliability of a software product from observation of its actual failure process during operation, using reliability growth modeling techniques described. Conclusions drawn from perfect working are examined. An assessment is also made of indirect sources of confidence in dependability. The author argues that one way forward might be to insist that certain safety-critical system should only be built if the safety case can be demonstrated to rely only on assurable levels of design dependability",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=257407,no,undetermined,0
From random testing of hardware to statistical testing of software,"Random or statistical testing consists in exercising a system by supplying to it valued inputs which are randomly selected according to a defined probability distribution on the input domain. The specific role one can expect from statistical testing in a software validation process is pointed out. The notion of test quality with respect to the experiment goal allows the testing time to be adjusted to a target test quality. Numerical results illustrate the strengths and limits of statistical testing as a software validation tool, in the present state-of-the-art",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=257382,no,undetermined,0
Connectionist speaker normalization and its applications to speech recognition,"Speaker normalization may have a significant impact on both speaker-adaptive and speaker-independent speech recognition. In this paper, a codeword-dependent neural network (CDNN) is presented for speaker normalization. The network is used as a nonlinear mapping function to transform speech data between two speakers. The mapping function is characterized by two important properties. First, the assembly of mapping functions enhances overall mapping quality. Second, multiple input vectors are used simultaneously in the transformation. This not only makes full use of dynamic information but also alleviates possible errors in the supervision data. Large-vocabulary continuous speech recognition is chosen to study the effect of speaker normalization. Using speaker-dependent semi-continuous hidden Markov models, performance evaluation over 360 testing sentences from new speakers showed that speaker normalization significantly reduced the error rate from 41.9% to 5.0% when only 40 speaker-dependent sentences were used to estimate CDNN parameters",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=239506,no,undetermined,0
An improved numerical algorithm for calculating steady-state solutions of deterministic and stochastic Petri net models,Introduces an algorithm for calculating steady-state solutions of DSPN models. The described method employs the randomization technique enhanced by a stable calculation of Poisson probabilities. A complete re-design and re-implementation of the appropriate components implemented in the version 1.4 of the software package GreatSPN has lead to significant savings in both computation time and memory space. These benefits are illustrated by DSPN models taken from the literature. The author considers DSPN models for an E<sub>r</sub>/D/1/K queueing system and a fault-tolerant clocking system. These examples show that the model solutions are calculated with significantly less computational effort and a better error control by the algorithm described than by the method implemented in the version 1.4 of the software package GreatSPN,1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=238803,no,undetermined,0
Reliability modeling of the MARS system: a case study in the use of different tools and techniques,"Analytical reliability modeling is a promising method for predicting the reliability of different architectural variants and to perform trade-off studies at design time. However, generating a computationally tractable analytic model implies in general an abstraction and idealization of the real system. Construction of such a tractable model is not an exact science, and as such, it depends on the modeler's intuition and experience. This freedom can be used in formulating the same problem by more than one approach. Such a N-version modeling approach increases the confidence in the results. In this paper, we analyze the MARS architecture with the dependability evaluation tools SHARPE and SPNP, employing several different techniques including: hierarchical modeling, stochastic Petri nets, folding of stochastic Petri nets, and state truncation. The authors critically examine these techniques for their practicability in modeling complex fault-tolerant computer architectures",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=238793,no,undetermined,0
Motion-compensated video image compression using luminance and chrominance components for motion estimation,"Summary form only given. A variable rate video image compression system using motion-compensated discrete cosine transform (MCDCT) coding is investigated. This paper focuses on the quality improvement achieved by using the luminance component (Y) and both chrominance components (Cr and Cb) for motion estimation. The design of the MCDCT system is based solely on existing technology. The complete system is currently simulated in software. The simulations of the discrete cosine transform processor and the motion estimation processor are based on the technical specifications of existing SGS-Thomson components. The performance of the MCDCT image compression system is evaluated using video images collected from a shuttle mission. The significant error count is shown to be sensitive to the luminance or color distortions in localized areas in an image sequence that are noticeable to viewers, but do not cause significant variations in the SNR measurement",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=213321,no,undetermined,0
Balancing the benchmark opportunities,"It is noted that benchmarking activities can be focused on two areas, internal and external benchmarks. Depending on the focus, different objectives are achieved. Based on an enterprise's reproducible development and delivery process, internal benchmarks are used to drive continuous improvements for quality and productivity as well as predictions of outcomes. Based on industry models or practices, external benchmarks identify competitive standing and improvement opportunities. As complementary activities, the external benchmark is used to identify new process requirements and the internal benchmark is used to focus the improvement efforts. Using benchmarks in the proper context allows an enterprise to make effective decisions on their software development and delivery processes",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=188378,no,undetermined,0
The detection of fault-prone programs,"The use of the statistical technique of discriminant analysis as a tool for the detection of fault-prone programs is explored. A principal-components procedure was employed to reduce simple multicollinear complexity metrics to uncorrelated measures on orthogonal complexity domains. These uncorrelated measures were then used to classify programs into alternate groups, depending on the metric values of the program. The criterion variable for group determination was a quality measure of faults or changes made to the programs. The discriminant analysis was conducted on two distinct data sets from large commercial systems. The basic discriminant model was constructed from deliberately biased data to magnify differences in metric values between the discriminant groups. The technique was successful in classifying programs with a relatively low error rate. While the use of linear regression models has produced models of limited value, this procedure shows great promise for use in the detection of program modules with potential for faults",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=135775,no,undetermined,0
Customer expectations for introduction of a large software product [for 5ESS switch],"Illinois Bell has developed a rigorous set of expectations, including formal evaluation criteria, for a new software release in its network elements. The combined introduction experience of Illinois Bell as the customer and AT&T as the vendor in meeting these expectations during the introduction of the 5E6 software release for the 5ESS switch is discussed. Topics outlined include customer requirements for successful retrofit, resolution process for critical problems, customer view of quality, software quality management prior to release, and metrics to assess improvement results over the in-service software release. This close customer-supplier relationship resulted in improved product delivery processes as well as improved end-customer service after delivery",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=188373,no,undetermined,0
A comparison of voting algorithms for n-version programming,"One approach to software fault-tolerance is to develop three versions of a program from a common specification, execute these versions in parallel, and vote on the output. If two versions agree, this majority vote is taken as the correct result. Errors in a single version are thus masked out. If there is no majority, a 3-version failure occurs. In earlier experimental work, groups of versions were tested in 3- version programming mode and the resulting average failure probability compared with the average failure probability of the individual versions. This paper describes the use of 5 alternative voting algorithms and compares the resulting failure probabilities against the failure probability for standard 3-version programming. All algorithms produced an improvement over standard 3-version programming, and one algorithm was sometimes able to produce entirely correct values even when none of the three versions has done so",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=183986,no,undetermined,0
A case study in traceability,"In 1987, a major engineering project took place whose aim was to develop a high speed electromechanical device which was to be controlled by some 30 microprocessors. The software development followed the YOURDON method supported by the Teamwork analysis and design tool. The project was not an unqualified success. One part delivered later than predicted, but the deliverables were complete and showed a lower defect frequency than previous products. The other part by contrast, delivered some two years late after several defeaturing exercises. It was suggested that the difference in productivity was partly due to the fact that the YOURDON guidelines had been followed more closely in the successful part. In addition the successful part had employed quite rigorous progress and quality measurement techniques. When a new project was begun it was desirable to ensure that progress would be tracked, that all parts of the YOURDON models could be traced to particular requirements and that quality assurance could detect nonconformance to the guidelines efficiently and effectively. The paper describes how the project team went about using the features offered by Teamwork to achieve the required improved control over the analysis and design process",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=182211,no,undetermined,0
Confidence intervals on performance estimates,"Performance estimates show how a computer system architecture will respond to the load placed upon it. Typical estimates required are response times to particular events, system throughputs and loading levels of CPUs and LANs. The purpose of performance estimation is to reduce risk. It aims to provide the best possible estimate of the system performance as cheaply as possible, and to highlight design weaknesses. This paper places emphasis on the practical applications of performance estimation, focusing on the approach taken early in the system life-cycle. Therefore, it is treated as a costed process, which makes definite recommendations on a design. To understand the approach, some basic concepts behind system modelling are laid down",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=181945,no,undetermined,0
IEE Colloquium on `Designing Quality into Software Based Systems' (Digest No.151),The following topics were dealt with: software quality review; software timing requirements; the anticipation of software change; controlling quality with quantified attribute check-lists; models for constructive quality assurance; confidence intervals on performance estimates; modelling timing constraints; systems development collaborative approach; automatic documentation for program maintenance; functional complexity analysis; the effect of incremental delivery on software quality and ensuring the quality and maintainability of software elements,1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=181938,no,undetermined,0
An on-line integrated industrial inspection system for the lighting industry,"Presents a set of knowledge-based algorithms suitable for on-line inspection, with particular reference to lamp inspection problems for the lighting industry. The system was developed to identify not only uncut wires, but also many other faults, such as missing solders on connector pads, insufficient soldering, and solder splashes on the backing disc. The paper describes the problem and identifies the types of faults to be detected by a machine vision system; and presents the methods proposed to solve this specific industrial inspection problem. A model system on which the software algorithms are tested and the performances are evaluated is also discussed",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=181292,no,undetermined,0
Hardware assisted real-time rollback in the advanced fault-tolerant data processor,"Rollback recovery offers an efficient method of recovering from transient faults and permanent faults when rollback is combined with spare resource reconfiguration. The hardware macro-rollback technique presented has been implemented in the advanced fault-tolerant data processor (AFTDP), which is a high-performance fault-tolerant shared memory multiprocessor. The architecture discussion focuses on the unique problems of achieving both low overhead and fast recovery in high-throughput cached multiprocessors. Macro-rollback recovery from transient faults and hard macro-rollback from permanent faults are examined. In addition, deadline analysis based on a semi-Markov model is presented",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=177179,no,undetermined,0
The Office of Naval Research initiative on ultradependable multicomputers and electronic systems,"The US Office of Naval Research initiative in ultradependable multicomputers and electronic systems is focused on the development of novel techniques for achieving high dependability in parallel and distributed systems. Some of the approaches being funded through the initiative and their role in the program as a whole are surveyed. The objective of the program is to achieve greater effectiveness and efficiency (than traditional massive redundancy) through the following: increased understanding of physical failure modes and their effects in real systems; increased sensitivity to the structure, semantics, and requirements of real applications; exploitation of the inherent redundancy in parallel systems; and examination of hardware/software tradeoffs. To achieve these objectives, an integrated program has been constructed around four major components: measurement and modeling of real systems; software-based methods such as robust algorithms; efficient hardware-based methods and real-time systems",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=177161,no,undetermined,0
Automated portable instruments for quantitative measurements of nuclear materials holdup and in-process inventories,"Summary form only given. Portable gamma-ray spectroscopy instrumentation has been combined with a versatile mechanical system and sophisticated software package to produce an automated instrument for determination of quantities of nuclear materials deposited in process equipment. The mechanical system enables using high-resolution spectroscopy methods in the variety of measurement locations and detector positions required to perform quantitative assays of nuclear materials holdup and in-process inventories. The, software automates the electronics setup, data acquisition, and analysis, output of results, and storage of results and spectra, as well as quality and measurement control tests and compensation for gain drift and rate loss. The analysis, which is based on a systematic method of generalizing the geometries of the nucl,ear material deposits, provides quantitative assay results at the time of the measurement. The software enables portable measurements to be performed in rapid succession with full utilization of readout and storage capabilities.<<ETX>>",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=259128,no,undetermined,0
Predicting faults with real-time diagnosis,The authors discuss a strategy for implementing real-time expert systems when the expert system hardware-software combination is too slow at first analysis. A major aspect of the system is its ability to predict faults. The techniques described have been fully implemented on a large-scale real-time expert system for British Steel at Ravenscraig in Scotland. This application is discussed in detail. The authors outline the requirement for a real-time system and how real-time analysis has been provided in spite of the fact that the computer system and expert system are notionally too slow for real-time input. The heart of the real-time capability is a result of planning ahead for various types of data analysis and providing adequate history management so that the analysis is available at the appropriate time. The authors also discuss the innovative strategy for allowing the system to predict faults based on the actual plant performance,1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=261820,no,undetermined,0
An animated interface for X-ray laminographic inspection of fine-pitch interconnect,"While the function of solder joint inspection is to assess joint quality, evolving fine-pitch mounting technologies have made some electronic assemblies impossible to inspect using human-oriented visual techniques. A group of automated inspection tools has recently been developed in response to this need, one of which uses X-ray laminography in conjunction with very elaborate software. The authors present the work that has been performed to provide an easy-to-use animated graphics-oriented, X-ray laminographic solder joint inspection system interface. The interface has been developed to aid in the study of performance-related interconnect inspection",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=279778,no,undetermined,0
A Fault Tolerant Four Wheel Steering System,"The paper describes an automotive fault tolerant control system applied to a 4WS system for high performance cars. The goal is to study the tradeoff among high performance, safety and low cost implementation.",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4791633,no,undetermined,0
Methodology for validating software metrics,"A comprehensive metrics validation methodology is proposed that has six validity criteria, which support the quality functions assessment, control, and prediction, where quality functions are activities conducted by software organizations for the purpose of achieving project quality goals. Six criteria are defined and illustrated: association, consistency, discriminative power, tracking, predictability, and repeatability. The author shows that nonparametric statistical methods such as contingency tables play an important role in evaluating metrics against the validity criteria. Examples emphasizing the discriminative power validity criterion are presented. A metrics validation process is defined that integrates quality factors, metrics, and quality functions",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=135774,no,undetermined,0
Reliability analysis of distributed systems based on a fast reliability algorithm,"The reliability of a distributed processing system (DPS) can be expressed by the analysis of distributed program reliability (DPR) and distributed system reliability (DSR). One of the good approaches to formulate these reliability performance indexes is to generate all disjoint file spanning trees (FSTs) in the DPS graph such that the DPR and DSR can be expressed by the probability that at least one of these FSTs is working. In the paper, a unified algorithm to efficiently generate disjoint FSTs by cutting different links is presented, and the DPR and DSR are computed based on a simple and consistent union operation on the probability space of the FSTs. The DPS reliability related problems are also discussed. For speeding up the reliability evaluation, nodes merged, series, and parallel reduction concepts are incorporated in the algorithm. Based on the comparison of number of subgraphs (or FSTs) generated by the proposed algorithm and by existing evaluation algorithms, it is concluded that the proposed algorithm is much more economic in terms of time and space than the existing algorithms",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=127256,no,undetermined,0
Channels and codes for magnetooptical recording,"The author outlines the technology used in the optical recording channel concentrating on magnetooptical recording and describes some of the major formats, codes, and data detection methods which are being developed. A sophisticated model of the optical recording channel was used as a way to make fair comparisons of these candidates, without skewing the results because of different disk or drive quality, degrees of hardware development, test procedures, etc. The author describes strengths and weaknesses of some of the leading contenders for current and future optical recording channels",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=124469,no,undetermined,0
Development of a model for predicting flicker from electric arc furnaces,"Owing to the characteristics of electric-arc phenomena electric arc furnace loads can result in serious electrical disturbances on a power system. Low level frequency modulation of the supply voltage of less than 0.5% can cause annoying flicker in lamps and invoke public complaints when the frequency lies in the range of 6-10 Hz. There is a need therefore to develop better techniques to predict flicker effects from single and multiple arc furnace loads and the effects of compensation schemes. The authors discuss the nature of arc furnace loads, and describe the instrumentation, field measurements, and signal analysis techniques which were undertaken to develop an arc furnace model. A single-phase furnace model is proposed suitable for use on digital simulation programs such as the EMTP or other appropriate commercial software simulation programs. Representative results based on actual furnace input data are included to show the validity of the model",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=108936,no,undetermined,0
Current antenna research at K. U. Leuven,"Four antenna research projects at the Katholieke Universiteit Leuven are described. In the first project, four different methods for the transformation of plane-polar near-field data to far-field antenna characteristics have been compared, and the method with the best performance (accuracy versus computer time) has been implemented on an HP 1000 and on a VAX under VMS. In the second project, a test method has been developed to detect faulty columns in three-dimensional array antennas. The method is based on a reconstruction technique, and has been implemented in an air-surveillance secondary radar system. In the third project, the transmission-line model developed for rectangular microstrip antennas has been implemented and verified for linear and planar arrays. The software package runs on a PC, as well as on VAX workstations. In the fourth project, a software package, running on a VAX under VMS, has been developed based on a moment-method approach for the analysis of multiprobe, multipatch configurations in stratified-dielectric media. The procedure developed contains features to improve the convergence, accuracy and efficiency of the moment method.<<ETX>>",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=97947,no,undetermined,0
The KAT (knowledge-action-transformation) approach to the modeling and evaluation of reliability and availability growth,"An approach for the modeling and evaluation of reliability and availability of systems using the knowledge of the reliability growth of their components is presented. Detailed models of reliability and availability for single-component systems are derived under much weaker assumption than usually considered. These models, termed knowledge models, enable phenomena to be precisely characterized, and a number of properties to be deduced. Since the knowledge models are too complex to be applied in real life for performing predictions, simplified models for practical purposes (action models) are discussed. The hyperexponential model is presented and applied to field data of software and hardware failures. This model is shown to be comparable to other models as far as reliability of single-component systems is concerned: in addition, it enables estimating and predicting the reliability of multicomponent systems, as well as their availability. The transformation approach enables classical Markov models to be transformed into other Markov models which account for reliability growth. The application of the transformation to multicomponent systems is described",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=90436,no,undetermined,0
Power system fault detection and state estimation using Kalman filter with hypothesis testing,An algorithm for detecting power system faults and estimating the pre and post-fault steady state values of the voltages and currents is described. The proposed algorithm is based on the Kalman filter and hypothesis testing. It is shown that a power system fault is ideally suited for single sample hypothesis testing. The performance of the proposed technique is examined in connection with the protection of parallel transmission lines. The proposed technique avoids many problems of parallel line protection. The simplicity and the avoidance of complicated software and hardware in addition to the stability of the relay under different switching conditions are some of the features of the proposed technique. Studies performed on a long double-circuit transmission line with and without series compensation show a trip time in all cases of around 5 ms,1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=85843,no,undetermined,0
A validity measure for fuzzy clustering,"The authors present a fuzzy validity criterion based on a validity function which identifies compact and separate fuzzy c-partitions without assumptions as to the number of substructures inherent in the data. This function depends on the data set, geometric distance measure, distance between cluster centroids and more importantly on the fuzzy partition generated by any fuzzy algorithm used. The function is mathematically justified via its relationship to a well-defined hard clustering validity function, the separation index for which the condition of uniqueness has already been established. The performance of this validity function compares favorably to that of several others. The application of this validity function to color image segmentation in a computer color vision system for recognition of IC wafer defects which are otherwise impossible to detect using gray-scale image processing is discussed",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=85677,no,undetermined,0
Analyzing partition testing strategies,"Partition testing strategies, which divide a program's input domain into subsets with the tester selecting one or more elements from each subdomain, are analyzed. The conditions that affect the efficiency of partition testing are investigated, and comparisons of the fault detection capabilities of partition testing and random testing are made. The effects of subdomain modifications on partition testing's ability to detect faults are studied",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=83906,no,undetermined,0
An experimental evaluation of software redundancy as a strategy for improving reliability,"The strategy of using multiple versions of independently developed software as a means to tolerate residual software design faults is discussed. The effectiveness of multiversion software is studied by comparing estimates of the failure probabilities of these systems with the failure probabilities of single versions. The estimates are obtained under a model of dependent failures and compared with estimates obtained when failures are assumed to be independent. The experimental results are based on 20 versions of an aerospace application developed and independently validated by 60 programmers from 4 universities. Descriptions of the application and development process are given, together with an analysis of the 20 versions",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=83905,no,undetermined,0
"Trends in measurement, estimation, and control (software engineering)","As an expert in quantitative aspects of software management, the author shares his view of where the industry is and where it is going. He identifies a few basic measures that have been used successfully in management and describes the direction that measurement is being driven by the pressure of total quality management.<<ETX>>",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=73762,no,undetermined,0
Predicting where faults can hide from testing,"Sensitivity analysis, which estimates the probability that a program location can hide a failure-causing fault, is addressed. The concept of sensitivity is discussed, and a fault/failure model that accounts for fault location is presented. Sensitivity analysis requires that every location be analyzed for three properties: the probability of execution occurring, the probability of infection occurring, and the probability of propagation occurring. One type of analysis is required to handle each part of the fault/failure model. Each of these analyses is examined, and the interpretation of the resulting three sets of probability estimates for each location is discussed. The relationship of the approach to testability is considered.<<ETX>>",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=73748,no,undetermined,0
Design And Performance Of A One-half Mv Rep-rate Pulser,"A rep-rate pulser system is under development for use as an Army user facility. This pulser is entering its final stages of testing prior to delivery. This paper presents results which show the pulser's performance demonstrated to date and describes future upgrades now in progress. The pulser system contains a power supply which energizes a resonant charging circuit which charges a PFN to 1 MV. This power supply can provide up to 300 kW in one second bursts to the pulser. The output of the power unit charges the pulser's 10.5 /spl mu/F primary capacitor bank to 80 kV in 5 ms. This bank is then discharged into the primary of a 1:13.5 iron-core pulse transformer. The transformer's core is reset by the charging current to the 10.2 /spl mu/F capacitor bank. A pair of low inductance, simultaneously triggered gas switches discharges the capacitors into the primary winding of the transformer. The transformer secondary resonant-charges a 5 /spl Omega/ PFN to 1 MV in 7 /spl mu/s. The stored PFN energy is thus 28 kJ. A single self- breaking output switch discharges the PFN into a low inductance 5 /spl Omega/ load resistor. The pulser section, extending from the primary capacitors to the resistive dummy load is contained within an oil tank. The pulser was tested in single pulse and low rep-rates. The specified 500 kV load voltage was achieved without breakdown or corona. A 10 to 90% risetime of approximately 30 ns was obtained. The pulsewidth of the flat-top, a critical parameter for this pulser, was 450 ns (within an amplitude range /spl plusmn/5% of peak) . The pulser and power sections are interfaced to a Macintosh Ilcx computer. This computer accepts commands while operating under the LabVIEW software system which allows the pre-setting of all required pulser settings, including rep-rate, number of shots, burst width and the amplitude and timing of voltages and currents within the power unit. The LabVIEW control program also incorporates fault detection and display rou- ines.",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=733450,no,undetermined,0
The Past Quarter Century and the Next Decade of Video Tape Recording (Invited),"Since the first commercially successful video tape recorder (VTR) was developed in 1956, many VTRs were developed and put on the commercial market. If the picture quality is almost the same, the tape consumption per hour (TCH) of a commercially successful VTR in its sales year will decrease according to a trend line of one tenth per ten years. If the picture quality is improves, the trend line will move toward a higher position on a parallel line. â€?We can forecast the future specification of the VTR as a function of TCH from such a trend chart. The miniaturization of the pitch and wavelength is a motivating force of VTR development. In this manner, high utility, high performance, but low cost VTR was developed and continues to improve. The harmonization of hardware and software is the next important problem. â€?In this paper, a short technical history of VTR, the future of VTR predicted from the trend chart, and the technical motivating force of VTR development are discussed.",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7267989,no,undetermined,0
A new DSP method for group delay measurement,"After a review of methods available for group delay measurement and their primary errors, a digital signal processing (DSP) method with excellent accuracy is proposed. The method is described, and a comparison is made with other methods. An example using the new method is given. When compared with the methods available, the new method does not involve compromises of the frequency intervals, and it eliminates the defects caused by the assumption of linearity of phase-frequency characteristics, the calculation of the phase-frequency characteristics, and the evaluation of the difference quotient, whose measurement accuracy depends only on the hardware and software used for measurement. It has the advantages of high precision and forthright computation",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=69941,no,undetermined,0
Analyzing error-prone system structure,"Using measures of data interaction called data bindings, the authors quantify ratios of coupling and strength in software systems and use the ratios to identify error-prone system structures. A 148000 source line system from a prediction environment was selected for empirical analysis. Software error data were collected from high-level system design through system testing and from field operation of the system. The authors use a set of five tools to calculate the data bindings automatically and use a clustering technique to determine a hierarchical description of each of the system's 77 subsystems. A nonparametric analysis of variance model is used to characterize subsystems and individual routines that had either many or few errors or high or low error correction effort. The empirical results support the effectiveness of the data bindings clustering approach for localizing error-prone system structure",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=67595,no,undetermined,0
Dynamic oscillations predicted by computer studies,"During the latter part of 1988, a study was begun to review the dynamic stability performance of Georgia Power Company's Plant Scherer. The scope of the study was to identify any operating conditions that might contribute to system oscillations and to examine alternative solutions that would control these oscillations. The study was performed in several phases. The authors briefly discuss the study process which utilized two different software packages for the analysis: dynamic stability studies using time-domain software and eigenvalue analysis using frequency-domain software.<<ETX>>",1991,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=65036,no,undetermined,0
QUIETEST: a quiescent current testing methodology for detecting leakage faults,"A hierarchical leakage fault analysis methodology is proposed for IDDQ (quiescent power supply current) testing of VLSI CMOS circuits. A software system, QUIETEST, has been developed on the basis of this methodology. The software can select a small number of test vectors for IDDQ testing from the provided functional test set. Therefore, the total test time for IDDQ measurements can be reduced significantly to make IDDQ testing of VLSI CMOS circuits feasible in a production test environment. For two VLSI circuits QUIETEST was able to select less than 1% of functional test vectors from the full test set for covering as many leakage faults as would be covered if IDDQ was measured upon the application of 100% of the vectors.<<ETX>>",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=129902,no,undetermined,0
Real-time computer network performance analysis based on ISO/OSI transport service definition,"A unified approach focuses on estimation of statistical parameters at the transport service access points from the states and transitions of the service endpoints in the transport layer. Unlike previous methods, this approach emphasizes simplicity, efficiency, and the possibility of real-time analysis. Other possible applications of this concept are discussed, e.g. timer value assignment, end-to-end global rate or throughput control. On the basis of the proposed concept, much more sophisticated real implementations, in end-to-end bases, could be considered for existing computer networks and for future open systems and integrated services",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=117309,no,undetermined,0
"PROOFS: a fast, memory efficient sequential circuit fault simulator","A super-fast fault simulator for synchronous sequential circuits, called PROOFS, is described. PROOFS achieves high performance by combining all the advantages of differential fault simulation, single fault propagation, and parallel fault simulation, while minimizing their individual disadvantages. PROOFS minimizes the memory requirements, reduces the number of events that need to be evaluated, and simplifies the complexity of the software implementation. PROOFS requires an average of one fifth the memory required for concurrent fault simulation and runs 6 to 67 times faster on the ISCAS sequential benchmarks",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=114913,no,undetermined,0
Validation of finite element analysis of the magnetic fields around fine cracks,"Reliable non-destructive testing of crack-like defects in welded structures, using magnetic particle inspection techniques, requires accurate information about the leakage flux resulting from applied current and fields. A range of cracks was prepared in carbon-manganese steel with maximum widths of 10 to 15Î¼m and depths of 0.5 to 4.5mm. The magnetic disturbance caused by such defects was measured using a 0.12Î¼m wide magneto-resistive thin film. Defects of similar dimensions were subsequently modelled using existing finite element software, tuned to meet the requirements imposed by the discretisation of regions having large aspect ratios. The predictions of the models, which were in close agreement with flux measurements, were used to determine the surface forces produced by magnetisation which act upon particles used in magnetic particle inspection.",1985,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1064218,no,undetermined,0
Evaluation of natural language processing systems: Issues and approaches,"This paper encompasses two main topics: a broad and general analysis of the issue of performance evaluation of NLP systems and a report on a specific approach developed by the authors and experimented on a sample test case. More precisely, it first presents a brief survey of the major works in the area of NLP systems evaluation. Then, after introducing the notion of the life cycle of an NLP system, it focuses on the concept of performance evaluation and analyzes the scope and the major problems of the investigation. The tools generally used within computer science to assess the quality of a software system are briefly reviewed, and their applicability to the task of evaluation of NLP systems is discussed. Particular attention is devoted to the concepts of efficiency, correctness, reliability, and adequacy, and how all of them basically fail in capturing the peculiar features of performance evaluation of an NLP system is discussed. Two main approaches to performance evaluation are later introduced; namely, black-box- and model-based, and their most important characteristics are presented. Finally, a specific model for performance evaluation proposed by the authors is illustrated, and the results of an experiment with a sample application are reported. The paper concludes with a discussion on research perspectives, open problems, and importance of performance evaluation to industrial applications.",1986,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1457848,no,undetermined,0
Assessment of Software Reliability Models,This paper proposes a method for assessing software reliability models and its application to the Musa and Littlewood-Verrall models. It is divided into two parts.,1985,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1702103,no,undetermined,0
A Technique for Estimating Performance of Fault-Tolerant Programs,A technique is presented for estimating the performance of programs written for execution on fail-stop processors. It is based on modeling the program as a discrete-time Markov chain and then using z-transforms to derive a probability distribution for time to completion.,1985,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1702053,no,undetermined,0
Identifying Error-Prone Softwareâ€”An Empirical Study,"A major portion of the effort expended in developing commercial software today is associated with program testing. Schedule and/ or resource constraints frequently require that testing be conducted so as to uncover the greatest number of errors possible in the time allowed. In this paper we describe a study undertaken to assess the potential usefulness of various product-and process-related measures in identifying error-prone software. Our goal was to establish an empirical basis for the efficient utilization of limited testing resources using objective, measurable criteria. Through a detailed analysis of three software products and their error discovery histories, we have found simple metrics related to the amount of data and the structural complexity of programs to be of value for this purpose.",1985,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1702015,no,undetermined,0
An Experimental Study of Software Metrics for Real-Time Software,"The rising costs of software development and maintenance have naturally aroused intere5t in tools and measures to quantify and analyze software complexity. Many software metrics have been studied widely because of the potential usefulness in predicting the complexity and quality of software. Most of the work reported in this area has been related to nonreal-time software. In this paper we report and discuss the results of an experimental investigation of some important metrics and their relationship for a class of 202 Pascal programs used in a real-time distributed processing environment. While some of our observations confirm independent studies, we have noted significant differences. For instance the correlations between McCabe's control complexity measure and Halstead's metrics are low in comparison to a previous study. Studies of the type reported here are important for understanding the relationship between software metrics.",1985,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1701992,no,undetermined,0
Software Fault Tolerance: An Evaluation,"In order to assess the effectiveness of software fault tolerance techniques for enhancing the reliability of practical systems, a major experimental project has been conducted at the University of Newcastle upon Tyne. Techniques were developed for, and applied to, a realistic implementation of a real-time system (a naval command and control system). Reliability data were collected by operating this system in a simulated tactical environment for a variety of action scenarios. This paper provides an overview of the project and presents the results of three phases of experimentation. An analysis of these results shows that use of the software fault tolerance approach yielded a substantial improvement in the reliability of the command and control system.",1985,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1701973,no,undetermined,0
Bayesian Extensions to a Basic Model of Software Reliability,"A Bayesian analysis of the software reliability model of Jelinski and Moranda is given, based upon Meinhold and Singpurwalla. Important extensions are provided to the stopping rule and prior distribution of the number of defects, as well as permitting uncertainty in the failure rate. It is easy to calculate the predictive distribution of unfound errors at the end of software testing, and to see the relative effects of uncertainty in the number of errors and in the detection efficiency. The behavior of the predictive mode and mean over time are examined as possible point estimators, but are clearly inferior to calculating the full predictive distribution.",1985,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1701969,no,undetermined,0
A Statistical Method for Software Quality Control,"This paper proposes a statistical method that can be used to monitor, control, and predict the quality (measured in terms of the failure intensity) of a software system being tested. The method consists of three steps: estimation of the failure intensity (failures per unit of execution time) based on groups of failures, fitting the logarithmic Poisson model to the estimated failure intensity data, and constructing confidence limits for the failure intensity process. The proposed estimation method is validated through a simulation study. A method for predicting the additional execution time required to achieve a failure intensity objective is also discussed. A set of failure data collected from a real-time command and control system is used to demonstrate the proposed method.",1985,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1701964,no,undetermined,0
Software and development process quality metrics,"This paper describes a framework for gathering and reporting software and development process quality metrics, as well as data engineering issues related to data collection and report generation. The framework is described in project-independent terms, including a methodology for applying metrics to a given software project. A key aspect of this application is the use of project milestones predicted by a failure rate model. For the purposes of this paper, a software project is one which delivers a software or system product, and involves between thirty and several hundred developers, testers, and project managers.",1984,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7271280,no,undetermined,0
Development of a High-Performance Stamped Character Reader,"A high-performance optical character reader (OCR) capable of reading low-quality stamped alphanumeric characters, on a metal rod, is described in this paper. Reliable and precise recognition is achieved by means of a special scanner and dedicated recognition module. The scanner revolves around the metal rod and detects characters, thus avoiding rotation of the heavy and long rod. Diffused illumination which produces a clear image, a powerful pattern-matching methodÃ‚Â¿the Multiple Similarity Method (MSM)Ã‚Â¿implemented by the recognition module, and the use of adaptive rescan control results in a correct recognition rate of 99.93 percent. One recognition module can process images from up to five different scanners. The processing speed is about 1.5 s/rod. The design and industrial application of the system are discussed.",1986,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4158705,no,undetermined,0
A Software Package for Statistical Quality Control of Instrumentation,"The software package contains both usual and new acceptance sampling procedures (essentially by variables) especially for instrumentation and automatic test systems. The main newer procedures implemented in this package are: * By attributes: with finite lot sizes, with classification errors * By variables: bilateral test of the mean measurement, nonparametric sequential, Bayes with minimization of the mean global cost, multidimensional measurements on each sample item, with provision for the internal structure of the items. This software package is aimed at implementation on microprocessors or personal computers, for acceptance sampling by variables and for data logging of measurements. The past four years during which the package has been in use, have been very positive, especially for those applications to electronic instrumentation and automatic test systems, where more standard methods were unsatisfactory or unfit.",1984,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5221768,no,undetermined,0
Evaluation of Error Recovery Blocks Used for Cooperating Processes,"Three alternatives for implementing recovery blocks (RB's) are conceivable for backward error recovery in concurrent processing. These are the asynchronous, synchronous, and the pseudorecovery point implementations. Asynchronous RB's are based on the concept of maximum autonomy in each of concurrent processes. Consequently, establishment of RB's in a process is made independently of others and unbounded rollback propagations become a serious problem. In order to completely avoid unbounded rollback propagations, it is necessary to synchronize the establishment of recovery blocks in all cooperating processes. Process autonomy is sacrificed and processes are forced to wait for commitments from others to establish a recovery line, leading to inefficiency in time utilization. As a compromise between asynchronous and synchronous RB's we propose to insert pseudorecovery points (PRP's) so that unbounded rollback propagations may be avoided while maintaining process autonomy. We developed probabilistic models for analyzing these three methods under standard assumptions in computer performance analysis, i.e., exponential distributions for related random variables. With these models we have estimated 1) the interval between two successive recovery lines for asynchronous RB's, 2) mean loss in computation power for the synchronized method, and 3) additional overhead and rollback distance in case PRP's are used.",1984,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5010298,no,undetermined,0
Static Data Flow Analysis of PL/I Programs with the PROBE System,"An experimental data flow analyzer for PL/I programs has been implemented within the PROBE system developed at the GM Research Laboratories. PROBE is an experimental software package that examines the internal structure of PL/I programs in order to expose error-prone design and programming features. This paper describes 1) the algorithms and data structures used by the data flow analyzer, 2) the salient aspects of PL/I usage in the analyzed production-level programs, and 3) the results of the data flow analysis.",1984,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5010259,no,undetermined,0
Dual Gated Nuclear Cardiac Images,"A data acquisition system has been developed to collect camera events simultaneously with continually digitized electrocardiograph signals and respiratory flow measurements. Software processing of the list mode data creates more precisely gated cardiac frames. Additionally, motion blur due to heart movement during breathing is reduced by selecting events within a specific respiratory phase. Thallium myocardium images of a healthy volunteer show increased definition. This technique of combined cardiac and respiratory gating has the potential of improving the detectability of small lesions, and the characterization of cardiac wall motion.",1984,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4333320,no,undetermined,0
Fault Tolerance in Binary Tree Architectures,"Binary tree network architectures are applicable in the design of hierarchical computing systems and in specialized high-performance computers. In this correspondence, the reliability and fault tolerance issues in binary tree architecture with spares are considered. Two different fault-tolerance mechanisms are described and studied, namely: 1) scheme with spares; and 2) scheme with performance degradation. Reliability analysis and estimation of the fault-tolerant binary tree structures are performed using the interactive ARIES 82 program. The discussion is restricted to the topological level, and certain extensions of the schemes are also discussed.",1984,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1676483,no,undetermined,0
A Study of Software Failures and Recovery in the MVS Operating System,"This paper describes an analysis of system detected software errors on the MVS operating system at the Center for Information Technology (CIT), Stanford University. The analysis procedure demonstrates a methodology by which systems with automatic recovery features can be evaluated. Most common error categories are determined and related to the program in execution at the time of the error. The severity of the error is measured by evaluating the criticality of the program for continued system operation. The system recovery and error correction features are then analyzed and an estimate of the system fault tolerance to errors of different levels of severity is made.",1984,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1676482,no,undetermined,0
A Performability Solution Method for Degradable Nonrepairable Systems,"An algorithm is developed for solving a broad class of performability models wherein system performance is identified with ""reward."" More precisely, for a system S and a utilization period T, the performance variable of the model is the reward derived from using S during T. The state behavior of S is represented by a finite-state stochastic process (the base model); reward is determined by reward rates associated with the states of the base model. Restrictions on the base model assume that the system in question is not repaired during utilization. It is also assumed that the corresponding reward model is a nonrecoverable process in the sense that a future state (reward rate) of the model cannot be greater than the present state. For this model class, we obtain a general method for determining the probability distribution function of the performance (reward) variable and, hence the performability of the corresponding system. Moreover, this is done for bounded utilization periods. The result is an integral expression which can be solved either analytically or numerically.",1984,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1676479,no,undetermined,0
Design and Evaluation of a Fault-Tolerant Multiprocessor Using Hardware Recovery Blocks,In this paper we consider the design and evaluation of a fault-tolerant multiprocessor with a rollback recovery mechanism.,1984,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1676403,no,undetermined,0
Robust Storage Structures for Crash Recovery,"A robust storage structure is intended to provide the ability to detect and possibly correct damage to the structure. One possible source of damage is the partial completion of an update operation, due to a ""crash"" of the program or system performing the update. Since adding redundancy to a structure increases the number of fields which must be changed, it is not clear whether adding redundancy will help or hinder crash recovery. This paper examines some of the general principles of using robust storage structures for crash recovery. It also describes a particular class of linked list structures which can be made arbitrarily robust, and which are all suitable for crash recovery.",1986,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1676761,no,undetermined,0
Probabilistic Short-Circuit Uprating of Station Strain Bus System - Mechanical Aspects,"Design of strength of station structures and clearances to accommodate forces and motions of strain bus during short circuit faults has been based on an energy method and a pendulum swing out model at Ontario Hydro. The energy method was developed some time ago to calculate the forces in a strain bus at the moment of zero motion following a fault. Several recent extensions of this approach are described in this paper. These include: (i) In-depth comparisons of predictions of peak tension, time of peak tension, and force on the structure with values from experiments on simple bus arrangements. (ii) Studies of insulator static and dynamic properties and incorporation of these values in the program. (iii) Extension of the theory to asymmetric faults and to four conductor busses. This method of calculation has recently been used in the probabilistic analysis of the uprating of Ontario Hydro's Middleport Transformer Station (TS) as well as a number of other design applications. This study demonstrates the overall accuracy of this relatively simple approach and its potential for application to more complex arrangements of strain bus. This paper and three companion papers present the probabilistic uprating process as applied to Middleport TS.",1986,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4307980,no,undetermined,0
A classification model of software comprehension,"Summary form only given. Measurements of understandability and maintainability of software have traditionally been developed of the basis of graph models, lexical counts, or information theory. The research reported proposes a theoretical foundation for a different way of evaluating software that can lead to a system of metrics. A model is provided that is based on human factors and information theory and can serve as a framework for development of cognitive-oriented measures of software quality.<<ETX>>",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11848,no,undetermined,0
Applications of error intensity measures to bearing estimation,"In this paper we will discuss a new class of performance approximations for maximum likelihood type bearing estimation systems. This class is based on the application of various point process models to a sequence of error prone points along the likelihood trajectory. The point process model is described by two quantities: the intensity function of the local maxima locations over the parameter space, and a selection rule for the global maximum from the set of local maxima. This class gives new estimators to the Mean-Square Error and specializes to the approximation techniques of [1] and [6].",1987,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1169633,no,undetermined,0
Risk Assessment Methodology for Software Supportability (RAMSS),"Concerns the Risk Assessment Methodology for Software Supportability (RAMSS), the overall approach being tested for use by the US Air Force Operational Test and Evaluation Center. Although the RAMSS is not a mature methodology, elements of the methodology have been used since 1979. The derived baseline database of maintenance actions across 80 systems and more than 300 block releases is being further developed, and evaluation metrics, techniques and procedures are being refined. Risk regression models have been derived for the estimated risk based on productivity factors and a historical maintenance database, and the evaluated risk based on supportability quality criteria and a historical evaluation database. The RAMSS has been derived for the Air Force, but the elements of the methodology apply to any software maintenance application which is based on the concept of version releases",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10177,no,undetermined,0
Production process modeling of software maintenance productivity,"Reports on efforts to develop and estimate a preliminary model of the software production process using pilot data from 65 software maintenance projects recently completed by a large regional bank's data processing department. The goals are to measure factors that affect software maintenance productivity, to integrate the quality and productivity dimensions of software measurements, and to examine the productivity of entire projects rather than only the programming phase, which typically accounts for less than half the effort on a software project. Variables relating to the quality of labor employed on the projects are included. To investigate the set of potential productivity factors, the technique of data envelopment analysis (DEA) is used to estimate the relationship between the inputs and products of software maintenance. The general approach to this research is to model software development as a microeconomic production process utilizing inputs and producing products",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10176,no,undetermined,0
Error localization during software maintenance: generating hierarchical system descriptions from the source code alone,"An empirical study is presented that investigates hierarchical software system descriptions that are based on measures of cohesion and coupling. The study evaluates the effectiveness of the hierarchical descriptions in identifying error-prone system structure. The measurement of cohesion and coupling is based on intrasystem interaction in terms of software data bindings. The measurement of error-proneness is based on software error data collected from high-level system design through system test; some error data from system operation are also included. The data bindings software analysis and supporting tools are described, followed by the data analysis, interpretations of the results, and some conclusions",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161,no,undetermined,0
An expert system for high voltage discharge tests,"Technical advances and reductions in cost of computer hardware and software have reached the point where personal computers can mimic the behavior of experts in assisting in both the performance of complex testing procedures, and in diagnosing the significance of the results. The computer program described here acts as an expert assisting in the performance and diagnosis of the results of discharge tests.",1987,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7736535,no,undetermined,0
"Towards a constructive quality model. Part 1: Software quality modelling, measurement and prediction","This paper considers the approach taken by the ESPRIT-funded REQUEST project to measuring, modelling and predicting software quality. The paper describes previous work on defining software quality and indicates how the work has been used by REQUEST to begin the formulation of a constructive quality model (COQUAMO). The paper concludes that the original goal of the project to produce a predictive quality model was unrealistic, but that a quality system incorporating prediction, analysis and advice is feasible.",1987,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4807894,no,undetermined,0
Test Data Selection and Quality Estimation Based on the Concept of Essential Branches for Path Testing,"A new coverage measure is proposed for efficient and effective software testing. The conventional coverage measure for branch testing has such defects as overestimation of software quality and redundant test data selection because all branches are treated equally. These problems can be avoided by paying attention to only those branches essential for path testing. That is, if one branch is executed whenever another particular branch is executed, the former branch is nonessential for path testing. This is because a path covering the latter branch also covers the former branch. Branches other than such nonessential branches will be referred to as essential branches.",1987,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1702251,no,undetermined,0
Queueing Analysis of Fault-Tolerant Computer Systems,"In this paper we consider the queueing analysis of a fault-tolerant computer system. The failure/repair behavior of the server is modeled by an irreducible continuous-time Markov chain. Jobs arrive in a Poisson fashion to the system and are serviced according to FCFS discipline. A failure may cause the loss of the work already done on the job in service, if any; in this case the interrupted job is repeated as soon as the server is ready to deliver service. In addition to the delays due to failures and repairs, jobs suffer delays due to queueing. We present an exact queueing analysig of the system and study the steady-state behavior of the number of jobs in the system. As a numerical example, we consider a system with two processors subject to failures and repairs.",1987,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1702223,no,undetermined,0
"A passenger vehicle onboard computer system for engine fault diagnosis, performance measurement and control","This paper describes a microprocessor based on-board instrumentation system with applications to engine fault diagnosis, performance measurement and control. A non contacting measurement of engine angular position, taken at the flywheel by means of a magnetic sensor, provides a signal proportional to crankshaft angular velocity. The signal is then appropriately digitized and interfaced to a microprocessor, along with timing information regarding engine position. Suitable processing of the signal by the microprocessor provides information in real time regarding crankshaft speed and torque nonuniformity, which is employed to detect engine faults and to yield a measure of engine performance. Experimental verification of a prototype instrument has been carried out on the road in a number of vehicles, with very successful results. A variety of experiments have confirmed the usefulness of the system for applications in fault diagnosis and performance measurement, and the potential of the system for some aspects of engine control.",1987,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1623585,no,undetermined,0
The Circulation and Water Level Forecast Atlas: A New Product in Development at NOS,"The National Ocean Service (NOS) is developing a new product aimed at improving the prediction of water level and currents in key waterways of the united States. The Circulation and Water Level Forecast Atlas will provide a series of charts and graphs for predicting the tide and tidal current anywhere in a particular estuary, and for providing information on the effects of wind, barometric pressure, and river discharge. The atlas will be produced using a numerical hydrodynamic model implemented for a particular estuary using an extensive oceanographic and meteorological data set. The atlas will represent the circulation at many more locations than the traditional NOS ""Tidal Current Charts,"" which represent currents only at those locations where current data have been obtained and analyzed. The additional locations will include those of special navigational or environmental interest where current measurements cannot be made using traditional moored current meters. The additional spatial coverage resulting from the use of a numerical model will be especially evident in the atlas' tidal height charts, which will depict the changing water level over the entire waterway for each hour of the tidal cycle. On each chart coheight lines will traverse the entire waterway; tide predictions will no longer be limited to a few coastal locations. Use of a numerical hydrodynamical model, combined with the results of statistical data analyses, will also allow the synthesis of important meteorological and river effects in a way that will permit factors be applied to the tide or tidal current prediction based on wind, pressure, or river discharge information available to a user. In a waterway where NOS real-time water level gages are in operation, the atlas will supplement the NOS data retrieval software package called TIDES ABC, by providing a means to extrapolate the real-time data from these gages to other locations in the waterway. The first Circulation and Water Level Forecast At- - las is planned for Delaware River and Bay, where NOS conducted a 2 1/2-year circulation survey. The observations from that survey will be used to implement and assess the skill of the high-resolution numerical model that will be used to produce the atlas.",1987,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1160716,no,undetermined,0
State Estimation Simulation for the Design of an Optimal On-Line Solution,"A state estimation simulation package (SES) developed for studies and educational purposes was further upgraded with on-line capabilities and implemented as a production grade program into a large-scale network control system. Although conceived for different applications and run-time environments, this transformation was realized smoothly and relative quickly without modifying the algorithmic kernel; the result is an efficient state estimation package (SE) with real-time capabilities. The benefits of this development strategy are: - feasibility studies such as optimal metering are possible with a unified SE approach in the early phase of a new control center project; - the existence of a thoroughly tested SE simulation package accelerates the debugging phase of the on-line algorithm; - the joint effort speeds up the know-how transfer and feed-back between R&D and reduces the overall costs. A short description of the SES structure is given. The requirements and design goals of an on-line SE are outlined. Special emphasis is laid upon functional extensions such as: data base interfaces, the mapping of the network connectivity at equipment (terminal) level into a node/branch model and vice-versa, man-machine-interfaces, etc. Methods for software-quality enhancement, performance measurements and results obtained with the on-line SE on a VAX 11/780 are presented.",1986,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4334882,no,undetermined,0
A multiprocessor system for AUV applications,"Autonomous Underwater Vehicles (AUV's) require high processing throughput and fault tolerance combined with low power and volume. High throughput is directly related to mission complexity and the degree of vehicle autonomy. Complex acoustic and optical sensors must be interpreted in real time and decisions based on uncertain data must be formulated and executed. Unexpected occurrances including environmental anomalies and enemy threats force immediate action and revised tactical mission planning. The development of the Autonomous Command and Control Demonstration (ACCD) system as a component development of the Autonomus Underwater Vehicle (AUV) program by Lockheed Advanced Marine Systems (AMS) has estimated peak rates greater than 100 MIPS are required for Command and Control processing. The processing itself will be a combination of signal, algorithmic and symbolic processing. Both deterministic and probabalistic algorithms will require computation. For vehicle operations in a fully autonomous mode, any component failures must be corrected by an automated fault tolerant system. The AUV software architecture, developed as part of a detailed analysis, established the performance requirements for the Command and Control hardware. The characteristics of various hardware architectures such as distributed, common memory, data flow, systolic, and connection type systems were established and assessed to determine the hardware best suited to the software structure. A heterogeneous hypercube was found to be optimum. The hypercube is reconfigurable for fault tolerance, is flexible, and can achieve high throughputs with low power and without special cooling systems. AMS will use a commercially-available hypercube as the basis of their Command and Control Architecture for an Unmanned Underwater Vehicle. This hypercube has a 32 bit architecture, extremely high message passing capability and will have image processing accelerators working in conjunction with standard cube nodes. Both symbolic and algorithmic processing will take place on the standard nodes. In this paper, system requirements, software hierarchy, computer architecture analysis and the resulting hypercube design will be discussed in detail.",1987,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1158567,no,undetermined,0
Knowledge-based management of failures in autonomous systems,"This paper presents the design of a fault management system (FMS) for an unmanned and untethered platform. The system must automatically detect, diagnose, localize and reconfigure the system to cope with failures. Traditional fault tolerant approaches used in telephone switching, manned and unmanned satellites, commercial banking, airline reservations, air traffic control, and others are reviewed. Expert system's technology is used to extend these traditional approaches to achieve a highly reliable design capable of sustaining operation over many months with little or no communication. An existing simulator has been modified to allow fault injection and to model fault propagation. This provides a testbed for evaluating system candidates. A specific fault management hardware and software architecture has been selected. Expert system diagnostic rules, which run on the fault tolerant base, are discussed. Diagnostic rule performance in detecting, localizing, and recovering from Autonomous Systems (AS) sensor, actuator, and computer subsystem failures during AS operation is analyzed.",1987,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1158558,no,undetermined,0
Modeling very large array phase data by the Box-Jenkins method,"The quality of radio astronomical images made with an antenna array depends upon atmospheric behavior. As baselines and frequencies increase, phase variations become increasingly erratic. The phase fluctuations are time dependent and we found them to be correlated In time order in each baseline. We can represent these correlations by stochastic models. Models obtained by the Box-Jenkins method are referred to as auto-regressive integrated moving average processes (ARIMA). ARIMA models of VLA phase provide good short-term predictions that may be useful for improving present calibration techniques. ARIMA models of VLA phase are data dependent and can be used in a variety of situations. A technique that works in all cases can be programmed into a software package such that modeling can be accomplished with no operator interactions. Another important application of ARIMA models involves the use of Kalman filtering to reduce the atmospheric effects when self-calibration does not work well. The performance of the Kalman filter critically depends upon the models of the processes. An ARIMA model of the phase fluctuation can be represented in a state space form as noise in the Kalman filter equations.",1986,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7774373,no,undetermined,0
A functional approach to program testing and analysis,"An integrated approach to testing is described which includes both static and dynamic analysis methods and which is based on theoretical results that prove both its effectiveness and efficiency. Programs are viewed as consisting of collections of functions that are joined together using elementary functional forms or complex functional structures. Functional testing is identified as the input-output analysis of functional forms. Classes of faults are defined for these forms, and results are presented which prove the fault-revealing effectiveness of well defined sets of tests. Functional analysis is identified as the analysis of the sequences of operators, functions, and data type transformations which occur in functional structures. Theoretical results are presented which prove that it is only necessary to look at interfaces between pairs of operators and data type transformations in order to detect the presence of operator or data type sequencing errors. The results depend on the definition of normal forms for operator and data type sequencing diagrams.",1986,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6313016,no,undetermined,0
Experiments in software reliability: Life-critical applications,"Digital computers are being used more frequently for process control applications in which the cost of system failure is high. Consideration of the potentially life-threatening risk, resulting from the high degree of functionality being ascribed to the software components of these systems, has stimulated the recommendation of various designs for tolerating software faults. The author discusses four reliability data gathering experiments which were conducted using a small sample of programs for two problems having ultrareliability requirements: <i>n</i>-version programming for fault detection, and repetitive run modeling for failure and fault rate estimation. The experimental results agree with those of M. Nagel and J.A. Skrivan (1982) in that the program error rates suggest an approximate log-linear pattern and the individual faults occurred with significantly different error rates.",1986,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6312925,no,undetermined,0
Field monitoring of software maintenance,"Life cycle maintenance cost must be predicted for any new product. Correct management practice then requires the prediction to be checked by measurement, so that planning and budgets can be adjusted. This requires knowledge of the nature of the market, quality of the product and activities of the service organisation, and how they mutually interact. This paper describes this interaction, and the problems of data collection peculiar to each of the three areas. A relational database structure is defined to enforce consistency on the data. Particular attention is paid to the problems of having different copies of the software in different states of repair, having several versions of several products in use simultaneously, and recording running time in order to be able to measure reliability.",1986,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4807668,no,undetermined,0
Design of a Testbed for Studying Rapidly Reconfigurable Tactical Communication Networks,"This paper describes the design of a testbed for studying rapidly reconfigurble ground and airborne tactical communication networks which use directed beam couplings between nodes. The networks are subject to rapid changes in connectivity and topology caused by links fading in and out or by mobile users moving into and out of the directed beams. Such networks, which provide improved security, jam resistance and probability of intercept, are viable candidates for tactical use in the 1990s. The testbed has been designed around three major subsystems: the prototype network equipment, the monitor, and the stimulator. The prototype network equipment consists of six nodes, with all required software, and direct wire connections between nodes. The monitor subsystem records the significant arrival and departure times for each packet at each node. It stores this data and processes it to determine appropriate performance parameters. The stimulator subsystem generates packets and injects bit errors and link and node faults into the prototype network. The testbed will be used to develop network architectures and protocols for constructing and reconfiguring networks and to study new types of adaptive routing algorithms and error control. Most of the planned studies will make use of the ability of the network to measure transient network behavior.",1986,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4805837,no,undetermined,0
Effect of System Processes on Error Repetition: A Probabilistic and Measurement Approach,"This paper presents an approach to system reliability modeling where failures and errors are not statistically independent. The repetition of failures and errors until their causes are removed is affected by the system processes and degrades system reliability. Four types of failures are introduced: hardware transients, software and hardware design errors, and program faults. Probability of failure, mean time to failure, and system reliability depend on the type of failure. Actual measurements show that the most critical factor for system reliability is the time after occurrence of a failure when this failure can be repeated in every process that accesses a failed component. An example involving measurements collected in an IBM 4331 installation validates the model and shows its applications. The degradation of system reliability can be appreciable even for very short periods of time. This is why the conditional probability of repetition of failures is introduced. The reliability model allows prediction of system reliability based on the calculation of the mean time to failure. The comparison with the measurement results shows that the model with process dependent repetition of failures approximates system reliability with better accuracy than the model with the assumption of independent failures.",1986,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4335527,no,undetermined,0
On selectivity properties of discrete-time linear networks,"A definition of the quality factor <img src=""/images/tex/6.gif"" alt=""Q""> and the resonant angle <img src=""/images/tex/5334.gif"" alt=""\theta_{0}""> for discrete-time networks is presented. The definitions are based on the bilinear transform. Using this transform, the selectivity properties for discrete-time networks, such as switched capacitor (SC) and digital filters, are consistent with analog networks. Expressions for <img src=""/images/tex/6.gif"" alt=""Q""> and <img src=""/images/tex/5334.gif"" alt=""\theta_{0}""> in terms of polar coordinate parameters <img src=""/images/tex/130.gif"" alt=""r""> and <img src=""/images/tex/10070.gif"" alt=""\theta_{p}""> for a complex-conjugate pole pair and <img src=""/images/tex/4835.gif"" alt=""x_{1}""> and <img src=""/images/tex/5085.gif"" alt=""x_{2}""> for real-axis poles are given as well as the inverse relations. Canonic expressions for generic forms of a biquadratic function are also given. Finally, an accurate method for measurement of the parameters in second-order SC-filters and digital filters implemented in software is described.",1984,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1085494,no,undetermined,0
Multivariate Sampling Plans in Quality Control: A Numerical Example,"This paper presents an attributes acceptance sampling procedure where several measurements are available on each item in the sample set. It is thus especially relevant when these measurements on each item are correlated; usual procedures cannot account for correlations. Correlations are considered by using a classification rule to assign the attribute good or bad to each item in the sample set; this rule uses the non-parametric k-nearest neighbor estimator. The test by which the lot is accepted or rejected, based on the outcomes of the rule for all items in the sample set, is the same as in acceptance sampling by attributes. The estimator has only to be compensated for errors at the level of the classification rule. This procedure has been used since 1978 in industry, thanks to a software package which implements it. Applications have mostly been to process quality control and mechanical parts. The procedure has the interesting feature of applying under relaxed assumptions on the distributions of the measurements on the bad items. The numerical example covers such a case. Because the classification rule operates individually on each sample item, nothing is changed in the definitions of the acceptable quality level and the limiting quality level. They apply to the whole lot. This is true because the multivariate measurements on each item are aggregated into a binary attribute on each item, as in classical sampling by attributes.",1983,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5221685,no,undetermined,0
Software Performance Modeling and Management,"This paper addresses methods to assess the impact of software on weapon system performance parameters such as reliability and operability/suitability. The latter is emphasized in major weapon-system go-ahead decisions. This paper discusses a system reliability model primarily intended to provide management insight and guidelines for identifying out-of-tolerance situations and needed corrective actions. Guidelines are discussed for judging if ``independent verification and test'' and ``weapon-system proof-of-compliance testing'' are successful. Guidelines are provided for comparing software and hardware in terms of total valid problems reported, resolution rates, and comparable difficulty of implementing and verifying the resolutions. This is done with respect to severity levels in MIL-STD-1679. The management of the operability/suitability issue is discussed and recommendations are made to both the procuring agency and the prime contractor. Software is an attractive medium in comparison to hardware in implementing complex functions because: a) there are more controllable means to reduce severe software defects, and b) it is easier to effect change. Properly managed software will have minimal difficulties with system reliability and operability/suitability. Proper software management includes: a) the application, during development, of proper design tools such as top-down design, structural programming, and programming teams; b) aggressive, independent testing and problem tracking activities; and c) application of the management elements presented in this paper. Such application requires contractor familiarity with user needs and capabilities as well as with the mission and operations of the system, so as to optimize the man/machine interface.",1983,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5221654,no,undetermined,0
Safeguard data-processing system: Maintenance and diagnostic subsystem,"The Safeguard Maintenance and Diagnostic Subsystem (M & DSS) is a unique, independent, hardware group within the data-processing system through which the nonreal-time functions of fault detection and isolation are performed. In this paper, the M & DSS hardware and fault detection software are described and system performance is reviewed.",1975,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6778879,no,undetermined,0
Algorithms for Monitoring Changes in Quality of Communication Links,"The automation of network service observing through message classification software permits a great increase in the rate at which observations can be made, thus allowing the monitoring of network links as well as switching nodes. This paper develops and analyzes a quality control algorithm which is suitable for monitoring the performance of direct (end-office to endoffice) links via high-volume service observing data. This algorithm processes a sample of successful as well as unsuccessful call attempts sequentially and in real time to quickly and effectively detect deteriorations in the performance of a link. Discussion of the performance qualities and the robustness of this algorithm is presented. The discussion is supported by quantitative results.",1979,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1094443,no,undetermined,0
A survey and methodology of reconfigurable multi-module systems,"<div style=""font-variant: small-caps; font-size: .9em;"">First Page of the Article</div><img class=""img-abs-container"" style=""width: 95%; border: 1px solid #808080;"" src=""/xploreAssets/images/absImages/00810568.png"" border=""0"">",1978,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=810568,no,undetermined,0
Controlling the Software Life Cycleâ€”The Project Management Task,"This paper describes project management methods used for controlling the life cycle of large-scale software systems deployed in multiple instaltations over a wide geographic area. A set of management milestones is offered along with requirements and techniques for establishing and maintaining control of the project. In particular, a quantitative measure of software quality is proposed based upon functional value, availability, and maintenance costs. Conclusions drawn are based on the study of several cases and are generally applicable to both commercial and military systems.",1978,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1702542,no,undetermined,0
Evaluation of Maintenance Software in Real-Time Systems,"Statistical theory can be applied to data collected by physical fault insertion of small, random fault samples to estimate the critical parameters of software responsible for fault recovery, detection, and resolution in real-time systems. Estimates of the critical parameters can be used to determine if system reliability objectives are satisfied. This method of software evaluation, modified because of the impracticality of selecting and physically inserting truly random fault samples, was verified against a diagnostic/ Trouble Locating Program (TLP) whose critical parameters were known and was then used to evaluate and optimize a new diagnostic/ TLP program of unknown quality. Empirical evidence indicates that there is a close correlation between system performance against real faults and against the selected subset of real faults used for sampling.",1978,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1675151,no,undetermined,0
Redundancy management of shuttle flight control sensors,"Aerospace programs such as the Space Shuttle increasingly demand that avionics be fault tolerant. This paper describes the concept used for sustaining two system failures in the Shuttle Flight Control System (FCS) using redundancy management of sensors as the specific area of emphasis. Moreover, an analytical approach is described which is capable of verifying performance on a statistical basis, addressing performance requirements concerned with ?? Avoidance of nuisance failures. ?? Detectability of faults. ?? Maximum vehicle transients incurred as a result of a failure. This approach will be the basis for preflight verification of Space Shuttle FCS redundancy management implemented in software.",1976,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4045637,no,undetermined,0
Fault-Tolerant Computing: A Introduction,"THE field of fault-tolerant computing is concerned with the analysis, design, verification, and diagnosis of computing systems that are subject to faults. A ""computing system,"" in this general context, can be a hardware system, a software system, or a computer which includes both hardware and internal software. A ""fault"" can reside in either hardware or software and can occur in the process of designing and implementing the system, or in the process of using the system once it is implemented. Major areas of technical interest include: 1) the design and analysis of computers which are able to execute specified algorithms correctly (according to specified correctness criteria) in the presence of hardware and/or software faults; 2) the testing and verification of the initial correctness of hardware and software systems prior to utilization; 3) the design and implementation of on-line fault detection, fault location, and system reconfiguration procedures that can be used to recover from hardware and software faults, to perform system maintenance, and to maintain security; and 4) the development of models, measures, and techniques for evaluating the reliability, availability and, in general, the effectiveness of fault-tolerant computing systems.",1976,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1674654,no,undetermined,0
Some preliminary experiments in the recognition of connected digits,"This paper describes an implementation of a speaker independent system which can recognize connected digits. The overall recognition system consists of two separate but interrelated parts. The function of the first part of the system is to segment the digit string into the individual digits which comprise the string; the second part of the system then recognizes the individual digits based on the results of the segmentation. The segmentation of the digits is based on a voiced-unvoiced analysis of the digit string, as well as information about the location and amplitude of minima in the energy contour of the utterance. The digit recognition strategy is similar to the algorithm used by Sambur and Rabiner [1] for isolated digits, but with several important modifications due to the impreciseness with which the exact digit boundaries can be located. To evaluate the accuracy of the system in segmenting and recognizing digit strings a series of experiments was conducted. Using high-quality recordings from a soundproof booth the segmentation accuracy was found to be about 99 percent, and the recognition accuracy was about 91 percent across ten speakers (five male, five female). With recordings made in a noisy computer room the segmentation accuracy remained close to 99 percent, and the recognition accuracy was about 87 percent across another group of ten speakers (five male, five female).",1976,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1162794,no,undetermined,0
Microprocessor Controlled Data Acquisition Systems,"The low cost and flexibility of microcomputers have created .a burst of activity in intelligent data acquisition systems. Radio frequency surveillance, electronic warfare systems, and navigational control are a few of the more visible applications. The power of well-designed microcomputers, when coupled with present day electronic detection and control equipment, is creating a state-of-the-art technological breakthrough in system designs. Noise control, pollution analysis, environmental quality in general, radar tracking and control, radio frequency analysis and interference detection are all candidates for distributed data acquisitions systems. This paper describes the use of microprocessors to control a radio frequency detection system.",1975,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7567877,no,undetermined,0
Concurrent software fault detection,"A graph theoretic model for software systems is presented which permits a system to be characterized by its set of allowable execution sequences. It is shown how a system can be structured so that every execution sequence affected by a control fault is obviously in error, i.e., not in the allowable set defined by the system model. Faults are detected by monitoring the execution sequence of every transaction processed by the system and comparing its execution sequence to the set of allowable sequences. Algorithms are presented both for structuring a system so that all faults can be detected and for fault detection concurrent with system operation. Simulation results are presented which support the theoretical development of this paper.",1975,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6312823,no,undetermined,0
Interval Reliability for Initiating and Enabling Events,"This paper describes generation and evaluation of logic models such as fault trees for interval reliability. Interval reliability assesses the ability of a system to operate over a specific time interval without failure. The analysis requires that the sequence of events leading to system failure be identified. Two types of events are described: 1) initiating events (cause disturbances or perturbations in system variables) that cause system failure and 2) enabling events (permit initiating events to cause system failure). Control-system failures are treated. The engineering and mathematical concepts are described in terms of a simplified example of a pressure-tank system. Later these same concepts are used in an actual industrial application in which an existing chlorine vaporizer system was modified to improve safety without compromising system availability. Computer codes that are capable of performing the calculations, and pitfalls in computing accident frequency in fault tree analysis, are discussed.",1983,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5221514,no,undetermined,0
Methods for the Determination of Specific Organic Pollutants in Water and Waste Water,"This paper reviews methods for sample collection and pretreatment and for isolation and determination of specific organic compounds with emphasis on methods for determination. Methods for organochlorine, organophosphorus, and organonitrogen pesticides and phenoxy acid herbicides are presented. Methods for the determination of other chlorinated organics such as polychlorinated biphenyls, organic solvents, and other selected organic compounds are also presented. Gas chromatography is the most widely applicable and popular method for detecting and measuring specific organic compounds in water, waste water, and other environmental media. When coupled with selective detectors, gas chromatography becomes the most sensitive and selective method for qualitative and quantitative determination of organic compounds that is available. The recent development and application of the computer-controlled gas chromatograph-mass spectrometer adds a much needed extra dimension to organic analysis, that of unequivocal identification of compounds that can be only tentatively identified by other means. Gas chromatographic methods are selectively applied either directly on the raw sample or after concentration by adsorption or solvent extraction and evaporation. A variety of other determinative methods such as infrared, ultraviolet, and fluorescent spectroscopy; liquid chromatography; and thin-layer chromatography have application in the broad spectrum of organic analysis. Selected examples of the application of these methods are presented.",1975,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4327765,no,undetermined,0
Fault-Tolerant Computing: An Introduction and a Perspective,"FAULT-TOLERANT computing has been defined as ""the ability to execute specified algorithms correctly regardless of hardware failures, total system flaws, or program fallacies"" [1]. To the extent that a system falls short of meeting the requirements of this definition, it can be labeled a partially fault-tolerant system [2]. Thus the definition of fault-tolerant computing provides a standard against which to measure all systems having a degree of fault tolerance. In particular, one can classify systems according to: 1), the amount of manual intervention required in performing three basic functions, and 2) the class of faults covered by three basic functions involved in fault tolerance: system validation, fault diagnosis, and fault masking or recovery. The word ""fault"" here is used to inclusively describe ""failures, flaws, and fallacies"" in the original definition. The first function is involved in the design and production of the system hardware and software, while the last two functions are embodied in the system itself. Likewise, the first function is directed to handling faults arising from design and production errors, whereas the last two functions are aimed at faults due to random hardware failures.",1975,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1672839,no,undetermined,0
Computer integration in the Kieler Multisonde,"The Multisonde is capable of measuring six parameters simultanously, which shall be exten- ded to an amount of sixteen parameters. Possible higher data rates can only be managed by using a digital magnetic tape unit and the easiest way to couple it to the Multisonde is by a minicomputer.This enables also an immediate check of the tape after having registrated one profile, and so prevents that a possible interface defect, leading to an incorrect recording, is not detected during the entire expedition. This paper discusses a real-time computer integrated to the multisonde for faster data processing.",1974,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1161424,no,undetermined,0
Associative memories and processors: An overview and selected bibliography,"A survey of associative processing, techniques is presented, together with a guide to the published literature in this field. Some familiarity with the basic concepts of associative-processing is assumed. The references have been divided into four groups dealing with architectural concepts, hardware implementation, software considerations, and application areas. The discussion of architectural concepts consists of a classification of associative devices into four major categories (fully parallel, bit-serial, word-serial, and block-oriented) and an enumeration of techniques for dealing with multiple responses and hardware faults. With respect to hardware implementation, considerations are given to the basic operations implemented, hardware elements used (e.g., cryoelectrics, magnetic elements, and semiconductors), and physical characteristics such as speed, size, and cost. The discussion of software aspects of associative devices deals with synthesis of algorithms, programming problems, and software simulation. The application areas discussed include solution of some mathematical systems, radar signal processing, information storage and retrieval, and performance of certain control functions in computer systems.",1973,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1451082,no,undetermined,0
Simulation modeling for air quality control,"Simulation modeling will have a major role in air quality programs for forecasting, developing both temporary and long-range controls and planning regional land use. Such air quality models use pollutant emissions and meteorological measurements to simulate concentrations of atmospheric pollutants. Box-and Gaussian-type models have serious short comings, but the new generation models (such as NEXUS) are approaching an adequate resolution and accuracy for routine applications. The NEXUS model simulated pollutant concentrations within 20 percent of the observed values.",1971,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4044830,no,undetermined,0
Computer Diagnosis Using the Blocking Gate Approach,"In previous papers [3]â€“[5] the authors considered the application of graph theory to represent and analyze a computer system. From such analysis of the graph (thus the system), we have shown that faults can be detected and located by means of strategically placed test points within the system.",1971,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1671723,no,undetermined,0
Recognition of Handprinted Numerals by Two-Stage Feature Extraction,"An optical character recognition system for handprinted numerals of noisy and low-resolution measurement is proposed. The system consists of the two-stage feature extraction process. In the first stage a set of primary features insensitive to the quality and format of a black-white bit pattern are extracted. In the second stage, a set of properties capable of discriminating the character classes is derived from primary features. The system is simple and reliable in that only three kinds of primary features are needed to be detected. The recognition is based on the decision tree which tests the logic statements of secondary features.",1970,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4082309,no,undetermined,0
A Very High Speed Electro-Optical-Mechanical Phototypesetting Machine,"Adanced-performance automated phototypesetting systems which have been introduced to the market recently fall into two general classificationsÃ‚Â¿the electro-optical-mechanical and the all-electric configurations. Each category has its own advantages in performance, price, and application. This paper describes an advanced electro-optical-mechanical machine, now in field test, which has many important advantages over existing machines: speed, exceedingly high font capacity, very high production/cost ratio, and excellent quality of type production, reliability, and versatility. These advantages are gained by designing a built-in digital computer to perform many of the mechanical and optical functions ordinarily associated with existing machines. During the exposure of a line of type, no optical components are in motion and only one mechanical assembly is in relatively slow continuous rotary motion. (See Fig. 3 of the text for the system configuration). The extremely large font capacity makes the machine ideal for typesetting oriental languages and complex scientific publications.",1968,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4322332,no,undetermined,0
Self-Testing Computers,"Built-in-test techniques exploit hardware redundancy to provide continuous on-line monitoring of computer performance. Hardware's declining cost makes these techniques attractive, especially for modular computers.",1979,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1658498,no,undetermined,0
Measuring Improvements in Program Clarity,"The sharply rising cost incurred during the production of quality software has brought with it the need for the development of new techniques of software measurement. In particular, the ability to objectively assess the clarity of a program is essential in order to rationally develop useful engineering guidelines for efficient software production and language development.",1979,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1702601,no,undetermined,0
Software Failure Modes and Effects Analysis,"This concept paper discusses the possible use of failure modes and effects analysis (FMEA) as a means to produce more reliable software. FMEA is a fault avoidance technique whose objective is to identify hazards in requirements that have the potential to either endanger mission success or significantly impact life-cycle costs. FMEA techniques can be profitably applied during the analysis stage to identify potential hazards in requirements and design. As hazards are identified, software defenses can be developed using fault tolerant or self-checking techniques to reduce the probability of their occurrence once the program is implemented. Critical design features can also be demonstrated a priori analytically using proof of correctness techniques prior to their implementation if warranted by cost and criticality.",1979,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5220578,no,undetermined,0
Technical Control of the Digital Transmission Facilities of the Defense Communications System,"As an aid in evaluating technical control techniques, an emulation facility that automatically performs the status monitoring, performance assessment, and fault isolation technical control functions as they apply to a future, predominantly digital Defense Communications System (DCS) has been developed. This emulation facility is a multicomputer system which automatically monitors and isolates faults for digital transmission equipments (power generators, RF distributions, digital radios, second level multiplexers, first level multiplexers, submultiplexers, and key generator units). The status monitoring and performance assessment functions are performed by two processors, the adaptive channel estimator (ACE) and an LSI 11/03, the composite being referred to as the CPMAS-D unit. When the software residing in the CPMAS-D unit detects a monitor point transition (alarm to/from nonalarm), it transmits the monitor point information to the CPMAS emulator, a PDP 11/60 minicomputer. These messages, called exception reports, enable the CPMAS emulator to perform its prime mission: fault isolation. A unique fault isolation algorithm has been developed for test with this emulation facility. The algorithm consists of three discrete steps. First, the equipment alarms are mapped into their effect upon each transmission path (link, supergroup, group, or channel). Second, the stations with the faulty equipment are located by deleting the impact of sympathetic alarms. Third, the faulty equipment is identified using the equipment alarm status. Testing of the fault isolation algorithm is enhanced by an emulated network consisting of up to 16 stations, 2048 equipments, and two nodal control areas. Monitor point simulators and T1-4000 multiplexers, which provide simulated and real inputs to two CPMAS-D units, are also part of the emulation facility. Technical control terminals are provided to evaluate man-machine operation in an automated technical control environment. An adaptive channel estimator (ACE) field test was conducted at the Rome Air Development Center (RADC) test facilities so that the ACE development unit could be evaluated as a means of assessing performance of high-speed digital radio transmission systems. Also, the fault isolation algorithm was tested using the pr- eviously described emulation facility. The test data and subsequent technique evaluations are discussed in this paper.",1980,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1094833,no,undetermined,0
Distributed Circuit Control for the DCS,"Processor-controlled time slot interchange equipments now in production for commercial applications could provide the cross-connect switching element of a Distributed Circuit Control (DCC) capability for a digital Defense Communications System (DCS). Real time automated rerouting of circuits around wartime damage could be effected by employing a routing algorithm implemented in software to generate channel cross-connect instructions for the time slot interchange hardware. Three candidate algorithms, designed originally for call or packet routing, were modified for routing and rerouting end-to-end dedicated circuits under the constraints of the National Communications System (NCS) restoration priority system. This paper focuses on the network modeling effort that was undertaken to provide quantitative data to support a comparative evaluation of the candidate algorithms. Predicted performance data are presented for the modified algorithms applied to a set of symmetric networks and operated in both allocation (routing) and trunk-fail (rerouting) modes. Parameters measured included circuit reroute time, quality of the routes, stability of the algorithm, network resources required, and processor sizing.",1983,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4794712,no,undetermined,0
Effect of the Software Coincidence Timing Window in Time-of-Flight Assisted Positron Emission Tomography,"The effect of the software coincidence timing window (SCW) has been studied and demonstrated experimentally using Super PETT I, a time-of-flight (TOF) assisted positron emission tomograph. This method utilizes the TOF information to eliminate unwanted random and scattered coincidence events outside of a region of interest. The effect is substantial for high count rate studies, and can dramatically improve the noise contribution from measured attenuation factors. The effective coincidence window is reduced to 3.3 nsec for reconstructions of the field of view, and reduced to only 2 nsec for measurement of the attenuation factors.",1983,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4332362,no,undetermined,0
"Low Contrast Sensitivity of Radiologic, CT, Nuclear Medicine, and Ultrasound Medical Imaging Systems","The physical sensitivity of a medical imaging system is defined as the square of the output signal-to-noise ratio per unit of radiation to the patient, or the information/radiation ratio. This sensitivity is analyzed at two stages: the radiation detection stage, and the image display stage. The signal-to-noise ratio (SNR) of the detection stage is a physical measure of the statistical quality of the raw detected data in the light of the imaging task to be performed. As such it is independent of any software or image processing algorithms which belong properly to the display stage. The fundamental SNR approach is applied to a wide variety of medical imaging applications and measured SNR values for signal detection at a given radiation exposure level are compared to the optimal values allowed by nature. It is found that the engineering falls short of the natural limitations by an inefficiency of about a factor two for most of the individual radiologic system components, allowing for great savings in the exposure required for a given imaging performance when the entire system is optimized. The display of the detected information is evaluated from the point of view of observer efficiency, the fraction of the displayed information that a human observer actually extracts.",1983,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4307624,no,undetermined,0
System Development and Technology Aspects of the IBM 3081 Processor Complex,"The IBM 3081 Processor Complex consists of a 3081 Processor Unit and supporting units for processor control, power, and cooling. The Processor Unit, completely implemented in LSI technology, has a dyadic organization of two central processors, each with a 26-ns machine cycle time, and executes System/370 instructions at approximately twice the rate of the IBM 3033. This paper presents an overview of the advances in technology and in the design process, as well as enhancements in the system design that were associated with the development of the IBM 3081. Application of LSI technology to the design of the 3081 Processor Unit, which contains almost 800,000 logic circuits, required extensions to silicon device packaging, interconnection, and cooling technologies. A key achievement in the 3081 is the incorporation of the thermal conduction module (TCM), which contains as many as 45,000 logic circuits, provides a cooling capacity of up to 300 W, and allows the elimination of one complete level of packagingâ€”the card level. Reliability and serviceability objectives required new approaches to error detection and fault isolation. Innovations in system packaging and organization, and extensive design verification by software and hardware models, led to the realization of the increased performance characteristics of the system.",1982,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5390538,no,undetermined,0
A Remote Data Acquisition and Display System,"A remote data acquisition and display system is described. The system deploys distributed intelligent microprocessors using a structured, high-level language with a simple operating system. The hardware and microprocessor software are modular; thus, the overall system is flexible and can be easily tailored to a wide variety of applications. The remote units accept a variety of analog and digital signals, perform automatic test and calibration of analog inputs, and perform conversion to customary units of measure. Information is digitally multiplexed from the input units to display units which format the data for alphanumeric or graphic displays or for conventional analog meters or recorders. Operator interactions are achieved by means of functional keys on the display or via a maintenance terminal. Software tests and on-line diagnostics are used to detect faults and aid in maintenance and repair.",1982,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4335993,no,undetermined,0
Databases and Units of Measure,"A serious impediment to the integrated use of databases across international boundaries, scientific disciplines and application areas is the use of different units of measure, e.g., miles or kilometers for distance, dollars or rupees for currency, and Btu or kWh for energy. Units of measure are not specified in the data definition language but associated by convention with values in the database. Inconsistent usage of values with different units cannot be checked atuomatically. Retrieval, storage, and manipulation of data values in units other than those associated with the values requires that the user perform the appropriate conversions. This process is inconvenient and error prone. These problems are illustrated by an international database example.",1982,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1702994,no,undetermined,0
A fault-tolerant/fail-safe command and control system for automated vehicles,"Redundancy and fault-tolerant computer technology are being applied to the development of a command and control system for automated vehicles. An ultrareliable command and control system is described which meets the availability and safety requirements for an automated transit system. The technology presented is applicable to a wide variety of computer-based controls where safety is involved or where interruption of the control process cannot be tolerated. High-performance computer-based controls are being developed by OTIS-TTD and Del Rey Systems to control the operation of automated transit systems. The command and control system will allow economical, flexible, personalized service while operating a large number of closely spaced (short headway) vehicles. The requirements for flexible service and short headway operation preclude the use of traditional failsafe design practices and components. To achieve the required performance, reliability, and safety, redundancy and fault-tolerant computer techniques are used. This paper describes how the reliability requirements for command and control systems are achieved through the application of fault tolerant computing. Three alternative computer architectures are described. Reliability analyses have been performed for each candidate architecture, and the results are presented. Based on the reliability analyses, a triple redundant computer is selected. Automatic failure detection and recovery is accomplished by software, thus allowing off-the-shelf hardware to be used.",1982,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1623052,no,undetermined,0
An autocorrelation pitch detector with error correction,"A particularly demanding application of pitch detection is in speech training of the deaf. The prevalance of defects such as breathy, creaky or hoarse voice means that pitch detectors are prone to errors. To overcome this an error correction algorithm has been developed for the Dubnowski, Schafer and Rabiner (D.S.R.) pitch detector. The resulting pitch detector overcomes these problems. It is a real-time procedure which can be implemented in hardware or software. Its performance is comparable with the nonreal-time SIFT algorithm and may thus have wider applications.",1982,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1171721,no,undetermined,0
"Microcomputers: Microprocessors and the M.D.: A new breed of smart medical equipment can diagnose, monitor, analyze, and rehabilitate","Microprocessor-based electrocardiography instruments are helping combat the No.1 killer of people in the United States: heart disease. The new ECG instruments not only capture and display real-time and recorded measurements of heart rates, waveforms, and rhythms, but they also process and transfer the data to a remote site for analysis by a cardiologist or a computer. Analysis can point to problems in the electrical conduction networks, heart muscles, valves or blood flow. The latest ECG equipment uses the microprocessor's software-derived intelligence to diagnose some heart defects or to record and warn of abnormal heart rhythms. Microprocessors are also turning up in other medical applications that are making diagnoses and even medication less prone to error. Among these applications are the following: Bedside monitoring of patient circulatory, respiratory, and central nervous systems, along with metabolic and acid/base control systems. Monitoring of heart signals in ambulatory patients through the recording of data on a tape cassette worn by patients. Feedback control of the breathing of bed patients and the automatic infusion of intravenous fluids. Automatic chemistry analyzers for laboratories that allow blood counts to be made by relatively unskilled personnel.",1981,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6369478,no,undetermined,0
Stochastic Reliability-Growth: A Model for Fault-Removal in Computer-Programs and Hardware-Designs,"An assumption commonly made in early models of software reliability is that the failure rate of a program is a constant multiple of the (unknown) number of faults remaining. This implies that all faults contribute the same amount to the failure rate of the program. The assumption is challenged and an alternative proposed. The suggested model results in earlier fault-fixes having a greater effect than later ones (the faults which make the greatest contribution to the overall failure rate tend to show themselves earlier, and so are fixed earlier), and the DFR property between fault fixes (assurance about programs increases during periods of failure-free operation, as well as at fault fixes). The model is tractable and allows a variety of reliability measures to be calculated. Predictions of total execution time to achieve a target reliability, and total number of fault fixes to target reliability, are obtained. The model might also apply to hardware reliability growth resulting from the elimination of design errors.",1981,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5221099,no,undetermined,0
The Energy Saver/Doubler Quench Protection Monitor System,"The microprocessor based system to detect quenches in the superconducting magnets is described. Tests conducted over the past two years using a string of twenty superconducting magnets have yielded results having major impact on the design of this system. A network of twenty-four 16-bit microprocessors will monitor the integrated voltage across magnet protection units every 60 Hz line period using isolated voltage to frequency converters. These measurements are compared with the inductive voltage expected. Under transient conditions, the distributed L-C nature of the magnets delays signal propagation as in a transmission line. To achieve the required sensitivity of resistive voltage detection, the inductive voltage for each magnet cell is calculated using two of the six measured dI/dt signals. Prevention of faults to the magnet system by the monitoring connections, minimization of cabling, and matched transient response of all monitoring channels have been considered together in design of the hardware and software.",1981,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4332081,no,undetermined,0
Application of Kalman Filtering in Computer Relaying,"During the first cycle following a power system fault, a high speed computer relay has to make a decision usually based on the 60 Hz information, which is badly corrupted by noise. The noise in this case is the nonfundamental frequency components in the transient current or voltage, as the case may be. For research and development purposes of computer relaying techniques, the precise nature of the noise signal is required. The autocorrelation function and variance of the noise signal was obtained based on the frequency of occurrence of the different types of faults, and the probability distribution of fault location. A new technique for modelling the signal and the measurements is developed based on Kalman Filtering theory for the optimal estimation of the 60 Hz information. The results indicate that the technique converges to the true 60 Hz quanitities faster than other algorithms that have been used. The new technique also has the lowest computer burden among recently published algorithms and appears to be within the state of the art of current microcomputer technology.",1981,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4111018,no,undetermined,0
Application of a Methodology for the Development and Validation of Reliable Process Control Software,"This paper discusses the necessity of a good methodology for the development of reliable software, especialy with respect to the final software validation and testing activities. A formal specification development and validation methodology is proposed. This methodology has been applied to the development and validation of a pilot software, incorporating typical features of critical software for nuclear power plant safety protection. The main features of the approach indude the use of a formal specification language and the independent development of two sets of specifications. Analyses on the specifications consists of three-parts: validation against the functional requirements consistency and integrity of the specifications, and dual specification comparison based on a high-level symbolic execution technique. Dual design, implementation, and testing are performed. Automated tools to facilitate the validation and testing activities are developed to support the methodology. These includes the symbolic executor and test data generator/dual program monitor system. The experiences of applying the methodology to the pilot software are discussed, and the impact on the quality of the software is assessed.",1981,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1702887,no,undetermined,0
Capture-Recapture Sampling for Estimating Software Error Content,Mills capture-recapture sampling method allows the estimation of the number of errors in a program by randomly inserting known errors and then testing the program for both inserted and indigenous errors. This correspondence shows how correct confidence limits and maximum likelihood estimates can be obtained from the test results. Both fixed sample size testing and sequential testing are considered.,1981,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1702812,no,undetermined,0
Experience from Quality Assurance in Nuclear Power Plant Protection System Software Validation,"Validation of a digital computer program to be used in a nuclear power plant protection system must meet quality assurance requirements. Digital systems have not traditionally been used on nuclear reactor protection systems. Licensing of digital system software requires providing assurance that the software performs its intended function. To provide added assurance, the Babcock and Wilcox Company performed software validation on the digital program intended for use on a protection system. Software validation of the Reactor Protection System-II digital program presented a multi-faceted challenge. Quality assurance requirements were imposed on the project. Certain validation ground rules were specified. No known methods existed for proving program correctness for nontrivial software. No precedence had been set to estimate the quality or quantity of testing required as a method of validation. Project schedule constraints were imposed. The need for more documentation than normally furnished was recognized, but how much and what kind was not clear. This paper relates how this challenge was met through a discussion of how the project was performed and the lessons learned through those experiences. A test method was devised within validation ground rules and project schedule constraints to validate that software performed the specified functions. Orderly methods of testing and evaluating were implemented and documented in compliance with a plan to provide auditable, traceable evidence of the validation effort and the digital component program performance.",1980,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4330948,no,undetermined,0
Some Stability Measures for Software Maintenance,"Software maintenance is the dominant factor contributing to the high cost of software. In this paper, the software maintenance process and the important software quality attributes that affect the maintenance effort are discussed. One of the most important quality attributes of software maintainability is the stability of a program, which indicates the resistance to the potential ripple effect that the program would have when it is modified. Measures for estimating the stability of a program and the modules of which the program is composed are presented, and an algorithm for computing these stability measures is given. An algorithm for normalizing these measures is also given. Applications of these measures during the maintenance phase are discussed along with an example. An indirect validation of these stability measures is also given. Future research efforts involving application of these measures during the design phase, program restructuring based on these measures, and the development of an overall maintainability measure are also discussed.",1980,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1702781,no,undetermined,0
A highly reliable mask inspection system,"An automatic system for inspecting micro mask defects with 1-Âµm minimum detectable size has been developed. An outline of the system is as follows: The pattern image obtained with a pickup tube is converted into binary video signals which are transferred into two parallel logic circuits for detecting pattern defects. One is based on the pattern-analyzing method, for which one of four algorithms for detecting micro defects is presented in detail. The other is based on the design-pattern data-comparing method, where the data compression scheme and a new idea for avoiding mask alignment errors are adopted. A software system outline, very important in assisting the hardware functions in this system, is also presented. The results of experiments for determining system performance indicate that the system can detect â‰?-Âµm diameter defects or loss patterns with high probability by complimentary use of the two methods. A 4-in by 4-in mask can be inspected within 100 rain.",1980,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1480815,no,undetermined,0
A model based on software quality factors which predicts maintainability,"Software quality metric were developed to measure the complexity of software systems. The authors relate the complexity of the system as measured by software metrics to the amount of maintenance necessary to that system. The authors have developed a model which uses several software quality metrics as parameters to predict maintenance activity. A description is given of three classifications of metrics that are used to measure the quality of source code: code metrics, which measure physical characteristics of the software, such as length or number of tokens; structure metrics, which measure the connectivity of the software, such as the flow of information through the program and flow of control; and hybrid metrics, which are a combination of code and structure metrics",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10191,no,undetermined,0
Analysis of Reed-Solomon coded frequency-hopping systems with non ideal interleaving in worst case partial band jamming,"The interleaving span of coded frequency-hopped systems is often constrained to be smaller than the decoder memory length, i.e. nonideal interleaving is performed. Analysis of the performance of a hard-decision decoder and an erasure-control decoder of Reed-Solomon codes is presented, both for ideal and nonideal interleaving. The interference consists of worst-case partial-band noise jamming and thermal noise. The frequency hopping system considered uses orthogonal MFSK modulation and noncoherent demodulation with quality bit output based on Viterbi's ratio-threshold technique. Optimization of the ratio-threshold and the erasure-control parameters is performed in worst-case partial-band jamming, and the resulting performance for several interleaver spans is presented.<<ETX>>",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=13430,no,undetermined,0
The software death cycle,"Some aspects of software quality have been fairly well explored-particularly those that pertain to the development phase. Other aspects, notably the measurement of the cost of keeping operational software in line with customer requirements, have received considerably less attention. The paper explores this area by proposing a means of assessing the ownership cost of such software. It shows how this may be used as a basis for deciding when a software system becomes obsolescent and should, on economic grounds, be retired from use",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=114261,no,undetermined,0
Fault-tolerant real-time task scheduling in the MAFT distributed system,"The development of the task scheduling mechanism for the multicomputer architecture for fault-tolerance (MAFT) is discussed. MAFT is a distributed computer system designed to provide high performance and extreme reliability in real-time control applications. The impact of the system's functional requirements, fault-tolerance requirements, and architecture on the development of the scheduling mechanism is examined. MAFT uses a priority-list scheduling algorithm modified to provide extreme reliability in the monitoring of tasks and the detection of scheduling errors. It considers such issues as modular redundancy, Byzantine agreement, and the use of multiversion software and dissimilar hardware. An example of scheduler performance with a realistic workload is presented",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=47154,no,undetermined,0
FOCUS: an experimental environment for validation of fault-tolerant systems-case study of a jet-engine controller,"A simulation environment that allows the run-time injection of transient and permanent faults and the assessment of their impact in complex systems is described. The error data from the simulation are automatically fed into the analysis software in order to quantify the fault-tolerance of the system under test. The features of the environment are illustrated with case study of a fault-tolerant, dual-configuration real-time jet engine controller. The entire controller, described at the logic and functional levels, is simulated, and transient fault injections are performed. In the controller, fault detection and reconfiguration are performed by transactions over the communication links. The simulation consists of the instructions specifically designed to exercise this cross-channel communication. The level of effectiveness of the dual configuration of the system to single and multiple transient errors is measured. The results are used to identify critical design aspects from a fault-tolerance viewpoint",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=63428,no,undetermined,0
Testing of reliability-Analysis tools,"An outline is presented of issues raised in verifying the accuracy of reliability analysis tools. State-of-the-art reliability analysis tools implement various decomposition, aggregation, and estimation techniques to compute the reliability of a diversity of complex fault-tolerant computer systems. However, no formal methodology has been formulated for validating the reliability estimates produced by these tools. The author presents three states of testing that can be performed on most reliability analysis tools to effectively increase confidence in a tool. These testing stages were applied to the SURE (semi-Markov unreliability range evaluator) reliability analysis tool, and the results of the testing are discussed",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=49651,no,undetermined,0
Computer-aided programming for message-passing systems: problems and solutions,"As the number of processors and the complexity of problems to be solved increase, programming multiprocessing systems becomes more difficult and error prone. Program development tools are necessary since programmers are not able to develop complex parallel programs efficiently. Parallel models of computation, parallelization problems, and tools for computer-aided programming (CAP) are discussed. As an example, a CAP tool that performs scheduling and inserts communication primitives automatically is described. It also generates the performance estimates and other program quality measures to help programmers in improving their algorithms and programs",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=48836,no,undetermined,0
Yet another software quality and productivity modeling-YAQUAPMO,"A modeling technique is proposed that uses weight functions to define factors of quality or productivity in terms of evaluation factors. As a result, an acyclic decomposition graph is obtained. Quality or productivity is then defined as the distance between the actual graph and a required graph. An assessment technique is proposed that permits decompositions of any reasonable depth. It is shown how specific models are to be generated by a set of elementary production rules. The approach proposed is also applicable to modeling cost or volume estimations",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=48110,no,undetermined,0
Software reliability prediction for large and complex telecommunication systems,"The problem of predicting the number of remaining faults in a software system are studied. Seven software projects are analyzed using a number of software structure metrices and reliability growth models. The following conclusions are drawn: there is no single model that can always be used, irrespective of the project conditions; software structure metrics (mainly size) do correlate with the number of faults; the assumptions of reliability growth models do not apply when the testing is structured and well organized; and sufficient data has to be collected from different projects to create a basis for predictions",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=48006,no,undetermined,0
PSACOIN level 0 intercomparison-an international verification exercise on a hypothetical safety assessment case study,"A comparison of probabilistic system assessment (PSA) codes relevant to radioactive waste disposal is reported. This level-0 intercomparison was the first of a series of planned code verification exercises and was based on a simple model describing a hypothetical disposal system. PSA codes deal with uncertainty and variability in input parameters using a Monte Carlo approach, so that the analysis of the intercomparison results was complicated by the random fluctuation associated with the output variables to be estimated. The exercise was an important step in the process of assuring the quality of computer programs involved. The methodology could be useful in different contexts, whenever large Monte Carlo outputs affected by random variability are to be analyzed",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=48001,no,undetermined,0
Insertion of fault detection mechanisms in distributed Ada software systems,"A technique for automatically inserting software mechanisms to detect single event upset (SEU) in distributed Ada systems is presented. SEUs may cause information corruption, leading to a change in program flow or causing a program to execute an infinite loop. Two cooperative software mechanisms for detecting the presence of these upsets are described. Automatic insertion of these mechanisms is discussed in relation to the structure of Ada software systems. A program, software modifier for upset detection (SMUD), has been written to automatically modify Ada application software and insert software upset detection mechanisms. As an example, the mechanisms have been incorporated into a system model that employs the MIL-STD-1553B communications protocol. This system model is used as a testbed for verifying that SMUD properly inserts the detection mechanisms. Ada is used for creating the simulation environment to exercise and verify the protocol. Simulation has been used to test and verify the proper functioning of the detection mechanisms. The testing methodology, a short description of the 1553B testbed, and a set of performance measures are presented",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=48000,no,undetermined,0
Application of Linear Adaptive Control to Some Advanced Benchmark Examples,"Linear Adaptive Control [1]-[8] is a new approach to adaptive controller design that uses a novel exogenous linear dynamical model of parameter perturbation ""effects,"" and an ordinary linear observer, to generate the required adaptive control signal u(t). By this means, the need for a nonlinear parameter estimator, as traditionally used in adaptive control, is eliminated and the resulting adaptive controller is completely linear and time-invariant, (all controller ""gains"" are constant). The performance capabilities of linear adaptive controllers have been demonstrated in [4], [6], [7], [8] using relatively simple examples. In this paper the linear adaptive control technique is applied to several examples having complex forms of plant uncertainty. Computer simulation studies are presented to demonstrate the quality of adaptive performance achieved.",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4790236,no,undetermined,0
Formant speech synthesis: improving production quality,"The authors describe analysis and synthesis methods for improving the quality of speech produced by D.H. Klatt's (J. Acoust. Soc. Am., vol.67, p.971-95, 1980) software formant synthesizer. Synthetic speech generated using an excitation waveform resembling the glotal volume-velocity was found to be perceptually preferred over speech synthesized using other types of excitation. In addition, listeners ranked speech tokens synthesized with an excitation waveform that simulated the effects of source-tract interaction higher in neutralness than tokens synthesized without such interaction. A series of algorithms for silent and voiced/unvoiced/mixed excitation interval classification, pitch detection, formant estimation and formant tracking was developed. The algorithms can utilize two channels of input data, i.e., speech and electroglottographic signals, and can therefore surpass the performance of single-channel (acoustic-signal-based) algorithms. The formant synthesizer was used to study some aspects of the acoustic correlates of voice quality, e.g., male/female voice conversion and the simulation of breathiness, roughness, and vocal fry",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=45534,no,undetermined,0
Visual representation methodology for the evaluation of software quality,"The visual representation methodology presents data in a pictorial form, such as face graphs, which enables developers and designers to detect problems that would otherwise be difficult to determine from tables or other statistical data, especially when the amount of data is large or the data elements are many and varied. The visual representation methodology is especially applicable to software development projects, which involve complicated data that are often subjectively biased by human factors, which can compromise data reliability. The visual representation methodology was used to evaluate a number of software projects. Quality and productivity improvements were achieved by applying the methodology to the quality control, process control, and subcontract management of telecommunications software",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=64045,no,undetermined,0
Availability analysis of a certain class of distributed computer systems under the influence of hardware and software faults,"There are modeling techniques which may be used to assess the availability of either the software or the hardware of distributed computer systems. However, from the practical point of view, it would be more useful to assess the availability when the system is subjected to the combined effect of hardware and software faults either during its normal operating time or repair time. Such an availability modelling technique, for the class of distributed computer systems which are used for process control is presented in this paper. To demonstrate the use of this technique the time function of the availability of a typical real-life distributed process control system is evaluated.",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=42821,no,undetermined,0
Cost effectiveness of software quality assurance,"The authors address the economic aspects of quality assurance through an error cost model that determines the impact of quality assurance activities on overall development cost and on the number of residual errors. This model is based on the DOD-STD-2167A development cycle and contrasts the cost of finding errors in each development phase with the cost incurred if the error is not found until a later phase. Using data from a wide range of sources, the mode demonstrates that an early investment in error prevention and discovery through quality assurance standards, reviews, audits, and inspections removes errors at a significantly lower cost than finding and eliminating errors during later integration test phases. Furthermore, improving error discovery and correction in early development phases significantly reduces the number of errors that must be found in the later testing phases, enabling this testing to be more effective and thereby reducing the residual errors in the delivered software product. The model also provides a framework for characterizing a particular software development project and for comparing it with other projects",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=40433,no,undetermined,0
The reliability of regeneration-based replica control protocols,"Several strategies for replica maintenance are considered, and the benefits of each are analyzed. Formulas describing the reliability of the replicated data object are presented, and closed-form solutions are given for the tractable cases. Numerical solutions, validated by simulation results, are used to analyze the tradeoffs between reliability and storage cost. With estimates of the mean times to site failure and repair in a given system, the numerical techniques presented can be applied to predict the fewest number of replicas required to provide the desired level of reliability",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=37978,no,undetermined,0
Concurrent correspondent modules: a fault tolerant Ada implementation,"The authors propose the concept of concurrent correspondent modules as a strategy to implement software fault tolerance. It is based on the concurrent execution of the primary, and its redundant correspondent versions. Error detection and forward error recovery are performed by the comparative test. Using Ada tasks in conjunction with the abort facility, concurrent correspondent modules are easily and efficiently implemented. The use of concurrent Ada enhances the power of concurrent correspondent modules, making this an elegant, effective and feasible alternative to other fault-tolerant approaches.<<ETX>>",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=37404,no,undetermined,0
A multiple DSP environment for on-line signature analysis of plant instrumentation,"A system consisting of four parallel digital signal processors that communicate with a computer host is proposed for implementing online fault detection and identification using industrial transducers. Programming of the fault detection algorithms is carried out in a block-diagram graphical language, specially designed for that purpose. The programming environment allows automatic program segmentation into parallel tasks, the generation of the corresponding code, the allocation of the coded tasks to the four processors, and simulation of the operation of the programmed system. An example (a turbine flow meter) is given to demonstrate the use of the software",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=36889,no,undetermined,0
Performance analysis of voting strategies for a fly-by-wire system of a fighter aircraft,"Findings of studies on input processing of a digital fly-by-wire system of a fighter aircraft are presented. Objectives were to select a suitable software structure complying with reliability and fault tolerance requirements and to assess its computational load. Ramp and constant input signals with noise were studied based on Monte-Carlo methods. Voting strategies studied and compared include lower-median, upper-median, and weighted average. Execution times and memory requirements of each strategy have also been assessed",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=35823,no,undetermined,0
Diagnostic software and hardware for critical real-time systems,"The authors describe a diagnostic software library which contains algorithms to test read-only memory, read/write memory, address lines, the main processor instruction set, the numeric data processor instruction set, and mutual exclusion hardware. The software library also contains algorithms to respond to unexpected software interrupts. A dedicated subsystem diagnostics board called the Multibus Diagnostic Monitor (MDM) is also described",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=34650,no,undetermined,0
An experimental evaluation of the effect of time-of-flight information in image reconstructions for the Scanditronix/PETT Electronics SP-3000 positron emission tomograph-preliminary results,"The effects of detector energy thresholds and time-of-flight information on image quality were investigated for the SP-3000 positron emission tomograph. Phantom studies were performed with different energy discriminator settings. A software preprocessor allowed images to be reconstructed from list mode data with different time-of-flight windows. In general, the inclusion of time-of-flight information in image reconstruction improved SNR (signal to-noise-ratio) over conventional filtered back-projection techniques. The data indicate that setting the energy discriminator level higher than the 170 keV currently used in the machine could be beneficial. The results demonstrated the need for good quality assurance testing for time-of-flight offsets and gains, and they indicate that the current look-ahead coincidence system may need to be modified",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=34611,no,undetermined,0
A distributed fault tolerant architecture for nuclear reactor control and safety functions,"A fault-tolerant architecture that provides tolerance to a broad scope of hardware, software, and communications faults is being developed. This architecture relies on widely available commercial operating systems, local area networks, and software standards. Thus development time is significantly shortened, and modularity allows for continuous and inexpensive system enhancement throughout the expected 20-year life. The fault-containment and parallel-processing capabilities of computers are exploited to provide a high-performance, high-availability network capable of tolerating a broad scope of hardware, software, and operating system faults. The system can tolerate all but one known (and avoidable) single fault, two known and avoidable dual faults, and it will detect all higher-order fault sequences and provide diagnostics to allow for rapid manual recovery",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=63572,no,undetermined,0
Elucidation of an ISDN performance management model,"A model for ISDN (integrated services digital network) performance management is discussed. According to this model, the three most important elements of performance management, in order of their importance, are autonomous receipt of failure and service degradation reports by the automated performance management environment (APME); development of tools to sectionalize problems in different layers of protocols; and incorporating correlation, analysis, and action functions, usually performed manually by technicians, into the APME. ISDN performance management tools can be organized into a hierarchy based on the open systems interconnection (OSI) layer model. The digital technology of the ISDN allows many of these tools to be designed in such a way that the service degradations are detected and reported to enable a pro-active approach, which maintains the service quality and isolated for repair the sections or elements causing the degradation in a way that is nondisruptive to service",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=64258,no,undetermined,0
Quality control of antenna calibration and site-attenuation data from open area test sites,"The use of swept-frequency methods coupled with some computation and graphics display capability, plus an understanding of the behavior of anomalies during data processing, is shown to provide an effective way to detect erroneous or suspicious data from antenna calibrations using the standard-site three-antenna method specified in ANSI Standard 63.4 (American National Standards Institute). How data from antenna and site calibration can be used to cross-check their own accuracy is demonstrated. With appropriate software, the evaluation can be done on site, immediately after the data have been taken. The same tools can be applied to evaluate the validity of site-attenuation data",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=14131,no,undetermined,0
A DSP-based active sensor interface performing on-line fault detection and identification,"Process-measuring instruments generating output signals in the form of a frequency are discussed. By applying a specific signal-analysis technique, which can be implemented by a mixture of hardware and software enhancements on a digital signal processor (DSP), it becomes possible to determine abnormal instrument operation and detect a faulty instrument component within minutes after its failure. The technique used comprises the transformation of the received signal to a train of unit pulses and its spectral analysis by the zero-crossing method. Although this technique presents a limited fault-discrimination ability, it can trace down quickly and economically the causes of major faults, as demonstrated by an application example",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=93424,no,undetermined,0
Market research of hardware and software with partial-order classification,"This work deals with one of the basic problems of market research: the evaluation of many different qualities, e.g. price, size, performance, and service. All the above are accumulated into comparable numbers and presented to the decision-makers as a total sum of qualities. To improve the techniques of data representation and accumulation, the author proposes the use of partial order classification for analyzing and representing sets of data vectors. He explains some of the basic principles of the theory and demonstrates its potential capabilities by a memory comparison between 33-MHz 386 PCs",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=113685,no,undetermined,0
Bridging the gap between design and testing of analog integrated circuits,"A global methodology for the design of analog and mixed analog/digital integrated systems is outlined. The methodology calls for a flexible framework that combines CAD software, simulators, measurement set-ups, and data-processing software. As an application example within such a framework, error modeling and estimation in multistage analog/digital converters is discussed using a new algorithm",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=112570,no,undetermined,0
Analytical techniques for diagnostic functional allocation,"The allocation of maintenance system elements to accomplish the diagnostic mission must be optimized to balance performance, size, weight, and cost. The authors discuss techniques used to create a functional dependency model of a subject system and derive an effectiveness and cost model for the diagnostic system. It is concluded that the diagnostic elements of a weapon system may be designed properly through the use of dependency models and adherence to a structured system engineering process. The critical process of functional allocation is the trade-off of diagnostic performance against operational system efficiencies. Models which have been created for this purpose assist the system engineer in the allocation and design processes, document the results for updates and changes, and help to achieve an optimized system design",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=111549,no,undetermined,0
Rapid recovery from transient faults in the fault-tolerant processor with fault-tolerant shared memory,"The Draper fault-tolerant processor with fault-tolerant shared memory (FTP/FTSM), which is designed to allow application tasks to continue execution during the memory alignment process, is described. Processor performance is not affected by memory alignment. In addition, the FTP/FTSM incorporates a hardware scrubber device to perform the memory alignment quickly during unused memory access cycles. The FTP/FTSM architecture is described, followed by an estimate of the time required for channel reintegration",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=111314,no,undetermined,0
DEPEND: a design environment for prediction and evaluation of system dependability,"The development of DEPEND, an integrated simulation environment for the design and dependability analysis of fault-tolerant systems, is described. DEPEND models both hardware and software components at a functional level, and allows automatic failure injection to assess system performance and reliability. It relieves the user of the work needed to inject failures, maintain statistics, and output reports. The automatic failure injection scheme is geared toward evaluating a system under high stress (workload) conditions. The failures that are injected can affect both hardware and software components. To illustrate the capability of the simulator, a distributed system which employs a prediction-based, dynamic load-balancing heuristic is evaluated. Experiments were conducted to determine the impact of failures on system performance and to identify the failures to which the system is especially susceptible",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=111267,no,undetermined,0
Controllable signature check pointing scheme for transient error detection,"A program can fail during execution owing to software, hardware, or extraneous faults. There are three types of errors caused by extraneous faults: transient, intermittent, and permanent. This work deals with transient faults. The authors present a flexible and controllable transient error detection scheme called the controllable signature check pointing scheme. The new signature monitoring differs from existing schemes in three major aspects: (1) the intervals in the program to be monitored are decided at a higher level of abstraction (high-level-language level); (2) the interval size is chosen not arbitrarily but by applying control-flow and bulk complexity measures; (3) the interval size can be controlled to suit the application. The location of the signature check points and the frequency of occurrence of the check points can be controlled by varying the interval control-flow complexity factor (INCCF). Thus the error coverage and latency, memory overhead, and performance degradation can also be tuned to suit the application. Future refinements and extensions include signature check pointing of sequential code and reducing signature embedding overhead",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=101739,no,undetermined,0
Comparison of two estimation methods of the mean time-interval between software failures,"The quantitative assessment of software reliability by using failure time data observed during software testing in the software development is addressed. On the basis of two models described by nonhomogeneous Poisson processes, exponential and delayed S-shaped software reliability growth models, the authors adopt the mean time between software failures as a reliability assessment measure and propose a method of software reliability assessment. Since the distributions of the time intervals between software failures for the two models are improper, the mean time intervals are obtained by standardization of the distributions. Further, applying the two models to actual data, numerical examples of the mean time between software failures are shown",1990,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=101651,no,undetermined,0
Case study of radial overhead feeder performance at 12.5 kV and 34.5 kV,"The authors compared the economic and electrical performance factors on one specific overhead radial distribution feeder that is in operation at 12.5 kV, but whose present load and growth require that it be upgraded. They examined the economics and electrical performance of several feeder upgrading options for a particular feeder on the Green Mountain Power system (USA) with respect to: feeder losses; feeder voltage regulation; capital costs and cost of losses: and voltage dips on the local 115 kV transmission system caused by feeder faults. The performance of four feeder configuration options is assessed. It is shown that there is no clear economic advantage to 34.5 kV and that there may be a disadvantage with respect to voltage dip performance",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=95536,no,undetermined,0
Industrial metrology and sensors,"Industrial metrology is concerned with sensors to measure movement of machine-tool parts and monitor tool wear and the dimensions of artifacts in machining centers, sensors for robots in flexible manufacturing systems, sensors to gauge mating parts for selective assembly or allowing for interchangeability, and sensors for inspection and testing of assembled or part-assembled products. In general, the dimensional, shape, and physical properties of functional parts need to be inspected. As a consequence of the need to be highly efficient and quality conscious, manufacturing metrology is evolving from traditional engineering metrology (dominated by the skills of quality inspectors at the end of production lines) to automatic inspection methods (offline, in-cycle, and in-line) and utilizing microelectronic, computer (hardware and software), and novel optical techniques. Suitable sensing techniques, sensors, and transducers are essential to this developing situation. The author reviews the subject and emphasizes significant advances",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=93397,no,undetermined,0
On predicting software reliability,"Definitions of software reliability are presented, and an overview of reliability is given. The system-software-availability relationship is also explored",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=65139,no,undetermined,0
Diagnostic tree design with model-based reasoning,"A reasoning procedure using quantitative models of connectivity and function has been developed to generate automatically multibranched diagnostic trees which can isolate faults within feedback loops and in the presence of multiple faults. The authors describe how the model-based reasoning system is used to generate automatically diagnostic trees that can have variable degrees of branching, from binary to ternary (nodes with high, OK, and low branches) to <e1>n</e1>-ary trees. With branching degrees at or above ternary, these trees are capable of fault isolating within loops and can in fact isolate multiple faults. The trees can utilize much of the information content in quantitative measurements to make efficient and accurate diagnoses not possible with the binary tree. Both efficiency and accuracy of diagnosis increase with the branching factor of the tree. Automated tree generation provides effective automated diagnostics to applications requiring low-cost hardware and fast response time",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=81115,no,undetermined,0
Application of state analysis technology to surveillance systems,"<div style=""font-variant: small-caps; font-size: .9em;"">First Page of the Article</div><img class=""img-abs-container"" style=""width: 95%; border: 1px solid #808080;"" src=""/xploreAssets/images/absImages/00751964.png"" border=""0"">",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=751964,no,undetermined,0
Does Imperfect Debugging Affect Software Reliability Growth?,"This paper discusses the improvement of conventional software reliability growth models by elimination of the unreasonable assumption that errors or faults in a program can be perfectly removed when they are detected. The results show that exponential-type soft- ware reliability growth models that deal with error- counting data could be used even if the perfect debugging assumption were not held, in which case the interpretation of the model parameters should be changed. An analysis of real project data is presented.",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=714425,no,undetermined,0
CAPRES: a software tool for modeling and analysis of fault-tolerant computer architectures,"An engineering software tool is described that was developed to support efficient and cost-effective design of large-scale, parallel, fault-tolerant computer architectures (FTCAs). The tool, termed CAPRES (computer-aided performance and reliability evaluation system), allows the user to predict, by analytical means, the performance, reliability, and performability of candidate computer architectures executing candidate workloads. The algorithms of CAPRES use multidisciplinary techniques from hierarchical queuing-network theory, semi-Markov processes, hybrid-state systems theory and numerical integration. The sophisticated user interface and high-level front-end of CAPRES make this software system a truly integrated package for FTCA analysis and evaluation. To illustrate the role of CAPRES as a tool to aid in the analysis of FTCAs, an example of an Encore Multimax multiprocessor executing a parallel weapon-target-assignment algorithm is presented",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=71372,no,undetermined,0
Software metric classification trees help guide the maintenance of large-scale systems,"The 80:20 rule states that approximately 20% of a software system is responsible for 80% of its errors. The authors propose an automated method for generating empirically-based models of error-prone software objects. These models are intended to help localize the troublesome 20%. The method uses a recursive algorithm to automatically generate classification trees whose nodes are multivalued functions based on software metrics. The purpose of the classification trees is to identify components that are likely to be error prone or costly, so that developers can focus their resources accordingly. A feasibility study was conducted using 16 NASA projects. On average, the classification trees correctly identified 79.3% of the software modules that had high development effort or faults",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=65202,no,undetermined,0
Assessing the adequacy of documentation through document quality indicators,"Case study results for a research effort funded by the US Naval Surface Warfare Center (NSWC) are presented. The investigation focuses on assessing the adequacy of project documentation based on an identified taxonomic structure relating documentation characteristics. Previous research in this area has been limited to the study of isolated characteristics of documentation and English prose, without considering the collective contributions of such characteristics. The research described takes those characteristics, adds others and establishes a well-defined approach to assessing the `goodness' of software documentation. The identification of document quality indicators (DQIs) provides the basis for the assessment procedure. DQIs are hierarchically defined in terms of document qualities, factors that refine qualities, and quantifiers that provide for the measurement of factors",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=65192,no,undetermined,0
Hyper-geometric distribution model to estimate the number of residual software faults,It is shown that the hyper-geometric distribution model can be applied to different types of test-and-debug data to estimate the number of initial software faults. Examples show that the fitness of the estimated growth curves of the model to real data is satisfactory. The relationship of the model to those proposed earlier is clarified. Some of them can be expressed as a special case of the proposed model. The S-shaped characteristic of the growth curve can also be introduced using this model,1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=65155,no,undetermined,0
Methodologies for meeting hard deadlines in industrial distributed real-time systems,"Four new approaches for coping with strict timing constraints as encountered in industrial process control applications are presented. For each node in a distributed system, an asymmetrical two-processor architecture capable of guaranteeing response times is employed. One processor in each node is dedicated to the kernel of a real-time operating system. Its three reaction levels are constructively described by outlining their functional units and control procedures. Contemporary real-time computers are unable to manipulate external parallel processes simultaneously, and their response times are generally unpredictable. It is shown that these problems can be solved by endowing the node computers with novel process peripherals. They work in parallel and perform I/O operations precisely at user-specified instants. To cope with a transient overload of a node resulting from an emergency situation, a fault tolerant scheme that handles overloads by degrading the system performance gracefully and predictably is used. The scheme is based on the concept of imprecise results and does not utilize load sharing, which is usually impossible in industrial process control environments",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=65149,no,undetermined,0
A cryogenic monitor system for the liquid argon calorimeter in the SLD detector,"The authors describe the monitoring electronics system design for the liquid argon calorimeter (LAC) portion of the SLD (Stanford Linear Collider Large Detector). This system measures temperatures and liquid levels inside the LAC cryostat and transfers the results over a fiber-optic serial link to an external monitoring computer. System requirements, unique design constraints, and detailed analog, digital, and software designs are presented. Fault tolerance and the requirement for a single design to work in several different operating environments are discussed",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=34542,no,undetermined,0
Recovery point selection on a reverse binary tree task model,"An analysis is conducted of the complexity of placing recovery points where the computation is modeled as a reverse binary tree task model. The objective is to minimize the expected computation time of a program in the presence of faults. The method can be extended to an arbitrary reverse tree model. For uniprocessor systems, an optimal placement algorithm is proposed. For multiprocessor systems, a procedure for computing their performance is described. Since no closed form solution is available, an alternative measurement is proposed that has a closed form formula. On the basis of this formula, algorithms are devised for solving the recovery point placement problem. The estimated formula can be extended to include communication delays where the algorithm devised still applies",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=31353,no,undetermined,0
The Sequoia approach to high-performance fault-tolerant I/O,"Summary form only given. The Sequoia computer utilizes a modular, tightly-coupled multiprocessor architecture, with hardware fault detection and software fault recovery to achieve high degrees of reliability, availability, and data integrity. The computer comprises from one to 64 processor elements (PE), and from two to 128 memory elements (ME) and I/O processing elements (IOE). Configurations can be tailored to meet the needs of the applications. Each processor element consists of two CPUs running in lockstep, with 256 kb of non-write-through cache. The non-write-through cache is the foundation of the fault-tolerant operation of the system. Each processor performs its work locally within its cache, periodically checkpointing its state to main memory. Each memory element consists of 16 Mb of memory, protected by ECC (error checking and correction), and 1024 test-and-set locks used to synchronize processor updates to shared data structures within the operating system. Writable data is shadowed in main memory; the data is stored on two different memory elements. Each I/O processing element consists of CPUs running in lockstep, with 2 Mb of local memory. The I/O processors are connected to an IEEE-standard 796 bus (the Multibus), which serves as an I/O bus connecting the IOE to up to 16 peripheral controllers of any type.<<ETX>>",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=301994,no,undetermined,0
A simplified method for RPE coding with improved performance,"The regular-pulse-excitation (RPE) technique for speech coding gives good speech quality, but requires a heavy computational effort. A method for the excitation sequence search is proposed in which the pulse amplitudes are computed one at a time, rather than simultaneously in block, by solving a linear system of reduced dimension. The algorithm also provides an appreciable improvement in the performance, because the noise energy is minimized taking into account both quantization errors and the effect of pulses in the next block. The algorithm is implemented in a 8-kb/s codec, using vector quantization for the vocal tract all-pole model parameters. Computer simulations over a large database of male and female speech signals (about 80s of speech) gave good results in terms of segmental signal-to-noise ratio, even in the absence of a patch predictor",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=25851,no,undetermined,0
The implementation and application of micro rollback in fault-tolerant VLSI systems,"The authors present a technique, called micro rollback, which allows most of the performance penalty for concurrent error detection to be eliminated. Detection is performed in parallel with the transmission of information between modules, thus removing the delay for detection from the critical path. Erroneous information may thus reach its destination module several clock cycles before an error indication. Operations performed on this erroneous information are undone using a hardware mechanism for fast rollback of a few cycles. The authors discuss the implementation of a VLSI processor capable of micro rollback as well as several critical issues related to its use in a complete system.<<ETX>>",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5325,no,undetermined,0
Experimental evaluation of software reliability growth models,"An experimental evaluation is presented of SRGMs (software reliability growth models). The experimental data sets were collected from compiler construction projects completed by five university students. The SRGMs studied are the exponential model, the hyperexponential model, and S-shaped models. It is shown that the S-shaped models are superior to the exponential model in both the accuracy of estimation and the goodness of fit (as determined by the Kolmogorov-Smirnov test). It is also shown that it is possible to estimate accurately residual faults from a subset of the test results. An estimation method is proposed for the hyperexponential model. It is based on the observation that the start time for testing is different for different program modules. It is shown that this method improves the goodness of fit significantly.<<ETX>>",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5313,no,undetermined,0
A comparison of four adaptation algorithms for increasing the reliability of real-time software,"In a large, parallel, real-time computer system, it is frequently most cost-effective to use different software reliability techniques (e.g., retry, replication, and resource-allocation algorithms) at different levels of granularity or within different subsystems, dependent on the reliability requirements and fault models associated with each subsystem. The authors describe the results of applying four reliability algorithms, via RESAS (REal-time Software Adaptation System), to a sample real-time program executing on a multiprocessor",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=51118,no,undetermined,0
Evaluation of the probability of dynamic failure and processor utilization for real-time systems,"It is shown how to determine closed-form expressions for task scheduling delay and active task time distributions for any real-time system application, given a scheduling policy and task execution time distributions. The active task time denotes the total time a task is executing or waiting to be executed, including scheduling delays and resource contention delays. The distributions are used to determine the probability of dynamic failure and processor utilization, where the probability of dynamic failure is the probability that any task will not complete before its deadline. The opposing effects of decreasing the probability of dynamic failure and increasing utilization are also addressed. The analysis first addresses workloads where all tasks are periodic, i.e., they are repetitively triggered at constant frequencies. It is then extended to include the arrival of asynchronously triggered tasks. The effects of asynchronous tasks on the probability of dynamic failure and utilization are addressed",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=51117,no,undetermined,0
Rajdoot: a remote procedure call mechanism supporting orphan detection and killing,"Rajdoot is a remote procedure call (RPC) mechanism with a number of fault tolerance capabilities. A discussion is presented of the reliability-related issues and how these issues have been dealt with in the RPC design. Rajdoot supports exactly-once semantics with call nesting capability, and incorporates effective measures for orphan detection and killing. Performance figures show that the reliability measures of Rajdoot impose little overhead",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4620,no,undetermined,0
SLS-a fast switch-level simulator [for MOS],"SLS, a large-capacity, high-performance switch-level simulator developed to run on an IBM System/370 architecture is described. SLS uses a model which closely reflects the behavior of MOS circuits. The high performance is the result of mixing a compiled model with the more traditional approach of event-driven simulation control, together with very efficient algorithms for evaluating the steady-state response of the circuit. SLS is used for design verification/checking applications and for estimating fault coverage",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=3214,no,undetermined,0
Concurrent error correction in systolic architectures,"It is shown how two features of systolic arrays in which the cells retain partial results rather than pass them on can be used to facilitate testing and fault localization with no modification of the systolic design; the monitoring is performed either by software in the host processor or by hardware following the system output. One feature is the ability to enter identical sequences of inputs into two adjacent processor elements; the other is the resemblance of the data flow to that of a scan design. In particular, a one-dimensional systolic architecture of interest is described and applied to a finite-impulse response filter. Both the error detection and correction coverages are considered in detail for the FIR filter. The concurrent error-correction technique is applicable only to operational reliability, but the concurrent error-detection technique is equally applicable to manufacturing tests, incoming tests, and periodic maintenance tests. A hardware implementation of the error-detection-and-correction technique is presented. The architecture is extended to a two-dimensional processor",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=3139,no,undetermined,0
Expert systems for image processing-knowledge-based composition of image analysis processes,"Expert systems for image processing are classified into four categories, and their objectives, knowledge representation, reasoning methods, and shortcomings are discussed. The categories are: (1) consultation systems for image processing; (2) knowledge-based program composition systems; (3) rule-based design systems for image segmentation algorithms; and (4) goal-directed image segmentation systems. The importance of choosing effective image analysis strategies is discussed. Two methods are proposed for characterizing image analysis strategies: one from a software engineering viewpoint and the other from a knowledge representation viewpoint. Several examples are given to demonstrate the effectiveness of these methods",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=28187,no,undetermined,0
An experimental investigation of software diversity in a fault-tolerant avionics application,"Highly reliable and effective failure detection and isolation (FDI) software is crucial in modern avionics systems that tolerate hardware failures in real time. The FDI function is an excellent opportunity for applying the principal of software design diversity to the fullest, i.e., algorithm diversity, in order to provide gains in functional performance as well as potentially enhancing the reliability of the software. The authors examine algorithm diversity applied to the redundancy management software for a hardware fault-tolerant sensor array. Results of an experiment are presented that show the performance gains that can be provided by utilizing the consensus of three diverse algorithms for sensor FDI",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=25781,no,undetermined,0
V&V in the next decade [software validation],"The author examines potential changes resulting from current research and evaluation studies of software verification and validation techniques and tools. She offers specific predictions with regard to: standards and guidelines; specification, modeling, and analysis; reviews, inspections, and walkthroughs; and tests.<<ETX>>",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=28123,no,undetermined,0
Estimating metrical change in fully connected mobile networks-a least upper bound on the worst case,"A least upper bound is derived on the amount of adjustment of virtual token passing (VTP) time needed to assure collision-free access control in VTP networks (i.e. networks that use `time-out' or scheduling function-based access protocols) and in which nodes change their spatial configuration due to motion (although always stay within range and in line-of-sight of each other). Since the new bound is a function of network geographical size, as well as of node maximal speeds and the time that passed since the previous adjustment, it allows VTP times that are shorter than those found in previous publications, especially when intervals between adjustments grow larger. For most VTP networks, with large mixed populations of mobile and stationary users, the average VTP time (which is a major factor in performance of the access protocol) allowed by the new bound is shorter than that of any configuration-independent protocol",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=2272,no,undetermined,0
Hybrid fault diagnosability with unreliable communication links,"Hybrid fault diagnosability in distributed multiprocessor systems is considered for the case in which, in addition to units being faulty, communication links among units can also be faulty. A hybrid fault situation is a (bounded) combination of hard and soft failing units. A novel hybrid fault diagnosability, denoted (<e1>t</e1>/<e1>t</e1><sub>s </sub>-unit: Ïƒ-link)-diagnosability is introduced (a total of <e1>t</e1> or fewer units can be faulty with at most <e1>t</e1><sub>s </sub> of them soft-failing; Ïƒ is the number of incorrect test outcomes caused by unreliable links)., This diagnosability is compatible with the previously known <e1>t</e1>/<e1>t</e1><sub>s</sub>-diagnosability and can additionally tolerate up to Ïƒ incorrect test outcomes to give always-correct diagnosis for any hybrid-fault (HF)-compatible syndrome from a hybrid fault situation. It is shown that an O(|<e1>E</e1>|) algorithm, proposed originally by the authors (see ibid., vol.C-35, p.503-10, 1986) for faulty unit identification in <e1>t</e1><sub>s</sub>-diagnosable systems, can be used to analyze a syndrome from a (<e1>t</e1><sub>s</sub>-unit Ïƒ-link)-diagnosable system efficiently without any preclassification of the syndrome in terms of HF-compatibility. It is shown that this algorithm not only identifies all the faulty units associated with an HF-compatible syndrome but also produces nonempty, always-correct diagnosis for many HF-incompatible syndromes",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=2146,no,undetermined,0
Evaluation of system BIST using computational performance measures,"The impact of built-in self-test (BIST) techniques and system maintenance strategies on the performance of a VLSI processor system is examined. The specific BIST technique used was shown to have a significant influence upon instantaneous and cumulative system reward. It was shown that the additional overhead of the distributed and BILBO approaches is justified for this model when area utilization and cumulative area utilization are considered. For the assumed design parameters, the results presented allow a VLSI system designer to choose an optimal configuration based on system requirements and individual component parameters. The optimal performance will also depend on system and component parameters such as processor failure rates and fault coverage. These results are relevant to the design, evaluation, and optimization of highly reliable, high-performance digital processing systems",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=207833,no,undetermined,0
Characterizing the software process: a maturity framework,A description is given of a software-process maturity framework that has been developed to provide the US Department of Defense with a means to characterize the capabilities of software-development organizations. This software-development process-maturity model reasonably represents the actual ways in which software-development organizations improve. It provides a framework for assessing these organizations and identifying the priority areas for immediate improvement. It also helps identify those places where advanced technology can be most valuable in improving the software-development process. The framework can be used by any software organization to assess its own capabilities and identify the most important areas for improvement.<<ETX>>,1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=2014,no,undetermined,0
Second IEE/BCS Conference: Software Engineering 88 (Conf. Publ. No.290),The following topics were dealt with: software prototyping; quality assurance; STARTS; measurements and metrics; software reliability; programming paradigms; cost control; software reuse; software maintenance; software testing; software tools; specification tools; specification testing; and embedded systems. Abstracts of individual papers can be found under the relevant classification codes in this or other issues,1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=196390,no,undetermined,0
Five years experience with a new method of field testing cross and quadrature polarized mho distance relays. II. Three case studies,"For pt.I see ibid., vol.3, no.3, p.880-6 (1988). In part I the authors discussed the testing of the faulted relay element of the cross and quadrature polarized mho distance relays. In this second part, they discuss three case studies in which the test method described in part I allowed the Saskatchewan Power Corporation (SaskPower), using off-the-shelf test equipment, to accurately predict the relay operating characteristic, improve relay performance, and more accurately set the distance relay for phase-to-phase (LL) and single-line-to-ground (SLG) faults",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=193865,no,undetermined,0
Applications of software reliability measurement to instruments and calculators,Software reliability growth models have been applied to several projects at two divisions of the Hewlett-Packard Corporation. The primary model used is the basic execution time model described by J.D. Musa et al. (1987). The model has been used to: predict software quality assurance duration prior to the beginning of testing; provide updated estimates of product reliability and projected release date; estimate software reliability at the completion of testing; and predict the customer's software product reliability experience after release. Model implementation and performance are discussed. Difficulties in the application of the model and suggested strategies for overcoming these difficulties are described.<<ETX>>,1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=17150,no,undetermined,0
Sequoia: a fault-tolerant tightly coupled multiprocessor for transaction processing,"The Sequoia computer is a tightly coupled multiprocessor that avoids most of the fault-tolerance disadvantages of tight coupling by using a fault-tolerant hardware-design approach. An overview is give of how the hardware architecture and operating system (OS) work together to provide a high degree of fault tolerance with good system performance. A description of hardware is followed by a discussion of the multiprocessor synchronization problem. Kernel support for fault recovery and the recovery process itself are examined. It is shown the kernel, through a combination of locking, shadowed memory, and controlled flushing of non-write-through cache, maintains a consistent main memory state recoverable from any single-point failure. The user shared memory is also discussed.<<ETX>>",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=17,no,undetermined,0
Fault tolerant parallel processor architecture overview,"The authors address issues central to the design and operation of a Byzantine resilient parallel computer. Interprocessor connectivity requirements are met by treating connectivity as a resource which is shared among many processing elements, allowing flexibility in their configuration and reducing complexity. Reliability analysis results are presented which demonstrate the reduced failure probability of such a system. Redundant groups are synchronized solely by message transmissions and receptions, which also provide input data consistency and output voting. Performance analysis results are presented which quantify the temporal overhead involved in executing such fault tolerance-specific operations.<<ETX>>",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5328,no,undetermined,0
Generating hierarchical system descriptions for software error localization,"The purpose of this study is to quantify ratios of coupling and cohesion and use them in the generation of hierarchical system descriptions. The ability of the hierarchical descriptions to localize errors by identifying error-prone system structure is evaluated using actual error data. Measures of data interaction called data bindings, are used as the basis for calculating software coupling and cohesion. A 135000 source line system from a production environment has been selected for empirical analysis. Software error data were collected from high-level system design through system test and from field operation of the system. A set of five tools is applied to calculate the data bindings automatically, and cluster analysis is used to determine a hierarchical description of each of the system's 77 subsystems. An analysis-of-variance model is used to characterize subsystems and individual routines that had either many/few errors or high/low error correction effort",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5357,no,undetermined,0
Automatically generated acceptance test: a software reliability experiment,"The author presents results of a software reliability experiment that investigates the feasibility of a new error detection method. The method can be used as an acceptance test and is solely based on empirical data about the behavior of internal states of a program. The experimental design uses the existing environment of a multiversion experiment and used the launch interceptor problem as a model problem. This allows the controlled experimental investigation of versions with well-known single and multiple faults, and the availability of an oracle permits the determination of the error detection performance of the test. Fault-interaction phenomena are observed that have an amplifying effect on the number of error occurrences. Preliminary results indicate that all faults examined so far are detected by the acceptance test. This shows promise for further investigations and for the use of this test method in other applications",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5375,no,undetermined,0
A class of analysis-by-synthesis predictive coders for high quality speech coding at rates between 4.8 and 16 kbit/s,"The general structure of this class of coders is reviewed, and the particulars of its members are discussed. The different analysis procedures are described, and the contributions of the various coder parameters to the performance of the coder are examined. Quantization procedures for each transmitted parameter are given along with examples of bit allocations. The speech quality produced by these coders is high at 16 kb/s and good at 8 kb/s, but only fair at 4.8 kb/s. The use of postprocessing techniques changes the performance at lower rates, but more research is needed to further improve the coders",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=612,no,undetermined,0
Software inspections: an effective verification process,"The authors explain how to perform software inspections to locate defects. They present metrics for inspection and examples of its effectiveness. The authors contend, on the basis of their experiences and those reported in the literature, that inspections can detect and eliminate faults more cheaply than testing.<<ETX>>",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=28121,no,undetermined,0
A quantitative approach to monitoring software development,"A preliminary overview of a procedure for monitoring software development with respect to quality is provided. The monitoring process is based on the extraction, analysis and interpretation of metrics relating to software products and processes. The problem of interpreting software metrics data in terms that are meaningful to project and quality managers is focused on. The authors present a four-stage model of the interpretation of metrics. The model could be implemented as an advice system for managers. The authors discuss the general principles of project monitoring throughout the development life-cycle and identify metrics that may be collected throughout, from requirements specification to integration testing. They suggest some general principles for assessing the cause of abnormal metric values, for distinguishing between causes, and for responding to unfavourable project circumstances.<<ETX>>",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=28084,no,undetermined,0
The Test Engineer's Assistant: a support environment for hardware design for testability,"A description is given of TEA (Test Engineer's Assistant), a CAD (computer-aided design) environment developed to provide the knowledge base and tools needed by a system designer for incorporating testability features into a design. TEA helps the designer meet the requirements of fault coverage and ambiguity group size. Fault coverage is defined as the percentage of faults that can be detected out of the population of all faults of a unit under test with a particular test set. An ambiguity group is defined as the smallest hardware entity in a given level of the system design hierarchy (that is, board, subsystem, and system) to which a fault can be isolated. The fault model considered throughout is the single stuck-at fault model. An example application of TEA is included.<<ETX>>",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=25383,no,undetermined,0
Whistle: a workbench for test development of library-based designs,"The authors have applied the difference fault model (DFM) approach to the simulation of high-level designs and built a complete environment for test generation and fault simulation of designs expressed in a hardware description language (HDL), such as VHDL. Their environment, called Whistle, consists of a set of programs that supports a library-based design style. They describe the tools in Whistle, with special emphasis on the tools used to analyze each block in the design library, and the generation of code for the simulation of the F-faults in the design. The authors then evaluate Whistle from several points of view. First, they illustrate the accuracy of the fault coverage estimates by comparing them with the actual fault coverage obtained by gate-level fault simulation. Second, they examine the correlation between the accuracy measurements as given by their evaluation functions and the behavior of various characterization functions for a given block. This illustrates how critical it is to choose a good characterization function and how it affects the accuracy of the results. A third aspect of Whistle that they look at is its test-generation capability.<<ETX>>",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=25380,no,undetermined,0
Perturbation techniques for detecting domain errors,"Perturbation testing is an approach to software testing which focuses on faults within arithmetic expressions appearing throughout a program. This approach is expanded to permit analysis of individual test points rather than entire paths, and to concentrate on domain errors. Faults are modeled as perturbing functions drawn from a vector space of potential faults and added to the correct form of an arithmetic expression. Sensitivity measures are derived which limit the possible size of those faults that would go undetected after the execution of a given test set. These measures open up an interesting view of testing, in which attempts are made to reduce the volume of possible faults which, were they present in the program being tested, would have escaped detection on all tests performed so far. The combination of these measures with standard optimization techniques yields a novel test-data-generation method called arithmetic fault detection",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=24727,no,undetermined,0
C-DOT DSS-maintenance software,"Switching systems are expected to meet stringent performance criteria, in order to meet the high availability requirements laid down for them. To meet these requirements, major emphasis is placed on reliability, quality of service and maintainability, in the Centre for Development of Telematics (C-DOT) digital switching system (DSS). A robust hardware/software distributed architecture is developed. Within this architecture a loosely coupled recovery strategy is adopted, in which an error is confined and recovered in the module where the error is detected. C-DOT DSS has a wide range of automated capabilities, which provide it with the capability to run with no need for manual intervention. These maintenance functions are capable of handling multiple failures and severe malfunctions. However, a wide variety of user friendly maintenance functions are provided which give the maintenance personnel overriding powers. The flexibility, fault resilience, and user friendly maintenance capabilities are described",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=177042,no,undetermined,0
Tools for measuring software reliability,"The author discusses a measure of software reliability and various models for characterizing it, the result of 15 years of theoretical research and experimental application, which are moving into practice and starting to pay off. These tools let developers quantify reliability, give them ways to predict how reliability will vary as testing progresses, and help them use that information to decide when to release software. He examines the distinction between failures and faults and how these affect reliability. He compares execution-time models with calendar-time models, which are less effective, and discusses the choice of execution-time models. The author then describes a generic, step-by-step procedure to guide software reliability engineers in using the reliability models.<<ETX>>",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=17360,no,undetermined,0
CHAOS/sup art/: support for real-time atomic transactions,"CHAOS/sup art/ is an object-based, real-time operating system kernel that provides an extended notion of atomic transactions as the basic mechanisms for programming real-time, embedded applications. These transactions are expressed as object invocations with guaranteed timing, consistency, and recovery attributes. The mechanisms implemented by CHAOS/sup art/ kernel provide a predictable, accountable, and efficient basis for programming with real-time transactions. These mechanisms are predictable because they have well-defined upper bounds on their execution times that are (can be) determined before their execution. They are accountable because their decisions are guaranteed to be honored as long as the system is in an application-specific safe state.<<ETX>>",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=105620,no,undetermined,0
Fault diagnosis for sparsely interconnected multiprocessor systems,"The authors present a general approach to fault diagnosis that is widely applicable and requires only a limited number of connections among units. Each unit in the system forms a private opinion on the status of each of its neighboring units based on duplication of jobs and comparison of job results over time. A diagnosis algorithm that consists of simply taking a majority vote among the neighbors of a unit to determine the status of that unit is then executed. The performance of this simple majority-vote diagnosis algorithm is analyzed using a probabilistic model for the faults in the system. It is shown that with high probability, for systems composed of n units, the algorithm will correctly identify the status of all units when each unit is connected to O(log n) other units. It is also shown that the algorithm works with high probability in a class of systems in which the average number of neighbors of a unit is constant. The results indicate that fault diagnosis can in fact be achieved quite simply in multiprocessor systems containing a low to moderate number of testing conditions.<<ETX>>",1989,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=105544,no,undetermined,0
Software safety management,"An approach to life-cycle management of software safety, continuing into the operational phases, is discussed. It is based on defect prevention, early-defect detection and removal, and critical-path analysis, with continuous measurement, analysis and evaluation taking place throughout the life cycle. This approach also takes into account the possibility that those responsible for software safety might not have strong software-analysis backgrounds nor the time to perform all of the software safety-related activities themselves.<<ETX>>",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9645,no,undetermined,0
Diagnostic automation-a total systems approach,"Diagnostics automation in the integrated support concept is described in terms of its primary attributes and their normal relationship to one another. The description is set forth with the understanding that each attribute may have both managerial and technological aspects and, in the broadcast sense, could be evaluated from the standpoint of how it contributes to the resolution of chronic support deficiencies",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9576,no,undetermined,0
A survey of software functional testing techniques,"The authors survey several technical articles in the area of software testing and provide a cross section of software functional testing techniques. In particular, they focus on the systematic methodologies because such methodologies demonstrate the absence of unwanted erroneous functions and divide the testing effort into manageable pieces for automating the testing effort into manageable pieces for automating the testing process. The authors also discuss the empirical approach to assessing software validation methods and focus on the static and the dynamic techniques used to characterize this approach. Several techniques were found to be very useful for discovering different types of errors. For instance, the symbolic evaluation can be used to prove the correctness of a program without executing it by symbolically evaluating the sequence of the assignment statements occurring in a program path. Structured walkthroughs and design inspections result in substantial improvements in quality and productivity through the use of formal inspections of the design and the code",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=95474,no,undetermined,0
Wideband polarimetric radar cross section measurement,"An ultrawideband polarimetric RCS (radar cross-section) measurement system built for investigations on basic vegetation scatterers like leaves and needles is presented. Due to a qualified 20 term calibration procedure and an extensive use of software tools, the system provides a higher quality standard level for future RCS target classification. Some of the outstanding features are fully calibrated monostatic and bistatic RCS target measurement, determination of the complex polarimetric scattering matrix in a frequency range from 1-26.5 GHz and complete elimination of spurious reflections. Additionally, time-domain capability makes it possible to get measurement results in both the frequency and time domains.<<ETX>>",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=94127,no,undetermined,0
Assessing the quality of abstract data types written in Ada,"A method is presented for assessing the quality of ADTs (abstract data types) in terms of cohesion and coupling. It is argued that an ADT that contains and exports only one domain and exports only operations that pertain to that domain has the best cohesive properties, and that ADTs that make neither explicit nor implicit assumptions about other ADTs in the system have the best coupling properties. Formal definitions are presented for each of the cohesion and coupling characteristics discussed. Their application to Ada packages is also investigated, and it is shown how a tool can be developed to assess the quality of an Ada package that represents an ADT",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=93696,no,undetermined,0
Learning from examples: generation and evaluation of decision trees for software resource analysis,"A general solution method for the automatic generation of decision (or classification) trees is investigated. The approach is to provide insights through in-depth empirical characterization and evaluation of decision trees for one problem domain, specifically, that of software resource data analysis. The purpose of the decision trees is to identify classes of objects (software modules) that had high development effort, i.e. in the uppermost quartile relative to past data. Sixteen software systems ranging from 3000 to 112000 source lines have been selected for analysis from a NASA production environment. The collection and analysis of 74 attributes (or metrics), for over 4700 objects, capture a multitude of information about the objects: development effort, faults, changes, design style, and implementation style. A total of 9600 decision trees are automatically generated and evaluated. The analysis focuses on the characterization and evaluation of decision tree accuracy, complexity, and composition. The decision trees correctly identified 79.3% of the software modules that had high development effort or faults, on the average across all 9600 trees. The decision trees generated from the best parameter combinations correctly identified 88.4% of the modules on the average. Visualization of the results is emphasized, and sample decision trees are included",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9061,no,undetermined,0
From data to models yet another modeling of software quality and productivity,"Software systems are often subdivided into the levels: application concept, data processing concept and implementation. Evaluation, measurement and assessment have to take these three levels into consideration. Quality models have to consider active and passive elements of the software as well as dynamic characteristics on all three levels. Since software systems and software projects are unique, quality models have to be flexible. A scheme for quality modeling uses weight functions to define quality factors in terms of evaluation factors. With respect to levels of software description a number of ratios are defined in order to describe software quantitatively. Using functions which reflect past experience, it becomes possible to generate estimates over the product extent and the production effort",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=72242,no,undetermined,0
An analysis of several software defect models,"Results are presented of an analysis of several defect models using data collected from two large commercial projects. Traditional models typically use either program matrices (i.e. measurements from software products) or testing time or combinations of these as independent variables. The limitations of such models have been well-documented. The models considered use the number of defects detected in the earlier phases of the development process as the independent variable. This number can be used to predict the number of defects to be detected later, even in modified software products. A strong correlation between the number of earlier defects and that of later ones was found. Using this relationship, a mathematical model was derived which may be used to estimate the number of defects remaining in software. This defect model may also be used to guide software developers in evaluating the effectiveness of the software development and testing processes",1988,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6170,no,undetermined,0
A method proposal for early software reliability estimation,"Presents a method proposal for estimation of software reliability before the implementation phase. The method is based upon a formal description technique and that it is possible to develop a tool for performing dynamic analysis, i.e. locating semantic faults in the design. The analysis is performed by applying a usage profile as input as well as doing a full analysis, i.e. locating all faults that the tool can find. The tool must provide failure data in terms of time since the last failure was detected. The mapping of the dynamic failures to the failures encountered during statistical usage testing and operation is discussed. The method can be applied either on the software specification or as a step in the development process by applying it on the design descriptions. The proposed method allows for software reliability estimations that can be used both as a quality indicator, and also for planning and controlling resources, development times etc. at an early stage in the development of software systems",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=285849,no,undetermined,0
Analysis of large system black-box test data,"Studies black box testing and verification of large systems. Testing data is collected from several test teams. A flat, integrated database of test, fault, repair, and source file information is built. A new analysis methodology based on the black box test design and white box analysis is proposed. The methodology is intended to support the reduction of testing costs and enhancement of software quality by improving test selection, eliminating test redundancy, and identifying error prone source files. Using example data from AT&T systems, the improved analysis methodology is demonstrated",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=285854,no,undetermined,0
ScanSAR processing using standard high precision SAR algorithms,"Processing ScanSAR or burst-mode SAR data by standard high precision algorithms (e.g., range/Doppler, wavenumber domain, or chirp scaling) is shown to be an interesting alternative to the normally used SPECAN (or deramp) algorithm. Long burst trains with zeroes inserted into the interburst intervals can be processed coherently. This kind of processing preserves the phase information of the data-an important aspect for ScanSAR interferometry. Due to the interference of the burst images the impulse response shows a periodic modulation that can be eliminated by a subsequent low-pass filtering of the detected image. This strategy allows an easy and safe adaptation of existing SAR processors to ScanSAR data if throughput is not an issue. The images are automatically consistent with regular SAR mode images both with respect to geometry and radiometry. The amount and diversity of the software for a multimode SAR processor are reduced. The impulse response and transfer functions of a burst-mode end-to-end system are derived. Special attention is drawn to the achievable image quality, the radiometric accuracy, and the effective number of looks. The scalloping effect known from burst-mode systems can be controlled by the spectral weighting of the processor transfer function. It is shown that the fact that the burst cycle period is in general not an integer multiple of the sampling grid distance does not complicate the algorithm. An image example using X-SAR data for simulation of a burst system is presented",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=481905,no,undetermined,0
Expert system for analysis of electric power system harmonics,"The proliferation of power electronics devices that cause harmonic voltages and currents and the widespread use of harmonics-sensitive electronic equipment are creating a near-epidemic of â€œpower qualityâ€?issues that are being addressed by both users and suppliers of electric energy. Numerous types, styles, and capabilities exist in the field of instrumentation and diagnostic equipment for the detection, recording, and analysis of the distorted waveform. This article reports on one effort to employ artificial intelligence (Al) in the form of a Harmonics Analysis Expert System (HAES). The resulting PC-based software program has captured the knowledge of expert power system engineers in a â€œrule baseâ€?which can be applied to information about the topology, power signature, operating practices, symptoms, equipment, type and ratings, and chronology of system development to diagnose the power system for harmonics-caused problems. The goal is to accomplish the analysis with a less experienced engineer, thereby multiplying the available nationwide pool of expertise. Additionally, repeated use of the system does result in impartation of knowledge from the system to the user. The process of applying this AI approach involves: (1) training the user, (2) measurements (optional), (3) diagnosis, and (4) solution of the power system for harmonics disturbances to acceptable power quality. The emphasis of this paper is on the diagnostic module (Al program) with a usage example presented",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=384625,no,undetermined,0
Assessing the effects of communication faults on parallel applications,"This paper addresses the problem of injection of faults in the communication system of disjoint memory parallel computers and presents fault injection results showing that 5% to 30% of the faults injected in the communication subsystem of a commercial parallel computer caused undetected errors that lead the application to generate erroneous results. All these cases correspond to situations in which it would be virtually impossible to detect that the benchmark output was erroneous, as the size of the results file was plausible and no system errors had been detected. This emphasizes the need for fault tolerant techniques in parallel systems in order to achieve confidence in the application results. This is especially true in massively parallel computers, as the probability of occurring faults increase with the number of processing nodes. Moreover, in disjoint memory computers, which is the most popular and scalable parallel architecture, the communication subsystem plays an important role, and is also very prone to errors. CSFI (Communication Software Fault Injector) is a versatile tool to inject communication faults in parallel computers. Faults injected with CSFI directly emulate communication faults and spurious messages generated by non fail-silent nodes by software, allowing the evaluation of the impact of faults in parallel systems and the assessment of fault tolerant techniques. The use of CSFI is nearly transparent to the target application as it only requires minor adaptations. Deterministic faults of different nature can be injected without user intervention and fault injection results are collected automatically by CSFI",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=395830,no,undetermined,0
An analysis of client/server outage data,"This paper examines client/server outage data and presents a list of outage causes extracted from the data. The outage causes include hardware, software, operations, and environmental failures, as well as outages due to planned reconfigurations. The study spans all client, server, and network devices in a typical client/server environment. The outage data is used to predict availability in a typical client/server environment and to evaluate various fault-tolerant architectures. The results are stated in terms of user outage minutes and have been validated by comparison with other outage surveys and data in the literature. The major results from the outage data study are: Each client in a client/server environment experiences an average of 12000 minutes (200 hours) annual downtime; Server outages, especially server software failures, are the dominant cause of client/server unavailability; Fault-tolerance in a client/server environment can reduce user outage minutes by nearly an order of magnitude",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=395822,no,undetermined,0
Performability evaluation: where it is and what lies ahead,"The concept of performability emerged from a need to assess a system's ability to perform when performance degrades as a consequence of faults. After almost 20 years of effort concerning its theory, techniques, and applications, performability evaluation is currently well understood by the many people responsible for its development. On the other hand, the utility of combined performance-dependability measures has yet to be appreciably recognized by the designers of contemporary computer systems. Following a review of what performability means, we discuss its present state with respect to both scientific and engineering contributions. In view of current practice and the potential design applicability of performability evaluation, we then point to some advances that are called for if this potential is indeed to be realized",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=395818,no,undetermined,0
Distribution engineering tool features a flexible framework,"The Electric Power Research Institute (EPRI) has been involved in developing a software package designed to meet the needs of distribution engineers. The result of these efforts is a workstation software package (DEWorkstation) that provides an open architecture environment with integrated data and applications. Analysis, design, and operation modules access database data and exchange data through common functions. The open architecture allows external application modules to be added to the workstation. External measurements such as voltage, current, and temperature may be imported and displayed on the circuit schematic. These imported variables are made available to any application program running within the workstation. This distribution engineering tool is designed to meet the analysis, planning, design, and operation needs of distribution engineering through its available application modules. Modules perform the following types of analyses: power flow, load estimation, line impedance calculation, fault analysis, capacitor placement, phase balancing, and more. External data such as customer information system data and distribution transformer data may be imported. The workstation is designed to provide utilities with a platform suitable for distribution automation evaluations",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=392021,no,undetermined,0
Signatures and software find high impedance faults,The detection of high-impedance faults on electrical distribution systems has been one of the most persistent and difficult problems facing the electric utility industry. High-impedance faults result from the unwanted contact of a primary circuit conductor with objects or surfaces that limit the current to levels below the detection thresholds of conventional protection devices. The author describes how recent advances in digital technology have enabled a practical solution for the detection of a high percentage of these previously undetectable faults,1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=392019,no,undetermined,0
Are we developers liars or just fools [software managers],"Things were good in the old days. Very few people understood software, so those of us who were cognoscenti could get away with murder. We could explain away cost and schedule overruns by blaming â€œcomplexity.â€?Performance problems were the fault of emerging technology and if management or customers got angry, what could they do? Although nearly everything that defined â€œthe old daysâ€?has changed, the author argues that many of the old foibles remain. According to the author, software managers and the people who work for them risk being viewed as liars or fools, a perception that can only harm the software community",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=391841,no,undetermined,0
Comparing detection methods for software requirements inspections: a replicated experiment,"Software requirements specifications (SRS) are often validated manually. One such process is inspection, in which several reviewers independently analyze all or part of the specification and search for faults. These faults are then collected at a meeting of the reviewers and author(s). Usually, reviewers use Ad Hoc or Checklist methods to uncover faults. These methods force all reviewers to rely on nonsystematic techniques to search for a wide variety of faults. We hypothesize that a Scenario-based method, in which each reviewer uses different, systematic techniques to search for different, specific classes of faults, will have a significantly higher success rate. We evaluated this hypothesis using a 3Ã—2<sup>4</sup> partial factorial, randomized experimental design. Forty eight graduate students in computer science participated in the experiment. They were assembled into sixteen, three-person teams. Each team inspected two SRS using some combination of Ad Hoc, Checklist or Scenario methods. For each inspection we performed four measurements: (1) individual fault detection rate, (2) team fault detection rate, (3) percentage of faults first identified at the collection meeting (meeting gain rate), and (4) percentage of faults first identified by an individual, but never reported at the collection meeting (meeting loss rate). The experimental results are that (1) the Scenario method had a higher fault detection rate than either Ad Hoc or Checklist methods, (2) Scenario reviewers were more effective at detecting the faults their scenarios are designed to uncover, and were no less effective at detecting other faults than both Ad Hoc or Checklist reviewers, (3) Checklist reviewers were no more effective than Ad Hoc reviewers, and (4) Collection meetings produced no net improvement in the fault detection rate-meeting gains were offset by meeting losses",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=391380,no,undetermined,0
A system approach to reliability and life-cycle cost of process safety-systems,"An analytic method, PDS, allows the designer to assess the cost effectiveness of computer-based process safety-systems based on a quantification of reliability and life-cycle cost. Using PDS in early system design, configurations and operating philosophies can be identified in which the reliability of field devices and logic control units is balanced from a safety and an economic point of view. When quantifying reliability, the effects are included of fault-tolerant and fault-removal techniques, and of failures due to environmental stresses and failures initiated by humans during engineering and operation. A failure taxonomy allows the analyst to treat hardware failures, human failures, and software failures of automatic systems in an integrated manner. The main benefit of this taxonomy is the direct relationship between failure cause and the means used to improve safety-system performance",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=387369,no,undetermined,0
Trends in reliability and test strategies,"Software fails because it contains faults. If we could be certain that our attempts to fix a fault had been effective and that we had introduced no new faults, we would know that we had improved the software's reliability. Unfortunately, no single testing method can be trusted to give accurate results in every circumstance. There are at least two areas of uncertainty in testing. First, we cannot predict when a failure will occur as a result of choosing an input that cannot currently be processed correctly. Second, we do not know what effect fixing a fault will have on the software's reliability. Most of today's reliability models consider the software system to be a black box, using indirect measures of failure by making strong assumptions about the test cases. White-box testing measures, which are direct measures, are generally not used in software-reliability models. The assumption is that the more the program is covered, the more likely that the software is reliable. Hence we must develop new testing strategies. For example, we do not understand very well the notion of stress as it applies to software; what effect the dependence between versions of the same application has on reliability; the relationship between software reliability and white-box testing measures; and how to achieve the ultra-high reliability levels obtained through formal verification and fault-tolerance methods. These are just some of the areas for future research into improved testing strategies",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=382175,no,undetermined,0
Dependability models for iterative software considering correlation between successive inputs,"We consider the dependability of programs of an iterative nature. The dependability of software structures is usually analysed using models that are strongly limited in their realism by the assumptions made to obtain mathematically tractable models and by the lack of experimental data. The assumption of independence between the outcomes of successive executions, which is often false, may lead to significant deviations from the real behaviour of the program under analysis. In this work we present a model in which dependencies among input values of successive iterations me taken into account in studying the dependability of iterative software. We consider also the possibility that repeated, nonfatal failures may together cause mission failure. We evaluate the effects of these different hypotheses on (1) the probability of completing a fixed-duration mission, and (2) a performability measure",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=395841,no,undetermined,0
Predicting behavior of induction motors during service faults and interruptions,A neural network-based identification for induction motor speed is proposed. The backpropagation neural network technique is used to provide real-time adaptive estimation of the motor speed. The validity and effectiveness of the proposed estimator as well as its sensitivity to parameter variation are verified by digital simulations. The proposed identification performs well under vector control and therefore can lead to an improvement in the performance of speed sensorless drives. The new approach is presented in a way that will contribute to a better understanding of neural network applications to motion control,1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=378057,no,undetermined,0
Industrial perspective on static analysis,"Static analysis within industrial applications provides a means of gaining higher assurance for critical software. This survey notes several problems, such as the lack of adequate standards, difficulty in assessing benefits, validation of the model used and acceptance by regulatory bodies. It concludes by outlining potential solutions and future directions.<<ETX>>",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=373902,no,undetermined,0
Short-circuit simulations help quantify wheeling flow,"New England transmission system owners use the results from short-circuit simulations to quantify the transmission wheeling services they provide to deliver power from the Connecticut, Maine, and Vermont Yankee Nuclear Power Plants. The wheeling service of a transmitting utility, defined in terms of MW-miles, is determined with the use of fault currents to reflect the flow of MW entitlements over the New England transmission system. The use of commercially available software to automate simulations, perform the MW-miles calculation, and tabulate the results significantly reduces the time and computational effort",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=372676,no,undetermined,0
Optimal test distributions for software failure cost estimation,"We generalize the input domain based software reliability measures by E.C. Nelson (1973) and by S.N. Weiss and E.J. Weyuker (1988), introducing expected failure costs under the operational distribution as a measure for software unreliability. This approach incorporates in the reliability concept a distinction between different degrees of failure severity. It is shown how to estimate the proposed quantity by means of random testing, using the Importance Sampling technique from Rare Event Simulation. A test input distribution that yields an unbiased estimator with minimum variance is determined. The practical application of the presented method is outlined, and a detailed numerical example is given",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=372149,no,undetermined,0
Faults and unbalance forces in the switched reluctance machine,"The paper identifies and analyzes a number of severe fault conditions that can occur in the switched reluctance machine, from the electrical and mechanical points of view. It is shown how the currents, torques, and forces may be estimated, and examples are included showing the possibility of large lateral forces on the rotor. The methods used for analysis include finite-element analysis, magnetic circuit models, and experiments on a small machine specially modified for the measurement of forces and magnetization characteristics when the rotor is off-center. Also described is a computer program (PC-SRD dynamic) which is used for simulating operation under fault conditions as well as normal conditions. The paper discusses various electrical configurations of windings and controller circuits, along with methods of fault detection and protective relaying. The paper attempts to cover several analytical and experimental aspects as well as methods of detection and protection",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=370280,no,undetermined,0
Test and measurement,"The author discusses the latest developments in test and measurement technology including: I<sub>DDQ</sub> testing, its benefits and disadvantages; pay-per-use testers in which a button is inserted in a fully configured system and programmed with test credits which must be purchased in advance; digital oscilloscopes; and Hewlett Packard's entry into the VXIplug and play Systems Alliance",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=366241,no,undetermined,0
Evaluating FTRE's for dependability measures in fault tolerant systems,"In order to analyze dependability measures in a fault tolerant system, we generally consider a nonstate space or a state space type model. A fault tree with repeated events (FTRE's) presents an important strategy for the nonstate space model. The paper deals with a conservative assessment to complex fault tree models, henceforth called CRAFT, to obtain an approximate analysis of the FTRE's. It is a noncutset, direct, bottom-up approach. It uses failure probability or failure rate as input and determines a bound on the probability of occurrence of the TOP event. CRAFT generalizes the concept of a cutting heuristic that obtains the signal probabilities for testability measurement in logic circuits. The method is efficient and solves coherent and noncoherent FTRE's having AND, OR, XOR, and NOT gates. In addition, CRAFT considers M/N priority AND, and two types of functional dependency, namely OR and AND types. Examples such as the Cm* architecture and a fault-tolerant software based on recovery block concept are used to illustrate the approach. The paper also provides a comparison with approaches such as SHARPE, HARP, and FTC",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=364538,no,undetermined,0
Fault-tolerant features in the HaL memory management unit,"This paper describes fault-tolerant and error detection features in HaL's memory management unit (MMU). The proposed fault-tolerant features allow recovery from transient errors in the MMU. It is shown that these features were natural choices considering the architectural and implementation constraints in the MMU's design environment. Three concurrent error detection and correction methods employed in address translation and coherence tables in the MMU are described. Virtually-indexed and virtually-tagged cache architecture is exploited to provide an almost fault-secure hardware coherence mechanism in the MMU, with very small performance overhead (less than 0.01% in the instruction throughput). Low overhead linear polynomial codes have been chosen in these designs to minimize both the hardware and software instrumentation impact",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=364529,no,undetermined,0
Design and analysis of efficient fault-detecting network membership protocols,"Network membership protocols determine the present nodes and links in a computer network and, therefore, contribute to making the huge amount of general distributed systems and their inherent redundancy available for fault tolerance. Two different protocols, GNL and GLV, are described in detail which solve the problem for a very general set of assumptions not covered by existing solutions of related research areas like network exploration, system level diagnosis and group membership: neither the topology nor a superset of the nodes are known in advance. Nodes do not require any special initial knowledge except a unique relative signature (Leu, 1994) for authentication. Faults may affect any number of nodes and can cause arbitrary behaviour except for breaking cryptographic fault detection mechanisms like digital signatures. The protocols are compared in terms of execution time and global communication costs based on simulation experiments on randomly generated topologies. It is shown that the GLV protocol dominates the GNL protocol clearly for nearly all practically relevant cases",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=395837,no,undetermined,0
On efficiently tolerating general failures in autonomous decentralized multiserver systems,We consider a multiserver system consisting of a set of servers that provide some service to a set of clients by accessing some shared objects. The goal is to provide reliable service in spite of client or server failures such that the overhead during normal operating periods is low. We consider a relatively general fault model where a faulty processor can write spurious data for a period of time before it is detected and removed from the system. We first develop a solution in an autonomous physical world of clients and servers. The basic approach is to divide the servers into groups such that each server has some limitations which prevent it from arbitrarily damaging the system. This solution is then mapped to a distributed system of processor and memory units followed by an assessment of its performance,1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=398986,no,undetermined,0
RTSPC: a software utility for real-time SPC and tool data analysis,"Competition in the semiconductor industry is forcing manufacturers to continuously improve the capability of their equipment. The analysis of real-time sensor data from semiconductor manufacturing equipment presents the opportunity to reduce the cost of ownership of the equipment. Previous work by the authors showed that time series filtering in combination with multivariate analysis techniques can be utilized to perform statistical process control, and thereby generate real-time alarms in the case of equipment malfunction. A more robust version of this fault detection algorithm is presented. The algorithm is implemented through RTSPC, a software utility which collects real-time sensor data from the equipment and generates real-time alarms, Examples of alarm generation using RTSPC on a plasma etcher are presented",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=350754,no,undetermined,0
Evaluation of software dependability based on stability test data,"The paper discusses a measurement-based approach to dependability evaluation of fault-tolerant, real-time software systems based on failure data collected from stability tests of an air traffic control system under development. Several dependability analysis techniques are illustrated with the data: parameter estimation, availability modeling of software from the task level, applications of the parameter estimation and model evaluation in assessing availability, identifying key problem areas, and predicting required test duration for achieving desired levels of availability and quantification of relationships between software size, the number of faults, and failure rate for a software unit. Although most discussion is focused on a typical subsystem, Sector Suite, the discussed methodology is applicable to other subsystems and the system. The study demonstrates a promising approach to measuring and assessing software availability during the development phase, which has been increasingly demanded by the project management of developing large, critical systems.<<ETX>>",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=466956,no,undetermined,0
Automated X-ray inspection of chip-to-chip interconnections of Si-on-Si MCM's,"The silicon-on-silicon (Si-on-Si) multichip module (MCM) is one of the newest system integration technologies that has the potential for high functional integration, enhanced performance, and cost effectiveness. Aside from functional tests, X-ray inspection is required to ensure good quality chip-to-chip interconnections during the fabrication process. This paper presents an automated inspection system that is capable of detecting defects such as â€œswollenâ€?solders, misaligned solders, missing solders, solder robbing, and solder bridging in a semi-finished MCM. The semi-finished MCM is in wafer form and has completed the following operations: stencil printing device placing, and solder reflowing. The defect detection methodology is detailed. Over a test set of 54 sample images of wafer tiles, 100% inspection accuracy was obtained. This system has the potential to automate the manual visual inspection operation which is tedious, slow, and error-prone",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=475273,no,undetermined,0
Development of computer-based measurements and their application to PD pattern analysis,"After referring briefly to the earlier developments of computer-based PD measurements in the 1970s, the paper outlines how the introduction of new technology has resulted in the design and production of a number of digital systems for use in test laboratories and on site. In order to avoid overgeneralization, the basis of the hardware required for detection and recording of the PD pulses is described by reference to a particular system, it being understood that many detailed variations are possible depending on the application. The paper describes a number of parameters that can be calculated by software and used to characterize the PD behavior. These include the IEC-270 integrated quantities, statistical moments and other fingerprints for recognizing the PD patterns within the power frequency cycle. Examples are given of the patterns obtained on equipment and samples. These are necessarily selective and are intended to indicate how the techniques may be applied. It is concluded that these new methods will become valuable in procedures for assessing the condition of electrical insulation, in particular faults or defects, but must be used in conjunction with existing techniques until improved confidence is achieved in interpreting results, especially in respect of the various phase-resolved distributions and calibration procedures",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=469978,no,undetermined,0
SEL's software process improvement program,"We select candidates for process change on the basis of quantified Software Engineering Laboratory (SEL) experiences and clearly defined goals for the software. After we select the changes, we provide training and formulate experiment plans. We then apply the new process to one or more production projects and take detailed measurements. We assess process success by comparing these measures with the continually evolving baseline. Based upon the results of the analysis, we adopt, discard, or revise the process",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=469763,no,undetermined,0
Using a neural network to predict test case effectiveness,"Test cases based on command language or program language descriptions have been generated automatically for at least two decades. More recently, domain based testing (DBT) was proposed as an alternative method. Automated test data generation decreases test generation time and cost, but we must evaluate its effectiveness. We report on an experiment with a neural network as a classifier to learn about the system under test and to predict the fault exposure capability of newly generated test cases. The network is trained on test case metric input data and fault severity level output parameters. Results show that a neural net can be an effective approach to test case effectiveness prediction. The neural net formalizes and objectively evaluates some of the testing folklore and rules-of-thumb that are system specific and often require many years of testing experience",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=468919,no,undetermined,0
The multi-element mercuric iodide detector array with computer controlled miniaturized electronics for EXAFS,"Construction of a 100-element HgI<sub>2</sub> detector array, with miniaturized electronics, and software developed for synchrotron applications in the 5 keV to 35 keV region has been completed. Recently, extended X-ray absorption fine structure (EXAFS) data on dilute (~1 mM) metallo-protein samples were obtained with up to seventy-five elements of the system installed. The data quality obtained is excellent and shows that the detector is quite competitive as compared to commercially available systems. The system represents the largest detector array ever developed for high resolution, high count rate X-ray synchrotron applications. It also represents the first development and demonstration of high-density miniaturized spectroscopy electronics with this high level of performance. Lastly, the integration of the whole system into an automated computer-controlled environment represents. A major advancement in the user interface for XAS measurements. These experiments clearly demonstrate that the HgI<sub>2</sub> system, with the miniaturized electronics and associated computer control functions well. In addition it shows that the new system provides superior ease of use and functionality, and that data quality is as good as or better than with state-of-the-art cryogenically cooled Ge systems",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=467914,no,undetermined,0
Fault-tolerance for off-the-shelf applications and hardware,"The concept of middleware provides a transparent way to augment and change the characteristics of a service provider as seen from a client. Fault tolerant policies are ideal candidates for middleware implementation. We have defined and implemented operating system based middleware support that provides the power and flexibility needed by diverse fault tolerant policies. This mechanism, called the sentry, has been built into the UNIX 4.3 BSD operating system server running on a Mach 3.0 kernel. To demonstrate the effectiveness of the mechanism several policies have been implemented using sentries including checkpointing and journaling. The implementation shows that complex fault tolerant policies can be efficiently and transparently implemented as middleware. Performance overhead of input journaling is less than 5% and application suspension during the checkpoint is typically under 10 seconds in length. A standard hard disk is used to store journal and checkpoint information with dedicated storage requirements of less than 20 MB.<<ETX>>",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=466997,no,undetermined,0
Implicit signature checking,"Proposes a control flow checking method that assigns unique initial signatures to each basic block in a program by using the block's start address. Using this strategy, implicit signature checking points are obtained at the beginning of each basic block, which results in a short error detection latency (2-5 instructions). Justifying signatures are embedded at each branch instruction, and a watchdog timer is used to detect the absence of a signature checking point. The method does not require the building of a program flow graph and it handles jumps to destinations that are not fixed at compile/link-time, e.g. subroutine calls using function pointers in the C language. This paper includes a generalized description of the control flow checking method, as well as a description and evaluation of an implementation of the method.<<ETX>>",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=466976,no,undetermined,0
On-line error monitoring for several data structures,"We present several examples of programs which efficiently monitor the answers from queries performed on data structures to determine if any errors are present. Our paper includes the first efficient on-line error monitor for a data structure designed to perform nearest neighbor queries. Applications of nearest neighbor queries are extensive and include learning, categorization, speech processing, and data compression. Our paper also discusses on-line error monitors for priority queues and splittable priority queues. On-line error monitors immediately detect if an error is present in the answer to a query. An error monitor which is not on-line may delay the time of detection until a later query is being processed which may allow the error to propagate or may cause irreversible state changes. On-line monitors can allow a more rapid and accurate response to an error.<<ETX>>",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=466960,no,undetermined,0
Localizing multiple faults in a protocol implementation,"<div style=""font-variant: small-caps; font-size: .9em;"">First Page of the Article</div><img class=""img-abs-container"" style=""width: 95%; border: 1px solid #808080;"" src=""/xploreAssets/images/absImages/00466373.png"" border=""0"">",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=466373,no,undetermined,0
Shape classification of flaw indications in three-dimensional ultrasonic images,"The rapid evolution of computing hardware technology now allows sophisticated software techniques to be employed which will aid the NDT data interpreter in the process of defect detection and classification. The paper describes an investigation into the area of three-dimensional ultrasonic image evaluation and, more specifically, the problem of characterising the shape of suspect flaw regions. A backpropagation neural network is used as the classifier for a series of four three-dimensional feature extraction methods which are individually assessed on two particular recognition problems. The optimum technique was determined for inclusion in an evaluation environment called the NDT Workbench, which has been designed for the processing of real data. Two acquired ultrasonic data sets are assessed using the best-performing classification method",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=401282,no,undetermined,0
The automatic generation of load test suites and the assessment of the resulting software,"Three automatic test case generation algorithms intended to test the resource allocation mechanisms of telecommunications software systems are introduced. Although these techniques were specifically designed for testing telecommunications software, they can be used to generate test cases for any software system that is modelable by a Markov chain provided operational profile data can either be collected or estimated. These algorithms have been used successfully to perform load testing for several real industrial software systems. Experience generating test suites for five such systems is presented. Early experience with the algorithms indicate that they are highly effective at detecting subtle faults that would have been likely to be missed if load testing had been done in the more traditional way, using hand-crafted test cases. A domain-based reliability measure is applied to systems after the load testing algorithms have been used to generate test data. Data are presented for the same five industrial telecommunications systems in order to track the reliability as a function of the degree of system degradation experienced",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=464549,no,undetermined,0
Intelligent alarm method by fuzzy measure and its application to plant abnormality prediction,"Traditional methods for diagnosis/prediction of plant abnormality rely on the acquisition of fragmentary knowledge or rules from experts and the application of the state vectors of plant to portions of the knowledge or rules thus obtained to derive a conclusion. Therefore as the complexity of the plant to be evaluated increases, the degree of uncertainty of knowledge obtained from experts increases and thus the accuracy of evaluation decreases. To solve this problem, this paper proposes an intelligent alarm method which uses a fuzzy integral model based on fuzzy measure that can provide an accurate model of human subjective evaluation mechanism. The intelligent alarm method is unique in that it can quantify the uncertainty of the information to be evaluated and of human being who evaluates. To achieve these objectives, this paper provides a simulation of the expert's overall plant evaluation by using an â€œinterpretation of plant state based on expert knowledgeâ€?and a â€œoverall judgement of plant state which amplifies the essence of the state interpretation informationâ€? The method was applied to the prediction of abnormality for a drying process of a chemical plant to demonstrate that abnormality prediction by a simulation method is possible, that even for a plant without abnormality the method can provide estimate of the potential of abnormality, and that the method is useful in alleviating the burden of the monitoring and checking work for plant",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=409709,no,undetermined,0
Making process improvement personal,"The personal software process (PSP) is a structured set of process descriptions, measurements and methods that can help engineers improve their personal performance. The PSP provides the forms, scripts and standards that help them estimate and plan their work. It shows them how to define processes and how to measure their quality and productivity. The PSP acknowledges that everyone is unique and that a method that suits one engineer may not suit another. The PSP helps each engineer measure and track his own work so that he can find the method best for him",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=406762,no,undetermined,0
Character recognition without segmentation,"A segmentation-free approach to OCR is presented as part of a knowledge-based word interpretation model. It is based on the recognition of subgraphs homeomorphic to previously defined prototypes of characters. Gaps are identified as potential parts of characters by implementing a variant of the notion of relative neighborhood used in computational perception. Each subgraph of strokes that matches a previously defined character prototype is recognized anywhere in the word even if it corresponds to a broken character or to a character touching another one. The characters are detected in the order defined by the matching quality. Each subgraph that is recognized is introduced as a node in a directed net that compiles different alternatives of interpretation of the features in the feature graph. A path in the net represents a consistent succession of characters. A final search for the optimal path under certain criteria gives the best interpretation of the word features. Broken characters are recognized by looking for gaps between features that may be interpreted as part of a character. Touching characters are recognized because the matching allows nonmatched adjacent strokes. The recognition results for over 24,000 printed numeral characters belonging to a USPS database and on some hand-printed words confirmed the method's high robustness level",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=406657,no,undetermined,0
Maximum likelihood voting for fault-tolerant software with finite output-space,"When the output space of a multiversion software is finite, several software versions can give identical but incorrect outputs. This paper proposes a maximum likelihood voting (MLV) strategy for multiversion software with finite output-space under the assumption of failure independence. To estimate the correct result, MLV uses the reliability of each software version and determines the most likely correct result. In addition, two enhancements are made to MLV: (1) imposition of a requirement Î±* that the most likely correct output must have probability of at least Î±*; and (2) the voter can estimate when it has received one or more outputs from the software versions. If the probability that the estimated result is correct and is at least Î±*, then it immediately gives this estimated output. Since the voter need not wait for all the outputs before it can estimate, the required mean execution time can be reduced. The numerical results show that these MLV strategies have better performance than consensus voting and majority voting, especially when the variation of software version reliability is large. Enhancement #2 can appreciably reduce the mean execution time, especially when the software versions have larger execution time standard-deviation",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=406576,no,undetermined,0
Complexity measure evaluation and selection,"A formal model of program complexity developed earlier by the authors is used to derive evaluation criteria for program complexity measures. This is then used to determine which measures are appropriate within a particular application domain. A set of rules for determining feasible measures for a particular application domain are given, and an evaluation model for choosing among alternative feasible measures is presented. This model is used to select measures from the classification trees produced by the empirically guided software development environment of R.W. Selby and A.A. Porter, and early experiments show it to be an effective process",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=403788,no,undetermined,0
Noise cancellation by a whole-cortex SQUID MEG system,"We report on the noise cancellation performance of several whole-cortex MEG systems operated under diverse noise conditions, ranging from unshielded environments to moderately shielded rooms. The noise cancellation is performed by means of spatial filtering using high order gradiometers (2nd or 3rd), which can be formed optionally either by SQUID electronics firmware or by software. The spatial dependence of the gradiometer responses was measured in an unshielded environment and compared with simulations to yield an estimate of the system common mode magnitudes relative to field, 1st, and 2nd gradients. High order gradiometers were also formed when the system was operated in moderately shielded rooms. The combination of the spatial filtering and room shielding resulted in very high combined noise rejections approaching that of high quality shielded rooms. Examples of noise cancellation under various conditions are shown.<<ETX>>",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=403001,no,undetermined,0
Asynchronous circuit synthesis with Boolean satisfiability,"Asynchronous circuits are widely used in many real time applications such as digital communication and computer systems. The design of complex asynchronous circuits is a difficult and error-prone task. An adequate synthesis method will significantly simplify the design and reduce errors. In this paper, we present a general and efficient partitioning approach to the synthesis of asynchronous circuits from general Signal Transition Graph (STG) specifications. The method partitions a large signal transition graph into smaller and manageable subgraphs which significantly reduces the complexity of asynchronous circuit synthesis. Experimental results of our partitioning approach with large number of practical industrial asynchronous circuit benchmarks are presented. They show that, compared to the existing asynchronous circuit synthesis techniques, this partitioning approach achieves many orders of magnitude of performance improvements in terms of computing time, in addition to the reduced circuit implementation area. This lends itself well to practical asynchronous circuit synthesis from general STG specifications",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=402496,no,undetermined,0
SEI capability maturity model's impact on contractors,"The Software Engineering Institute's Capability Maturity Model measures software development companies' ability to produce quality software within budget and on schedule. The CMM rates software development companies and government agencies at one of five maturity levels. Two key features are assessments and evaluations. Assessments represent a company's or government agency's voluntary, internal efforts to identify its current CMM maturity level and areas needing improvement. Evaluations represent an independent assessor's examination of a company's maturity level. Industry responses to the model vary. Some companies have reported that savings significantly offset the costs of achieving and maintaining a maturity level. Others have attacked the CMM on many fronts, such as its failure to incorporate Total Quality Management principles. Nevertheless, either enthusiastically or reluctantly companies are attempting to achieve higher CMM levels. The authors present both favorable and unfavorable reactions to the model as well as insight gained through discussions with various companies. They conclude that since no approach that enforces improvements will be universally acceptable in all aspects to all concerned, the CMM, on balance, can be considered a very successful model, particularly when combined with TQM principles. There may be uncertainty as to whether the costs of attaining and maintaining a CMM level will be recouped through reduced software production costs and more efficient software engineering practices (as published studies report). But with continued strong DoD sponsorship, there will likely be an increasing number of companies that base their software process improvement efforts on the CMM",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=362633,no,undetermined,0
Determining software schedules,"Improving software productivity, shortening schedules or time to market, and improving quality are prominent topics in software journals in both contributed articles and advertising copy. Unfortunately, most of these articles and advertisements have dealt with software schedules, productivity, or quality factors in abstract terms. Now we can measure these factors with reasonable accuracy and collect empirical data on both average and best-in-class results. We are particularly interested in the wide performance gaps between laggards, average enterprises, and industry leaders, as well as differences among the various software domains. The function-point metric lets us establish a meaningful database of software performance levels. A simple algorithm raises function points to a total to obtain a useful first-order schedule estimate",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=348003,no,undetermined,0
A neural network approach for predicting software development faults,"Accurately predicting the number of faults in program modules is a major problem in the quality control of a large scale software system. In this paper, the use of the neural networks as a tool for predicting the number of faults in programs is explored. Software complexity metrics have been shown to be closely related to the distribution of faults in program modules. The objective in the construction of models of software quality is to use measures that may be obtained relatively early in the software development life cycle to provide reasonable initial estimates of quality of an evolving software system. Measures of software quality and software complexity to be used in this modeling process exhibit systematic departures of normality assumptions of regression modeling. This paper introduces a new approach for static reliability modeling and compares its performance in the modeling of software reliability from software complexity in terms of the predictive quality and the quality of fit with more traditional regression modeling techniques. The neural networks did produce models with better quality of fit and predictive quality when applied to one data set obtained from a large commercial system",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=285855,no,undetermined,0
An experimental comparison of software methodologies for image based quality control,"In this paper we study the cost structure of a software system for the visual classification of industrial parts. We investigate two different development methodologies and compare them using a suitable cost function. The first approach aims to the development of a piece of software fully tailored to the specific problem, while the second one exploits the capabilities of adaptive signal processing techniques to reduce the software production test",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=398115,no,undetermined,0
From the software process to software quality: BOOTSTRAP and ISO 9000,"The aim of the BOOTSTRAP project was to develop a method for software process assessment, quantitative measurement and improvement. BOOTSTRAP enhanced and refined the SEI method for software process assessment and adapted it to the needs of software producing units (SPUs) in the European software industry. BOOTSTRAP assesses both the SPU and the projects of the SPU with respect to policies and guidelines and their implementation. The BOOTSTRAP maturity level algorithm determines quantitative process quality profiles for individual process quality attributes. This quality profile serves as a basis for making decisions about process improvements and allows to determine the degree of satisfaction of ISO 9000",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=465262,no,undetermined,0
Nonseparable orthogonal linear phase perfect reconstruction filter banks and their application to image compression,"We have designed a 4-channel 10Ã—10 nonseparable orthogonal linear phase perfect reconstruction filter bank (PRFB), hereafter NSEP, and examined its performance in image compression applications. It is shown that, in addition to the orthogonality property, NSEP offers an excellent compression performance. Because of their limited spatial support, the nonseparable filters can be implemented at a reasonable computational cost. The orthogonality property also allows the use of image compression algorithms which exploit orthogonality for bit allocation purposes. We demonstrate that the NSEP filters have superior spatial localization properties when compared to filters arising in a 2-D separable PRFB generated from a l-D Daubechies FB of length 10. This superior spatial localization results in less visible ringing distortion in compressed images. In addition, the linear phase property of NSEP distributes the ripples located at image edges evenly on both sides of each edge, thus reducing further their visibility. Because of these advantages, NSEP is a viable and attractive candidate for high quality, low bit rate image compression applications",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=413832,no,undetermined,0
Temporal resolution scalable video coding,"Scalable video coding is important in a number of applications where video needs to be decoded and displayed at a variety of spatial and temporal resolution scales or when resilience to errors is necessary. In this paper, we investigate a temporal-domain approach for resolution scalable video coding. This approach was originally proposed to and since then has been included in the Moving Picture Experts Group (MPEG-2) standard. Temporal scalability, although not limited to, is particularly useful for applications such as, HDTV as well as for high quality telecommunication applications that may require high frame rate (e.g., 60 Hz) progressive video. Furthermore, it allows flexibility in use of progressive or interlaced video formats for coding even though the source video may be progressive with high frame rate. Our technique builds on the motion-compensated DCT framework of nonscalable (single layer) MPEG-2 video coding and adds motion compensated inter-layer coding. In our simulations we focus on the 2- layer temporally scalable coding and compare the performance of using progressive video format for both layers to that when interlaced video format is used for both layers; all interim video formats used are derived from original high temporal resolution progressive video. In each case we compare the performance of two inter-layer prediction configurations and evaluate their prediction efficiency",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=413495,no,undetermined,0
Error Recovery in Parallel Systems of Pipelined Processors with Caches,"This paper examines the problem of recovering from processor transient faults in pipelined multiprocessor systems. A pipelined machine allows out of order instruction execution and branch prediction to increase performance, thus a precise computation state may not be available. We propose a modified scheme to implement the precise computation state in a pipelined machine. The goal of this research is to implement checkpointing and rollback for error recovery in a pipelined system based on the technique to achieving precise computation state. Detailed analysis has been performed to demonstrate the effectiveness of this method.",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4115730,no,undetermined,0
Systematic approach to selecting H<sub>âˆ?/sub> weighting functions for DC servos,"This paper proposes an experimental solution for H<sub>âˆ?</sub> weighting functions selection by exploiting an experimental planning method which is widely used in quality control. Conducting matrix experiments via orthogonal array, several parameters in H<sub>âˆ?/sub> weighting functions can be determined efficiently so that the resulting controller is able to satisfy all design specifications. Of great importance, the proposed experimental method provides additional robust performance property to conventional H<sub>âˆ?/sub> control which can only prove robust stability and nominal performance. Velocity controller design of DC servomotors is demonstrated to show the remarkable robustness and superior servo performance provided by experimental H<sub>âˆ?/sub> design. A hardware environment is constructed to correlate the theoretical prediction and the measurement data. All specifications of robust stability and performance are validated by the experimental results",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=411290,no,undetermined,0
Quantitative evaluation of expert systems,"The rapidly increasing application of expert systems has lead to the need to assure their quality, especially in the critical areas where a fault may be serious. Metrics can provide a quantitative basis for such assurance activities. In this paper, we investigate several formal metrics that are designed for measuring the three important characteristics of expert systems: the size, search space, and complexity. The applications of these metrics to assess or predict the quality of expert systems are also addressed. In addition, a comprehensive evaluation and comparison of the metrics is conducted with the aim to assess their validity when applying them to quantify expert systems",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=400190,no,undetermined,0
Software risk management: requirements-based risk metrics,"There are several problems associated with current approaches to requirements-based risk metrics for software risk management. For example, the approaches are qualitative and limit risk considerations to probability of occurrence and severity of impact. They also do not take into account the relative size of the requirements category of which they are a part, i.e. the development effort needed to implement that requirements category. Finally, quantitative factors are not used. The advanced integrated requirements engineering system (AIRES) provides automated support for the quantitative identification and extraction of requirements-based software risk metrics throughout the requirements engineering life cycle. The AIRES risk assessment framework and techniques for the integrated application of both semantic and syntactic rules provide for effective, efficient, and comprehensive identification of requirements at risk in large complex multiple segment systems. A review of a portion of the requirements for a Navy Gun Mount System is included to provide an example of the use of AIRES to support the development of requirements-based risk metrics",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=399939,no,undetermined,0
Prediction of image performance and real-time quality improvement for the SIR-C/X-SAR SRL-1 mission,"For the SIR-C/X-SAR (Space Radar Lab-1) mission, launched the 9th of April 94, the expected image performance for selected sites was evaluated before the mission in terms of S/N ratio, radiometric and spatial resolution and ambiguity ratio related to the best suitable radar settings according to the nominal orbit. The software tool, named Performance Estimator (PE), which was used to calculate the parameters for such an image quality prediction is explained in detail. This tool was also used during the mission to be able to recalculate the radar settings such as PRF, Data Window Position (DWP), etc. According to any changes in the timeline to be sure that the sensor was always correctly pointing the desired scenario. Such changes can be caused either by not acceptable deviations of the shuttle from the planned attitude and orbit, or by the insertion of new experiments required by the scientist according to instantaneous appearing special conditions or effects e.g. volcanic eruptions or can be forced by reacting to contingencies. During the mission a further software tool was used to improve the image quality in real-time. This tool, called Performance Analyzer (PA), is also explained in detail. The purpose of this tool is to analyze the quality of the high rate data (off-line) and to assess the quality of the echo profile obtained with the telemetry data (on-Line) in order to allow, if necessary, the Performance engineer to optimize real-time the echo profile",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=399698,no,undetermined,0
A transputer-based waveform synthesizer for protection relay test and parameter estimation applications,"Power system faults can give rise to highly complex transient voltage and current waveforms, particularly during the periods immediately following the faults. Modern microprocessor-based protection relays can in principle respond intelligently to such input signals. This gives rise to the need for test equipment with the capability to evaluate protection relay performance for representative waveform data recorded for actual field conditions, or obtained alternatively by simulation using software packages such as the Electromagnetic Transients Program (EMTP). This paper describes a computer-based, multiphase, arbitrary waveform synthesizer for protection relay test applications. The system distinguishes itself from other such arrangements in that it features a highly programmable and versatile transputer-based digital to analog converter facility developed primarily with the view to obtain optimized performance for protection test applications",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=398093,no,undetermined,0
A fault tolerant pipelined adaptive filter,"A fault tolerant pipelined architecture for high sampling rate adaptive filters is presented. The architecture, which is based on the computational requirements of delayed LMS and adaptive lattice filters, offers robust performance in the presence of single hardware faults, and software faults resulting from numerical instability. The reliability of the proposed system is analyzed and compared to the existing implementations strategies, and methods for fault detection, fault location, and recovery via hardware reconfiguration are discussed. Finally, simulation results illustrating recovery from processor fault are presented",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=471615,no,undetermined,0
The automatic generation of march tests,"Many memory tests have been designed in the past, one class of tests which has been proven to be very efficient in terms of fault coverage as well as test time, is the class of march tests. Designing march tests is a tedious, manual task. This paper presents a method which can, given a set of fault models, automatically generate the required march tests. It has been implemented in the programming language C and shown to be effective",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=397192,no,undetermined,0
Assessing the fuzziness of general system characteristics in estimating software size,"Function point analysis (FPA) is a widely used method for estimating software size. However, there are subjective elements involved in the process of function point assessment. The single estimate given by FPA does not show the confidence interval. Therefore, there is no way to assess the confidence level of the estimate. The paper introduces a fuzzified function point analysis (FFPA) model to help software size estimators to express their judgment and use fuzzy B spline membership function (BMF) to derive their assessment values. Through a case study for an inhouse software development department, the paper presents the experiences of using FFPA to estimate the software size and compares it with the conventional FPA",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=396923,no,undetermined,0
Conversion of scanned documents to the open document architecture,"The paper presents a system for the conversion of scanned documents into the open document architecture. Unlike previous work in this field the authors use a combination of evidence sources to achieve greater robustness to document defects and noise introduced in the scanning process. Furthermore, they use optical character recognition in conjunction with other forms of image analysis as a means of detecting document structure. This enables enhanced document feature extraction and improved performance. They demonstrate the performance of the system on a specific class of input document",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=389557,no,undetermined,0
Performance evaluation of admission policies in ATM based embedded real-time systems,"We study the effect of the output link scheduling discipline of an ATM switch on the ability of an ATM LAN to admit real-time connections. Three output link scheduling policies are studied: first come first served (FCFS), round robin (RR), and packet-by-packet generalized processor sharing (PGPS). We derive connection admission criteria for the three scheduling policies. To evaluate the performance of the three scheduling policies, we introduce the metric of admission probability. The admission probability gives the probability that a randomly chosen set of real-time connections will be admitted into the network. The admission probability allows system designers to study the performance of different scheduling policies over a wide range of network loads. We observe that the performance of the three scheduling policies is sensitive to message deadlines. When the deadlines are small, PGPS outperforms both RR and FCFS, and RR outperforms FCFS. When the deadlines are large, all three scheduling policies perform the same. We also note that although PGPS is better than RR and FCFS most of the time, its improved performance is achieved at the cost of high implementation complexity and run time overheads",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=386607,no,undetermined,0
Creating a diagnostic engineering tool set,"The development of supportability techniques and tools which keep pace with mission equipment technology is essential to minimize long term maintenance costs. The DoD's solution is the use of an integrated diagnostic (ID) process. An integral part of MDA's plan to implement ID is the use of automated tools to effectively assess diagnostic capabilities during the design process. The development and implementation of a procedure to effectively diagnose an item of mission equipment involves several analyses performed during various phases of the equipment development. A mixture of automated tools and manual methods are currently used to perform these analyses. The effort required to perform these diagnostic analyses can be significantly reduced and the quality of the analyses can be significantly improved by developing an automated method of implementing those portions of the analysis which are currently performed manually. Additional improvements can be realized by integrating those automated tools, utilized separately to perform each analysis, into a unified tool set operating on a common set of design data",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=381564,no,undetermined,0
"Internal/external test harmonization, verticality and performance metrics-a systems approach",On-line BIT (built in test) and off-line TPS (test program sets) both suffer from a history of chronic performance deficiencies. Many of these deficiencies can be remedied by more closely coordinating concurrent BIT/TPS development and by applying the same quantitative test performance metrics to both areas under a vertical test design approach,1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=381529,no,undetermined,0
Improving neural network predictions of software quality using principal components analysis,"The application of statistical modeling techniques has been an intensely pursued area of research in the field of software engineering. The goal has been to model software quality and use that information to better understand the software development process. Neural network modeling methods have been applied to this field. The results reported indicate that neural network models have better predictive quality than some statistical models when predicting reliability and the number of faults. In this paper, we will explore the application of principal components analysis to neural network modeling as a way of improving the predictive quality of neural network quality models. We trained two neural nets with data collected from a large commercial software system, one with raw data, and one with principal components. Then, we compare the predictive quality of the two competing neural net models",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=374764,no,undetermined,0
Multi-layer perceptron ensembles for increased performance and fault-tolerance in pattern recognition tasks,"Multilayer perceptrons (MLPs) have proven to be an effective way to solve classification tasks. A major concern in their use is the difficulty to define the proper network for a specific application, due to the sensitivity to the initial conditions and to overfitting and underfitting problems which limit their generalization capability. Moreover, time and hardware constraints may seriously reduce the degrees of freedom in the search for a single optimal network. A very promising way to partially overcome such drawbacks is the use of MLP ensembles: averaging and voting techniques are largely used in classical statistical pattern recognition and can be fruitfully applied to MLP classifiers. This work summarizes our experience in this field. A real-world OCR task is used as a test case to compare different models",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=374692,no,undetermined,0
Information fusion method in fault detection and diagnosis,"In order to improve the reliability of fault detection and diagnosis (FDD) for dynamic system, it is important to make full use of the information from system knowledge and system measurements. This paper presents a scheme of information fusion in FDD. In the proposed scheme, the local fusion adopts multiple FDD strategies aiming at one measurement of the system, then the results of these strategies are fused. The global fusion will fuse the results of every local fusion",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=467147,no,undetermined,0
A software metrics program,"Software metrics play a central role in software management and technology, as well as in support of general business goals. Metrics are utilized at all levels of a corporation. This paper focuses on software metrics at the multi-divisional level. It views metrics as an integral part of software process. The ideas presented are based on experiences and lessons learned with the metrics and process program within GTE Government Systems Corporation (GSC). The software metrics program at GTE GSC is described, including metrics for software estimation, tracking, and control of software project progress and quality. Trends in software metrics are identified",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=472674,no,undetermined,0
Automatic Data Layout for High Performance Fortran,"High Performance Fortran (HPF) is rapidly gaining acceptance as a language for parallel programming. The goal of HPF is to provide a simple yet efficient machine independent parallel programming model. Besides the algorithm selection, the data layout choice is the key intellectual step in writing an efficient HPF program. The developers of HPF did not believe that data layouts can be determined automatically in all cases, Therefore HPF requires the user to specify the data layout. It is the task of the HPF compiler to generate efficient code for the user supplied data layout. The choice of a good data layout depends on the HPF compiler used, the target architecture, the problem size, and the number of available processors. Allowing remapping of arrays at specific points in the program makes the selection of an efficient data layout even harder. Although finding an efficient data layout fully automatically may not be possible in all cases. HPF users will need support during the data layout selection process. In particular, this support is necessary if the user is not familiar with the characteristics of the target HPF compiler and target architecture, or even with HPF itself. Therefore, tools for automatic data layout and performance estimation will be crucial if the HPF is to find general acceptance in the scientific community. This paper discusses a framework for automatic data layout for use in a data layout assistant tool for a data-parallel language such as HPF. The envisioned tool can be used to generate a first data layout for a sequential Fortran program without data layout statements, or to extend a partially specified data layout in a HPF program to a totally specified data layout. Since the data layout assistant is not embedded in a compiler and will run only a few times during the tuning process of an application program, the framework can use techniques that may be too computationally expensive to be included in a compiler. A prototype data layout assistant tool based on our framework has been implemented as part of the D system currently under development at Rice University. The paper reports preliminary experimental results. The results indicate that the framework is efficient and generates data layouts of high quality.",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1383213,no,undetermined,0
A software to aid reliability estimation,"Estimating the reliability of software is becoming increasingly important. There are a large number of models available for estimating reliability. Most of these models are computationally complex and require statistical inferencing to estimate the parameters. In addition, techniques for evaluating the results obtained from a given model themselves are computationally expensive. We describe a software that can apply a model of the choice of the tester (from a set of models) for estimating the reliability of a given software, and also provide data for evaluating the predictions of the model",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=526384,no,undetermined,0
Simulation statistical software: an introspective appraisal,"Simulation experiments are sampling experiments by their very nature. Statistical issues dominate all aspects of a well-designed simulation study-model validation, selection of input distributions and associated parameters, experiment design frameworks, output analysis methodologies, model sensitivity, and forecasting are examples of some of the issues which must be dealt with by simulation experimenters. There are many factors which complicate analyses, such as multivariate input distributions, serially correlated model inputs and outputs, multiple performance measures, and non-linear system response, to name a few. The purpose of this panel is to discuss any and all issues related to software tools available for dealing with these and other problems. I asked five experts from within the simulation community to share their opinions and insights on the availability and quality of software to meet the statistical needs of the simulation community. The position statements provided by them are intended to serve as a springboard for a more extensive exchange of ideas during the discussion at the conference.",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=717524,no,undetermined,0
Achieving customer satisfaction through robust design,"A winning product has to consistently outperform its rivals by providing value in today's highly competitive markets. At the heart of consistent value is robust design â€?the process of ensuring that a product will perform well despite variations in raw materials, components, manufacturing, and operating conditions. It consists of steps for identifying key parameters that affect the customer, analyzing to predict and improve robustness, and measuring to confirm performance. As it exists in AT&Ts Transmission Systems Business Unit (TSBU) for hardware development, robust design is a real, effective process. It should be used to ensure design robustness for all designs, especially those intended for multi-use platforms.",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6770219,no,undetermined,0
Non-linear circuit effects on analog VLSI neural network implementations,"We present an analog VLSI neural network for texture analysis; in particular we show that the filtering block, which is the most critical block of the architecture for precision of computation, can be implemented using simple and compact analog circuits, without significant loss in classification performance. Through an accurate analysis of the circuits it is possible to model the real circuit characteristics in the software simulation environment; the weights calculated in the learning phase (which is performed off-line using the adaptive simulated annealing algorithm), can be properly coded into analog circuit variables in order to implement the correct operation of the network",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=593739,no,undetermined,0
Estimating the intrinsic difficulty of a recognition problem,"Describes an experiment in estimating the Bayes error of an image classification problem: a difficult, practically important, two-class character recognition problem. The Bayes error gives the â€œintrinsic difficultyâ€?of the problem since it is the minimum error achievable by any classification method. Since for many realistically complex problems, deriving this analytically appears to be hopeless, the authors approach the task empirically. The authors proceed first by expressing the problem precisely in terms of ideal prototype images and an image defect model, and then by carrying out the estimation on pseudorandomly simulated data. Arriving at sharp estimates seems inevitably to require both large sample sizes-in the authors' trial, over a million images-and careful statistical extrapolation. The study of the data reveals many interesting statistics, which allow the prediction of the worst-case time/space requirements for any given classifier performance, expressed as a combination of error and reject rates",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=576899,no,undetermined,0
Backplane test bus selection criteria,"The author discusses the architecture of the IEEE 1149.1 Standard, its applications and selection criteria. Although the evaluation and selection of a multi-drop addressable 1149.1 architecture are completed, the hardware has been designed such that alternative architectures could be implemented, if warranted, without a complete redesign of the payload digital electronics. The primary advantage of this architecture is the natural boundary scan test support at all levels of integration. Closed loop chains exist at each integration level and can be exercised to isolate 1149.1-detectable defects that occur at that level prior to any disassembly of the product",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=528055,no,undetermined,0
Testing CMOS logic gates for: realistic shorts,"It is assumed that tests generated using the single stuck-at fault model will implicitly detect the vast majority of fault-causing defects within logic elements. This may not be the case. In this paper we characterize the possible shorts in the combinational cells in a standard cell library. The characterization includes errors on the cell outputs, errors on the cell inputs, and excessive quiescent current. The characterization provides input vectors to stimulate these errors. After characterizing the faults that occur due to possible electrical shorts, we compare the coverage of the logic faults using a single stuck-at test set and tests developed specifically to detect these shorts. We discuss the effectiveness of I<sub>DDQ</sub> testing for these faults",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=527981,no,undetermined,0
Reliability growth model for object oriented software system,Object oriented design techniques are the most powerful way of developing software in the foreseen future. The techniques help in reducing the cost of development and increase the reliability of the software. We develop a software reliability growth model for an object oriented software system. The model is validated by simulating the error data. The goodness of fit and predictive validity is also tested,1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=526400,no,undetermined,0
A software reliability model in the embedded system,"We propose a software reliability model for estimating, measuring, and controlling software reliability of embedded systems, and a software test stopping equation for determining software testing time. It is not easy to correct errors occurring in embedded systems on site",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=526385,no,undetermined,0
Investigating coverage-reliability relationship and sensitivity of reliability to errors in the operational profile,"The focus of the work is an investigation into the correlation between â€œtrueâ€?reliability of a software system and the white box testing measures such as block coverage, c-uses and p-uses coverage. We believe that software reliability and testing measures, especially white box testing, are inherently related. Results from experiments are presented to support this belief. We also demonstrate that the estimated reliability is sensitive to the operational profile defined for the software and hence errors in the operational profile may lead to incorrect reliability estimates",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=526380,no,undetermined,0
The role of measurement in software engineering,"The software engineering community has been slow to adapt measurement into industrial practice. This article reviews some of the probable causes of this situation, suggests a general approach to metrification, and identifies the most common industrial applications of measurement. These basic applications include project estimation, project tracking, product quality control, product design improvement, process quality control, and process improvement",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=472702,no,undetermined,0
A discrete software reliability growth model with testing effort,"We propose a discrete software reliability growth model with testing effort. The behaviour of the testing effort is described by a discrete Rayleigh curve. Assuming that the discrete failure intensity to the amount of current testing effort is proportional to the remaining error content, we formulate the model as a non-homogeneous poisson process. Parameters of the model are estimated. We then discuss a release policy based on cost and failure intensity criteria. Numerical results are also presented",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=526379,no,undetermined,0
Real-time supervision of software systems using the belief method,"With the critical role played by software systems in the operation of telecommunications networks, the ability to detect and report software failures has become of great importance. The paper presents the failure detection approach of real-time supervision based on the belief method. Real-time supervision allows failures in a software system to be detected in real-time based only on boundary signals and a specification of its external behaviour. This feature is of particular benefit when software is purchased from other vendors and there is no direct access to source code. An implementation of real-time supervision has been shown to be capable of failure detection in a small telephone exchange",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=513042,no,undetermined,0
Personal `progress functions' in the software process,"Individual developers can expect improvement in software productivity as a consequence of (i) a growing stock of knowledge and experience gained by repeatedly doing the same task (first-order learning) or (ii) due to technological and training programs supported by the organization (second-order learning). Organizations have used this type of progress behavior in making managerial decisions regarding cost estimation and budgeting, production and staff scheduling, product pricing, etc. Such progress is studied in productivity, product-quality and personal skills, in an experiment involving a sample of 12 software developers, who complete one project every week for ten weeks. Second-order training is provided to the subjects through Humphrey's Personal Software Process. A modified GQM method for measurement is used to execute the research method",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=512780,no,undetermined,0
Improving reliability of large software systems through software design and renovation,"Software renovation is a method to improve existing software and to incorporate new features and system functions without developing entirely new software. The paper proposes a method to increase software reliability and to determine software components that need to be renovated. The method considers the changed code in base and non-base software, probability of a software error, and sets of static and dynamic connections in large software applications. The software is renovated if the probability of error exceeds threshold or the sets of static and dynamic connections are changed and the error in code exceeds threshold. A number of experiments with changing model parameters are used to show the usefulness of the method. The study is of interest in telecommunications",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=512709,no,undetermined,0
Evaluating HACMP/6000: a clustering solution for high availability distributed systems,The requirements for highly available computer systems have evolved over time. In a commercial environment end-users and consumers do not tolerate systems that are unavailable when needed. This paper describes and evaluates the High Availability Cluster Multi-Processing (HACMP/6000) product. HACMP/6000 is a clustering solution for high availability distributed systems. Our approach in showing the effectiveness of HACMP/6000 is both qualitative and quantitative. We use availability analysis as a means of our quantitative analysis. Availability analysis is undertaken using a state-of-the-art availability analysis tool-System Availability Estimator (SAVE). Results obtained from the analysis are used for comparison with a non-HACMP/6000 system,1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=494467,no,undetermined,0
A tool for assessing the quality of distributed software designs,"This paper reports a software tool for computing McCabe's cyclomatic complexity measure of a distributed system design specified in LOTOS, without presenting the underlying theory and algorithms in detail. Such a measure can be used to assess the quality of the design and determine the maximum number of independent cycles in an all-path test coverage. The tool accepts as an input a LOTOS specification in either textual or graphical form. It provides two modes for computation, the first one based on an actual transformation of the entire LOTOS specification to a Petri-net and the second one based on a set of formulas for computing the cyclomatic values constructively",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=475316,no,undetermined,0
Characterization inconsistencies in CdTe and CZT gamma-ray detectors,"In the past few years, significant developments in cadmium telluride (CdTe) and cadmium zinc telluride (CZT) semiconductor materials have taken place with respect to both quality and yield. Many of the more recent developments have occurred in the area of CZT crystal growth. This has resulted in an explosion of interest in the use of these materials in ambient temperature gamma-ray detectors. Most, if not all, of the manufacturers of CdTe and CZT have acquired government funding to continue research in development and applications, indicating the importance of these improvements in material quality. We have examined many detectors, along with the accompanying manufacturer's data, and it has become apparent that a clear standard does not exist by which each manufacturer characterizes the performance of their material. The result has been a wide variety of performance claims that have no basis for comparison and normally cannot be readily reproduced. In this paper we will first support our observations and then propose a standard that all manufacturers and users of these materials may use for characterization",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=474371,no,undetermined,0
CAMAC standard high-speed precise spectrometer,"The paper describes the CAMAC Standard Precise High-Speed Spectrometer for X-ray and gamma spectrometry designed for applications in a high-speed spectroscopy systems with requirement to obtain the high spectroscopy performance. The spectrometer contains the controlled. HV (high voltage) source, spectroscopy amplifier (later-amplifier), ADC (analog to digital converter) with buffer memory, CAMAC Grate controller hardware and software for PC. In this paper only the most important units, that determine quality and throughput performance of spectrometer are considered. The spectrometer has main features: maximum registration speed, pps, no less-150000; maximum input count rate, pps, -1000000; integral nonlinearity, %, no more -0.07; differential nonlinearity, %, no more -0.5; shaping symmetrical quasi-Gaussian; pile-up rejector dead time, ns, -100; ADC dead time, Î¼s, -1, 7. Spectrometric data are processed and displayed on PC",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=474358,no,undetermined,0
The impact of software enhancement on software reliability,"This paper exploits the `relationship between functional-enhancement (FE) activity and the distribution of software defects' to develop a discriminant model that identifies high-risk program modules, `FE activity' and `defect data captured during the FE of a commercial programming language processing utility' serve to fit and test the predictive quality of this model. The model misclassification rates demonstrate that FE information is sufficient for developing discriminant models identifying high-risk program modules. Consideration of the misclassified functions leads us to suggest that: (1) the level of routines in the calling hierarchy introduces variation in defect distribution; and (2) the impact of a defect indicates the risk that it presents. Thus consideration of defect impact can improve the discriminant results",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=476001,no,undetermined,0
A software system evaluation framework,"The objective of a software system evaluation framework is to assess the quality and sophistication of software from different points of view. The framework explicitly links process and product aspects with the ultimate utility of systems and it provides a basic set of attributes to characterize the important dimensions of software systems. We describe such a framework and its levels of categorization, and we analyze examples of project classifications. Then we draw some conclusions and present ideas for further research. This evaluation framework assesses a software system's quality by consolidating the viewpoints of producers, operators, users, managers, and stakeholders",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=476196,no,undetermined,0
Inside front cover,"We present the first closed loop image segmentation system which incorporates a genetic algorithm to adapt the segmentation process to changes in image characteristics caused by variable environmental conditions such as time of day, time of year, clouds, etc. The segmentation problem is formulated as an optimization problem and the genetic algorithm efficiently searches the hyperspace of segmentation parameter combinations to determine the parameter set which maximizes the segmentation quality criteria. The goals of our adaptive image segmentation system are to provide continuous adaptation to normal environmental variations, to exhibit learning capabilities, and to provide robust performance when interacting with a dynamic environment. We present experimental results which demonstrate learning and the ability to adapt the segmentation performance in outdoor color imagery.",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=478442,no,undetermined,0
Applying TQ principles to the requirements phase of system development,"This paper examines techniques that have evolved for specifying user requirements definition during systems development. It concentrates on potential impacts of applying total quality management (TQM) principles to the user requirements phase of the system development life cycle. TQM methodologies have been successful in many arenas. There are high expectations that TQM can also improve the quality of the most critical aspect of systems development, that is, defining user requirements",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=524583,no,undetermined,0
Trillium: a model for the assessment of telecom software system development and maintenance capability,"Since 1982 Bell Canada has been developing a model to assess the software development process of existing and prospective suppliers as a means to minimize the risks involved and ensure both the performance and timely delivery of software systems purchased. This paper presents the revised Trillium model (version 3.0). The basis of the process assessment models for software relies on benchmarking, e.g. comparing your practices with the best and successful organizations. It is also a basic tool that you will find in the TQM literature. For software assessment we have used initially two levels of benchmarks to develop the model: professional, national and international standards; and comparisons with other organizations in the same market segment. The software assessment model should therefore map to existing engineering standards as well as quality standards. It should also provide an output that can be used easily to benchmark against â€œbest-in-classâ€?organizations",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=525963,no,undetermined,0
Using fault injection to assess software engineering standards,"Standards for quality software are increasingly important, especially for critical systems. Development standards and practices must be subjected to quantitative analyses; it is no longer adequate to encourage practices because they â€œmake senseâ€?or â€œseem reasonable.â€?Process improvement must be demonstrated by a history of improved products. Fault-injection methods can be used to assess the quality of software itself and to demonstrate the effectiveness of software processes. Fault-injection techniques can help developers move beyond the practical limitations of testing. Fault-injection techniques focus on software behavior, not structure; process-oriented techniques cannot measure behavior as precisely. Fault-injection methods are dynamic, empirical, and tractable; as such, they belie the notion that measuring the reliability of critical software is futile. Before focusing too narrowly on the assessment of software development processes, we should further explore the measurement of software behaviors",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=525959,no,undetermined,0
"Software quality measurement and modeling, maturity, control and improvement","The goal of the paper is to provide a measurement structure needed for the improvement of software quality by uniting the Rome Laboratory Software Quality Framework, a reconstructed concept of measurement based maturity, statistical analysis, data quality and requisite information structures. The structure involves both standards of practice and standards of performance relating to product and process, and the need for extended forms of analysis for them to be developed. Such a structure is essential if software quality is to be assessed and improved in a timely manner",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=525951,no,undetermined,0
On the organization level structure of the hierarchical intelligent machine,A novel structure of the organization level of the hierarchical intelligent machine is proposed. The proposed model has the ability to search complete activities by learning without referring to the knowledge base of the incompatible event pairs. Also it has fault tolerant capability by using directly a performance measure from low levels as a probability. Application of the novel structure to a simple mobile robot organization problem is considered to demonstrate its effectiveness by computer simulations,1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=525297,no,undetermined,0
Automated test generation for improved protocol testing,"To enhance the overall quality of its network protocol testing process, Bellcore has developed TACIT, a toolkit for automated conformance and interoperability testing based on a formal and automated approach to test script generation. We first give a technical description of TACIT in which the different steps of the automated test script generation method are outlined, such as protocol specification and compilation, generic test script generation and executable test script encoding. We then relate how TACIT was experimentally used for testing of ISDN primary rate interface protocol. The results of this experiment are finally assessed and compared to the current script generation approach used at Bellcore",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=525158,no,undetermined,0
A leader election algorithm in a distributed computing system,A leader election algorithm based on the performance and the operation rate of nodes and links as proposed. The performance of the existing leader election algorithms should be improved because a distributed system pauses until a node becomes a unique leader among all nodes in the system. The pre-election algorithm that elects a provisional leader while the leader is executing is also introduced,1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=525021,no,undetermined,0
Design and analysis of a multiprocessor system with extended fault tolerance,"Hardware approaches to fault-tolerance in designing a scalable multiprocessor system are discussed. Each node is designed to support multi-level fault-tolerance enabling a user to choose the level of fault-tolerance with a possible resource or performance penalty. Various tree-type interconnection networks using switches are compared in terms of reliability, latency, and implementation complexity. A practical duplicate interconnection network with the increased reliability is proposed in consideration of implementation issues under the physical constraint",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=524998,no,undetermined,0
Parallel software engineering - Goals 2000,"The challenge of parallel software engineering is to provide predictable and effective products and processes for a new breed of application. To do this, parallel computing must apply to all concurrent application types. Assured delivery and predictable time response are the metrics to determine success. In addition, high integrity factors (availability, reliability, security and graceful reaction to changes) are necessary. Infrastructure and life cycle support issues are important to consider when creating large scale information and command and control systems. The approach is to define an encompassing engineering process that accounts for quantitative and predictive measures of time response, integrity and application complexity",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=524764,no,undetermined,0
Using neural networks for functional testing,"This paper discusses using Neural Networks for diagnosing circuit faults. As a circuit is tested, the output signals from a Unit Under Test can vary as different functions are invoked by the test. When plotted against time, these signals create a characteristic trace for the test performed. Sensors in the ATS can be used to monitor the output signals during test execution. Using such an approach, defective components can be classified using a Neural Network according to the pattern of variation from that exhibited by a known good card. This provides a means to develop testing strategies for circuits based upon observed performance rather than domain expertise. Such capability is particularly important with systems whose performance, especially under faulty conditions, is not well documented or where suitable domain knowledge and experience does not exist. Thus, neural network solutions may in some application areas exhibit better performance than either conventional algorithms or knowledge-based systems. They may also be retrained periodically as a background function, resulting with the network gaining accuracy over time.",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=522718,no,undetermined,0
ORCA's oceanographic sensor suite,"The Mapping, Charting and Geodesy Branch of the Naval Research Laboratory (NRL) at Stennis Space Center, MS, is conducting a multi-year program for the development of unmanned, untethered sensor systems for the collection of tactical oceanographic data in littoral regions. This paper reviews the sensor systems, program progress to date and the future plans for a comprehensive oceanographic survey system. The prototype platform currently in use for this project is the ORCA semi-submersible. The ORCA is an air-breathing vessel which travels just below the water surface. The vessel utilizes a direct radio link for real-time data and control communications, as well as a DGPS system for precise platform positioning. The primary sensor installed on ORCA is the Simrad EM950 system which collects bathymetry and collocated acoustic imagery in water up to 300 meters in depth. With realtime data telemetry to the ORCA host vessel and the NRL developed HMPS bathymetry post-processing software, the system is capable of same-day chart production. In contrast to a full size survey vessel, ORCA is able to collect bathymetric data of the same quantity and quality, but will have one fortieth the life-cycle costs. Other sensors being integrated into ORCA include an acoustic sediment classification system, an acoustic Doppler current profiler, and obstacle avoidance systems",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=526907,no,undetermined,0
An intelligent approach to sensor fusion-based diagnostics,"This paper describes the integration of diverse sensor technology capable of analyzing units under test (UUT) from different perspectives, with advanced analysis techniques that provide new insight into static, dynamic, and historical UUT performance. Such methods can achieve greater accuracy in failure diagnosis and fault prediction; reduction in cannot duplicate (CND), retest-OK (RTOK) rates, and ambiguity group size; and improved confidence in performance testing that results in the determination of UUT ready for issue status.",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=522715,no,undetermined,0
Recasting the avionic support paradigm,"Cost and performance problems associated with the current avionic support paradigm, which had its origins in the 1960s and is based largely on TPS/ATS technology, would suggest that a fundamental change in paradigm should be considered. A new paradigm, that more closely integrates the mission related and support related aspects of both acquisition and development, could resolve many chronic deficiencies while being more compatible with evolving prime system technology.",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=522675,no,undetermined,0
Understanding the domain of the Army's CTS,"This paper describes the development of the elements of the domain of the Army's Contact Test Set (CTS). This work is motivated by the anticipated proliferation of applications of the CTS, the testing classifications, and the roles of Test Measurement and Diagnostic Equipment (TMDE) and Software Engineering Directorate (SED). This work includes both the top-down development of the domain, following the Domain-Specific Software Architecture (DSSA) Pedagogical Example and the bottom-up analysis of existing Contact Test Set software. Several tools and methods are being evaluated, and currently two applications are currently being studied with others being added. A pilot application is to be implemented.",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=522648,no,undetermined,0
Evaluation of a multiple-model failure detection system for the F-16 in a full-scale nonlinear simulation,"A multiple model adaptive estimation (MMAE) algorithm is implemented with the fully nonlinear six-degree-of-motion, simulation rapid-prototyping facility (SRF) VISTA F-16 software simulation tool. The algorithm is composed of a bank of Kalman filters modeled to match particular hypotheses of the real world. Each presumes a single failure in one of the flight-critical actuators, or sensors, and one presumes no failure. The algorithm is demonstrated to be capable of identifying flight-critical aircraft actuator and sensor failures at a low dynamic pressure (20,000 ft, .4 Mach). Research considers single hardover failures. Tuning methods for accommodating model mismatch, including addition of discrete dynamics pseudonoise and measurement pseudonoise, are discussed and demonstrated. Robustness to sensor failures provided by MMAE-based control is also demonstrated",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=521988,no,undetermined,0
Fault tolerant behavior of redundant flight controls as demonstrated with the sure and assist modeling programs,"A comparison and evaluation between a triple redundant and quadruple redundant flight control system (FCS) is performed through the use of NASA's ASSIST and SURE software programs. To enhance the productivity of these programs, the development of a user friendly, front-end, menu driven interface routine was initiated. The front-end tool provides a user environment based on a system block diagram concept and automatically converts the system inputs into ASSIST code. Two test case models were used to compute the Probability of Loss of Control (PLOC), the Probability Of Mission Abort (PMA), and the Probability of Hazard Condition (PHC) values for the two different architectures. The solutions obtained by these tools were compared against existing solutions obtained from approximate equations calculations for a certain set of system requirements. The data produced by these software tools displayed more accuracy then the conventional approximation approach; however, the overall system predictions remained consistent for both approaches. The quadruple architecture design surpassed the triple architecture design for the given system parameters by a significant safety factor margin. The software tools approach also demonstrated how the coverage percentage values, which represent the voter capabilities in recognizing and handling failures, significantly affected the system performance. The ability of the software tools to provide user friendly interactions that produce accurate results in a quick and easy manner make this method of predicting fault-tolerant reliabilities for flight control systems the preferred choice of analysis",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=521972,no,undetermined,0
Integration testing based on software couplings,"Integration testing is an important part of the testing process, but few integration testing techniques have been systematically studied or defined. This paper presents an integration testing technique based on couplings between software components. The coupling-based testing technique is described, and 12 coverage criteria are defined. The coupling-based technique is also compared with the category-partition method on a case study, which found that the coupling-based technique detected more faults with fewer test cases than category-partition. This modest result indicates that the coupling-based testing approach can benefit practitioners who are performing integration testing on software. While it is our intention to develop algorithms to fully automate this technique, it is relatively easy to apply by hand",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=521883,no,undetermined,0
"QUEST-a tool for automated performance, dependability, and cost tradeoff support","The Quantitative Expert System Tool (QUEST) provides automated support for quantitative evaluation and tradeoff analysis of complex computer system designs. Three basic components are provided. The design capture component is based on an information model that incorporates performance, dependability, and cost considerations within a single, integrated representation. The quantitative modeling and analysis component provides a suite of mathematical modeling capabilities that calculate a range of performance, dependability, and cost metrics relative to candidate designs. The knowledge-based tradeoff support component interprets design information and modeling results to assess system compliance with requirements, identify design problems contributing to non-compliance, and recommend design alternatives that will bring designs into compliance with all system requirements.",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=521875,no,undetermined,0
Experimental evaluation of the impact of processor faults on parallel applications,"This paper addresses the problem of processor faults in distributed memory parallel systems. It shows that transient faults injected at the processor pins of one node of a commercial parallel computer, without any particular fault-tolerant techniques, can cause erroneous application results for up to 43% of the injected faults (depending on the application). In addition to these very subtle faults, up to 19% of the injected faults (almost independent on the application) caused the system to hang up. These results show that fault-tolerant techniques are absolutely required in parallel systems, not only to ensure the completion of long-run applications but, and more important, to achieve confidence in the application results. The benefits of including some fairly simple behaviour based error detection mechanisms in the system were evaluated together with Algorithm Based Fault Tolerance (ABFT) techniques. The inclusion of such Mechanisms in parallel systems seems to be very important for detecting most of those subtle errors without greatly affecting the performance and the cost of these systems",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=518719,no,undetermined,0
Detecting program modules with low testability,"We model the relationship between static software product measures and a dynamic quality measure, testability. To our knowledge, this is the first time a dynamic quality measure has been modeled using static software product measures. We first give an overview of testability analysis and discriminant modeling. Using static software product measures collected from a real time avionics software system, we develop two discriminant models and classify the component program modules as having low or high testability. The independent variables are principal components derived from the observed software product measures. One model is used to evaluate the quality of fit and one is used to assess classification performance. We show that for this study, the quality of fit and classification performance of the discriminant modeling methodology are excellent and yield a potentially useful insight into the relationship between static software measures and testability",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=526546,no,undetermined,0
Production I<sub>DDQ</sub> testing with passive current compensation,"In an ideal world, all test vectors selected as I<sub>DDQ</sub> test vectors would have very low measurement current. In many circumstances, however, a complete set of tests cannot be defined when relying on zero-current considerations. I<sub>DDQ</sub> test vectors selected from existing test sets must be capable of compensating for passive currents. The identification of valid test vectors, and the magnitude of the passive current, can be predicted in the ASIC environment via macrocell-specific information. This paper presents one methodology to support this test generation using digital simulation, including the selection of multiple I<sub>DDQ</sub> test vectors",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=529876,no,undetermined,0
Predictions for increasing confidence in the reliability of safety critical software,We show how residual faults and failures and time to next failure can be used in combination to assist in assuring the safety of the software in safety critical systems like the NASA Space Shuttle Primary Avionics Software System,1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=479312,no,undetermined,0
Architectural timing verification of CMOS RISC processors,"We consider the problem of verification and testing of architectural timing models (""timers"") coded to predict cycles-per-instruction (CPI) performance of advanced CMOS superscalar (RISC) processors. Such timers are used for pre-hardware performance analysis and prediction. As such, these software models play a vital role in processor performance tuning as well as application-based competitive analysis, years before actual product availability. One of the key problems facing a designer, modeler, or application analyst who uses such a tool is to understand how accurate the model is, in terms of the actual design. In contrast to functional simulators, there is no direct way of testing timers in the classical sense, since the â€œcorrectâ€?execution time (in cycles) of a program on the machine model under test is not directly known or computable from equations, truth tables, or other formal specifications. Ultimate validation (or invalidation) of such models can be achieved under actual hardware availability, by direct comparisons against measured performance. However, deferring validation solely to that stage would do little to achieve the overall purpose of accurate pre-hardware analysis, tuning, and projection. We describe a multilevel validation method which has been used successfully to transform evolving timers into highly accurate pre-hardware models. In this paper, we focus primarily on the following aspects of the methodology: a) establishment of cause-effect relationships in terms of model defects and the associated fault signatures; b) derivation of application-based test loop kernels to verify steady-state (periodic) behavior of pipeline flow, against analytically predicted signatures; and c) derivation of synthetic test cases to verify the â€œcoreâ€?parameters characterizing the pipeline-level machine organization as implemented in the timer model. The basic tenets of the theory and its application are described in the co- - ntext of an example processor, comparable in complexity to an advanced member of the PowerPCâ„?6XX processor family.",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5389507,no,undetermined,0
Synchronization of multimedia data for a multimedia news-on-demand application,"We present a complete software control architecture for synchronizing multiple data streams generated from distributed media-storing database servers without the use of a global clock. Independent network connections are set up to remote workstations for multimedia presentations. Based on the document scenario and traffic predictions, stream delivery scheduling is performed in a centralized manner. Certain compensation mechanisms at the receiver are also necessary due to the presence of random network delays. A stream synchronization protocol (SSP) allows for synchronization recovery, ensuring a high quality multimedia display at the receiver. SSP uses synchronization quality of service parameters to guarantee the simultaneous delivery of the different types of data streams. In the proposed architecture, a priority-based synchronization control mechanism for MPEG-2 coded data streams is also provided. A performance modeling of the SSP is presented using the DSPN models. Relevant results such as the effect of the SSP, the number of synchronization errors, etc., are obtained",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=481710,no,undetermined,0
Early quality prediction: a case study in telecommunications,"Predicting the quality of modules lets developers focus on potential problems and make improvements earlier in development, when it is more cost-effective. The authors applied discriminant analysis to identify fault-prone modules in a large telecommunications system prior to testing",1996,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=476287,no,undetermined,0
Fast bounds for ATM quality of service parameters,We demonstrate how fast estimates of quality of service parameters can be obtained for models of VBR traffic queuing in the buffer of an ATM multiplexer. The technique rests on conservative estimates of cell-loss ratios in the buffer. These are obtained through Martingale methods; the cell-loss ratio from a buffer of size x is bounded as Prob[cell-loss]&les;e<sup>-u-Î´x</sup>. An estimate of the mean cell delay can also be derived from this. We discuss briefly the algorithms used to compute the bounds. One attraction of the method is that the speed of computation is independent of the number of sources present in the traffic stream arriving at the multiplexer. This gives great advantage over estimates derived from a complete solution of the model queueing problem: these generally require the analysis of matrices whose dimension is proportional to the number of sources present. We give an application using a model of bursty traffic proposed by Finnish Telecom: a Markovian traffic model in which burst and silence are geometrically distributed subject to a fixed maximum burst length. A demonstration of a prototype software package based on the above methods is included,1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=676644,no,undetermined,0
Providing Predictable Performance To A Network-Based Application Over ATM,"<div style=""font-variant: small-caps; font-size: .9em;"">First Page of the Article</div><img class=""img-abs-container"" style=""width: 95%; border: 1px solid #808080;"" src=""/xploreAssets/images/absImages/00662025.png"" border=""0"">",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=662025,no,undetermined,0
Ground-truthing and benchmarking document page segmentation,"We describe a new approach for evaluating page segmentation algorithms. Unlike techniques that rely on OCR output, our method is region-based: the segmentation output, described as a set of regions together with their types, output order etc., is matched against the pre-stored set of ground-truth regions. Misclassifications, splitting, and merging of regions are among the errors that are detected by the system. Each error is weighted individually for a particular application and a global estimate of segmentation quality is derived. The system can be customized to benchmark specific aspects of segmentation (e.g., headline detection) and according to the type of error correction that might follow (e.g., re-typing). Segmentation ground-truth files are quickly and easily generated and edited using GroundsKeeper, an X-Window based tool that allows one to view a document, manually draw regions (arbitrary polygons) on it, and specify information about each region (e.g., type, parent)",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=601968,no,undetermined,0
Prediction of OCR accuracy using simple image features,"A classifier for predicting the character accuracy achieved by any Optical Character Recognition (OCR) system on a given page is presented. This classifier is based on measuring the amount of white speckle, the amount of character fragments, and overall size information in the page. No output from the OCR system is used. The given page is classified as either â€œgoodâ€?quality (i.e. high OCR accuracy expected) or â€œpoorâ€?(i.e. low OCR accuracy expected). Results of processing 639 pages show a recognition rate of approximately 85%. This performance compares favorably with the ideal-case performance of a prediction method based upon the number of reject-markers in OCR generated text",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=599003,no,undetermined,0
Clustering OCR-ed texts for browsing document image database,"Document clustering is a powerful tool for browsing throughout a document database. Similar documents are gathered into several clusters and a representative document of each cluster is shown to users. To make users infer the content of the database from several representatives, the documents must be separated into tight clusters, in which documents are connected with high similarities. At the same time, clustering must be fast for user interaction. We propose an O(n<sup>2</sup>) time, O(n) space cluster extraction method. It is faster than the ordinal clustering methods, and its clusters compare favorably with those produced by Complete Link for tightness. When we deal with OCR-ed documents, term loss caused by recognition faults can change similarities between documents. We also examined the effect of recognition faults to the performance of document clustering",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=598969,no,undetermined,0
Logic vs. current testing: an evaluation using CrossCheck Technology's CM-iTest,"This paper examines the effectiveness of using the CrossCheck software tool CM-iTest to select high-fault coverage, robust test vectors for measuring I<sub>DDQ</sub>. In the experiments section of the paper data from 4 ASIC designs is presented which examines the defect detection of stuck-at fault (SAF) functional testing versus I<sub>DDQ </sub> current testing for varying degrees of SAF and transistor-level I <sub>DDQ</sub> fault coverages. The I<sub>DDQ</sub> current testing further compares the defect detection of using CrossCheck CM-iTest selected I<sub>DDQ</sub> vectors versus randomly selected and customer selected I<sub>DDQ</sub> vectors",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=580716,no,undetermined,0
Image quality and readability,"Determining the readability of documents is an important task. Human readability pertains to the scenario when a document image is ultimately presented to a human to read. Machine readability pertains to the scenario when the document is subjected to an OCR process. In either case, poor image quality might render a document unreadable. A document image which is human readable is often not machine readable. It is often advisable to filter out documents of poor image quality before sending them to either machine or human for reading. This paper is about the design of such a filter. We describe various factors which affect document image quality and the accuracy of predicting the extent of human and machine readability possible using metrics based on document image quality",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=538547,no,undetermined,0
Yield learning via functional test data,This paper presents a methodology to estimate the defect Pareto in an IC process through the use of production functional test data. This Pareto can then be used for yield improvement activities. We demonstrate the concept on several benchmark circuits. We show how limited IDDQ current testing can significantly improve the Pareto accuracy,1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=529891,no,undetermined,0
A neural fuzzy system to evaluate software development productivity,"Managing software development and maintenance projects requires early knowledge about quality and effort needed for achieving this quality level. Quality-based productivity management is introduced as one approach for achieving and using such process knowledge. Fuzzy rules are used as a basis for constructing quality models that can identify outlying software components that might cause potential quality problems. A special fuzzy neural network is introduced to obtain the fuzzy rules combining the metrics as premises and quality factors as conclusions. Using the law of DeMorgan, this net structure is able to learn premises just by changing the weights. Note that the authors change neither the number of neurons nor the number of connections. This new type of net allows for the extraction of knowledge acquired by training on the past process data directly in the form of fuzzy rules. Beyond that, it is possible to transfer all the known rules to the neural fuzzy system in advance. The suggested approach and its advantages towards common simulation and decision techniques is illustrated with experimental results. Its application area is in maintenance productivity. A module quality model-with respect to changes-provides both quality of fit (according to past data) and predictive accuracy (according to ongoing projects)",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=538521,no,undetermined,0
On-line sensing and modeling of mechanical impedance in robotic food processing,"Measurement of mechanical impedance is useful in robotic food processing. In cutting meat, fish, and other inhomogeneous objects by means of a robotic cutter, for instance, it is useful to sense the transition regions between soft meat, hard meat, fat, shin, and bone. Product quality and yield can be improved through this, by accurately separating the desired product from the parts that should be discarded. Mechanical impedance at the cutter-object interface is known to provide the necessary information for this purpose. Unfortunately, the conventional methods of measuring mechanical impedance require sensing of both force and velocity simultaneously. Instrumenting a robotic end-effector for these measurements can make the cutter unit unsatisfactorily heavy, sluggish, and costly. An approach for on-line sensing of mechanical impedance, using the current of the cutter motor and the displacement (depth of cut) of the cutter, has been developed by us. A digital filter computes the mechanical impedance on this basis. For model-based estimation, performance evaluation of the on-line sensor, and also for model-based cutter control, it is useful to develop a model of the cutter-object interface. This paper illustrates these concepts using a laboratory system consisting of a robotic gripper and a flexible object. The prototype consists of an industrial-quality robotic gripper, a control computer, and associated hardware and software for data acquisition and processing. A model of the process interface between the end-effector and object has been developed. Some illustrative results from laboratory experiments are given",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=538018,no,undetermined,0
Image processing for CT-assisted reverse engineering and part characterization,"Computed tomography (CT) systems have the ability to rapidly and nondestructively produce images of both exterior and interior surfaces and regions. Because CT images are densitometrically accurate, complete morphological and part characterization information can be obtained without need of physical sectioning. CT data can be processed to create computer-aided design (CAD) representations of the part, to extract dimensional measurement, or to detect, size and locate defects. The key image processing steps which are required both for CT-assisted reverse engineering and CT-assisted part characterization are examined. Two desirable characteristics of the underlying image processing algorithms are accuracy of the produced results and the ability to handle large volumetric images of complex parts",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=537573,no,undetermined,0
An Adaptive Distributed System-Level Diagnosis Algorithm and Its Implementation,"<div style=""font-variant: small-caps; font-size: .9em;"">First Page of the Article</div><img class=""img-abs-container"" style=""width: 95%; border: 1px solid #808080;"" src=""/xploreAssets/images/absImages/00532652.png"" border=""0"">",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=532652,no,undetermined,0
ON THE DIAGNOSABILITY OF DIGITAL SYSTEMS,"<div style=""font-variant: small-caps; font-size: .9em;"">First Page of the Article</div><img class=""img-abs-container"" style=""width: 95%; border: 1px solid #808080;"" src=""/xploreAssets/images/absImages/00532650.png"" border=""0"">",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=532650,no,undetermined,0
STAREX SELF-REPAIR ROUTINES: SOFTWARE RECOVERY IN THE JPL-STAR COMPUTER,"<div style=""font-variant: small-caps; font-size: .9em;"">First Page of the Article</div><img class=""img-abs-container"" style=""width: 95%; border: 1px solid #808080;"" src=""/xploreAssets/images/absImages/00532635.png"" border=""0"">",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=532635,no,undetermined,0
PACS in the operating room: experience at the Baltimore VA Medical Center,The objective of this study was to evaluate the performance of a picture archiving and communication system (PACS) in the operating room (OR) environment in a filmless hospital. Surgeons and OR staff were interviewed to assess the degree to which the PACS in the OR met their requirements and expectations. PACS was favored over film in the OR for the majority of those surveyed. Image quality was perceived as equivalent to or better than film. Image availability was cited as the major advantage over film. Areas for improvement in workstation design and PACS training were described and solutions offered. A filmless OR was preferable to a film environment for surgeons and staff. Attention to the specific requirements of different hospital environments such as the OR during PACS design and implementation may increase the utility of PACS for non-radiologist users,1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=532592,no,undetermined,0
From hardware to software testability,"This paper presents the application of some hardware testability concepts to data-flow software. Testability is concerned with three difficulties: generating test sets, interpreting test results and diagnosing faults. This threefold aspect of testability is discussed and estimates are proposed",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=529901,no,undetermined,0
An operational expert system to analyze and locate faults occurring on MV networks,"This paper describes an operational expert system analyzing faults occurring on medium voltage networks. After a brief description of the auscultation method, that led to the development of the software, the LAURE functions and architecture are explained. The analysis and calculation module is detailed, as well as the configuration and the request parts. The paper also displays the experimentation results and the organization held to operate the system",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=515199,no,undetermined,0
Analyzing empirical data from a reverse engineering project,"This study reports the analysis of data collected during the execution of a reverse engineering process in a real environment. Essentially, the analysis assessed productivity aspects. The experience showed the need for automatic tools which can be adapted to the context the process is performed in and the possibility of anchoring the effort required for the reverse engineering to the desired product quality",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=514699,no,undetermined,0
A verification tool to measure software in critical systems,"Previously, software metrics have been established to evaluate the software development process throughout the software life cycle, and have been effective in helping to determine how a software design is progressing. These metrics are used to uncover favorable and unfavorable design trends and identify potential problems and deficiencies early in the development process to reduce costly redesign or the delivery of immature error prone software. One area where design metrics plays an important role is in the identification of misunderstandings between the software engineer and the system or user requirements due to incorrect or ambiguous statements of requirements. However, the metrics developed to date do not consider the additional interface to the safety engineer when developing critical systems. Because a software error in a computer controlled critical system can potentially result in death, injury, loss of equipment or property, or environmental harm, a safety metrics set was developed to ensure that the safety requirements are well understood and correctly implemented by the software engineer. This paper presents a safety metrics set that can be used to evaluate the maturity of hazard analysis processes and its interaction with the software development process",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=513263,no,undetermined,0
Performance and reliability evaluation of passive replication schemes in application level fault tolerance,"Process replication is provided as the central mechanism for application level software fault tolerance in SwiFT and DOORS. These technologies, implemented as reusable software modules, support cold and warm schemes of passive replication. The choice of a scheme for a particular application is based on its availability and performance requirements. In this paper we analyze the performability of a server software which may potentially use these technologies. We derive closed form formulae for availability throughput and probability of loss of a job. Six scenarios of loss are modeled and for each, these expressions are derived. The formulae can be used either of time or online to determine the optimal replication scheme.",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=781067,no,undetermined,0
On the analysis of subdomain testing strategies,Weyuker and Jeng (1991) have investigated the conditions that affect the performance of partition testing and have compared analytically the fault-detecting ability of partition testing and random testing. Chen and Yu (1994) have generalized some of Weyuker and Jeng's results. We extend the analysis to subdomain testing in which subdomains may overlap. We derive several results for a special case and demonstrate a technique to extend some of our results to more general cases. We believe that this technique should be very useful in further investigating the behaviour of subdomain testing,1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=496970,no,undetermined,0
A defect identification algorithm for sequential and parallel computers,"The comparison of images containing a single object of interest, where one of them contains the model object, is frequently used for defect identification. This is often a problem of interest to industrial applications. This paper introduces sequential and parallel versions of an algorithm that compares original (reference) and processed images in the time and frequency domains. This comparison combined with histogram data from both domains can identify differences in the images. Extracted data is also compared to database data in an attempt to pinpoint specific changes, such as rotations, translations, defects, etc. The first application considered here is recognition of an object which has been translated and/or rotated. For illustration purposes, an original image of a computer-simulated centered needle is compared to a second image of the hypodermic needle in a different position. This algorithm will determine if both images contain the same object regardless of position. The second application identifies changes (defects) in the needle regardless of position and reports the quality of the needle. This quality will be a quantitative measurement depending on error calculations in the spatial and frequency domains and comparisons to database data. Finally, the performance of sequential and parallel versions of the algorithm for a Sun SPARCstation and an experimental in-house built parallel DSP computer with eight TMS320C40 processors is included. The results show that significant speedup can be achieved through incorporation of parallel processing techniques",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=496625,no,undetermined,0
A CSIC implementation with POCSAG decoder and microcontroller for paging applications,"This paper presents a CSIC (Customer Specification Integrated Circuit) implementation, which includes a 512/1200/2400 bps POCSAG decoder, PDI2400 and MC68HC05 changed by PANTECH. It can receive all the data with the rate of 512/1200/2400 bps of a single clock of 76.8 KHz. It is designed to have maximum 2 own frames for service enhancement. To improve receiver quality, a preamble detection considering frequency tolerance and a SCW (Synchronization Code Word) detection at every 4 bit is suggested. Also we consider an error correction of address and message up to 2 bits. Furthermore, it is possible with proposed PF (Preamble Frequency) error to achieve a battery life increase due to the turn-off of RF circuits when the preamble signal is detected with noises. The chip is designed using VHDL code from PDI2400 micro-architecture level. It is verified with VHDL simulation software of PowerView. Its logic diagrams are synthesized with VHDL synthesis software of PowerView. Proposed decoder and MC68HC05 CPU of MOTOROLA are integrated with about 88000 transistors by using 1.0 Î¼m HCMOS process and named MC68HC05PD6. It is proved that the wrong detection numbers of preamble of noises are significantly reduced in the pager system that uses our chip through the real field test. The system receiving performance is improved by 20% of average, compared with other existing systems",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=486210,no,undetermined,0
Testability analysis of co-designed systems,"This paper focus on the testability analysis of co-designed data-flow specifications. The co-designed specification level implies a high level testability analysis, independent of the implementation choices. With respect to testability, the difficulties of generating test sets, detecting and diagnosing faults are discussed and estimates are proposed. A hardware modelling, based on information transfers and called the Information Transfer Graph, is adapted to the specifications. A real case study supplied by Aerospatiale illustrates all the evaluations",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=485338,no,undetermined,0
Distributed off-line testing of parallel systems,This paper deals with off-line testing of parallel systems. The applicability of distributed self-diagnosis algorithms is investigated. Both static and adaptive testing assignment strategies are studied and evaluated using a queueing network model of the system under test. Key results include testing latency and message load of each algorithm,1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=485308,no,undetermined,0
Electrical two and three dimensional modelling of high-speed board to board interconnections,"In today's high speed electronic systems, board-to-board connectors can contribute to signal degradation, reflections, and crosstalk. Methods exist for using computer modeling to predict the electrical behavior of interconnections from a knowledge of their materials and construction. A typical modeling process utilizes a combination of 2 or 3 dimensional electromagnetic field solvers to derive an approximate circuit model of the connector. Predictions of connector electrical performance are then obtained by plugging this connector model into a model test circuit and simulating the combination with circuit simulation software such as SPICE. As an example a six-row 2 mm-grid backplane connector is discussed in this paper",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=485259,no,undetermined,0
Networked analysis systems,"Summary form only given. In-line and off-line metrology measurement and interpretation are a critical, but time consuming part of yield learning. Yield learning occurs during pilot line development and early volume manufacture. Networking analysis systems and whole wafer analysis tools can greatly reduce the cycle time associated with metrology during yield learning. Although in-line metrology tools have whole wafer capability, other tools such as scanning electron microscopes (equipped with energy dispersive spectroscopy: SEM/EDS) for defect review are recent developments. These SEM/EDS defect review tools (DRT) have coordinate locating stages and software capable of reading wafer defect maps from optical defect detection systems. In this paper, we discuss networked analysis systems and whole wafer tools for in/off-line analysis. The issues associated with data interpretation and management become greater with each new technology generation. We also discuss new network capabilities such as presorting electrical defects into similar types before physical characterization using ""model yield learning"".",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=484341,no,undetermined,0
Application of virtual reality devices to the quantitative assessment of manual assembly forces in a factory environment,"The paper describes a system which has been developed to meet an industrial requirement for performing in-situ quantitative measurements of manual forces and postures applied by human assembly line operators. Virtual reality devices are shown to offer a valuable means of meeting the necessary sensory requirements. A set of force transducers incorporated into a pair of Datacq gloves together with a force triggered image capturing facility all controlled by a PC compatible, comprise the main hardware elements of the basic system. Comprehensive software provides online and playback facilities, allowing nonmedical staff to assess the assembly process, including maximum forces applied and a measure of the effort or work done per assembly cycle. Invaluable physiological information is made available for post processing and further medical analysis as well as for database archiving purposes. The instrument is seen to have generic application for ergonomic design of manufacturing processes involving humans, in the quest to avoid repetitive health problems such as strain injury in the upper body. The system can also serve as a tool to monitor the design of new product types as well as setting ergonomically relevant quality targets for subcontractor supplied components",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=483940,no,undetermined,0
Assessing the capabilities of military software maintenance organizations,"The objectives of the assessment of military avionics software maintenance organizations (SMOs) are to determine the current and future software maintenance capabilities at a specific organization, evaluate the software products and processes, review the organizational infrastructure, identify areas of technical, programmatic and cost risk, estimate the costs associated with maintenance, and identify actions which could reasonably be taken to improve business efficiency and software quality. This paper focuses primarily on our process for assessing SMOs and then highlights our findings by identifying a number of issues that are pervasive across military software maintenance organizations. These assessments provide us with practical insight into the world of software maintenance and allow us to provide direction so that SMOs can plan and be prepared for more legacy avionics software in the future",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=482914,no,undetermined,0
Built-in-test adequacy-evaluation methodology for an air-vehicle system,"Rapid improvement in built-in-test (BIT) system capability is probably one of the most important activities in an air-vehicle system development test and evaluation program. An effective BIT adequacy evaluation methodology can accomplish this. The methodology must provide the development tester BIT parameters that serve as indicators of air-vehicle BIT system performance. Deficiencies that are pinpointed can then be corrected to improve the overall BIT system's performance. A BIT adequacy evaluation methodology developed and implemented during a recently conducted Air Force development and test program did just that. It provided real-time technical information at a particular period that indicated the status of the BIT system in relation to specified requirements. It also served as a springboard to evaluate the BIT system, and to document the results, which provided the manufacturer with valuable data on the BIT system's performance. Finally, it gave the manufacturer the opportunity to implement corrective actions within the allocated schedule, and develop the air-vehicle BIT system concurrently",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=513260,no,undetermined,0
GPS ground antenna and monitor station upgrades: software requirements analysis,"This paper discusses the software aspects of the Global Positioning System (GPS) remote site modernization study, specifically, the software requirements analysis. At the beginning of the software requirements analysis phase, a qualitative trade-off study was performed to determine if a structured analysis or an object-oriented analysis (OOA) approach would be followed. The latter was chosen because it offers a number of advantages over the life-cycle of a software development project. This paper outlines these advantages. In conjunction with this methodology study, a study of Computer-aided Software Engineering (CASE) tools was performed to ascertain if available software requirements analysis and design tools would aid the software development process, in general, and requirements analysis, in particular. The results of this study are also summarized in this paper. The paper also discusses the experiences of Draper Laboratory software engineers in using OOA to perform software requirements analysis for a large software project (over 100,000 lines of Ada source code estimated), and how this approach was followed while specifying requirements using the Software Requirements Specification Data Item Description (DID) that accompanies DOD-STD-2167A",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=482832,no,undetermined,0
Traffic Alert and Collision Avoidance System (TCAS) transition program (TTP): a status update,"In response to United States (U.S.) law mandating installation, by December 31, 1993, of TCAS IT on all air carrier aircraft with more than thirty passenger seats operating in U.S. Airspace, the first in-service flight of a certified TCAS II production unit occurred in June 1990. Currently, worldwide installations of TCAS IT in air carrier and business aircraft are estimated at over 7,000 units. Total accumulated flight operations exceed 50 million hours, with over 1.5 million additional hours accumulated each month. This paper provides a status update on the TCAS Transition Program (TTP) which was established to monitor the operational performance and assist with the introduction of TCAS II avionics into the National Airspace System (NAS). The update is provided by means of a qualitative performance comparison between the software logic initially fielded (Version 6.02) and the latest version (Version 6.04A) developed to address various flight performance dynamics and interface issues between TCAS and the air traffic control (ATC) system. This latest software logic has been in operation in all TCAS II-equipped aircraft since January 1, 1995",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=482819,no,undetermined,0
Military requirements constrain COTS utilization,"In the past, our military systems have required technology not readily available in the industrial and commercial areas. This is no longer the case. Commercial products and practices offer the potential of reduced costs, lower risk and faster technology infusion. In order to achieve the potential benefits, our current methods of acquisition need change but it is important to review the background behind the institution of military standards and to assess their continued validity and to understand why. Also, before we rush to the conclusion that military specifications and standards are preferred for avionics over commercial products and practices, it is useful to review the specific requirements. In the past, in the absence of military specifications and standards, situations arose where items were developed using existing industrial and commercial products, processes and practices that at times led to catastrophic results or were inadequate to meet the threat. It is comforting that those who demand change show wisdom by asking that change be conducted with care and the impact understood. However, some use hyperbole in condemning the reliance on military specifications and standards and insist that their elimination is a good thing. In the domain of high performance aircraft avionics the potential for change is likely to be limited without an in-depth comprehension of the military environment, requirements and the capability of commercial products to satisfy them",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=482815,no,undetermined,0
Computer aided partial discharge testing of electrical motors and capacitors,"The present methods for testing of low voltage equipment, in particular electrical motors and capacitors are discussed. It is demonstrated that those tests realising high voltage overstress of the object tested cannot guarantee reliable performance of solid insulation throughout the equipment life. Partial discharge (PD) measurement as an alternative is a true nondestructive test method providing detailed information about the duality of manufacturing of the equipment tested and the selection of suitable insulation materials. As mentioned, above two different types of test objects have been investigated. The measurements have been performed by means of a computerised partial discharge measuring system with particular software features which simplify its application. Based on experimental results it is demonstrated that PD tests are of great importance for quality assurance of electrical motors and capacitors. Moreover it seems to be possible to predict the quality of the final product by precise PD investigations and measurements of the materials and components used",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=482503,no,undetermined,0
In-service signal quality estimation for TDMA cellular systems,"In-service interference plus noise power (I+N) and signal-to-interference plus noise power SI(I+N) estimation methods are examined for TDMA cellular systems. A simple (I+N) estimator is developed whose accuracy depends on the channel and symbol estimate error statistics. Improved (I+N) and S/(I+N) estimators are developed whose accuracy depends only on the symbol error statistics. The proposed estimators are evaluated through software simulation with an IS-54 frame structure. For high speed mobiles, it is demonstrated that S/(I+N) can be estimated to within 2 dB in less than a second",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=480986,no,undetermined,0
Implementation and demonstration of a multiple model adaptive estimation failure detection system for the F-16,"A multiple model adaptive estimation (MMAE) algorithm is implemented with the fully nonlinear six-degree-of-motion simulation rapid-prototyping facility (SRF) VISTA F-16 software simulation tool. The algorithm is composed of a bank of Kalman filters modeled to match particular hypotheses of the real world. Each presumes a single failure in one of the flight-critical actuators, or sensors, and one presumes no failure. For dual failures, a hierarchical structure is used to keep the number of on-line filters to a minimum. The algorithm is demonstrated to be capable of identifying flight-critical aircraft actuator and sensor failures at a low dynamic pressure (20,000 ft, 4 Mach). Research includes single and dual complete failures. Tuning methods for accommodating model mismatch, including addition of discrete dynamics pseudonoise and measurement pseudonoise, are discussed and demonstrated. Scalar residuals within each filter are also examined and characterized for possible use as an additional failure declaration voter. An investigation of algorithm performance off the nominal design conditions is accomplished as a first step towards full flight envelope coverage",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=480616,no,undetermined,0
A real time software-only H.261 codec,"Video and audio conferencing over networks is becoming increasingly popular due to the availability of video and audio I/O as standard equipment on many computer systems. So far, many algorithms have concentrated on playback only capability. This generally results in unacceptable real-time performance with respect to latency and encoder complexity. We describe a software-only system that allows full duplex video communication. For our analysis and implementation we chose a DCT based method that uses motion estimation and is modelled on the CCITT H.261 standard. We discuss the algorithm, followed by an analysis of the computational requirements for each major block. The results presented show the effect of computational simplifications on signal to noise ratio and image quality. We also examine the processing needs for full resolution coding and project when this will become available",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=480123,no,undetermined,0
Using speculative execution for fault tolerance in a real-time system,"Achieving fault-tolerance using a primary-backup approach involves overhead of recovery such as activating the backup and propagating execution states, which may affect the timeliness properties of real-time systems. We propose a semi-passive architecture for fault-tolerance and show that speculative execution can enhance overall performance and hence shorten the recovery time in the presence of failure. The compiler is used to detect speculative execution, to insert check-points and to construct the updated messages. Simulation results are reported to show the contribution of speculative execution under the proposed architecture",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=479357,no,undetermined,0
An integrated approach for criticality prediction,"The paper provides insight in techniques for criticality prediction as they are applied within the development of Alcatel 1000 S12 switching software. The primary goal is to identify critical components and to make failure predictions as early as possible during the life cycle and hence reduce managerial risk combined with too early or too late release. The approach is integrated in the development process and starts with complexity based criticality prediction of modules. Modules identified as overly complex are given additional tests or review efforts. Release time prediction and field performance prediction are both based on tailored ENHPP reliability models. For the complete approach of criticality prediction, recent data from the development of a switching system with around 2 MLOC is provided. The switching system is currently in operational use, thus allowing for validation and tuning of the prediction models",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=497639,no,undetermined,0
Detection of fault-prone program modules in a very large telecommunications system,"Telecommunications software is known for its high reliability. Society has become so accustomed to reliable telecommunications, that failures can cause major disruptions. This is an experience report on application of discriminant analysis based on 20 static software product metrics, to identify fault prone modules in a large telecommunications system, so that reliability may be improved. We analyzed a sample of 2000 modules representing about 1.3 million lines of code, drawn from a much larger system. Sample modules were randomly divided into a fit data set and a test data set. We simulated utilization of the fitted model with the test data set. We found that identifying new modules and changed modules mere significant components of the discriminant model, and improved its performance. The results demonstrate that data on module reuse is a valuable input to quality models and that discriminant analysis can be a useful tool in early identification of fault prone software modules in large telecommunications systems. Model results could be used to identify those modules that would probably benefit from extra attention, and thus, reduce the risk of unexpected problems with those modules",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=497640,no,undetermined,0
Analysis of review's effectiveness based on software metrics,"The paper statistically analyzes the relationship between review and software quality and the relationship between review and productivity, utilizing software metrics on 36 actual projects executed in the OMRON Corporation from 1992 to 1994. Firstly, by examining the relationship between review effort and field quality (the number of faults after delivery) of each project, and the relationship between the number of faults detected in review and field quality of each project, we reasoned that: (1) greater review effort helps to increase field quality (decrease the number of faults after delivery); (2) source code review is more effective in order to increase field quality than design review; (3) if more than 10% of total design and programming effort is spent on review, one can achieve a quite stable field quality. We noticed that no relevant effects were recognized in productivity (LOC/staff month) with respect to a review rate of up to 20%. As a result of the analysis above, we recommended that 15% of review effort is a suitable percentage to use as a guideline for our software project management",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=497641,no,undetermined,0
Hyper-geometric distribution software reliability growth model with imperfect debugging,"Debugging actions during the test/debug phase of software development are not always performed perfectly. That is, not all the software faults detected are perfectly removed without introducing new faults. This phenomenon is called imperfect debugging. The hyper-geometric distribution software reliability growth model (HGDM) was developed for estimating the number of software faults initially in a program. We propose an extended model based on the HGDM incorporating the notion of imperfect debugging",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=497658,no,undetermined,0
Software-quality improvement using reliability-growth models,"In the traditional software development model, the system test phase is the last stage where the reliability, capability, performance and other important dimensions of the quality of a system can be evaluated. During this stage, a point is eventually reached where a decision must be made to either continue or stop testing. This is a crucial task that requires an objective assessment of the benefits and costs associated with each alternative. This paper outlines a methodology that improves the effectiveness of management decisions by providing estimates of the total number of errors to be found through testing, the number of remaining errors, the additional testing time needed to achieve reliability goals and the impact of these parameters on product quality, project cost and project duration",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=513244,no,undetermined,0
Parallel test generation with low communication overhead,"In this paper we present a method of parallelizing test generation for combinational logic using boolean satisfiability. We propose a dynamic search-space allocation strategy to split work between the available processors. This strategy is easy to implement with a greedy heuristic and is economical in its demand for inter-processor communication. We derive an analytical model to predict the performance of the parallel versus sequential implementations. The effectiveness of our method and analysis is demonstrated by an implementation on a Sequent (shared memory) multiprocessor. The experimental data shows significant performance improvement in parallel implementation, validates our analytical model, and allows predictions of performance for a range of time-out limits and degrees of parallelism",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=512088,no,undetermined,0
A pricing policy for scalable VOD applications,"Many video applications are scalable due to human tolerance to the degradation in picture quality, frame loss and end-to-end delay. Scalability allows the network to utilize its resources efficiently for supporting additional connections, thereby increasing revenue and number of supported customers. This can be accomplished by a dynamic admission control scheme which scales down existing connections to support new requests. However, users will not be willing to tolerate quality degradation unless it is coupled with monetary or availability incentives. We propose a pricing policy and a corresponding admission control scheme for scalable VOD applications. The pricing policy is two-tiered, based on a connection setup component and a scalable component. Connections which are more scalable are charged less but are more liable to be degraded. The proposed policy trades off performance degradation with monetary incentives to improve user benefit and network revenue, and to decrease the blocking probability of connection requests. We demonstrate by means of simulation that this policy encourages users to specify the scalability of an application to the network",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=509563,no,undetermined,0
Dealing with Non-Functional Requirements: Three Experimental Studies of a Process-Oriented Approach,"Quality characteristics are vital for the success of software systems. To remedy the problems inherent in ad hoc development, a framework has been developed to deal with non-functional requirements (quality requirements or NFRs). Taking the premise that the quality of a product depends on the quality of the process that leads from high-Ievel NFRs to the product, the framework's objectives are to represent NFR-specific requirements, consider design tradeoffs, relate design decisions to IYFRs, justify the decisions, and assist defect detection. The purpose of this paper is to give an initial evaluation of the extent to which the framework's objectives are met. Three small portions of information systems were studied by the authors using the framework. The framework and empirical studies are evaluated herein, both from the viewpoint of domain experts who have reviewed the framework and studies, and ourselves as framework developers and users. The systems studied have a variety of characteristics, reflecting a variety of real application domains, and the studies deal with three important classes of NFRs for systems, namely, accuracy, security, and performance. The studies provide preliminary support for the usefulness of certain aspects of the framework, while raising some open issues.",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5071089,no,undetermined,0
Performance prediction of WLAN speech and image transmission,"The effects of multipath, noise, and cochannel interference on performance are shown for two wireless local area networks (WLAN). Measured impulse responses from an office environment provided the multipath distortion. The first WLAN system uses frequency hopped (FH) Gaussian frequency shift keying (GFSK) and the second uses direct sequence (DS) differential binary phase shift keying (DBPSK). Both modulations are part of the proposed WLAN IEEE 802.11 standard. Vocoded speech and image quality were predicted for wireless systems through software simulation by relating quality classes to the following system and channel operational parameters: bit error rate (BER), signal-to-noise ratio (SNR), and carrier-to-interference ratio (CIR). Experimental results are summarized by overlaying image and vocoded speech quality classes on a BER, SNR, and CIR graph",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=504930,no,undetermined,0
Topology detection for adaptive protection of distribution networks,A general purpose network topology detection technique suitable for use in adaptive relaying applications is presented in this paper. Three test systems were used to check the performance of the proposed technique. Results obtained from the tests are included. The proposed technique was implemented in the laboratory as a part of the implementation of the adaptive protection scheme. The execution times of the topology detection software were monitored and were found to be acceptable,1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=500768,no,undetermined,0
Probing and fault injection of protocol implementations,"Ensuring that a distributed system with strict dependability constraints meets its prescribed specification is a growing challenge that confronts software developers and system engineers. This paper presents a technique for probing and fault injection of fault-tolerant distributed protocols. The proposed technique, called script-driven probing and fault injection, can be used for studying the behavior of distributed systems and for detecting design and implementation errors of fault-tolerant protocols. The focus of this work is on fault injection techniques that can be used to demonstrate three aspects of a target protocol: i) detection of design or implementation errors, ii) identification of violations of protocol specifications, and iii) insight into design decisions made by the implementers. The emphasis of our approach is on experimental techniques intended to identify specific â€œproblemsâ€?in a protocol or its implementation rather than the evaluation of system dependability through statistical metrics such as fault coverage. To demonstrate the capabilities of this technique, the paper describes a probing and fault injection tool, called the PFI tool (Probe/Fault Injection Tool), and a summary of several extensive experiments that studied the behavior of two protocols: the transmission control protocol (TCP) and a group membership protocol (GMP)",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=500039,no,undetermined,0
Hybrid systems long term simulation,"The paper deals with the estimation of the long term performances of â€œhybrid systemsâ€?for electric energy production. Such systems integrate conveniently different kind of generators and usually they are constituted by traditional electric energy plants as diesel generating set combined with renewable energy generators. A software for the energetic/economic evaluation of the performances of hybrid systems as a function of their configuration and components characteristics as well as of the renewable energy sources availability is presented. Such a package has been developed as part of the Hybrid Simulator Facility built under an EC-JOULE II project. Tests performed on the facility are presented to show the capability of these kind of systems to cover the needs of small communities. Also giving a very good quality of the electric service. Furthermore it is demonstrated how to reduce the cost of the kWh with respect to the one produced by diesel stations for electric energy production in isolated communities. Finally, a simulation relative to the Greek island of Angathonisi is reported and the results show the fuel savings obtained with the integration of a 20 kW wind turbine and 10 kWp PV field",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=499157,no,undetermined,0
Computerised fault diagnosis of induction motor drives,"In spite of the fact that the probability of breakdowns of 3-phase induction motors is very low, 2 to 3% per annum, monitoring their operating condition is essential, particularly when they are working in sophisticated workshops (for example automated production lines). In many industries lots of machines depend on mutual operation, and the cost of unexpected breakdowns is very high. In such cases the breakdowns of machines in operation usually involve higher losses in the production process than the cost of their repairs or even the initial costs of the machines themselves. In addition to the faults, i.e. short circuits, winding failures, bearing seizures, causing an immediate stop in the production process often has safety implications and may even expose human beings to danger. Both technical and economical considerations suggested the development of a new computer-based diagnostic system to predict the dates and places of expected faults by condition monitoring. The computer-based measurement and analysis system was designed and tested under a cooperation scheme between the Middlesex University, London and the University of Miskolc, Hungary. This system, after analysing various quantities of the drive unit, predicts the possible breakdowns, their expected dates and places. The authors discuss the criteria for a new diagnostic system, and then describe the hardware and software of the new system",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=497720,no,undetermined,0
A DSP based scheme for the on-line monitoring of performance and parameter determination of three phase induction motors,"The paper discusses developments, and practical implementation of a novel digital signal processor (DSP) based system for the online real time monitoring of performance and parameter determination of three-phase induction motors. A software based algorithm is used to measure the performance and calculate the parameters of the three-phase induction motor in real-time, without the need to connect a mechanical load or drive to the machine's rotor shaft. The new algorithm takes into account the machine parameter variations due to changes in electric supply, skin effect, mechanical load and any fault in the overall system, so the system can be used for condition monitoring of any electrical drive system and can be permanently or temporarily connected to such a system for diagnosis",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=497717,no,undetermined,0
The current analysis program-a software tool for rotor fault detection in three phase induction motors,This paper shows that it is possible to monitor the supply current to an induction motor during the motors acceleration period and be able to detect the nonstationary frequency components previously monitored whilst stationary during the motors steady state operation. Results show that the increase in component amplitude observed with a faulty rotor when monitoring in steady state under full load conditions can clearly be observed when monitoring the transient signal under no load conditions. Using a signal processing technique known as wavelet decomposition it is shown that the sidebands may be tracked during the transient period. This technique has been simplified and implemented along with a data acquisition system to form a complete current transient health monitoring system which determines the health of the motor from its no load supply current transient signal. The software which controls both the acquisition and analysis part of the monitoring system has been developed so that the operator has access to all main parameters used within the analysis,1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=497715,no,undetermined,0
ROBUST: a next generation software reliability engineering tool,"In this paper, we propose an approach for linking the isolated research results together and describe a tool supporting the incorporation of the various existing modeling techniques. The new tool, named ROBUST, is based on recent research results validated using actual data. It employs knowledge of static software complexity metrics, dynamic failure data and test coverages for software reliability estimation and prediction at different software development phases",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=497680,no,undetermined,0
Parameter estimation of hyper-geometric distribution software reliability growth model by genetic algorithms,"Usually, parameters in software reliability growth models are not known, and they must be estimated by using observed failure data. Several estimation methods have been proposed, but most of them have restrictions such as the existence of derivatives on evaluation functions. On the other hand, genetic algorithms (GA) provide us with robust optimization methods in many fields. We apply GA to the parameter estimation of the hyper-geometric distribution software reliability growth model. Experimental result shows that GA is effective in the parameter estimation and removes restrictions from software reliability growth models",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=497673,no,undetermined,0
Towards a unified approach to the testability of co-designed systems,"The paper deals with the testability analysis of dataflow co designed systems. As a data flow specification is independent from the hardware/software implementation choice, a uniform approach may be used to evaluate the specification with respect to testability. The difficulty of generating test sets, and of detecting and diagnosing faults is discussed and estimated. We chose to use an existing hardware testability model which is suitable for data flow software specification; this model, based on information transfers, is called the Information Transfer Graph. A real case study supplied by Aerospatiale illustrates the proposed testability estimates",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=497668,no,undetermined,0
Predicting software's minimum-time-to-hazard and mean-time-to-hazard for rare input events,"The paper turns the concept of input distributions on its head to exploit inverse input distributions. Although such distributions are not always true mathematical inverses, they do capture an intuitive property: inputs that have high frequencies in the original distribution will have low frequencies in the inverse distribution, and vice versa. We can use the inverse distribution in several different quality checks during development. We provide a fault based (fault injection) method to determine minimum time to failure and mean time to failure for software systems under normal operational and non normal operational conditions (meaning rare but legal events). In our calculations, we consider how various programmer faults, design errors, and incoming hardware failures are expected to impact the observability of the software system",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=497662,no,undetermined,0
Performability modeling of N version programming technique,"The paper presents a detailed, but efficiently solvable model of the N version programming for evaluating reliability and performability over a mission period. Employing a hierarchical decomposition we reduce the model complexity and provide a modeling framework for evaluating the NVP failure and execution time behavior and the operational environment, as well. The failure and execution rates are treated as random variables and the operational profile is analyzed on the microstructure level, looking at probabilities of occurrence, failure and execution rates for each partition of input space. The reliability submodel that represents per run behavior of NVP, includes both functional failures and timing failures thus resulting in system reliability which accounts for performance requirements. The successive runs are modeled by the performance submodel, that represents the iterative nature of the software execution. Combining the results of both submodels, we assess the performability over a mission period that represents the collective effect of multiple system attributes on the NVP effectiveness",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=497660,no,undetermined,0
A new method for increasing the reliability of multiversion software systems using software breeding,"The paper proposes a new method for increasing the reliability of multiversion software systems. The software using software breeding is more reliable than one using N version programming. But software breeding is not suitable for real time application because program versions are executed several times for detecting faulty modules. In the proposed method, the detection of faulty modules is performed in the background when program versions fail and the software continues the execution in the foreground. When the detection of faulty modules is finished, the combination of module versions in program versions are changed. Ten simulations, each of which executed program versions 10<sup>6</sup> times, were performed to analyse the effectiveness of the new method. This resulted in the reduction of the number of failures to range from 33% to 76% with an average of 56%",1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=497659,no,undetermined,0
A neural network based perceptual audio coder,"The implementations and performance results of a neural network based perceptual audio coder is reported. The coder uses the configuration of the ISOIMPEG audio layer II coder with the perceptual analysis block replaced by a 2 layer network trained to estimate the masking thresholds required for the bit allocation. The 2 layer network is trained by a back propagation algorithm using the energies in the subbands as inputs and the masking thresholds of obtained from psychoacoustic model II of the ISOIMPEG audio coder as the reference outputs. The result is a coder which performs favourably in quality against the ISOIMPEG audio layer 2 coder at bit rates of 256 kbit/s and 192 kbit/s stereo. Performance at a bit rate of 128 kbit/s stereo was however, found to be poorer",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=369178,no,undetermined,0
Class hierarchy based metric for object-oriented design,"Object-oriented technology, including object-oriented analysis (OOA), object-oriented design (OOD), and object-oriented programming (OOP), is a new promising approach to developing software systems to reduce software costs and to increase software extensibility, flexibility, and reusability. Software metrics are widely used to measure software complexity and assure software correctness. This paper proposes a metric to measure object-oriented software. Also, an important factor called URIs, is conducted to build the metric. This approach describes a graph-theoretical method for measuring the complexity of the class hierarchy. The proposed metric shows that inheritance has a close relation with object-oriented software complexity and reveals that misuse of repeated (multiple) inheritance will increase software complexity and be prone to implicit software errors",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=369163,no,undetermined,0
On the relationship between partition and random testing,"Weyuker and Jeng (ibid., vol. SE-17, pp. 703-711, July 1991) have investigated the conditions that affect the performance of partition testing and have compared analytically the fault-detecting ability of partition testing and random testing. This paper extends and generalizes some of their results. We give more general ways of characterizing the worst case for partition testing, along with a precise characterization of when this worst case is as good as random testing. We also find that partition testing is guaranteed to perform at least as well as random testing so long as the number of test cases selected is in proportion to the size of the subdomains",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=368132,no,undetermined,0
Performance trending for management of high-speed packet-switched networks,"Due to technological advances, computer communication networks are becoming increasingly complex. In addition, applications require networks to provide high performance and reliability. Placing such demands on these networks necessitates an efficient network management. Many existing systems perform data collection and reporting only, and so operators must use their own expertise to do aid the trouble shooting. To provide a dynamic solution, it is necessary to provide a real-time automated control of network operation parameters. The objective of this research is to develop and demonstrate the capability of applying modeling and simulation techniques to maintain the network integrity of a high-speed packet-switched network. In the paper a new technique called performance trending is presented, in which key network parameters are monitored and network failure is predicted. Performance trending is also used for the network performance maintenance by means of analysis and tuning to predict and prevent network failures. A typical packet-switched network is simulated to verify the performance trending for network performance management. End-to-end delay and bit error rate (BER) are used to illustrate performance trending and their effects on the network performance",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=318373,no,undetermined,0
A parametric method for the evaluation of human-computer interfaces,"The paper proposes a method of interface quality evaluation intended for product selection and based on a checklist. The method is flexible, easily understood and covers all aspects of an interface. Creation of the list rests upon a selection of more than 300 ergonomic criteria drawn from published reports and research articles in the field of interface design and evaluation. The criteria are high-level and relatively independent of the type of interface and technology. Their collection has been organised according to a new synthetic classification scheme which bears upon the functional capabilities and friendliness factors of an interface. Furthermore, the proposed method uses a parametric model to calculate the overall quality of an interface as a function of the values and weights assigned to the criteria and classes. The user can also adjust the model behaviour by modifying its sensitivity parameters. The method of evaluation has been implemented in an electronic calculator",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=332217,no,undetermined,0
Automated error detection in multibeam bathymetry data,"For a variety of reasons, multibeam swath sounding systems produce errors that can seriously corrupt navigational charts. To address this problem, the authors have developed two algorithms for identifying subtle outlier errors in a variety of multibeam systems. The first algorithm treats the swath as a sequence of images. The algorithm is based on robust estimation of autoregressive (AR) model parameters. The second algorithm is based on energy minimization techniques. The data are represented by a weak-membrane or thin-plate model, and a global optimization procedure is used to find a stable surface shape. Both of these algorithms have undergone extensive testing at bathymetric processing centers to assess performance. The algorithms were found to have a probability of detection high enough to be useful and a false-alarm rate that does not significantly degrade the data quality. The resulting software is currently being used both at processing centers and at sea as an aid to bathymetric data processors",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=326072,no,undetermined,0
Control reconfiguration in the presence of software failures,"In this paper, we discuss a special approach for software fault tolerance in control applications. A full-function, high-performance, but complex control system is complemented by an error-free implementation of a highly reliable control system of lower functionality. When the correctness of the high-performance controller is in doubt, the reliable control system takes over the execution of the task. An innovative feature of the approach is the disparity between the two control systems, which is used to exploit the relative advantages of the simple/reliable vs. complex/high-performance systems. Another innovative feature is the fault detection mechanism, which is based on measures of performance and of safety of the control system. The example of a ball and beam system is used to illustrate the concepts, and experimental results obtained on a laboratory set-up are presented",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=325604,no,undetermined,0
Robust FDI systems and H<sub>âˆ?/sub>-optimization-disturbances and tall fault case,"This paper is dedicated to the design of robust FDI systems by utilizing the H<sub>âˆ?/sub>-optimization technique. Results are derived based on the optimization of a sensitivity (performance) index. The overall effectiveness of the residual generator is examined in terms of faults vs. uncertainties, with the assumption of a bounded norm for disturbances. The results show that, for a well-conditioned system, a robust residual generator exists for all possible disturbances. This paper discusses the robust design of a tall fault system (i.e. the number of faults is no greater than the number of outputs) with additive disturbances",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=325481,no,undetermined,0
Comparison between a neural fuzzy system- and a backpropagation-based fault classifiers in a power controller,"A real-time neural fuzzy (NF) power control system is developed and compared with a backpropagation neural network (BNN) system. The objective is to develop computation hardware and software in order to implement the fault classification of a three-phase motor in real-time response. With online training capability, the NF system can be adaptive to the particular characteristics of a particular motor and can be easily modified for the customer's needs in the future. The preprocessing of a BNN-based fault classifier normalizes the magnitude between [-1,1] and transforms the number of samples to 32 for a cycle of waveform. The trained BNN is used to classify faults from the input waveforms. Real-time response is achieved through the use of a parallel processing system and the partition of the computation into parallel processing tasks. Compared with a four-processor BNN system, the NF system requires smaller cost (three processors) and recognizes waveforms faster. Moreover, with the appropriate feature extraction, the NF system can recognize temporally variant spike and chop occurring within a sin waveform",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=324221,no,undetermined,0
A new approach for discriminating between switching and fault generated broadband noise on EHV transmission lines,"A novel technique for the detection of fault generated high frequency noise on transmission lines has been under development within the Power and Energy System Group at Bath University. The technique employs an arrangement involving the use of conventionally connected power line communication signal traps and capacitor voltage transformers (CVTs). This paper examines the performance of the digital fault detection technique in the presence of high frequency signals due to both arcing faults and spurious noise, with a view to improving the reliability and dependability of the technique, particularly in the presence of the latter. The performance is illustrated in relation to high frequency responses attained from a typical 400 kV vertical construction line, under both faults and switching operations on the system and for these purposes, the simulation studies for generating the high frequency phenomenon includes the incorporation of very realistic nonlinear arc models. The well known EMTP software package is used.<<ETX>>",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=320689,no,undetermined,0
CCD camera based automatic dimension measurement and reproduction of 3-D objects,"The paper presents an automatic CCD camera vision based measurement system having capabilities to measure dimensional parameters and reproduce a 3-D object on computer numerical controlled machines (CNC). The operations of image acquisition, digitization, storage and transfer to computer are performed by the vision system. The vision system provides the required coordinates to computer for the movement of machine tools. The CNC machines integrated with the vision system form an automatic object reproduction system. This automation enhances the quality and quantity of production. The associated analysis software employs graphic techniques like edge detection, surface filling and mensuration of regular 3D objects. The linear parameter (i.e. dimensions) of the objects are obtained by area measurement using the surface filling algorithm. The measured dimensions are in the form of coordinates to be input to the CNC part program.<<ETX>>",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=320548,no,undetermined,0
Highly-constrained neural networks with application to visual inspection of machined parts,"The authors investigate techniques for embedding domain specific spatial invariances into highly constrained neural networks. This information is used to reduce drastically the number of weights which have to be determined during the learning phase, thus allowing application of artificial neural networks to problems characterized by a relatively small number of available examples. As an application of the proposed technique, the problem of optical inspection of machined parts is studied. More specifically, the performance of a network created according to this strategy which accepts images of the parts under inspection at its input and issues at its output a flag which states whether the part is defective, is characterized. The results obtained so far show that such a classifier provides a potentially relevant approach for the quality control of metallic objects since it offers at the same time accuracy and short software development time.<<ETX>>",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=319197,no,undetermined,0
Total reliability management for telecommunications software,"Total reliability management for telecommunications software is essential in achieving high quality network products and a high level of network integrity. The special characteristics and reliability goals of telecommunications software are reviewed, followed by a description of the components of an effective strategy for comprehensive software reliability management. The programs of fault prevention, fault detection, fault removal, failure detection, failure diagnosis, and failure tolerance are shown to accomplish these goals. In addition, specific and advanced techniques for building these programs are discussed. The relationship of these techniques to the special characteristics of telecommunications software, and their coordination with each other is explained. The total reliability management for telecommunications software requires efforts from both suppliers and services providers",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=318132,no,undetermined,0
Comparison and optimization of digital read channels using actual data,"Various digital equalization and detection schemes have been proposed for disk drive read channels and simulated as a Lorentzian channel [Moon and Carley, 1990]. With the design of linear recording densities greater than two channel bits per half amplitude pulsewidth (PW50), concerns over nonlinear distortions, such as partial erasure, require channel simulation with techniques other than the linear Lorentzian model. The paper describes the considerations a disk drive designer faces when optimizing the performance of digital read channels with a channel simulator that uses actual data obtained from a disk file environment. This real data channel simulator accomplishes equalization and detection in software using sampled data from a spinstand allowing the flexibility of investigating the performance of different channel schemes without having to build expensive hardware. Sample offtrack data for selected channels at various user (information) densities are shown, along with ontrack Lorentzian simulation. Goodness numbers for each channel are presented which approximately minimize bit error rate (BER) in a quick way, providing algorithms for a disk drive to check channel quality during adaptive adjustments",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=342293,no,undetermined,0
High resolution measurements of lepton beam transverse distributions with the LEP wire scanners,"A large number of improvements were carried-out on the LEP wire-scanners in preparation for the 1992 running period. They include modifications of the monitors mechanics to decrease the vibrations and the heating of the wire by the beam generated electromagnetic fields. Improvements of the detector chain and a software reorganization at the various levels for better noise rejection, improved user interface and â€œoff-lineâ€?data analysis capabilities. It is now also possible to acquire the profiles of each of the sixteen circulating bunches, electrons and positrons, during the same sweep. As a consequence of these actions the quality of the collected data is much improved. The results are presented and discussed",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=309370,no,undetermined,0
CEBAF beam viewer imaging software,"This paper discusses the various software used in the analysis of beam viewer images at CEBAF. This software, developed at CEBAF, includes a three-dimensional viewscreen calibration code which takes into account such factors as multiple camera/viewscreen rotations and perspective imaging, and maintaining a calibration database for each unit. Additional software allows single-button beam spot detection, with determination of beam location, width, and quality, in less than three seconds. Software has also been implemented to assist in the determination of proper chopper RF control parameters from digitized chopper circles, providing excellent results",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=309361,no,undetermined,0
MPS VAX monitor and control software architecture,"The new Machine Protection System (MPS) now being tested at the SLAC Linear Collider (SLC) includes monitoring and controlling facilities integrated into the existing VAX control system. The actual machine protection is performed by VME micros which control the beam repetition rate on a pulse-by-pulse basis based on measurements from fault detectors. The VAX is used to control and configure the VME micros, configure custom CAMAC modules providing the fault detector inputs, monitor and report faults and system errors, update the SLC database, and interface with the user. The design goals of the VAX software include a database-driven system to allow configuration changes without code changes, use of a standard TCP/IP-based message service for communication, use of existing SLCNET micros for CAMAC configuration, security and verification features to prevent unauthorized access, error and alarm logging and display updates as quickly as possible, and use of touch panels and X-windows displays for the user interface",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=309164,no,undetermined,0
PLL subsystem for NSLS booster ring power supplies,"A high-performance digital phase-lock loop subsystem has been designed as part of the upgrade to the magnet power supplies in the booster ring at the National Synchrotron Light Source. The PLL subsystem uses a dedicated floating-point digital signal processor to implement the required filters and the startup, fault-handling, and running logic. The subsystem consists of two loops; the first loop tracks long-term changes in the line frequency, while the second tracks more rapid variations. To achieve the required performance, the order of the loop transfer functions was taken to be five, in contrast to the second-or third-order loops usually used. The use of such high-order loops required design techniques different from those normally used for PLL filter design. The hardware and software elements of the subsystem are described, and the design methodology used for the filters is presented. Performance is described and compared to theoretical predictions",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=309047,no,undetermined,0
An automatic finder of field defects in a large A.G. machine,"A series of program modules has been written in C, which can perform various tasks to analyze closed orbit measurements. They have been grouped into a software package which can be used by an operator to find field defects from orbit measurements. The basic algorithms used are well known and simple, based on fitting betatron oscillations. The effort has been put in the execution speed and ease of use. New algorithms have been introduced to detect wrong measurements and check the relevance of the kick calculation, which are a decisive step towards automatisation. It is presently possible to localize all relevant dipole field defects in a machine as large as LEP within less than one hour, including the check of the orbit readings",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=308981,no,undetermined,0
Faults and unbalance forces in the switched reluctance machine,"The author identifies and analyzes a number of severe fault conditions that can arise in the switched reluctance machine from the electrical and mechanical points of view. It is shown how the currents, torques, and forces may be estimated, and examples are included showing the possibility of large lateral forces on the rotor. The methods used for analysis include finite element analysis, magnetic circuit models, and experiments on a small machine specially modified for the measurement of forces and magnetization characteristics when the rotor is off-centered. The author also describes a computer program (PC-SRD Dynamic) which is used for simulating operation under fault, as well as normal, conditions. The author discusses various electrical configurations of windings and controller circuits, along with methods of fault detection and protective relaying",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=298909,no,undetermined,0
Systems reliability and availability prediction and comparison with field data for a mid-range computer,"The author describes reliability and availability prediction based on systems evaluation through Markov modelling and comparison of results with field observations of a sample of mid-range computer systems with mirrored disks. The system uses single level architecture with disk striping. Reliability evaluation revealed that, with mirroring, the disk system and the intermediate FRUs (field replaceable units) between the disk system and the processor will be duplicated. Hence, system failures would be limited to the processor which is not duplicated. The validity of the prediction was checked by field monitoring of the performance of four systems with mirrored disks. Good agreement between the predicted and observed reliability and availability estimates was observed reliability and availability estimates was observed for the system hardware. As a result of disk monitoring the disk system is totally fault tolerant",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=296879,no,undetermined,0
Reliability growth of fielded software,"A reliability growth model is presented that translates quality measures to time-dependent MTBFs (mean times between failures). Software reliability growth stems from the detection and removal of faults that are latent in the code after delivery. The fielded reliability performance is measured over time; the `noise' is smoothed; and a general form for the reliability growth profile is generated. The basic reliability growth profile is enhanced to include the impact of code updates, fault recurrence, and usage. It is concluded that the combination of theoretical analysis and actual field data presented should provide key guidance to make operational reliability decisions and determine the support analysis needs of fielded software",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=296829,no,undetermined,0
Automatic test case generation for Estelle,"An automatic test case generation method for Estelle is proposed. A formal model is introduced to describe the dynamic properties of Estelle specifications so as to verify the difference between the behavior of the specification and the behavior of its models. Based on the difference, an algorithm is presented to produce test cases that can detect such implementation faults. The algorithm can generate test cases not only for single module specifications but also for systems containing multiple modules that run concurrently. In addition, heuristics are suggested to improve the performance of the test case generation process",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=340915,no,undetermined,0
A computer hardware configuration expert system: Examination of its software reliability,"Describes the implementation of an expert system called SCAT for a computer hardware configuration task. It assists system engineers in enumerating all the parts necessary to build a computer system and prepares the documents for succeeding tasks. The emphasis is placed on the software reliability metrics and the development management methodology employed in SCAT. To evaluate the software reliability of SCAT, a traditional metric was employed the number of faults detected at each stage of the test process was counted and compared with the standard value. By analyzing the fault data derived from two versions of SCAT, it was found that the code review for production rules was not as effective as it was for procedural languages. It was also found that there was a possibility that the number of faults embedded in production rules in the coding process was smaller than that in procedural languages",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=366595,no,undetermined,0
SONET-based integrated digital loop carrier (IDLC) software reliability modeling,"An overview of the mathematical framework for the nonhomogeneous Poisson process (NHHP) model, and the design and implementation of the software reliability test tool are presented. The operational profile-driven testing in measuring software quality from the user's point of view is presented. It is shown how NHPP can be used to predict software failure behavior and estimate how much testing time is needed to achieve given reliability objectives. It is recommended that building an automated test tool will save time and improve accuracy in failure identification",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=587728,no,undetermined,0
Hardware design-for-test process improvements and resulting testability tool requirements,"Testability engineering has relevance in all program phases, at all levels of a weapon system, and for all levels of diagnostics and maintenance. Engineering tool development for testability, however, lags far behind tools for operational hardware and software development. This paper addresses the fundamental elements of the testability process in the hardware design phase and recommends attributes of tools needed to support a truly concurrent engineering environment",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=396322,no,undetermined,0
Assessing real-valued transform algorithms for fast convolution in electric power quality calculations,"It is argued that real- versus complex-valued trigonometric transform algorithms may deserve increased awareness in systems analysis in general, and application to nonsinusoidal waveform propagation in electric power systems in particular, due to their favorable convolution property when linear, time-invariant systems are studied. The performance of the three real-valued trigonometric transforms is assessed based on timing study results. The importance of instructions other than the multiplicative and additive complexities, namely those instructions involved in data transfer operations (e.g. loads and stores), is pointed out",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=522753,no,undetermined,0
Certification trails and software design for testability,"This paper investigates design techniques which may be applied to make program testing easier. We present methods for modifying a program to generate additional data which we refer to as a certification trail. This additional data is designed to allow the program output to be checked more quickly and effectively. Certification trails have heretofore been described primarily from a theoretical perspective. In this paper, we report on a comprehensive attempt to assess experimentally the performance and overall value of the certification trail method. The method has been applied to nine fundamental, well-known algorithms for the following problems: convex hull, sorting, huffman tree, shortest path, closest pair, line segment intersection, longest increasing subsequence, skyline, and voronoi diagram. Run-time performance data for each of these problems is given, and selected problems are described in more detail. Our results indicate that there are many cases in which certification trails allow for significantly faster overall program execution time than a two-version programming approach, and also give further evidence of the breadth of applicability of this method",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=470701,no,undetermined,0
A comparison of stuck-at fault coverage and I<sub>DDQ</sub> testing on defect levels,"This paper presents experimental data comparing the effect of functional stuck-at fault testing and I<sub>DDQ</sub> testing on defect levels. Results obtained for parts tested with varying stuck-at fault coverage are compared with results from I<sub>DDQ</sub> testing. Empirical data are analyzed against two theoretical fault models to demonstrate that correlation can be obtained. The results indicate that significant benefits can be gained from I<sub>DDQ</sub> testing, even with non-deterministic test locations and relatively few measurements. Data are also presented on the impact of I<sub>DDQ</sub> testing on burn-in monitoring and customer field failures of ASIC designs. Finally some practical issues associated with implementing I<sub>DDQ</sub> tests are discussed",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=470684,no,undetermined,0
Towards a test standard for board and system level mixed-signal interconnects,"This paper describes basic requirements for a standard bus for testing analog interconnects. The viewpoint is that of prospective users. An architecture for such a standard bus is proposed. The basic conception is of a mixed-signal version of boundary-scan and is compatible with, indeed built upon, ANSI/IEEE Std 1149.1. The goals in mind are the detection of faults and the measurement of analog interconnect parameters. Among the desired benefits are test and test data commonality throughout an assembly hierarchy, from manufacturing to field service",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=470683,no,undetermined,0
VLSI neural network architectures,"VLSI architectures for neural networks are presented. Neural networks have wide-ranging applications in classification, control, and optimization. With the need for real-time performance, VLSI neural networks have gained significant attention. Digital, analog, and mixed-mode designs are used for this application. Modular and reconfigurable designs are necessary so that various neural network models can be easily configured",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=410845,no,undetermined,0
A fault-tolerant dynamic scheduler for distributed hard-real-time systems,"A dynamic run-time scheduler is proposed that enhances the effects of a pre-run-time scheduling algorithm for real-time applications. The tasks are of a periodic nature and each task has two schedulable versions: primary and alternate. The primary produces an accurate result while the alternate produces an approximate result but takes less time and should be scheduled if the primary fails to meet the deadline. The objective of the dynamic scheduler is to maximize the number of primaries that are scheduled. The scheduling algorithm and performance results for different failure rates are also given. The performance results show that, for lower failure probability of the primaries scheduled during pre-run-time, the algorithm succeeds in scheduling a higher number of primaries during run-time",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=407172,no,undetermined,0
Software architecture review for telecommunications software improvement,"Software architecture reviews (SARs) have been established to help assure that a Bellcore client company (BCC) receives network elements and network switching systems of sufficiently high quality to meet its needs. The motivation for the SAR is discussed along with its methodology, checklists, application and expected benefits. The SAR is designed to assess three aspects of software reliability and quality. The impact of implementing new requirements in existing software architectures is assessed. The robustness of software to faults that occur in the field is described, and the fault prevention methods used before product deployment to reduce the number of software faults are characterized",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=397349,no,undetermined,0
Design and implementation of the multilevel test philosophy,"This paper describes the design and implementation of a Test Program Set (TPS) based on a multilevel test philosophy. The multilevel test philosophy not only incorporates the concepts of vertical testability and directed diagnostics, but also explores the test strategy and test encapsulation concepts being defined by the IEEE A Broad Based Environment for Test (ABBET) subcommittee",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=396370,no,undetermined,0
A Bayesian approach to confidence in system testing,The authors present a purely Bayesian technique for computing the probability that the conclusion reached during a fault isolation is correct given only the results of the (potentially unreliable) tests that have been run and information about the reliability of those tests. This technique is then applied to both off-line and real-time system analysis as a means of determining the potential correctness of a fault isolation conclusion as well as to guarantee (if possible) an arbitrarily selected probability that a conclusion is correct,1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=396317,no,undetermined,0
An expert system for reviewing commodity substitutions in the consumer price index,"An expert system is being developed to help assure the quality of data in the consumer price index (CPI). The system replicates the reasoning of commodity analysis, determining if a substitute product is comparable to a product priced in the previous month. If comparable, its price can be considered in producing the CPI. The system reasons with expertise directly entered by commodity analysts. To help the commodity analysts serve as their own knowledge engineers, they are provided with a skeletal system to which they add their knowledge of particular product categories. The system was tested on six months of CPI data. Results indicate that commodity analysts can accurately encode their own expertise and use the technology to detect their own oversights. A primary function of the project is to document and refine analyst expertise that is otherwise implicit. This makes the knowledge permanent and represents it in a way that allows it to be applied consistently",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=366596,no,undetermined,0
STAR: a fault-tolerant system for distributed applications,The paper presents a fault-tolerant manager for distributed applications. This manager provides an efficient recovery of hosts' failures on networks of workstations. An independent checkpointing is used to automatically recover application processes affected by host failures. Domino-effects are avoided by means of message logging and file versions management. STAR provides an efficient software failure detection by structuring hosts in a logical ring. Performance measurements in a real environment show the interest and the limits of our system,1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=395471,no,undetermined,0
Real-time distributed program reliability analysis,"Distributed program reliability has been proposed as a reliability index for distributed computing systems to analyze the probability of the successful execution of a program, task, or mission in the system. However, current reliability models proposed for distributed program reliability evaluation do not capture the effects of real-time constraints. We propose an approach to the reliability analysis of distributed programs that addresses real-time constraints. Our approach is based on a model for evaluating transmission time, which allow us to find the time needed to complete execution of the program, task, or mission under evaluation. With information on time-constraints, the corresponding Markov state space can then be defined for reliability computation. To speed up the evaluation process and reduce the size of the Markov state space, several dynamic reliability-preserving reductions are developed. A simple distributed real-time system is used as an example to illustrate the feasibility and uniqueness of the proposed approach",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=395454,no,undetermined,0
Reducing the cost of test pattern generation by information reusing,"A new source of computational saving for test pattern generation, i.e., information reusing, is presented. The proposed technique can make full use of the pattern generation information from the last pattern to derive a set of new tests by means of critical path transitions. By so doing, fault propagation procedure is no longer required in the next pattern generation process and the line justification procedure is simplified. Experiments using the ISCAS-85 benchmark circuits show that when the technique is used with a deterministic test pattern generation algorithm (DTPG), computational cost is greatly reduced without a substantial increase in test length",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=393360,no,undetermined,0
1993 IEEE Instrumentation and Measurement Technology Conference,The following topics are dealt with: microwaves; medical instrumentation; quantization; sensors and transducers; antennas and electromagnetics; metrology and calibration; analog-to-digital converter architecture; system identification; analog-to-digital converter testing; optics and fiber optics; time and frequency analysis; integrated measurement; robotics; digital signal processing; fault diagnosis; neural networks; computer-based measurements and software; material characterization; artificial intelligence (AI) and fuzzy logic; data acquisition; and electronic circuits. Abstracts of individual papers can be found under the relevant classification codes in this or other issues,1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=382693,no,undetermined,0
A transputers-based data acquisition and processing tool for measurements on power systems,"The authors describe a transputer-based data acquisition and processing system that is used for real time accurate measurements of various quantities which characterize a three-phase system. The measurement system is based on PC-bus data processing boards. The measurements of the quantities of interest are determined starting from the harmonic voltage and current amplitudes and phases obtained by the discrete Fourier transform results. A system architecture is also proposed to detect and record the variations of the power quality. Data relating to the hardware and software structure, and descriptions of the system components and their interactions are presented. The design and operation of the system are reported along with the advantages of using transputers and a PC base system. An example showing the applications of this system is also reported",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=382537,no,undetermined,0
A comparative study of predictive models for program changes during system testing and maintenance,"By modeling the relationship between software complexity attributes and software quality attributes, software engineers can take actions early in the development cycle to control the cost of the maintenance phase. The effectiveness of these model-based actions depends heavily on the predictive quality of the model. An enhanced modeling methodology that shows significant improvements in the predictive quality of regression models developed to predict software changes during maintenance is applied here. The methodology reduces software complexity data to domain metrics by applying principal components analysis. It then isolates clusters of similar program modules by applying cluster analysis to these derived domain metrics. Finally, the methodology develops individual regression models for each cluster. These within-cluster models have better predictive quality than a general model fitted to all of the observations",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=366954,no,undetermined,0
Improving the quality of three products through improved testing: A case study,"The keys to efficient testing are to detect problems as early as possible in the testing phase and to try and detect problems before they reach the customer. Three product offerings of a major software company are examined, and recommendations on how the testing and maintenance processes for these products can be improved are given. The rationale behind these recommendations is based on an analysis of the types of errors being reported in each product, when these errors are found, and in which modules they are found",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=366944,no,undetermined,0
Structure-based clustering of components for software reuse,"The characterization of the code reuse practices in existing production environments provides fundamental data and lessons for the establishment or improvement of effective reuse-oriented policies, and for the adoption of up-to-date technologies supporting them. The method and results of an experience of metric-aided clustering of software components, aimed at detecting and characterizing implicit reuse of code and reuse potential in a large-scale data processing environment, are presented. Similar function may be in fact replicated many times, customizing an existing source code component, but this phenomenon may be only partially apparent in the form of explicit reuse. A set of software metrics has been used to create clusters of existing components whose internal structures appear very similar. Functional similarity checks involving human experts were then performed. This was done in the context of a large reuse project, where quantitative software quality indicators are also combined with the feedback collected in pilot groups who know the applications from which the candidate components were extracted. The potential and limitations of metric support in this field are considered in the discussion of the results obtained",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=366941,no,undetermined,0
REACT: a synthesis and evaluation tool for fault-tolerant multiprocessor architectures,"A software testbed that performs automated life testing of a variety of multiprocessor architectures through simulated fault injection is discussed. It is being developed to meet the need for a generalized simulation tool which can evaluate system reliability and availability metrics while avoiding several of the limitations associated with combinatorial and Markov modeling. Incorporating detailed system, workload, and fault/error models, REACT can be more accurate and easier to use than many dependability prediction tools based on analytical approaches. The authors motivate the development of REACT, describe its features, and explain its use. Example applications for the software are provided, and its limitations are discussed",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=296819,no,undetermined,0
The application of activity based management to a software engineering environment,"This paper describes a method that supports the use of budgets based on activities and the integration of measurable continuous improvement in the formulation of software development plans. The application of this method fosters the clear definition and documentation of the software development process, outputs, costs and process controls. These definitions enable the evaluation of performance in real-time for quality and cost of outputs, timely deliveries and activity effectiveness instead of waiting for financial results months later. The proper application of this method will result in more accurate estimates processed in days instead of weeks, identification and elimination of non-value added activities, closer correlation between estimates and actual performance and optimum utilization of resources",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=290971,no,undetermined,0
Component tolerance and circuit performance: a case study,"The author presents a continuation of work presented at APEC 1992 (see p.28-35), in which a simple circuit was simulated to predict yield in the manufacturing process. The simulation results had some anomalies. This was because the component values were assumed to be uniformly distributed between the tolerance limits. The author updates that assumption, and uses normally distributed component values. Various simulations are run with varying component tolerance and quality levels. From the mean and variance of the simulated results, plots are made that show the specification limits that would have to be set to meet a given yield requirement",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=290679,no,undetermined,0
The past Quarter-Century and the Next Decade of Videotape Recording,"Since the first commercially successful videotape recorder (VTR) was introduced in 1956, many VTRs have been developed and put on the market. If the picture quality is almost the same, the tape consumption per hour (TCH) of a commercially successful VTR in its sales year will decrease according to a trend line of one-tenth per ten years. In other words, the recording density will be increased by a rule relating to the trend line. If the picture quality is improved, the trend line will move toward a higher TCH position on a parallel line. The future specifications of the VTR as a function of TCH can be predicted from such a trend chart. The miniaturization of the recording pitch and wavelength is a motivating force of VTR development. A high-utility, high-performance, but low-cost VTR was developed and continues to improve. The harmonization of hardware and software is the next important challenge. In this article, a short technical history of the VTR, the future of the VTR predicted from the trend chart, and the technical motivating force of VTR development are discussed.",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7236023,no,undetermined,0
A novel non-unit protection scheme based on fault generated high frequency noise on transmission lines,"This paper is concerned with outlining a novel technique for the detection of power transmission line fault generated noise by employing an arrangement involving the use of conventionally connected, power line communication signal line traps and CVTs. A specially designed stack tuner (tuned to a certain frequency bandwidth) is connected to the coupling capacitor of the CVT in order to capture the high frequency signals. Digital signal processing is then applied to the captured information to determine whether the fault is inside or outside the protected zone. The paper concludes by presenting results based on extensive simulation studies carried out on a typical 400 kV vertical construction line, using the Electromagnetic Transient Program (EMTP) software. In particular, the performance of the nonunit protection scheme is examined for faults on systems with heavy loading and for high resistance arcing earth faults",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=224563,no,undetermined,0
"Performance evaluation of channel-sensitive, stabilized multiple access schemes for land-mobile satellite services using a hybrid simulation tool","A hybrid simulation tool for real-time performance measurements of complete networks is presented. The simulation tool combines the advantages of software and hardware modules. Recorded channel data are included. They allow the reproduction of realistic situations in the laboratory. Synchronization problems and noise are considered. As an example, stabilized slotted ALOHA access protocols were implemented and investigated in land-mobile satellite environments. A slotted ALOHA-based modified channel access strategy for highly unreliable channels is proposed requiring all mobile stations to estimate the link quality before starting a transmission. The different strategies are analyzed by simulation as well as the analytical methods. Transmission parameters are determined and optimized using a combination of analysis and simulation. The results show the power of the simulation tool and the better performance of stabilized slotted ALOHA with link-quality estimation in city channels",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=219557,no,undetermined,0
Simulation analysis of a communication link with statistically multiplexed bursty noise sources,"Variable bit rate (VBR) coding techniques have received great research interest as very promising tools for transmitting bursty multimedia traffic with low bandwidth requirements over a communication link. Statistically multiplexing the multimedia bursty traffic is a very efficient method of maximizing the utilization of the link capacity. The application of computer simulation techniques in analyzing a rate-based access control scheme for multimedia traffic such as voice traffic is discussed. The control scheme regulates the packetized bursty traffic at the user network interface of the link. Using a suitable congestion measure, namely, the multiplexer buffer length, the scheme dynamically controls the arrival rate by switching the coder to a different compression ratio (i.e., changing the coding rate). VBR coding methods can be adaptively adjusted to transmit at a lower rate with very little degradation in the voice quality. Reported results prove that the scheme greatly improves the link performance, in terms of reducing the probability of call blocking and enhancing the statistical multiplexing gain",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=219556,no,undetermined,0
Software-reliability growth with a Weibull test-effort: a model and application,"Software reliability measurement during the testing phase is essential for examining the degree of quality or reliability of a developed software system. A software-reliability growth model incorporating the amount of test effort expended during the software testing phase is developed. The time-dependent behavior of test-effort expenditures is described by a Weibull curve. Assuming that the error detection rate to the amount of test effort spent during the testing phase is proportional to the current error content, the model is formulated by a nonhomogeneous Poission process. Using the model, the method of data analysis for software reliability measurement is developed. This model is applied to the prediction of additional test-effort expenditures to achieve the objective number of errors detected by software testing, and the determination of the optimum time to stop software testing for release",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=210278,no,undetermined,0
Getting started on software metrics,"The principles on which the Software Management Metrics system is based are discussed. The system collects metrics at regular intervals and represents current estimates of the work to be done, the work accomplished, the resources used, and the status of products being generated. The lessons learned in the eight years since Software Management Metrics were first imposed on the US Air Force's software contractors are reviewed.<<ETX>>",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=207238,no,undetermined,0
Recent advances in software estimation techniques,"This paper describes some recent developments in the field of software estimation. Factors to be considered in software estimation are discussed, a framework for reconciling those factors is presented, software estimation techniques are categorized, and recent advances are described. The paper concludes with a forecast of likely future trends in software estimation.",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=753515,no,undetermined,0
The software engineering laboratory - an operational software experience factory,"For 15 years, the Software Engineering Laboratory (SEL) has been carrying out studies and experiments for the purpose of understand- ing, assessing, and improving software and software processes within a production software development environment at the National Aeronautics and Space Administration/Goddard Space Flight Center (NASA/GSFC). The SEL comprises three major organizations: NASA/GSFC, Flight Dynamics Division University of Maryland, Department of Computer Science Computer Sciences Corporation, Flight Dynamics Technology Group - These organizations have jointly carried out several hundred software studies, producing hundreds of reports, papers, and documents, all of which de scribe some aspect of the software engineering technology that has been analyzed in the flight dynamics environment at NASA. The studies range from small, controlled experiments (such as analyzing the effectiveness of code readingversus that of functional testing) tolarge, multiple- project studies (such as assessing the impacts of Ada on a production environment). The organization's driving goal is to improve the software process continually, so that sustained improvement may be observed in the resulting products. This paper discusses the SEL as a functioning example of an operational software experience factory and summarizes the characteristics of and major lessons learned from 15 years of SEL operations.",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=753514,no,undetermined,0
Software Reuse Economics: Cost-benefit Analysis On A Large-scale Ada Project,"The software industry is painfully realizing that a software reuse effort, if not carefully planned and properly carried out, oftentimes becomes an inhibitor rather than a catalyst to software productivity and quality. Despite numerous ar ticles in the areas of domain analysis, component classification, automated component storage/retrieval tools, reuse metrics, etc., only a handful have managed to address the economics of software reuse. In order to be successful, not only must a reuse program be technically sound, it must also be economically worthwhile. After all, reducing costs and increasing quality were the two main factors that drove software reuse into the software mainstream. This paper presents a number of reuse economics models used to perform an economic assessment of a reuse effort on a large-scale Ada project: the United States Federal Aviation Administration's Advanced Automation System (FAA/AAS). Reuse economics models, cost/benefit tracking methods, and reuse catalystslinhibitors are addressed.",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=753511,no,undetermined,0
OPTIC - A High Performance Image Processor,"The OPTIC is a high performance VLSI device designed for use in general-purpose real-time vision systems. The chip incorporates a fast proprietary (see [1]) shape- and edge-recognition algorithm, which does not require any preprocessing of the grey-level image. This makes the device especially suited to difficult machine vision problems where the image quality varies over time. The full-custom 1.5Î¼m CMOS circuit contains 80,000 transistors and operates at an image sampling rate of 25MHz.",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5468386,no,undetermined,0
System for defect detection on material,"A hardware and software solution allowing, in a given experimental environment, defect detection during textile manufacture is defined. The method is based on detecting breaks in the periodicity of the fabric's pattern. If the periodic characteristic of the material is deleted by filtering, the defect is detected more easily",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=289826,no,undetermined,0
Estimating the fault rate function,"Paging activity can be a major factor in determining whether a software workload will run on a given computer system. A program's paging behavior is difficult to predict because it depends not only on the workload processed by the program, but also on the level of storage contention of the processor. A program's fault rate function relates storage allocation to the page fault rate experienced while processing a given workload. Thus, with the workload defined, the fault rate function can be used to see how the program's storage allocation is affected by varying levels of storage contention, represented by varying fault rates. This paper presents a technique to represent program workloads and estimate the fault rate function, and describes how these results can be used in analyzing program performance.",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5387417,no,undetermined,0
Adaptable technique for collecting MWD density logging research data using Windows software,The goal of the Density Project at Develco is to further develop gamma-gamma technology and support a commercial instrument that measures bulk density of Earth formations while drilling. A large database is automatically generated by the data collection system developed in the R&D department. This data collection system also performs several quality assurance functions for the data as they are collected. Commercial software applications are used in lieu of software generated in-house. Real-time analysis can be accomplished with any software application that supports the Dynamic Data Exchange (DDE) features of Windows applications. Mathcad was used initially to develop an algorithm for locating photo peaks and subsequently fitting to the peak a Gaussian function,1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=301360,no,undetermined,0
Measuring the field quality of wide-distribution commercial software,"The problem of quantifying the field quality of wide-distribution commercial software is addressed. The authors argue that for this type of software the proper quality metric is an estimate of the number of defects remaining in the code. They observe that the apparent number of defects remaining in this type of software is a function of the number of users as well as the number of actual defects in the code. New releases of commercial software normally consist of some code from prior releases and some new or modified code. The authors continue to discover new defects in the code from prior releases even after a new release is in the field. In fact, the new release code appears to stimulate discovery of defects latent in the `old' code and cause a `next release effect.' Field defect data from several releases of a widely distributed commercial software product are shown to a demonstrate this effect",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=285906,no,undetermined,0
Providing an empirical basis for optimizing the verification and testing phases of software development,"Applying equal testing and verification effort to all parts of a software system is not very efficient, especially when resources are limited and scheduling is tight. Therefore, one needs to be able to differentiate low/high fault density components so that testing/verification effort can be concentrated where needed. Such a strategy is ejected to detect more faults and thus improve the resulting reliability of the overall system. The authors present an alternative approach for constructing such models that is intended to fulfil specific software engineering needs, (i.e. dealing with partial/incomplete information and creating models that are easy to interpret). The approach to classification is to: measure the software system to be considered; and to build multivariate stochastic models for prediction. The authors present experimental results obtained by classifying FORTRAN components into two fault density classes: low and high. They also evaluate the accuracy of the model and the insights it provides into the software process",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=285903,no,undetermined,0
Standards and practices for reliable safety-related software systems,"Safety and reliability are of increasing concern as mechanical systems are replaced or upgraded with computer-based software technology as we move into the 21st century. The article is the result of an evaluation of domestic and international, industry and government accepted practices and standards relating to software safety and reliability. Standards were evaluated for similarities and variance. There are three areas of interest when discussing the development of safety-related software systems. These are the quality enhancing development processes, safety analysis techniques, and measurement techniques for assessing reliability or safety",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=285902,no,undetermined,0
Software quality metrics in space systems,"The French Space Agency uses a special computer environment to collect quality and reliability data on the software it develops, and to analyze and predict the performance potential of that software throughout its life cycle. The authors present the environment, along with the way data is organized and collected during software development and maintenance. They show the impact their work in this field has on complexity metrics and software reliability measurements. They present practical applications and results concerning quite a large number of space projects in a real context",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=285897,no,undetermined,0
Software reliability measurements in N-Version software execution environment,"The author quantitatively examines the effectiveness of the N-Version programming approach. He looks into the details of an academia/industry joint project employing six programming languages, and studies the properties of the resulting program versions. He explains how exhaustive testing was applied to the project, and measure the error probability in different N-Version Software execution configurations. He also applies mutation testing techniques to measure the safety coverage factor for the N-Version software system. Results from this investigation reveals the potential of N-Version software in improving software reliability. Another observation of this research is that the per fault error rate does not remain constant in this computation-intensive project. The error rates associated with each program fault differ from each other dramatically, and they tend to decrease as testing progresses",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=285866,no,undetermined,0
The nature of fault exposure ratio,"The fault exposure ratio <e1>K</e1> is an important factor that controls the per-fault hazard rate, and hence the effectiveness of software testing. The paper examines the variations of <e1>K</e1> with fault density which declines with testing time. Because faults get harder to find, <e1>K</e1> should decline if testing is strictly random. However, it is shown that at lower fault densities <e1>K</e1> tends to increase, suggesting that real testing is more efficient than random testing. Data sets from several different projects are analyzed. Models for the two factors controlling <e1>K</e1> are suggested, which jointly lead to the logarithmic model",1992,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=285861,no,undetermined,0
Performability enhancement of fault-tolerant software,"Model-based performability evaluation is used to assess and improve the effectiveness of fault-tolerant software. The evaluation employs a measure that combines quantifications of performance and dependability in a synergistic manner, thus capturing the interaction between these two important attributes. The specific systems evaluated are a basic realization of <e1>N</e1>-version programming (NVP) (<e1>N </e1>=3) along with variants thereof. For each system, its corresponding stochastic process model is constructed in two layers, with performance and dependability submodels residing in the lower layer. The evaluation results reveal the extent to which performance, dependability, and performability of a variant are improved relative to the basic NVP system. More generally, the investigation demonstrates that such evaluations are indeed feasible and useful with regard to enhancing software performability",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=229492,no,undetermined,0
A general framework for developing adaptive fault-tolerant routing algorithms,"It is shown that Cartesian product (CP) graph-based network methods provide a useful framework for the design of reliable parallel computer systems. Given component networks with prespecified connectivity, more complex networks with known connectivity and terminal reliability can be developed. CP networks provide systematic techniques for developing reliable fault-tolerant routing schemes, even for very complex topological structures. The authors establish the theoretical foundations that relate the connectivity of a CP network, the connectivity of the component networks, and the number of faulty components: present an adaptive generic algorithm that can perform successful point-to-point routing in the presence of faults: synthesize, using the theoretical results, this adaptive fault-tolerant algorithm from algorithms written for the component networks: prove the correctness of the algorithm: and show that the algorithm ensures following an optimal path, in the presence of many faults, with high probability",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=229494,no,undetermined,0
"Distributed, collaborative software inspection","The Collaborative Software Inspection (CSI) tool, which provides a distributed, structured environment for performing inspections on all software-development products, including specifications, designs, code, and test cases, is described. The inspection environment lets geographically distributed inspection participants meet with people in other cities through workstations at their desks. The current version of all material is accessible online. Inspection products are created online, so secondary data entry to permanent records is not necessary. The inspection information is also available for review and metrics collection. To assess the effectiveness of inspection in the distributive collaborative environment and compare it with face-to-face meetings, a case-study approach with replication logic is presented.<<ETX>>",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=232404,no,undetermined,0
Effects of manufacturing errors on the magnetic performance of the SCC dipole magnets,"The authors summarize the work performed to evaluate the effects of manufacturing errors on the field quality of the Superconducting Super Collider dipole magnets. The multiple sensitivities to the conductor displacements were computed. There are two types of magnetic field multipole specifications: systematic (average over the entire ring) and RMS (standard deviation of the multipole distribution). The RMS multipoles induced by random variations in the magnet cross section were predicted using VSA software and the current design tolerances. In addition, the effects of variations in the beam tube, yoke and cryostat dimensions were also analyzed.<<ETX>>",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=233784,no,undetermined,0
Experimental evidence of sensitivity analysis predicting minimum failure probabilities,"The authors discuss a theoretical statistical technique complementary to black-box testing, called sensitivity analysis. Black-box testing establishes an upper limit on the likely probability of software failure. Software sensitivity analysis establishes a lower limit on the probability of failures that are likely to occur. Together, these estimates can be used to establish confidence that software does not contain faults. Experimental results show that sensitivity analysis predicts a realistic, lower limit on the probability of failure. This limit is lower than can be generally predicted using testing results only. Sensitivity analysis was applied to three versions of NASA's specification for the sensor management of a redundant strapped down inertial measurement unit (RSDIMU). An RSDIMU is a component of a modern inertial navigation system to provide acceleration data that is integrated to determine velocity and position. The programs used in the experiment were originally produced for use in an N-version system for the RSDIMU",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=288851,no,undetermined,0
Software quality: a market perspective,"The author investigates software quality in a market setting. He points out that imperfections in reputation place downward pressure on a product's price and lower the average quality of the delivered software. Testing, verification, and validation improve quality only when reputation is important to end users, when demand satiation is low, and when the marginal cost of improving quality is low in comparison with the improvement in quality. As firms improve total quality control, testing, verification, and validation become less relevant, until they may actually become counterproductive. Where complex products are produced, high quality may lower marginal costs of software, and this is shown to provide an accelerator effect that enhances other competitive strategies. Warranties can ameliorate the underprovision of quality by improving software reputation. But even with warranties, the authors shows that software developers will tend to underprovide quality",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=284261,no,undetermined,0
Achieving software quality through Cleanroom software engineering,"Describes a success story in the use of modern technologies for software quality improvement. The Cleanroom software engineering process for zero-defect software has been successfully applied on development projects in a variety of environments with remarkable quality results. Cleanroom is based on formal, theory-based methods for software specification, design, correctness verification, and statistical quality certification. The authors survey a number of Cleanroom projects and demonstrate achievement of the following objectives: superior quality through Cleanroom software development; successful, cost-effective Cleanroom technology transfer to software development teams; and sharp reduction in effort to maintain and evolve Cleanroom software products",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=284260,no,undetermined,0
A low-power generator-based FIFO using ring pointers and current-mode sensing,"The authors present a submicron full CMOS FIFO (first in, first out) memory generator for ASIC (application-specific integrated circuit) and full-custom applications that combines asynchronous access, shift register pointers, a direct word line comparison technique (DWLC) for flag generation, and current sensing in a regular structured architecture that uses circuit compaction and automated parameteric characterization. It features overread and write protection and retransmit capability at clock rates of 70-100 MHz at 5 V and 40-60 MHz at 3.3 V. The regular structure yields predictable performance and simplifies timing model synthesis for simulators. Special hardware features support full fault coverage with BIST (built-in self-test) algorithms available in hardware or software.<<ETX>>",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=280031,no,undetermined,0
Nonhomogeneous Poisson process software-debugging models with linear dependence,"Past research in software reliability concentrated on reliability growth of one-version software. This work proposes models for describing the dependency of N-version software. The models are illustrated via a logarithmic Poisson execution-time model by postulating assumptions of dependency among nonhomogeneous Poisson processes (NHPPs) of debugging behavior. Two-version, three-version, and general N-version models are proposed. The redundancy techniques discussed serve as a basis for fault-tolerant software design. The system reliability, related performance measures, and parameter estimation of model parameters when N=2 are presented. Based on the assumption of linear dependency among the NHPPs, two types of models are developed. The analytical models are useful primarily in estimating and monitoring software reliability of fault-tolerant software. Without considering dependency of failures, the estimation of reliability would not be conservative",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=273590,no,undetermined,0
Towards the evaluation of software engineering standards,"The paper presents the results of work undertaken during the first half of a project whose aim is to develop a measurement based approach for assessing software engineering standards. Using a simple model of a standard as a set of requirements the authors found that it is impossible to make an objective assessment of conformance to the majority of requirements. They make recommendations for improving standards. These include the restriction of individual requirements to a specific product, process or resource, with conformance able to be assessed objectively; the restriction of standards to cohesive collections of requirements; a clear statement of benefit. The approach to evaluation of a specific standard has two stages: firstly the rationalisation and decomposition into mini-standards and secondly the (ideal) measurement-based assessment. The initial stages of a major industrial case study being used to assess the approach are described. They show, by the example of the evaluation of a specific mini-standard, that data the company already collects, and which are similar to the type of data collected by other companies, can be used to form the basis of the assessment",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=263960,no,undetermined,0
A framework for evaluation and prediction of metrics program success,"The need for metrics programs as a part of the improvement process for software development and enhancement has been recognized in the literature. Suggestions have been made for the successful implementation of such programs but little empirical research has been conducted to verify these suggestions. This paper looks at the results of three metrics programs in three different software organizations with which the authors have worked, compares the metrics program development process in those organizations with an evaluation framework developed from the literature, and suggests extensions which may be needed to this framework",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=263803,no,undetermined,0
Constructing and testing software maintainability assessment models,"Software metrics are used to quantitatively characterize the essential features of software. The paper investigates the use of metrics in assessing software maintainability by presenting and comparing seven software maintainability assessment models. Eight software systems were used for initial construction and calibrating the automated assessment models, and an additional six software systems were used for testing the results. A comparison was made between expert software engineers' subjective assessment of the 14 individual software systems and the maintainability indices calculated by the seven models based on complexity metrics automatically derived from those systems. Initial tests show very high correlations between the automated assessment techniques and the subjective expert evaluations",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=263800,no,undetermined,0
Dynamic system complexity,"Both operational environment and source code complexity influence the reliability of software systems. The complex modules of software systems often contain a disproportionate number of faults. However, if in a given environment, the complex modules are rarely exercised, then few of these faults are likely to become expressed as failures. Different environments will exercise a system's modules differently. A system's dynamic complexity is high in an environment that exercises the system's complex modules with high probability. It is likely that one or more potential scenarios induce inordinately large dynamic complexity. Identifying these scenarios is the first step in assuring that they receive the testing time that they warrant. The paper presents two metrics which evaluate the dynamic complexity of systems and of subsystems in the intended operational environment. The authors' analyze two software systems using both dynamic metrics",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=263793,no,undetermined,0
Identifying and measuring quality in a software requirements specification,Numerous treatises exist that define appropriate qualities that should be exhibited by a well written software requirements specification (SRS). In most cases these are vaguely defined. The paper explores thoroughly the concept of quality in an SRS and defines attributes that contribute to that quality. Techniques for measuring these attributes are suggested,1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=263792,no,undetermined,0
Safety & hazard analysis for software controlled medical devices,"Advanced medical devices are progressively more controlled by software. In many cases software plays a central role in the safety critical devices. Historically, the question of safety has received more attention with respect to hardware than software. However, software can also make a significant contribution to the safeness of a device. In this paper device safety is examined in a system context. Since the study of software safety is relatively new, at first the nature of software failures is examined. This is followed by touching on the vocabulary of safety analysis. Some methods of analysis, detection, control and containment of hazards are described and some good practices for developing safe medical devices are suggested",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=262997,no,undetermined,0
Parallel processing of fault trees on a locally distributed multiple-processor network,"Large fault trees are evaluated in a distributed fashion by pooling the computing power of several networked LISP processors. Direct evaluation of fault trees of complex systems is computationally intensive and can take a long time when performed on a single processor. An iterative top-down algorithm successively recognizes and reduces known patterns, and decomposes the problem into subtasks at each level of iteration. These subtasks are distributed to multiple machines on the network. Thus, subtree evaluations which are tackled serially on a single processor, are performed in parallel by the distributed network. The reductions in elapsed computer time afforded by this approach are examined. Questions of optimal resource allocation and of scheme limitations are considered",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=257827,no,undetermined,0
FINE: A fault injection and monitoring environment for tracing the UNIX system behavior under faults,"The authors present a fault injection and monitoring environment (FINE) as a tool to study fault propagation in the UNIX kernel. FINE injects hardware-induced software errors and software faults into the UNIX kernel and traces the execution flow and key variables of the kernel. FINE consists of a fault injector, a software monitor, a workload generator, a controller, and several analysis utilities. Experiments on SunOS 4.1.2 are conducted by applying FINE to investigate fault propagation and to evaluate the impact of various types of faults. Fault propagation models are built for both hardware and software faults. Transient Markov reward analysis is performed to evaluate the loss of performance due to an injected fault. Experimental results show that memory and software faults usually have a very long latency, while bus and CPU faults tend to crash the system immediately. About half of the detected errors are data faults, which are detected when the system is tries to access an unauthorized memory location. Only about 8% of faults propagate to other UNIX subsystems. Markov reward analysis shows that the performance loss incurred by bus faults and CPU faults is much higher than that incurred by software and memory faults. Among software faults, the impact of pointer faults is higher than that of nonpointer faults",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=256857,no,undetermined,0
A case study of software process improvement during development,We present a case study of the use of a software process improvement method which is based on the analysis of defect data. The first step of the method is the classification of software defects using attributes which relate defects to specific process activities. Such classification captures the semantics of the defects in a fashion which is useful for process correction. The second step utilizes a machine-assisted approach to data exploration which allows a project team to discover such knowledge from defect data as is useful for process correction. We show that such analysis of defect data can readily lead a project team to improve their process during development,1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=249661,no,undetermined,0
An optimal graph-construction approach to placing program signatures for signature monitoring,"A new approach produces optimal signature placement for concurrent detection of processor and program-memory errors using signature monitoring. A program control-how graph, labeled with the overhead for placing a signature on each node and arc, is transformed into an undirected graph. For an order-independent signature function such as an XOR or arithmetic checksum, the undirected graph and a spanning tree algorithm are shown to produce an optimal placement in O(n log Î²(n, m)) time. Cyclic codes, which are order dependent, are shown to allow significantly lower overhead than order-independent functions. Prior work suggests overhead is unrelated to signature-function type. An O(n) graph-construction algorithm produces an optimal signature placement for cyclic codes. Experimental data show that using a cyclic code and horizontal reference signatures, the new approach can reduce average performance overhead to a fraction of a percent for the SPEC89 benchmark suite, more than 9 times lower than the performance overhead of an existing O(n<sup>2</sup>) placement algorithm",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=247847,no,undetermined,0
Provable improvements on branch testing,"This paper compares the fault-detecting ability of several software test data adequacy criteria. It has previously been shown that if C<sub>1</sub> properly covers C<sub>2</sub>, then C<sub>1</sub> is guaranteed to be better at detecting faults than C<sub>2</sub>, in the following sense: a test suite selected by independent random selection of one test case from each subdomain induced by C<sub>1</sub> is at least as likely to detect a fault as a test suite similarly selected using C<sub>2</sub>. In contrast, if C<sub>1</sub> subsumes but does not properly cover C<sub>2</sub>, this is not necessarily the case. These results are used to compare a number of criteria, including several that have been proposed as stronger alternatives to branch testing. We compare the relative fault-detecting ability of data flow testing, mutation testing, and the condition-coverage techniques, to branch testing, showing that most of the criteria examined are guaranteed to be better than branch testing according to two probabilistic measures. We also show that there are criteria that can sometimes be poorer at detecting faults than substantially less expensive criteria",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=245738,no,undetermined,0
An empirical study of testing and integration strategies using artificial software systems,"There has been much discussion about the merits of various testing and integration strategies. Top-down, bottom-up, big-bang, and sandwich integration strategies are advocated by various authors. Also, some authors insist that modules be unit tested, while others believe that unit testing diverts resources from more effective verification processes. This article addresses the ability of the aforementioned integration strategies to detect defects, and produce reliable systems. It also explores the efficacy of spot unit testing, and compares phased and incremental versions of top-down and bottom-up integration strategies. Relatively large artificial software systems were constructed using a code generator with ten basic module templates. These systems were seeded with known defects and tested using the above testing and integration strategies. A number of experiments were then conducted using a simulator whose validity was established by comparing results against these artificial systems. The defect detection ability and resulting system reliability were measured for each strategy. Results indicated that top-down integration strategies are generally most effective in terms of defect correction. Top-down and big-bang strategies produced the most reliable systems. Results favored neither those strategies that incorporate spot unit testing nor those that do not; also, results favored neither phased nor incremental strategies",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=245736,no,undetermined,0
The business case for software reuse,"To remain competitive, software development organizations must reduce cycle time and cost, while at the same time adding function and improving quality. One potential solution lies in software reuse. Because software reuse is not free, we must weigh the potential benefits against the expenditures of time and resources required to identify and integrate reusable software into products. We first introduce software reuse concepts and examine the cost-benefit trade-offs of software reuse investments. We then provide a set of metrics used by IBM to accurately reflect the effort saved by reuse. We define reuse metrics that distinguish the savings and benefits from those already gained through accepted software engineering techniques. When used with the return-on-investment (ROI) model described in this paper, these metrics can effectively establish a sound business justification for reuse and can help assess the success of organizational reuse programs.",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5387336,no,undetermined,0
IDDQ test results on a digital CMOS ASIC,"The test program of a digital CMOS ASIC (application-specific integrated circuit) was instrumented for IDDQ current measurement. The design test included a low-fault-coverage functional set of vectors as well as high-fault-coverage scan set of vectors. Analysis of current distributions of parts failing and passing vector sets provides insight into the potential of static current testing. Many issues remain, but the data suggest that real quality improvements can be obtained from implementing static current testing on a small subset of device vectors",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=590781,no,undetermined,0
On a unified framework for the evaluation of distributed quorum attainment protocols,"Quorum attainment protocols are an important part of many mutual exclusion algorithms. Assessing the performance of such protocols in terms of number of messages, as is usually done, may be less significant than being able to compute the delay in attaining the quorum. Some protocols achieve higher reliability at the expense of increased message cost or delay. A unified analytical model which takes into account the network delay and its effect on the time needed to obtain a quorum is presented. A combined performability metric, which takes into account both availability and delay, is defined, and expressions to calculate its value are derived for two different reliable quorum attainment protocols: D. Agrawal and A. El Abbadi's (1991) and Majority Consensus algorithms (R.H. Thomas, 1979). Expressions for the primary site approach are also given as upper bound on performability and lower bound on delay. A parallel version of the Agrawal and El Abbadi protocol is introduced and evaluated. This new algorithm is shown to exhibit lower delay at the expense of a negligible increase in the number of messages exchanged. Numerical results derived from the model are discussed",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=368122,no,undetermined,0
High-resolution estimation and on-line monitoring of natural frequency of aircraft structure specimen under random vibration,"The growth of structural damage reduces the stiffness which results in decrease in the natural frequency of a specimen (coupon) under random vibration. However, the identification of the natural frequency is not a trivial problem because the coupons are shaken in a random fashion in which many closely-spaced frequencies are involved. The signal may have other redundant frequencies involved which are close to the natural frequency The simplest way to identify the frequencies of multiple sinusoids is the Fourier transform spectral analysis. However, when the frequencies of interest are closely spaced, the Fourier transform spectrum analysis fails to resolve these frequencies. In this paper these facts are shown for a real vibration signal. High resolution frequency estimation approach, such as the forward-backward linear prediction (FBLP) method, is adopted here to identify all the prominent frequencies in a certain bandwidth. From the p<sup>th</sup> order FBLP model, we estimate p positive frequencies. Among these p frequencies, the one that will decrease as the structure degenerates, is the natural frequency and is monitored in an on-line mode. An estimation of natural frequency and its prediction is made regularly (every one second) based on all available estimated natural frequencies once the structure enters the fatigue zone. The system is useful for on-line health monitoring of any structure under random vibration. We have applied this system to monitor several aluminum coupons of aircraft structure. The tests were performed at the Wright Research Lab and the software monitoring system was developed at Tennessee State University",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=332866,no,undetermined,0
Empirical studies of predicate-based software testing,"We report the results of three empirical studies of fault detection and stability performance of the predicate-based BOR (Boolean Operator) testing strategy. BOR testing is used to develop test cases based on formal software specification, or based on the implementation code. We evaluated the BOR strategy with respect to some other strategies by using Boolean expressions and actual software. We applied it to software specification cause-effect graphs of a safety-related real-time control system, and to a set of N-version programs. We found that BOR testing is very effective at detecting faults in predicates, and that BOR-based approach has consistently better fault detection performance than branch testing, thorough (but informal) functional testing, simple state-based testing, and random testing. Our results indicate that BOR test selection strategy is practical and effective for detection of faulty predicates and is suitable for generation of safety-sensitive test-cases",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=341348,no,undetermined,0
An exploratory analysis of system test data,We conduct an exploratory data analysis of a specific system test data set that includes concomitant variables measuring aspects of test effort as well as the failure count and time variables considered in standard software reliability analyses. We consider a family of Poisson process models for which failure rates may depend on time as well as on the test effort variables. We analyze failure rates computed from this test data using least squares parameter estimation and model selection via the predicted residual sum of squares (PRESS),1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=341344,no,undetermined,0
Applying various learning curves to hyper-geometric distribution software reliability growth model,"The hyper-geometric distribution software reliability growth model (HGDM) has been shown to be able to estimate the number of faults initially resident in a program at the beginning of the test-and-debug phase. A key factor of the HGDM is the â€œsensitivity factorâ€? which represents the number of faults discovered and rediscovered at the application of a test instance. The learning curve incorporated in the sensitivity factor is generally assumed to be linear in the literature. However, this assumption is apparently not realistic in many applications. We propose two new sensitivity factors based on the exponential learning curve and the S-shaped learning curve, respectively. Furthermore, the growth curves of the cumulative number of discovered faults for the HGDM with the proposed learning curves are investigated. Extensive experiments have been performed based on two real test/debug data sets, and the results show that the HGDM with the proposed learning curves estimates the number of initial faults better than previous approaches",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=341342,no,undetermined,0
A class of recursive interconnection networks: architectural characteristics and hardware cost,"We propose and analyze a new class of interconnection networks, RCC, for interconnecting the processors of a general purpose parallel computer system. The RCC is constructed incrementally through the recursive application of a complete graph compound. The RCC integrate positive features of both complete networks and a given basic network. This paper presents the principles of constructing RCC and analyzes its architectural characteristics, its message routing capability, and its hardware cost. A specific instance of this class, RCC-CUBE, is shown to have desirable network properties such as small diameter, small degree, high density, high bandwidth, and high fault tolerance and is shown that they compare favorably to those of the hypercube, the 2-D, mesh, and the folded hypercube. Moreover, RCC-CUBE is shown to emulate the hypercube in constant time under any message distribution. The hardware cost and physical time performance are estimated under some packaging constraints for RCC-CUBE and compared to those of the hypercube, the folded hypercube, and the 2-D mesh demonstrating an overall cost effectiveness for RCC-CUBE. Hence, the RCC-CUBE appears to be a good candidate for next generation massively parallel computer systems",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=340843,no,undetermined,0
"Comments on ""Analysis of self-stabilizing clock synchronization by means of stochastic Petri nets""","We point out some errors in the combinatorial approach for the steady-state analysis of a deterministic and stochastic Petri net (DSPN) presented by M. Lu, D. Zhang, and T. Murata (1990). However, the methodology for analyzing the self-stability of fault-tolerant clock synchronisation (FCS) systems introduced in that paper is applicable for FCS systems with many clocking modules, even if the combinatorial approach is not valid. This is due to the progress in improving the efficiency of the DSPN solution algorithm made in recent years. We show that the explicit computation of the steady-state solution of the DSPN can be performed with reasonable computational effort on a modern workstation by the software package DSPNexpress.<<ETX>>",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=338111,no,undetermined,0
On available bandwidth in FDDI-based reconfigurable networks,"Networks used in mission-critical applications must be highly fault-tolerant. The authors propose an FDDI-based reconfigurable network (FBRN) that can survive multiple faults. An FBRN consists of multiple FDDI trunk rings and has the ability to reconfigure itself in the face of extensive damage. They consider three classes of FBRNs depending on their degree of reconfigurability: N-FBRN (nonreconfigurable), P-FBRN (partially reconfigurable) and F-FBRN (fully reconfigurable). They analyze the performance of FBRNs in terms of their available bandwidth in the presence of faults. In mission-critical systems, it is not enough to ensure that the network is connected in spite of faults; the traffic carrying capacity of the network must be sufficient to guarantee the required quality of service. They present a probabilistic analysis of the available bandwidth of FBRNs given the number of faults. They find that a fully reconfigurable FBRN can provide a high available bandwidth even in the presence of a large number of faults. A partially reconfigurable FBRN is found to be an excellent compromise between high reliability and ease of implementation",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=337554,no,undetermined,0
Complexity metrics for rule-based expert systems,"The increasing application of rule-based expert systems has led to the urgent need to quantitatively measure their quality, especially the maintainability which is harder and more expensive than that of conventional software because of the dynamic and evolutionary features of rules. One of the main factors that affect the maintainability of rule-based expert systems is their complexity; but so far little effort has been devoted to measure it. The paper investigates several complexity metrics for rule-based expert systems, and presents some evaluation methods based on statistical testing, analysis and comparison to assess the validity of these metrics. 71 rule-based expert systems are collected as test data from different application areas. The results reveal that a properly defined complexity metric, like our proposed RC, can be used as an effective means to measure the complexity of rule-based expert systems",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=336756,no,undetermined,0
An innovative tool for designing fault tolerant cockpit display symbology,"This research focuses on the design and development of a software package to aid display designers in creating fault tolerant fonts and symbology for monochrome dot-matrix displays. Since dot-matrix displays are subject to non-catastrophic failures [rows, columns, and individual picture elements], display designers find it necessary to address hardware reliability as a key design element when avoidance of operator reading errors is mission critical. This paper addresses row and column failure modes. Building redundancy into the design of font characters and symbology can provide additional protection from reading errors. The software package developed for the design of fault tolerant fonts, referred to herein as FontTool, operates on an IBM PC or compatible hardware platform within a Microsoft DOS environment. FontTool can simulate row or column dot-matrix display failures and â€œpredictâ€?likely human reading errors. Based on limited testing, FontTool reading error â€œpredictionsâ€?were found to be consistent with actual human performance reading error data about 86% of the time. FontTool uses Euclidean distance between 2-D Fourier transformed representations of dot-matrix characters as a metric for predicting character â€œsimilarityâ€? Although this metric has been applied previously, FontTool is a major advance in aiding display designers to build more fault tolerant cockpit display symbology",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=332968,no,undetermined,0
Fault-tolerant routing in mesh architectures,"It is important for a distributed computing system to be able to route messages around whatever faulty links or nodes may be present. We present a fault-tolerant routing algorithm that assures the delivery of every message as long as there is a path between its source and destination. The algorithm works on many common mesh architectures such as the torus and hexagonal mesh. The proposed scheme can also detect the nonexistence of a path between a pair of nodes in a finite amount of time. Moreover, the scheme requires each node in the system to know only the state (faulty or not) of each of its own links. The performance of the routing scheme is simulated for both square and hexagonal meshes while varying the physical distribution of faulty components. It is shown that a shortest path between the source and destination of each message is taken with a high probability, and, if a path exists, it is usually found very quickly",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=329665,no,undetermined,0
Software trustability,"A measure of software dependability called trustability is described. A program p has trustability T if we are at least T confident that p is free of faults. Trustability measurement depends on detectability. The detectability of a method is the conditional probability that it will detect faults. The trustability model characterizes the kind of information needed to justify a given level of trustability. When the required information is available, the trustability approach can be used to determine strategies in which methods are combined for maximum effectiveness. It can be used to determine the minimum amount of resources needed to guarantee a required degree of trustability, and the maximum trustability that is achievable with a given amount of resources",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=341366,no,undetermined,0
A Markov chain model for statistical software testing,"Statistical testing of software establishes a basis for statistical inference about a software system's expected field quality. This paper describes a method for statistical testing based on a Markov chain model of software usage. The significance of the Markov chain is twofold. First, it allows test input sequences to be generated from multiple probability distributions, making it more general than many existing techniques. Analytical results associated with Markov chains facilitate informative analysis of the sequences before they are generated, indicating how the test is likely to unfold. Second, the test input sequences generated from the chain and applied to the software are themselves a stochastic model and are used to create a second Markov chain to encapsulate the history of the test, including any observed failure information. The influence of the failures is assessed through analytical computations on this chain. We also derive a stopping criterion for the testing process based on a comparison of the sequence generating properties of the two chains",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=328991,no,undetermined,0
A PC-based system to assist researchers in electric power distribution systems analysis,"A PC-based system has been developed as an aid to research in power engineering. This system was developed using off-the-shelf hardware cards, and a commercially available software package that provides a high-level programming â€œshellâ€?for hardware interfacing/control and graphic user interface development. The system consists of a data acquisition unit and a waveform generator. The data acquisition unit can monitor and record actual field measurements, whereas the waveform generator is able to recreate analog versions of stored data in a laboratory environment. The system has been used to test a prototype high-impedance fault detection unit which is under development. The system can also be used as an aid in classroom instruction. The design and development of the system reflects a simple cost-effective way for students to gain valuable practical experience in developing hardware/software-based systems within a reasonable length of time",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=328385,no,undetermined,0
Speech coding: a tutorial review,"The past decade has witnessed substantial progress towards the application of low-rate speech coders to civilian and military communications as well as computer-related voice applications. Central to this progress has been the development of new speech coders capable of producing high-quality speech at low data rates. Most of these coders incorporate mechanisms to: represent the spectral properties of speech, provide for speech waveform matching, and â€œoptimizeâ€?the coder's performance for the human ear. A number of these coders have already been adopted in national and international cellular telephony standards. The objective of this paper is to provide a tutorial overview of speech coding methodologies with emphasis on those algorithms that are part of the recent low-rate standards for cellular communications. Although the emphasis is on the new low-rate coders, we attempt to provide a comprehensive survey by covering some of the traditional methodologies as well. We feel that this approach will not only point out key references but will also provide valuable background to the beginner. The paper starts with a historical perspective and continues with a brief discussion on the speech properties and performance measures. We then proceed with descriptions of waveform coders, sinusoidal transform coders, linear predictive vocoders, and analysis-by-synthesis linear predictive coders. Finally, we present concluding remarks followed by a discussion of opportunities for future research",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=326413,no,undetermined,0
Input preprocessor for creating MCNP models for use in developing large calculational databases,"With the rapid increase in workstation performance in the last decade, computational power is no longer the limiting factor for using Monte Carlo modeling to design and characterize nuclear well-logging tools. Instead, the development of tool models, modification of tool models, and the interpretation of computational results limit the throughput of MCNP modelers. A new software program called MCPREP has been developed that greatly increases the productivity of nuclear modelers in the development of large calculational databases. In addition, the use of the preprocessor can improve quality control of the modeling effort, can allow novice modelers to run complex and realistic problems, and can decrease the post-processing of the MCNP output files",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=322860,no,undetermined,0
What is software reliability?,"Reliability refers to statistical measures an engineer uses to quantify imperfection in practice. Often we speak imprecisely of an object having â€œhigh reliabilityâ€? but technically, unless the object cannot fail at all, its reliability is arbitrarily close to zero for a long enough period of operation. This is merely an expression of the truism that an imperfect object must eventually fail. At first sight, it seems that software should have a sensible reliability, as other engineered objects do. But the application of the usual mathematics is not justified. Reliability theory applies to random (as opposed to systematic) variations in a population of similar objects, whereas software defects are all design flaws, not at all random, in a unique object. The traditional cause of failure is a random process of wear and tear, while software is forever as good (or as bad!) as new. However, software defects can be thought of as lurking in wait for the user requests that excite them, like a minefield through which the user must walk",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=318457,no,undetermined,0
On measurement of operational security [software reliability],"Ideally, a measure of the security of a system should capture quantitatively the intuitive notion of `the ability of the system to resist attack'. That is, it should be operational, reflecting the degree to which the system can be expected to remain free of security breaches under particular conditions of operation (including attack). Instead, current security levels at best merely reflect the extensiveness of safeguards introduced during the design and development of a system. Whilst we might expect a system developed to a higher level than another to exhibit `more secure behaviour' in operation, this cannot be guaranteed; more particularly, we cannot infer what the actual security behaviour will be from knowledge of such a level. In the paper we discuss similarities between reliability and security with the intention of working towards measures of `operational security' similar to those that we have for reliability of systems. Very informally, these measures could involve expressions such as the rate of occurrence of security breaches (cf. rate of occurrence of failures in reliability), or the probability that a specified `mission' can be accomplished without a security breach (cf. reliability function). This new approach is based on the analogy between system failure and security breach, but it raises several issues which invite empirical investigation. We briefly describe a pilot experiment that we have conducted to judge the feasibility of collecting data to examine these issues",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=318447,no,undetermined,0
Software quality measurement based on fault-detection data,"We develop a methodology to measure the quality levels of a number of releases of a software product in its evolution process. The proposed quality measurement plan is based on the faults detected in field operation of the software. We describe how fault discovery data can be analyzed and reported in a framework very similar to that of the QMP (quality measurement plan) proposed by B. Hoadley (1986). The proposed procedure is especially useful in situations where one has only very little data from the latest release. We present details of implementation of solutions to a class of models on the distribution of fault detection times. The conditions under which the families: exponential, Weibull, or Pareto distributions might be appropriate for fault detection times are discussed. In a variety of typical data sets that we investigated one of these families was found to provide a good fit for the data. The proposed methodology is illustrated with an example involving three releases of a software product, where the fault detection times are exponentially distributed. Another example for a situation where the exponential fit is not good enough is also considered",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=317425,no,undetermined,0
A stochastic reward net model for dependability analysis of real-time computing systems,"Dependability assessment plays an important role in the design and validation of fault-tolerant real-lime computer systems. Dependability models provide measures such as reliability, safety and mean time to failure as functions of the component failure rates and fault/error coverage probabilities. In this paper we present a decomposition technique that accounts for both the hardware and software architectural characteristics of the modelled systems. Stochastic reward nets are employed as a unique modeling framework. Dependability of a railroad control computer, which relies an software techniques for fault/error handling, is analysed as an application example",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=316162,no,undetermined,0
On the impact of software product dissimilarity on software quality models,"The current software market favors software development organizations that apply software quality models. Software engineers fit quality models to data collected from past projects. Predictions from these models provide guidance in setting schedules and allocating resources for new and ongoing development projects. To improve model stability and predictive quality, engineers select models from the orthogonal linear combinations produced using principal components analysis. However, recent research revealed that the principal components underlying source code measures are not necessarily stable across software products. Thus, the principal components underlying the product used to fit a regression model can vary from the principal components underlying the product for which we desire predictions. We investigate the impact of this principal components instability on the predictive quality of regression models. To achieve this, we apply an analytical technique for accessing the aptness of a given model to a particular application",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=341360,no,undetermined,0
Putting assertions in their place,"Assertions that are placed at each statement in a program can automatically monitor the internal computations of a program execution. However, the advantages of universal assertions come at a cost. A program with such extensive internal instrumentation will be slower than the same program without the instrumentation. Some of the assertions may be redundant. The task of instrumenting the code with correct assertions at each location is burdensome, and there is no guarantee that the assertions themselves will be correct. We advocate a middle ground between no assertions at all (the most common practice) and the theoretical ideal of assertions at every location. Our compromise is to place assertions only at locations where traditional testing is unlikely to uncover software faults. One type of testability measurement, sensitivity analysis, identifies locations where testing is unlikely to be effective",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=341367,no,undetermined,0
Designing reliable software,"In this paper we examine the quantitative software attributes that are known to be related to software faults and subsequently software failures. The object is to develop design guidelines based on these quantitative attributes that will assist the evaluation of alternatives in the software design process. In order for the several design alternatives to provide meaningful input to the design process, the software system must first be characterized in terms of a series of operational profiles of its projected use",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=624273,no,undetermined,0
Measuring program structure with inter-module metrics,"A good structure is an important quality aspect of a program. Well-structured, modular programs are less costly to maintain than unstructured monolithic ones. Quantitatively assessing structure and modularity of programs can be useful to help ensure that a program is well-structured by indicating a potential need for restructuring of poorly structured code. However, previous structure metrics do not sufficiently account for the modular features used in modern, object-oriented programming languages. We propose four novel measures to assess the modular structure of a software project. Our measures are based on the principle of vocabulary hiding and measure a form of cohesion. A metrics prototype tool has been implemented for the Modula-3 programming language. Informal tests suggest that they are indeed useful to assess the quality of the program structure",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=342817,no,undetermined,0
Efficient test sequence generation for localization of multiple faults in communication protocols,"Conformance test for communication protocols is indispensable for the production of reliable communications software. A lot of conformance test techniques have been developed. However, most of them can only decide whether an implemented protocol conforms to its specification. That is, the exact locations of faults are not determined by them. This paper presents some conditions that enable to find location of multiple faults, and then proposes a test sequence generation technique under such conditions. The characteristics of this technique are to generate test sequences based on protocol specifications and interim test results, and to find locations of multiple faults in protocol implementations. Although the length of the test sequence generated by the proposed technique is a little longer than the one generated by the previous one, the class to which the proposed technique can be applied is larger than that to which the previous one can be applied",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=367228,no,undetermined,0
Application of artificial neural networks to parameter estimation of dynamical systems,Neural network (NN) based estimators of dynamical system parameters are introduced and compared to the least-squares-error estimators. Equations are derived to discuss the NN estimator existence and to express its covariance matrix. The results are illustrated using a numerical example of a 3-parameter system represented by multiexponential model,1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=352109,no,undetermined,0
Fault detection using topological case based modelimg and its application to chiller performance deterioration,"We apply a new modeling technique, Topological Case Based Modeling (TCBM), to fault diagnosis. In this paper we propose a new model which represents a continuous input/output relation using a set of numerical data, and it is possible to describe nonlinear systems. We employ the idea of case based reasoning (CBR). CBR infers a new case from stored cases and these relation.[1] It is important to define the similarity, that is a relation among the cases. We propose to define the similarity as the neighbourhood in input space corresponding to output accuracy and its accuracy is arbitrarily set before constructing the model. We name this technique Topological Case Based Modeling. In addition, we describe that TCBM has several advantages over other models. Finally we also show the example of TCBM to detect the deterioration for a chiller system",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=352030,no,undetermined,0
The personal process in software engineering,"The personal software process (PSP) provides software engineers a way to improve the quality, predictability, and productivity of their work. It is designed to address the improvement needs of individual engineers and small software organizations. A graduate level PSP course has been taught at six universities and the PSP is being introduced by three industrial software organizations. The PSP provides a defined sequence of process improvement steps coupled with performance feedback at each step. This helps engineers to understand the quality of their work and to appreciate the effectiveness of the methods they use. Early experience with the PSP shows that average test defect rate improvements of ten times and average productivity improvements of 25% or more are typical",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=344422,no,undetermined,0
Improving the performance of DSM systems via compiler involvement,"Distributed shared memory (DSM) systems provide an illusion of shared memory on distributed memory systems such as workstation networks and some parallel computers such as the Cray T3D and Convex SPP-1. On these systems, users can write programs using a shared memory style of programming instead of message passing which is tedious and error prone. Our experience with one such system TreadMarks, has shown that a large class of applications does not perform well on these systems. TreadMarks is a software distributed shared memory system designed by Rice University researchers to run on networks of workstations and massively parallel computers. Due to the distributed nature of the memory system, shared memory synchronization primitives such as locks and barriers often cause significant amounts of communication. We have provided a set of powerful primitives that will alleviate the problems with locks and barriers on such systems. We have designed two sets of primitives the first set maintains coherence and is easy to use by a programmer, the second set does not maintain coherence and is best used by a compiler. These primitives require that the underlying DSM be enhanced. We have implemented some of our primitives on the TreadMarks DSM system and obtained reasonable performance improvements on application kernels from molecular dynamics and numerical analysis. Furthermore, we have identified key compiler optimizations that use the noncoherent primitives to reduce synchronization overhead and improve the performance of DSM systems",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=344342,no,undetermined,0
Measurement-driven quality improvement in the MVS/ESA operating system,"Achieving gains in software quality requires both the use of software metrics and the desire to make measurement-driven process improvements. This paper describes the experiences, changes, and kinds of metrics used to drive quantifiable results in a very large and complex software system. Developers on the IBM Multiple Virtual Storage (MVS) operating system track, respond and initiate better ways of controlling the software development process through the use of metric-based defect models, orthogonal defect classification (ODC), and object-oriented techniques. Constant attention to progress with respect to the measurements and working toward goals based on the metrics show that measurement-driven process improvement can lead to noticeably better products",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=344232,no,undetermined,0
Are the principal components of software complexity data stable across software products?,"The current software market is not suitable for organizations that place competitive bids, set schedules, or control projects without regard to past performance. Software quality models based upon data collected from past projects can help engineers to estimate costs of future development efforts, and to control ongoing efforts. Application of principal components analysis can improve the stability and predictive quality of software quality models. However, models based upon principal components are only appropriate for application to products having similar principal components. We apply a statistical technique for quantifying the similarity of principal components. We find that distinct but similar products developed by the same organization can share similar principal components, and that distinct products developed by distinct organizations will likely have dissimilar principal components",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=344227,no,undetermined,0
A software project management system supporting the cooperation between managers and workers-design and implementation,"The authors aim at constructing an integrated software project management system with an object database. This paper describes a process model for software project management that forms the basis of the system. This paper also describes a framework for inducing the sequence of operations that the project manager must perform: issuing instructions to workers in accordance with a plan based on the process model, progress evaluation based on progress reports sent from the workers, and analysis of the impacts of any problems detected. In order to provide these facilities, we analyse the actions triggered per unit of activity from the viewpoint of project management. Based on this analysis, this paper proposes a finite state machine model involving the actions and the states changed by the actions. When an action is triggered, the system checks whether the action is effective or not, and provides the necessary information to the appropriate persons by referring to the relationships specified in the process model. This paper also describes prototype system based on the framework",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=342834,no,undetermined,0
Simulation based metrics prediction in software engineering,"The software industry is increasingly working towards the goal of software reuse. The use of metrics to assess software modules has long been advocated by the US Army. A combination of these two trends should make it possible for a software developer to predict the metrics of the composite software system given the metrics of the component software modules. It should also enable the developer to make an appropriate choice among many possible modules available in the software repository. The paper presents an approach to solve the above two problems. The metrics suite used is an extension of the US Army STEP metrics, the approach is simulation based, and its theoretical validation is provided from approximation theory",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=342808,no,undetermined,0
Connecting test coverage to software dependability,"It is widely felt that software quality in the form of reliability or â€œtrustworthinessâ€? can be demonstrated by the successful completion of testing that â€œcoversâ€?the software. However, this intuition has little experimental or theoretical support. The paper considers why the intuition is so powerful and yet misleading. Formal definitions of software â€œdependabilityâ€?are suggested, along with new approaches for measuring this analogy of trustworthiness",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=341368,no,undetermined,0
Bounding worst-case instruction cache performance,"The use of caches poses a difficult tradeoff for architects of real-time systems. While caches provide significant performance advantages, they have also been viewed as inherently unpredictable, since the behavior of a cache reference depends upon the history of the previous references. The use of caches is only suitable for real-time systems if a reasonably tight bound on the performance of programs using cache memory can be predicted. This paper describes an approach for bounding the worst-case instruction cache performance of large code segments. First, a new method called static cache simulation is used to analyze a program's control flow to statically categorize the caching behavior of each instruction. A timing analyzer, which uses the categorization information, then estimates the worst-case instruction cache performance for each loop and function in the program",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=342718,no,undetermined,0
Assessing the dynamic strength of software systems using interference analysis,"The concept of dynamic strength is closely related to reliability: the probability that a software system does not encounter a latent fault during execution. Dynamic strength is assessed by analyzing the interference between the execution profile, a probability density for system size, and the composite static strength distributions. Composite static strength is the sum of the relative software complexity metrics of each of the system's software modules. Composite static strength is a density function with size as the variate. Latent faults occur in the region of interference when the execution distribution exceeds a critical level of the strength distribution. An important relationship between the execution profile and strength distributions is characterized by the return period function",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=341401,no,undetermined,0
Sensitivity of field failure intensity to operational profile errors,"Sensitivity of field failure intensity estimates to operational profile occurrence probability errors is investigated. This is an important issue in software reliability engineering, because these estimates enter into many development decisions. Sensitivity was computed for 59,200 sets of conditions, spread over a wide range. For 99.4% of these points, the failure intensity was very robust with respect to occurrence probability errors, the error in failure intensity being more than a factor of 5 smaller than the occurrence probability error. Thus projects do not have to spend extra effort to obtain high precision in measuring the operational profile, nor do they need to worry about moderate changes in system usage with time",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=341399,no,undetermined,0
Reliability growth modelling of software for process control systems,"Reliability growth models are the most widely known and used in practical applications. Nevertheless, the performances of these models limit their applicability to the control and assessment of the attainment of very low reliability levels. One of the reasons for their insufficient predictive quality is the scarcity of information they use for reliability estimation. They do not take into account information that could be easily collected from code observation, such as the code complexity, or during testing such as the coverage obtained. The paper analyzes the problems encountered using reliability growth models for the evaluation of software used in a measurement equipment control system",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=341396,no,undetermined,0
A case study to investigate sensitivity of reliability estimates to errors in operational profile,"We report a case study to investigate the effect of errors in an operational profile on reliability estimates. A previously reported tool named TERSE was used in this study to generate random flow graphs representing programs, model errors in operational profile, and compute reliability estimates. Four models for reliability estimation were considered: the Musa-Okumoto model, the Goel-Okumoto model, coverage enhanced Musa-Okumoto model, and coverage enhanced Goel-Okumoto model. It was found that the error in reliability estimates from these models grows nonlinearly with errors in operational profile. Results from this case study lend credit to the argument that further research is necessary in development of more robust models for reliability estimation",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=341387,no,undetermined,0
A generalized software reliability process simulation technique and tool,"The paper describes the structure and rationale of the generalized software reliability process and a set of simulation techniques that may be applied for the purpose of software reliability modeling. These techniques establish a convenient means for studying a realistic, end-to-end software life cycle that includes intricate subprocess interdependencies, multiple defect categories, many factors of influence, and schedule and resource dependencies, subject to only a few fundamental assumptions, such as the independence of causes of failures. The goals of this research are dual: first, to generate data for truly satisfying the simplified assumptions of various existing models for the purpose of studying their comparative merits, and second, to enable these models to extend their merits to a less idealized, more realistic reliability life cycle. This simulation technique has been applied to data from a spacecraft project at the Jet Propulsion Laboratory; results indicate that the simulation technique potentially may lead to more accurate tracking and more timely prediction of software reliability than obtainable from analytic modeling techniques",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=341385,no,undetermined,0
The relationship between test coverage and reliability,"Models the relationship between testing effort, coverage and reliability, and presents a logarithmic model that relates testing effort to test coverage: statement (or block) coverage, branch (or decision) coverage, computation use (c-use) coverage, or predicate use (p-use) coverage. The model is based on the hypothesis that the enumerables (like branches or blocks) for any coverage measure have different detectability, just like the individual defects. This model allows us to relate a test coverage measure directly to the defect coverage. Data sets for programs with real defects are used to validate the model. The results are consistent with the known inclusion relationships among block, branch and p-use coverage measures. We show how the defect density controls the time-to-next-failure. The model can eliminate variables like the test application strategy from consideration. It is suitable for high-reliability applications where automatic (or manual) test generation is used to cover enemerables which have not yet been tested",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=341373,no,undetermined,0
Modelling an imperfect debugging phenomenon with testing effort,"A software reliability growth model (SRGM) based on non-homogeneous Poisson processes (NHPP) is developed. The model describes the relationship between the calendar time, the testing effort consumption and the error removal process under an imperfect debugging environment. The role of learning (gaining experience) with the progress of the testing phase is taken into consideration by assuming that the imperfect debugging probability is dependent on the current software error content. The model has the in-built flexibility of representing a wide range of growth curves. The model can be used to plan the amount of testing effort required to achieve a pre-determined target in terms of the number of errors removed in a given span of time",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=341371,no,undetermined,0
MONSET-a prototype software development estimating tool,"The development of large-scale computer software has traditionally been a difficult cost estimation problem. Software has been developed for more than thirty years and it is reasonable to expect that the experience gained in this time, would make software development effort predictions more reliable. One way by which an organisation can benefit from past projects is to measure, track and control each project and use the collected results to assist future project estimation. This paper describes a hybrid model for software effort prediction and its validation against available data on some large software projects. A prototype software development estimation system (MONSET-Monash Software Estimating Tool) based on the proposed model is described. The system aims to provide guidance for project managers during the software development process",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=315761,no,undetermined,0
PARSA: a parallel program software development tool,"This paper introduces the PARSA (PARallel program Scheduling and Assessment) parallel software development tool to address the efficient partitioning and scheduling of parallel programs on multiprocessor systems. The PARSA environment consists of a user-friendly (visual), interactive, compile-time environment for partitioning, scheduling, and performance evaluation/tuning of parallel programs on different parallel computer architectures",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=315759,no,undetermined,0
Modeling and analysis of system dependability using the System Availability Estimator,"This paper reviews the System Availability Estimator (SAVE) modeling program package. SAVE is used to construct and analyze models of computer and communication systems dependability. The SAVE modeling language consists of a few constructs for describing the components in a system, their failure and repair characteristics, the interdependencies between components, and the conditions on the individual components for the system to be considered available. SAVE parses an input file and creates a Markov chain model. For small models numerical solution methods can be used, but for larger models (the state space grows exponentially with the number of components in the model), fast simulation techniques using importance sampling have to be used. We provide software demonstrations using both these techniques.<<ETX>>",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=315647,no,undetermined,0
SACEM: A fault tolerant system for train speed control,"The authors give an overview of the SACEM system which controls the train movements on RER A in Paris, which transports daily one million passengers. The various aspects of the dependability of the system are described, including the techniques aimed at insuring safety (online error detection, software validation). Fault tolerance of the onboard-ground compound system is emphasized.",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=627365,no,undetermined,0
Cost impact of telecommunications switch software problems on divested Bell operating companies,"Bellcore has developed a model to support the periodic calculation and reporting of customer costs resulting from poor quality in telecommunications switching system software. This model is designed to provide, as a primary output, total switching system software cost of poor quality, along with a breakdown of this total by major cost component. The primary input to the model is software RQMS data, described in Bellcore's reliability and qualify measurements for telecommunications systems, TR-TSY-000929, and reported by switching system suppliers to the divested Bell operating companies (telcos) on systems operated by these companies. Telco and switching system supplier alike are expected to benefit from the ongoing application of the model, through which accumulated maintenance cost information will be made available to support telco switching system management functions and supplier quality improvement efforts",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=272888,no,undetermined,0
A comparative study of pattern recognition techniques for quality evaluation of telecommunications software,"The extreme risks of software faults in the telecommunications environment justify the costs of data collection and modeling of software quality. Software quality models based on data drawn from past projects can identify key risk or problem areas in current similar development efforts. Once these problem areas are identified, the project management team can take actions to reduce the risks. Studies of several telecommunications systems have found that only 4-6% of the system modules were complex [LeGall et al. 1990]. Since complex modules are likely to contain a large proportion of a system's faults, the approach of focusing resources on high-risk modules seems especially relevant to telecommunications software development efforts. A number of researchers have recognized this, and have applied modeling techniques to isolate fault-prone or high-risk program modules. A classification model based upon discriminant analytic techniques has shown promise in performing this task. The authors introduce a neural network classification model for identifying high-risk program modules, and compare the quality of this model with that of a discriminant classification model fitted with the same data. They find that the neural network techniques provide a better management tool in software engineering environments. These techniques are simpler, produce more accurate models, and are easier to use",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=272878,no,undetermined,0
Managing code inspection information,"Inspection data is difficult to gather and interpret. At AT&T Bell Laboratories, the authors have defined nine key metrics that software project managers can use to plan, monitor, and improve inspections. Graphs of these metrics expose problems early and can help managers evaluate the inspection process itself. The nine metrics are: total noncomment lines of source code inspected in thousands (KLOC); average lines of code inspected; average preparation rate; average inspection rate; average effort per KLOC; average effort per fault detected; average faults detected per KLOC; percentage of reinspections; defect-removal efficiency.<<ETX>>",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=268958,no,undetermined,0
Designing an agent synthesis system for cross-RPC communication,"Remote procedure call (RPC) is the most popular paradigm used today to build distributed systems and applications. As a consequence, the term â€œRPCâ€?has grown to include a range of vastly different protocols above the transport layer. A resulting problem is that programs often use different RPC protocols, cannot be interconnected directly, and building a solution for each case in a large heterogeneous environment is prohibitively expensive. We describe the design of a system that can synthesize programs (RPC agents) to accommodate RPC heterogeneities. Because of its synthesis capability, the system also facilitates the design and implementation of new RPC protocols through rapid prototyping. We have built a prototype system to validate the design and to estimate the agent development costs and cross-RPC performance. The evaluation shows that the synthesis approach provides a more general solution than existing approaches do, and with lower software development and maintenance costs, while maintaining reasonable cross-RPC performance",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=268920,no,undetermined,0
Exploiting instruction-level parallelism for integrated control-flow monitoring,"Computer architectures are using increased degrees of instruction-level machine parallelism to achieve higher performance, e.g., superpipelined, superscalar and very long instruction word (VLIW) processors. Full utilization of such machine parallelism is difficult to achieve and sustain, resulting in the occurrence of idle resources at run time. This work explores the use of such idle resources for concurrent error detection in processors employing instruction-level machine parallelism. The Multiflow TRACE 14/300 processor, a VLIW machine, is chosen as an experimental vehicle. Experiments indicate that significant idle resources are likely to exist across a wide range of scientific applications for the TRACE 14/300. A methodology is presented for detecting transient control-flow errors, called available resource-driven control-flow monitoring (ARC), whose resource use can be tailored to the existence of idle resources in the processor. Results of applying ARC to the Multiflow TRACE 14/300 processor show that >99% of control-flow errors are detected with negligible performance overhead. These results demonstrate that ARC is highly effective in using the idle resources of a processor to achieve concurrent error detection at a very low cost",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=262118,no,undetermined,0
TTP-a protocol for fault-tolerant real-time systems,"The Time-Triggered Protocol integrates such services as predictable message transmission, clock synchronization, membership, mode change, and blackout handling. It also supports replicated nodes and replicated communication channels. The authors describe their architectural assumptions, fault hypothesis, and objectives for the TTP protocol. After they elaborate on its rationale, they give a detailed protocol description. They also discuss TTP characteristics and compare its performance with that of other protocols proposed for control applications",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=248873,no,undetermined,0
A microprocessor-based measuring unit for high-speed distance protection,"This paper describes a microprocessor-based measuring unit which is suitable for use in high-speed digital distance relays. The unit uses an algorithm that provides accurate estimates of apparent resistance and inductance of the line in relatively short times. The algorithm deals effectively with the presence of nonfundamental frequency components in signals without making assumptions about their compositions. The hardware and software of the unit are described. Test results indicate that the estimates converge to their true values within about 10 ms after fault inception. Also, the ability of the unit to provide accurate estimates is not adversely affected by the system frequencies drifting from its nominal value and by the distortion of the input signals.",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6592926,no,undetermined,0
A systematic and comprehensive tool for software reliability modeling and measurement,"Sufficient work has been done to demonstrate that software reliability models can be used to monitor reliability growth over a useful range of software development projects. However, due to the lack of appropriate tools, the application of software reliability models as a means for project management is not as widespread as it might be. The existing software reliability modeling and measurement programs are either difficult for a nonspecialist to use, or short of a systematic and comprehensive capability in the software reliability measurement practice. To address the ease-of-use and the capability issues, the authors have prototyped a software reliability modeling tool called CASRE, a Computer-Aided Software Reliability Estimation tool. Implemented with a systematic and comprehensive procedure as its framework, CASRE will encourage more widespread use of software reliability modeling and measurement as a routine practice for software project management. The authors explore the CASRE tool to demonstrate its functionality and capability.",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=627369,no,undetermined,0
Efficient memory access checking,"A new approach provides efficient concurrent checking of memory accesses using a signature embedded into each instance of a data structure, and using a new LOAD/STORE instruction that reads the data structure signature as a side effect. Software memory-access faults are detectable using this approach, including corrupted pointers, uninitialized pointers, stale pointers, and copy overruns. Hardware memory-access faults are also detectable, including faults in the memory access path and the register file. Instruction scheduling minimizes the cost of the side-effect reads, and signatures are checked with little overhead using the hardware monitor previously proposed for signature monitoring. Benchmark results for the MIPS R3000 executing code scheduled by a modified GNU C Compiler show that an average of 53% of the memory accesses are checked, and that access checking causes an average of less than 5% performance overhead.",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=627359,no,undetermined,0
Multiple fault detection in parity checkers,"Parity checkers are widely used in digital systems to detect errors when systems are in operation. Since parity checkers are monitoring circuits, their reliability must be guaranteed by performing a thorough testing. In this work, multiple fault detection of parity checkers is investigated. We have found that all multiple stuck-at faults occurring on a parity tree can be completely detected using test patterns provided by the identity matrix plus zero vector. The identity matrix contains 1's on the main diagonal and 0's elsewhere; while the zero vector contains 0's. The identity matrix vectors can also detect all multiple general bridging faults, if the bridgings result in a wired-AND effect. However, test patterns generated from the identity matrix and binary matrix are required to detect a majority of the multiple bridging faults which yield wired-OR connections. Note that the binary matrix contains two 1's at each column of the matrix",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=312118,no,undetermined,0
Simulation of software behavior under hardware faults,"A simulation-based software-model that permits application specific dependability analysis in the early design stages is introduced. The model represents an application program by decomposing it into a graph model consisting of a set of nodes, a set of edges that probabilistically determine the flow from node to node, and a mapping of the nodes to memory. The software model simulates the execution of the program while errors are injected into the program's memory space. The model provides application-dependent parameters such as detection and propagation times and permits evaluation of function on system level error detection and recovery schemes. A case study illustrates the interaction between an application program and two detection schemes. Specifically, Gaussian elimination programs running on a Tandem Integrity S2 system with memory scrubbing are studied. Results from the simulation-based software model are validated with data measured from an actual Tandem Integrity S2 system. Application dependent coverage values obtained with the model are compared with those obtained via traditional schemes that assume uniform or ramp memory access patterns. For the authors' program, some coverage values obtained with the traditional approaches were found to be 100% larger than those obtained with the software model.",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=627325,no,undetermined,0
The variation of software survival time for different operational input profiles (or why you can wait a long time for a big bug to fail),"Experimental and theoretical evidence for the existence of contiguous failure regions in the program input space (blob defects) is provided. For real-time systems where successive input values tend to be similar, blob defects can have a major impact on the software survival time because the failure probability is not constant. For example, with a random walk input sequence, the probability of failure decreases as the time from the last failure increases. It is shown that the key factors affecting the survival time are the input trajectory, the rate of change of the input values, and the surface area of the defect (rather than its volume).",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=627312,no,undetermined,0
"Faults, symptoms, and software fault tolerance in the Tandem GUARDIAN90 operating system","The authors present a measurement-based study of software failures and recovery in the Tandem GUARDIAN90 operating system using a collection of memory dump analyses of field software failures. They identify the effects of software faults on the processor state and trace the propagation of the effects to other areas of the system. They also evaluate the role of the defensive programming techniques and the software fault tolerance of the process pair mechanism implemented in the Tandem system. Results show that the Tandem system tolerates nearly 82% of reported field software faults, thus demonstrating the effectiveness of the system against software faults. Consistency checks made by the operating system detect 52% of software problems and prevent any error propagation in 31% of software problems. Results also show that 72% of reported field software failures are recurrences of known software faults and 70% of the recurrence groups have identical characteristics.",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=627304,no,undetermined,0
A neural network modeling methodology for the detection of high-risk programs,"The profitability of a software development effort is highly dependent on both timely market entry and the reliability of the released product. To get a highly reliable product to the market on schedule, software engineers must allocate resources appropriately across the development effort. Software quality models based upon data drawn from past projects can identify key risk or problem areas in current similar development efforts. Knowing the high-risk modules in a software design is a key to good design and staffing decisions. A number of researchers have recognized this, and have applied modeling technqiues to isolate fault-prone or high-risk program modules early in the development cycle. Discriminant analytic classification models have shown promise in performing this task. We introduce a neural network classification model for identifying high-risk program modules, and we compare the quality of this model with that of a discriminant classification model fitted with the same data. We find that the neural network techniques provide a better management tool in software engineering environments",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=624300,no,undetermined,0
Infinite failure models for a finite world: A simulation study of the fault discovery process,"Many software reliability growth models have been published in journals and conference proceedings over the last 20-25 years. Each one has been justified based on theoretical or empirical evidence. Although there are many ways of classifying these models, a particularly interesting classification involves distinguishing models based upon the asymptotic expected number of total failures. These models are termed infinite and finite failure models since each one expects infinite and finite failures respectively as time approaches infinity. As might be expected, theoretic and especially empirical justification for the appropriateness of infinite failure models came after justifications for finite failure models. Infinite failure models were associated with weak fault repair systems or possibly with highly nonuniform usage, although this term is non-specific. We demonstrate through simulations of black-box testing (and/or field usage) that infinite failure models are appropriate in situations where there is perfect and near-perfect repair and where usage is uniform for the vast majority of the system",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=624298,no,undetermined,0
Adding capability checks enhances error detection and isolation in object-based systems,"Error detection and error isolation are becoming stringent requirements for many computational problems requiring high reliability in addition to high performance. This paper presents CAPACETI, a technique for utilizing capabilities at the application level in order to achieve dependable operations. The proposed technique is further augmented with executable assertions and other software error detection techniques. The effectiveness of the techniques to detect errors, their contribution to the overall coverage, and their performance overhead were experimentally obtained using fault/error injection. Results obtained from these experiments show that high coverage with a low performance overhead can be achieved by selectively combining different error detection techniques",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=624287,no,undetermined,0
Dependability modeling and evaluation of recovery block systems,"The paper presents performance modeling and evaluation of recovery block systems. In order to produce a dependability model for a complete fault tolerant system we consider the interaction between the faults in the alternatives and the faults in the acceptance test. The study is based on finite state continuous time Markov model, and unlike previous works, we carry out the analysis in the time domain. The undetected and total failure probabilities (safety and reliability), as well as the average recovery block execution time expressions are obtained. Derived mathematical relations between failure probabilities (i.e. reliability and safety) and modeling parameters enable us to gain a great deal of quantitative results",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=624280,no,undetermined,0
Enhancing accuracy of software reliability prediction,"The measurement and prediction of software reliability require the use of the software reliability growth models (SRGMs). The predictive quality can be measured by the average end-point projection error. In this paper, the effects of two orthogonal classes of approaches to improve prediction capability of a SRM have been examined using a large number of data sets. The first approach is preprocessing of data to filter out short term noise. The second is to overcome the bias inherent in the model. The results show that proper application of these two approaches can be more important than the selection of the model",1993,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=624276,no,undetermined,0
The derivation and experimental verification of clock synchronization theory,"The objective of this work is to validate mathematically derived clock synchronization theories and their associated algorithms through experiment. Two theories are considered, the Interactive Convergence Clock Synchronization Algorithm and the Mid-Point Algorithm. Special clock circuitry was designed and built so that several operating conditions and failure modes (including malicious failures) could be tested. Both theories are shown to predict conservative upper bounds (i.e., measured values of clock skew were always less than the theory prediction). Insight gained during experimentation led to alternative derivations of the theories. These new theories accurately predict the clock system's behavior. It is found that a 100% penalty is paid to tolerate worst case failures. It is also shown that under optimal conditions (with minimum error and no failures) the clock skew can be as much as 3 clock ticks. Clock skew grows to 6 clock ticks when failures are present. Finally, it is concluded that one cannot rely solely on test procedures or theoretical analysis to predict worst case conditions",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=286301,no,undetermined,0
Compressionless Routing: a framework for adaptive and fault-tolerant routing,"Compressionless Routing (CR) is a new adaptive routing framework which provides a unified framework for efficient deadlock-free adaptive routing and fault-tolerance. CR exploits the tight-coupling between wormhole routers for flow control to detect potential deadlock situations and recover from them. Fault-tolerant Compressionless Routing (FCR) extends Compressionless Routing to support end-to-end fault-tolerant delivery. Detailed routing algorithms, implementation complexity and performance simulation results for CR and FCR are presented. CR has the following advantages: deadlock-free adaptive routing in torus networks with no virtual channels, simple router designs, order-preserving message transmission, applicability to a wide variety of network topologies, and elimination of the need for buffer allocation messages. FCR has the following advantages: tolerates transient faults while maintaining data integrity (nonstop fault-tolerance), tolerates permanent faults, can be applied to a wide variety of network topologies, and eliminates the need for software buffering and retry for reliability. These advantages of CR and FCR not only simplify hardware support for adaptive routing and fault-tolerance, they also can simplify communication software layers",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=288141,no,undetermined,0
The Replica Management System: a scheme for flexible and dynamic replication,"The actual gains achieved by replication are a complex function of the number of replicas, the placement of those replicas, the replication protocol, the nature of the transactions performed on the replicas, and the availability and performance characteristics of the machines and networks composing the system. This paper describes the design and implementation of the Replica Management System, which allows a programmer to specify the quality of service required for replica groups in terms of availability and performance. From the quality of service specification, information about the replication protocol to be used, and data about the characteristics of the underlying distributed system, the RMS computes an initial placement and replication level. As machines and communications systems are detected to have failed or recovered, or performance characteristics change, the RMS can be re-invoked to compute an updated mapping of replicas which preserves the desired quality of service. The result is a flexible, dynamic and dependable replication system",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=289936,no,undetermined,0
Integrated maintainability analysis: a practical case study,"Working from preliminary time estimates and software expectations, a system for premature maintainability analysis and prediction for a parallel signal processing system was offered on schedule to the customer. Upon review, a mutual plan was agreed that further data was needed and would be obtained, throughout the fault isolation software development and formal testing. Early fault insertion data provided a new software design direction, physical time studies verified selection of the best tools, formal BIT diagnostic testing isolated some software corrections and provided significant data points along with the formal maintainability demonstration. Using the initial maintainability model as the controlling metric throughout the program, each time a change was observed that could effect one or more of the parameters in the model, a new prediction was calculated. These reiterative calculations, integrating the progression of data, provided a consistent measure of the teams progress and assured a final best estimate of the systems maintainability capability with a level of confidence acceptable to ourselves and our customer",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=291074,no,undetermined,0
Modeling the relationship between source code complexity and maintenance difficulty,"Canonical correlation analysis can be a useful exploratory tool for software engineers who want to understand relationships that are not directly observable and who are interested in understanding influences affecting past development efforts. These influences could also affect current development efforts. In this paper, we restrict our findings to one particular development effort. We do not imply that either the weights or the loadings of the relations generalize to all software development efforts. Such generalization is untenable, since the model omitted many important influences on maintenance difficulty. Much work remains to specify subsets of indicators and development efforts for which the technique becomes useful as a predictive tool. Canonical correlation analysis is explained as a restricted form of soft modeling. We chose this approach not only because the terminology and graphical devices of soft modeling allow straightforward high-level explanations, but also because we are interested in the general method. The general method allows models involving many latent variables having interdependencies. It is intended for modeling complex interdisciplinary systems having many variables and little established theory. Further, it incorporates parameter estimation techniques relying on no distributional assumptions. Future research will focus on developing general soft models of the software development process for both exploratory analysis and prediction of future performance.<<ETX>>",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=312036,no,undetermined,0
Using metrics to manage software projects,"Five years ago, Bull's Enterprise Servers Operation in Phoenix, Arizona, used a software process that, although understandable, was unpredictable in terms of product quality and delivery schedule. The process generated products with unsatisfactory quality levels and required significant extra effort to avoid major schedule slips. All but the smallest software projects require metrics for effective project management. Hence, as part of a program designed to improve the quality, productivity, and predictability of software development projects, the Phoenix operation launched a series of improvements in 1989. One improvement based software project management on additional software measures. Another introduced an inspection program, since inspection data was essential to project management improvements. Project sizes varied from several thousand lines of code (KLOC) to more than 300 KLOC. The improvement projects enhanced quality and productivity. In essence, Bull now has a process that is repeatable and manageable, and that delivers higher quality products at lower cost. We describe the metrics we selected and implemented, illustrating with examples drawn from several development projects.<<ETX>>",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=312035,no,undetermined,0
A fast method for determining electrical and mechanical qualities of induction motors using a PC-aided detector,"A unique computer-aided detector is presented that determines the mechanical and electrical qualities of single-phase and three-phase induction motors. Measurements of voltage, current, power factor, active power, and reactive power as well as evaluations of centrifugal switch and airgap eccentricity, i.e., misalignment between rotor and stator, of induction motors can be rapidly conducted, and test results are available on the screen within a few seconds. The feasibility and usefulness of this detector are impressively demonstrated through application examples",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=310178,no,undetermined,0
Validating metrics for ensuring Space Shuttle flight software quality,"In this article, we cover the validation of software quality metrics for the Space Shuttle. Experiments with Space Shuttle flight software show that the Boolean OR discriminator function can successfully validate metrics for controlling and predicting quality. Further, we found that statement count and node count are the metrics most closely associated with the discrepancy reports count, and that with data smoothing their critical values can be used as predictors of software quality. We are continuing our research and comparing validated metrics with actual results in an attempt to determine whether the use of additional metrics provides sufficiently greater discriminative power to justify the increased inspection costs.<<ETX>>",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=303613,no,undetermined,0
Software process evolution at the SEL,"The Software Engineering Laboratory of the National Aeronautics and Space Administration's Goddard Space Flight Center has been adapting, analyzing, and evolving software processes for the last 18 years (1976-94). Their approach is based on the Quality Improvement Paradigm, which is used to evaluate process effects on both product and people. The authors explain this approach as it was applied to reduce defects in code. In examining and adapting reading techniques, we go through a systematic process of evaluating the candidate process and refining its implementation through lessons learned from previous experiments and studies. As a result of this continuous, evolutionary process, we determined that we could successfully apply key elements of the cleanroom development method in the SEL environment, especially for projects involving fewer than 50000 lines of code (all references to lines of code refer to developed, not delivered, lines of code). We saw indications of lower error rates, higher productivity, a more complete and consistent set of code comments, and a redistribution of developer effort. Although we have not seen similar reliability and cost gains for larger efforts, we continue to investigate the cleanroom method's effect on them.<<ETX>>",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=300090,no,undetermined,0
Bootstrap: fine-tuning process assessment,"Bootstrap was a project done as part of the European Strategic Program for Research in Information Technology. Its goal was to develop a method for software-process assessment, quantitative measurement, and improvement. In executing that goal, Bootstrap enhanced and refined the Software Engineering Institute's process-assessment method and adapted it to the needs of the European software industry-including nondefense sectors like banking, insurance, and administration. This adaptation provided a method that could be applied to a variety of software-producing units, small to medium software companies or departments that produce software within a large company. Although the Bootstrap project completed in 1993, its attribute-based method for assessing process maturity continues to evolve. The authors describe the elements of the method, how it can be used to determine readiness for ISO 9001 certification, and how it was applied in two instances to substantially improve processes.<<ETX>>",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=300080,no,undetermined,0
Achieving higher SEI levels,"Two years or more can pass between formal SEI (Software Engineering Institute) assessments using the Capability Maturity Model (CMM). An organization seeking to monitor its progress to a higher SEI level needs a method for internally conducting incremental assessments. The author provides one that has proven successful at Motorola. A method was developed for assessing progress to higher SEI levels that lets engineers and managers evaluate an organization's current status relative to the CMM and identify weak areas for immediate attention and improvement. This method serves as an effective means to ensure continuous process improvement as well as grassroots participation and support in achieving higher maturity levels. This progress-assessment process is not intended as a replacement for any formal assessment instruments developed by the SEI, but rather as an internal tool to help organizations prepare for a formal SEI assessment. Although the author provides examples in terms of CMM version 1.1, both the self-evaluation instrument and the progress-assessment process are generic enough for use with any (similar) later version of the SEI CMM by updating the worksheets and charts used.<<ETX>>",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=300079,no,undetermined,0
Collaborative modeling and visualization: an EPA HPCC initiative,The transfer of high-performance computing and visualization technologies to state environmental protection agencies is an integral part of the US Environmental Protection Agency's High-Performance Computing and Communications (HPCC) Program. One aspect of this process involves collaborative modeling for air quality efforts associated with the 1990 Clean Air Act. Another is the use of visualization as a diagnostic tool to depict modeling scenarios for various pollution control strategies. A fundamental step in this process involves using the existing Internet and the Nationwide Information Infrastructure (NII) for sharing data and electronic communication. Another step includes the development of visualization tools and user-friendly interfaces for collaborative exploration of environmental data sets.<<ETX>>,1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=299419,no,undetermined,0
A programmer performance measure based on programmer state transitions in testing and debugging process,"To organize and manage software development teams, it as important to evaluate the capability of each programmer based on reliable and easily collected data. We present a system which automatically monitors programmer activities, and propose a programmer debugging performance measure based on data monitored by the system. The system automatically categorizes programmer activity in real time into three types (compilation, program execution, and program modification) by monitoring and analyzing key strokes of a programmer. The resulting outputs are the time sequences of monitored activities. The measure we propose is the average length of debugging time per fault, D, estimated from the data sequences monitored by the system. To estimate the debugging time per fault, we introduce a testing and debugging process model. The process model has parameters associated with the average length of a program modification, d, and the probability of a fault being fixed completely by a program modification, r. By taking account of r as well as d, the debugging time per fault can be estimated with high accuracy. The model parameters, such as d and r, are computed from the monitored data sequences by using a maximum likelihood estimation method",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=296772,no,undetermined,0
An experiment to assess different defect detection methods for software requirements inspections,"Software requirements specifications (SRS) are usually validated by inspections, in which several reviewers read all or part of the specification and search for defects. We hypothesize that different methods for conducting these searches may have significantly different rates of success. Using a controlled experiment, we show that a scenario-based detection method, in which each reviewer executes a specific procedure to discover a particular class of defects has a higher defect detection rate than either ad hoc or checklist methods. We describe the design, execution and analysis of the experiment so others may reproduce it and test our results for different kinds of software developments and different populations of software engineers",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=296770,no,undetermined,0
Checkpointing SPMD applications on transputer networks,"Providing fault-tolerance for parallel/distributed applications is a problem of paramount importance, since the overall failure rate of the system increases with the number of processors, and the failure of just one processor can lend to the complete crash of the program. Checkpointing mechanisms are a good candidate to provide the continuity of the applications in the occurrence of failures. In this paper, we present an experimental study of several variations of checkpointing for SPMD (single process, multiple data) applications. We used a typical benchmark to experimentally assess the overhead, advantages and limitations of each checkpointing scheme",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=296709,no,undetermined,0
Certification of software components,"Reuse is becoming one of the key areas in dealing with the cost and quality of software systems. An important issue is the reliability of the components, hence making certification of software components a critical area. The objective of this article is to try to describe methods that can be used to certify and measure the ability of software components to fulfil the reliability requirements placed on them. A usage modelling technique is presented, which can be used to formulate usage models for components. This technique will make it possible not only to certify the components, but also to certify the system containing the components. The usage model describes the usage from a structural point of view, which is complemented with a profile describing the expected usage in figures. The failure statistics from the usage test form the input of a hypothesis certification model, which makes it possible to certify a specific reliability level with a given degree of confidence. The certification model is the basis for deciding whether the component can be accepted, either for storage as a reusable component or for reuse. It is concluded that the proposed method makes it possible to certify software components, both when developing for and with reuse",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=295896,no,undetermined,0
Optimal design of large software-systems using N-version programming,"Fault tolerant software uses redundancy to improve reliability; but such redundancy requires additional resources and tends to be costly, therefore the redundancy level needs to be optimized. Our optimization models determine the optimal level of redundancy within a software system under the assumption that functionally equivalent software components fail independently. A framework illustrates the tradeoff between the cost of using N-version programming and the improved reliability for a software system. The 2 models deal with: a single task, and multitask software. These software systems consist of several modules where each module performs a subtask and, by sequential execution of modules, a major task is performed. Major assumptions are: 1) several versions of each module, each with an estimated cost and reliability, are available, 2) these module versions fail independently. Optimization models are used to select the optimal set of versions for each module such that the system reliability is maximized and total cost remains within budget",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=295021,no,undetermined,0
Fuzzy methods for multisensor data fusion,"This paper presents a new general framework for information fusion based on fuzzy methods. In the proposed architecture, feature extraction and feature fusion are performed by adopting a class of (possibly multidimensional) membership functions which take care of the possibility and admissibility of the feature itself. Test cases of one-dimensional and image data fusion are presented",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=293435,no,undetermined,0
Real-time communication in FDDI-based reconfigurable networks,"We report our ongoing research in real-time communication with FDDI-based reconfigurable networks. The original FDDI architecture was enhanced in order to improve its fault-tolerance capability while a scheduling methodology, including message assignment, bandwidth allocation, and bandwidth management is developed to support real-time communication. As a result, message deadlines are guaranteed even in the event of network faults",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=292562,no,undetermined,0
Fuzzy Weibull for risk analysis,"Reliability and the life of components are frequently prime safety considerations. Extensive qualitative analysis employing probabilistic risk assessment has been widely performed to minimize hazards or accidents. Weibull probability data and information is a vital tool of quantitative risk assessments, but so are qualitative methods such as fault tree analysis. Qualitative aspects of product risk are subjective and contain many uncertainties. Most risk analyses do not have a means of dealing with uncertainty. Fuzzy set theoretical methods deal with supporting reasoning with uncertainty efficiently and conclusively. A unique fuzzy logic method employing Weibulls to represent membership functions for a set of fuzzy values along with â€œcrispâ€?values has been developed for addressing uncertainties. The paper describes a type of AI software for risk analysis. A complete description is presented with parallels to previous methods. The model discussed is called fuzzy fault tree (FFT). It employs â€œfuzzy Weibullâ€?membership functions which have been demonstrated in a working prototype. The prototype system which provides maximum utility in minimizing risk uncertainties is programmed in C for Apple Macintosh platforms. The results validate this application of fuzzy logic to qualitative risk assessment modeling, and lend credibility to the validity of the approach. The fuzzy Weibulls used in the modeling process perform quite well. Before developing FFT into an operational system, several calibration trials with a variety of risk assessment problems will be attempted",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=291151,no,undetermined,0
Achieving reliability growth on real-time systems,"This paper addresses the principles used to predict and attain reliability growth on real-time systems. System reliability modeling techniques that include software reliability, maintenance effectiveness, and failure recovery are discussed in detail. Several software reliability growth models are discussed with emphasis on measured reliability growth of fielded software. The impact of maintenance effectiveness, which is a measure of the maintainer's skill and training levels, is shown. The need to develop and measure the robustness of failure recovery algorithms is emphasized in this paper. All of these factors are combined with the failure and repair characteristics of hardware to create comprehensive reliability growth models for real-time systems. Through the authors' research, they have determined that effective failure recovery algorithms are the key to attaining highly reliable systems. Without them, redundant computer systems that run banking and air traffic control systems will come crashing down with possibly disastrous results. The modeling and measurement techniques discussed in this paper provide the reliability practitioner with the methods to predict and achieve reliability growth resulting from improved software reliability and recovery algorithms. A fault tolerant system's ability to recover from hardware and software failures is gauged by a parameter called coverage. Coverage is the conditional probability of recovery given that a failure has occurred. Because of its huge impact on system reliability, the measurement of coverage is emphasized",1994,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=291096,no,undetermined,0
A distributed safety-critical system for real-time train control,An architecture and methodology for executing a train control application in an ultra-safe manner is presented in this paper. Prior work in advanced train control systems are summarized along with their assumptions and drawbacks. A flexible architecture that allows fault-tolerant and fail-safe operation is presented for a distributed control system. A safety assurance technique which detects errors in software and hardware for simplex systems is presented in this paper. Performance results important to real-time control are obtained from modeling and simulating the architecture. An experimental prototype implementing the architecture and safety assurance technique is described together with experimental results,1995,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=483824,no,undetermined,0
Sensors signal processing using neural networks,"The authors proposed to use neural networks (single- and multi-layer perceptrons) for the prediction of measurement channel components (including sensors). This allows one to improve the processing quality of measurement information and the design of intelligent sensing instrumentation systems. The hardware and software of such systems, computer power distribution and processing algorithms are considered",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=820846,no,undetermined,0
Forecasting with fuzzy neural networks: a case study in stock market crash situations,"Neural networks have been used for forecasting purposes for some years now. The problem of the black-box approach often arises, i.e., after having trained neural networks to a particular problem, it is almost impossible to analyse them for how they work. Fuzzy neuronal networks allow one to add rules to neural networks. This avoids the black-box-problem. Additionally they are supposed to have a higher prediction precision in different situations. A case study describes a comparison of fuzzy neural networks and the classical approach during the stock market crashes of 1987 and 1998. It can be found that rules generate a more stable prediction quality, while the performance is not as good as when using classical neural networks",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=781726,no,undetermined,0
Does your result checker really check?,"A result checker is a program that checks the output of the computation of the observed program for correctness. Introduced originally by Blum, the result-checking paradigm has provided a powerful platform assuring the reliability of software. However, constructing result checkers for most problems requires not only significant domain knowledge but also ingenuity and can be error prone. In this paper we present our experience in validating result checkers using formal methods. We have conducted several case studies in validating result checkers from the commercial LEDA system for combinatorial and geometric computing. In one of our case studies, we detected a logical error in a result checker for a program computing max flow of a graph.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1311909,no,undetermined,0
Using simulation to empirically investigate test coverage criteria based on statechart,"A number of testing strategies have been proposed using state machines and statecharts as test models in order to derive test sequences and validate classes or class clusters. Though such criteria have the advantage of being systematic, little is known on how cost effective they are and how they compare to each other. This article presents a precise simulation and analysis procedure to analyze the cost-effectiveness of statechart-based testing techniques. We then investigate, using this procedure, the cost and fault detection effectiveness of adequate test sets for the most referenced coverage criteria for statecharts on three different representative case studies. Through the analysis of common results and differences across studies, we attempt to draw more general conclusions regarding the costs and benefits of using the criteria under investigation.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1317431,no,undetermined,0
Computing depth under ambient illumination using multi-shuttered light,"Range imaging has become a critical component of many computer vision applications. The quality of the depth data is of critical importance, but so is the need for speed. Shuttered light-pulse (SLP) imaging uses active illumination hardware to provide high quality depth maps at video frame rates. Unfortunately, current analytical models for deriving depth from SLP imagers are specific to the number of shutters and have a number of deficiencies. As a result, depth estimation often suffers from bias due to object reflectivity, incorrect shutter settings, or strong ambient illumination such as that encountered outdoors. These limitations make SLP imaging unsuitable for many applications requiring stable depth readings. This paper introduces a method that is general to any number of shutters. Using three shutters, the new method produces invariant estimates under changes in ambient illumination, producing high quality depth maps in a wider range of situations.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1315169,no,undetermined,0
Probability models for high dynamic range imaging,"Methods for expanding the dynamic range of digital photographs by combining images taken at different exposures have recently received a lot of attention. Current techniques assume that the photometric transfer function of a given camera is the same (modulo an overall exposure change) for all the input images. Unfortunately, this is rarely the case with today's camera, which may perform complex nonlinear color and intensity transforms on each picture. In this paper, we show how the use of probability models for the imaging system and weak prior models for the response functions enable us to estimate a different function for each image using only pixel intensity values. Our approach also allows us to characterize the uncertainty inherent in each pixel measurement. We can therefore produce statistically optimal estimates for the hidden variables in our model representing scene irradiance. We present results using this method to statistically characterize camera imaging functions and construct high-quality high dynamic range (HDR) images using only image pixel information.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1315160,no,undetermined,0
ANN application in electronic diagnosis-preliminary results,"In this paper artificial neural networks (ANNs) are applied to diagnosis of catastrophic defects in a linear analog circuit. In fact, today the technical diagnosis is great challenge for design engineers because the diagnostic problem is generally underdeterminate. It is also a deductive process with one set of data creating, in general, unlimited number of hypotheses among which one should try to find the solution. So, the diagnosis methods are mostly based on proprietary knowledge and personal experience, although they were built into integrated diagnostic equipment. ANN approach is proposed here as an alternative to existing solutions, based on the fact that ANNs are expected to encompass all phases of the diagnostic process: symptom detection, hypothesis generation, and hypothesis discrimination. The approach is demonstrated on the example of a simple resistive electrical circuit, and the generalization property is shown by supplying noisy data to ANNs inputs during diagnosis.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1314898,no,undetermined,0
Scalable network assessment for IP telephony,Multimedia applications such as IP telephony are among the applications that demand strict quality of service (QoS) guarantees from the underlying data network. At the predeployment stage it is critical to assess whether the network can handle the QoS requirements of IP telephony and fix problems that may prevent a successful deployment. In this paper we describe a technique for efficiently assessing network readiness for IP telephony. Our technique relies on understanding link level QoS behavior in a network from an IP telephony perspective. We use network topology and end-to-end measurements collected from the network in locating the sources of performance problems that may prevent a successful IP telephony deployment. We present an empirical study conducted on a real network spanning three geographically separated sites of an enterprise network. The empirical results indicate that our approach efficiently and accurately pinpoints links in the network incurring the most significant delay.,2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1312762,no,undetermined,0
A scalable distributed QoS multicast routing protocol,"Many Internet multicast applications such as teleconferencing and remote diagnosis have quality-of-service (QoS) requirements. It is a challenging task to build QoS constrained multicast trees with high performance, high success ratio, low overhead, and low system requirements. This paper presents a new scalable QoS multicast routing protocol (SoMR) that has very small communication overhead and requires no state outside the multicast tree. SoMR achieves the favorable tradeoff between routing performance and overhead by carefully selecting the network sub-graph in which it conducts the search for a path that can support the QoS requirement, and by auto-tuning the selection according to the current network conditions. Its early-warning mechanism helps to detect and route around the real bottlenecks in the network, which increases the chance of finding feasible paths for additive QoS requirements. SoMR minimizes the system requirements; it relies only on the local state stored at each router. The routing operations are completely decentralized.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1312682,no,undetermined,0
QoS of timeout-based self-tuned failure detectors: the effects of the communication delay predictor and the safety margin,"Unreliable failure detectors have been an important abstraction to build dependable distributed applications over asynchronous distributed systems subject to faults. Their implementations are commonly based on timeouts to ensure algorithm termination. However, for systems built on the Internet, it is hard to estimate this time value due to traffic variations. Thus, different types of predictors have been used to model this behavior and make predictions of delays. In order to increase the quality of service (QoS), self-tuned failure detectors dynamically adapt their timeouts to the communication delay behavior added of a safety margin. In this paper, we evaluate the QoS of a failure detector for different combinations of communication delay predictors and safety margins. As the results show, to improve the QoS, one must consider the relation between the pair predictor/margin, instead of each one separately. Furthermore, performance and accuracy requirements should be considered for a suitable relationship.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1311946,no,undetermined,0
Availability measurement and modeling for an application server,"Application server is a standard middleware platform for deploying Web-based business applications which typically require the underlying platform to deliver high system availability and to minimize loss of transactions. This paper presents a measurement-based availability modeling and analysis for a fault tolerant application server system - Sun Java System Application Server, Enterprise Edition 7. The study applies hierarchical Markov reward modeling techniques on the target software system. The model parameters are conservatively estimated from lab or field measurements. The uncertainty analysis method is used on the model to obtain average system availability and confidence intervals by randomly sampling from possible ranges of parameters that cannot be accurately measured in limited time frames or may vary widely in customer sites. As demonstrated in this paper, the combined use of lab measurement, analytical modeling, and uncertainty analysis is a useful evaluation approach which can provide a conservative availability assessment at stated confidence levels for a new software product.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1311937,no,undetermined,0
An intelligent admission control scheme for next generation wireless systems using distributed genetic algorithms,"A different variety of services requiring different levels of quality of service (QoS) need be addressed for mobile users of the next generation wireless system (NGWS). An efficient handoff technique with intelligent admission control can accomplish this aim. In this paper, a new, intelligent handoff scheme using distributed genetic algorithms (DGA) is proposed for NGWS. This scheme uses DGA to achieve high network utilization, minimum cost and handoff latency. A performance analysis is provided to assess the efficiency of the proposed DGA scheme. Simulation results show a significant improvement in handoff latencies and costs over traditional genetic algorithms and other admission control schemes.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1311817,no,undetermined,0
Making resource decisions for software projects,"Software metrics should support managerial decision making in software projects. We explain how traditional metrics approaches, such as regression-based models for cost estimation fall short of this goal. Instead, we describe a causal model (using a Bayesian network) which incorporates empirical data, but allows it to be interpreted and supplemented using expert judgement. We show how this causal model is used in a practical decision-support tool, allowing a project manager to trade-off the resources used against the outputs (delivered functionality, quality achieved) in a software project. The model and toolset have evolved in a number of collaborative projects and hence capture significant commercial input. Extensive validation trials are taking place among partners on the EC funded project MODIST (this includes Philips, Israel Aircraft Industries and QinetiQ) and the feedback so far has been very good. The estimates are sensible and the causal modelling approach enables decision-makers to reason in a way that is not possible with other project management and resource estimation tools. To ensure wide dissemination and validation a version of the toolset with the full underlying model is being made available for free to researchers.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1317462,no,undetermined,0
Requirements driven software evolution,"Software evolution is an integral part of the software life cycle. Furthermore in the recent years the issue of keeping legacy systems operational in new platforms has become critical and one of the top priorities in IT departments worldwide. The research community and the industry have responded to these challenges by investigating and proposing techniques for analyzing, transforming, integrating, and porting software systems to new platforms, languages, and operating environments. However, measuring and ensuring that compliance of the migrant system with specific target requirements have not been formally and thoroughly addressed. We believe that issues such as the identification, measurement, and evaluation of specific re-engineering and transformation strategies and their impact on the quality of the migrant system pose major challenges in the software re-engineering community. Other related problems include the verification, validation, and testing of migrant systems, and the design of techniques for keeping various models (architecture, design, source code) during evolution, synchronized. In this working session, we plan to assess the state of the art in these areas, discuss on-going work, and identify further research issues.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1311070,no,undetermined,0
Impact of process variation phenomena on performance and quality assessment,"Summary form only given. Logic product density and performance trends have continued to follow the course predicted by Moore's Law. To support the trends in the future and build logic products approaching one billion or more transistors before the end of the decade, several challenges must be met. These challenges include: 1) maintaining transistor/interconnect feature scaling, 2) the increasing power density dilemma, 3) increasing relative difficulty of 2-D feature resolution and general critical dimension control, 4) identifying cost effective solutions to increasing process and design database complexity, and 5), improving general performance and quality predictability in the face of the growing control, complexity and predictability issues. The trend in transistor scaling can be maintained while addressing the power density issue with new transistor structures, design approaches, and product architectures (e.g. high-k, metal gate, etc.). Items 3 to 5 are the focus of this work and are also strongly inter-related. The general 2-D patterning and resolution control problems will require several solution approaches both through design and technology e.g. reduce design degrees of freedom, use of simpler arrayed structures, improved uniformity, improved tools, etc. The data base complexity/cost problem will require solutions likely to involve use of improved data structure, improved use of hierarchy, and improved software and hardware solutions. Performance assessment, predictability and quality assessment will benefit from solutions to the control and complexity issues noted above. In addition, new design techniques/tools as well as improved process characterization models and methods can address the general performance/quality assessment challenge.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1309897,no,undetermined,0
"Requirements triage: what can we learn from a ""medical"" approach?","New-product development is commonly risky, judging by the number of high-profile failures that continue to occur-especially in software engineering. We can trace many of these failures back to requirements-related issues. Triage is a technique that the medical profession uses to prioritize treatment to patients on the basis of their symptoms' severity. Trauma triage provides some tantalizing insights into how we might measure risk of failure early, quickly, and accurately. For projects at significant risk, we could activate a ""requirements trauma system"" to include specialists, processes, and tools designed to correct the issues and improve the probability that the project ends successfully. We explain these techniques and suggest how we can adapt them to help identify and quantify requirements-related risks.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1309651,no,undetermined,0
Model-driven reverse engineering,"Reverse engineering is the process of comprehending software and producing a model of it at a high abstraction level, suitable for documentation, maintenance, or reengineering. But from a manager's viewpoint, there are two painful problems: 1) It's difficult or impossible to predict how much time reverse engineering will require. 2) There are no standards to evaluate the quality of the reverse engineering that the maintenance staff performs. Model-driven reverse engineering can overcome these difficulties. A model is a high-level representation of some aspect of a software system. MDRE uses the features of modeling technology but applies them differently to address the maintenance manager's problems. Our approach to MDRE uses formal specification and automatic code generation to reverse the reverse-engineering process. Models written in a formal specification language called SLANG describe both the application domain and the program being reverse engineered, and interpretations annotate the connections between the two. The ability to generate a similar version of a program gives managers a fixed target for reverse engineering. This, in turn, enables better effort prediction and quality evaluation, reducing development risk.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1309646,no,undetermined,0
The use of unified APC/FD in the control of a metal etch area,"An adaptive neural network-based advanced process control software, the Dynamic Neural Controllerâ„?(DNC), was employed at National Semiconductor's 200 mm fabrication facility, South Portland, Maine, to enhance the performance of metal etch tools. The installation was performed on 5 identical LAM 9600 TCP Metal etchers running production material. The DNC produced a single predictive model on critical outputs and metrology for each tool based on process variables, maintenance, input metrology and output metrology. Although process metrology is usually measured on only one wafer per lot, the process can be closely monitored on a wafer-by-wafer basis with the DNC models. The DNC was able to provide recommendations for maintenance (replacing components in advance of predicted failure) and process variable adjustments (e.g. gas flow) to maximize tool up time and to reduce scrap. This enabled the equipment engineers to both debug problems more quickly on the tool and to make adjustments to tool parameters before out-of-spec wafers were produced. After a comparison of the performance of all 5 tools for a 2-month period prior to DNC installation vs. a 2-month post-DNC period, we concluded that the software was able to predict when maintenance actions were required, when process changes were required, and when maintenance actions were being taken but were not required. We observed a significant improvement in process C<sub>pk</sub>s for the metal etchers in this study.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1309573,no,undetermined,0
Triage: performance isolation and differentiation for storage systems,"Ensuring performance isolation and differentiation among workloads that share a storage infrastructure is a basic requirement in consolidated data centers. Existing management tools rely on resource provisioning to meet performance goals; they require detailed knowledge of the system characteristics and the workloads. Provisioning is inherently slow to react to system and workload dynamics, and in the general case, it is impossible to provision for the worst case. We propose a software-only solution that ensures predictable performance for storage access. It is applicable to a wide range of storage systems and makes no assumptions about workload characteristics. We use an on-line feedback loop with an adaptive controller that throttles storage access requests to ensure that the available system throughput is shared among workloads according to their performance goals and their relative importance. The controller considers the system as a ""black box"" and adapts automatically to system and workload changes. The controller is distributed to ensure high availability under overload conditions, and it can be used for both block and file access protocols. The evaluation of Triage, our experimental prototype, demonstrates workload isolation and differentiation, in an overloaded cluster file-system where workloads and system components are changing.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1309358,no,undetermined,0
Exact analysis of a class of GI/G/1-type performability models,"We present an exact decomposition algorithm for the analysis of Markov chains with a GI/G/1-type repetitive structure. Such processes exhibit both M/G/1-type & GI/M/1-type patterns, and cannot be solved using existing techniques. Markov chains with a GI/G/1 pattern result when modeling open systems which accept jobs from multiple exogenous sources, and are subject to failures & repairs; a single failure can empty the system of jobs, while a single batch arrival can add many jobs to the system. Our method provides exact computation of the stationary probabilities, which can then be used to obtain performance measures such as the average queue length or any of its higher moments, as well as the probability of the system being in various failure states, thus performability measures. We formulate the conditions under which our approach is applicable, and illustrate it via the performability analysis of a parallel computer system.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1308668,no,undetermined,0
Classification of power quality events using optimal time-frequency representations-Part 1: theory,"Better software and hardware for automatic classification of power quality (PQ) disturbances are desired for both utilities and commercial customers. Existing automatic recognition methods need improvement in terms of their capability, reliability, and accuracy. This paper presents the theoretical foundation of a new method for classifying voltage and current waveform events that are related to a variety of PQ problems. The method is composed of two sequential processes: feature extraction and classification. The proposed feature extraction tool, time-frequency ambiguity plane with kernel techniques, is new to the power engineering field. The essence of the feature exaction is to project a PQ signal onto a low-dimension time-frequency representation (TFR), which is deliberately designed for maximizing the separability between classes. The technique of designing an optimized TFR from time-frequency ambiguity plane is for the first time applied to the PQ classification problem. A distinct TFR is designed for each class. The classifiers include a Heaviside-function linear classifier and neural networks with feedforward structures. The flexibility of this method allows classification of a very broad range of power quality events. The performance validation and hardware implementation of the proposed method are presented in the second part of this two-paper series.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1308384,no,undetermined,0
Bi-criteria models for all-uses test suite reduction,"Using bi-criteria decision making analysis, a new model for test suite minimization has been developed that pursues two objectives: minimizing a test suite with regard to a particular level of coverage while simultaneously maximizing error detection rates. This new representation makes it possible to achieve significant reductions in test suite size without experiencing a decrease in error detection rates. Using the all-uses inter-procedural data flow testing criterion, two binary integer linear programming models were evaluated, one a single-objective model, the other a weighted-sums bi-criteria model. The applicability of the bi-criteria model to regression test suite maintenance was also evaluated. The data show that minimization based solely on definition-use association coverage may have a negative impact on the error detection rate as compared to minimization performed with a bi-criteria model that also takes into account the ability of test cases to reveal error. Results obtained with the bi-criteria model also indicate that test suites minimized with respect to a collection of program faults are effective at revealing subsequent program faults.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1317433,no,undetermined,0
Quality of service provisioning for VoIP applications with policy-enabled differentiated services,"The recent advancement in network technologies has made it possible for the convergence of voice and data networks. In particular, the differentiated services (DiffServ) model has helped in bridging the diverse performance requirements between voice and non real-time applications such as file transfer and e-mail. One key feature of the DiffServ model is its promise to bring scalable service discrimination to the Internet and intranets. With the heterogeneous nature of today's distributed systems, however, there is a demand for flexible management systems that can cope with not only changes in resource needs and the perceived quality of applications (which is often user-dependent and application-specific), but also changes in the configuration of the distributed environment. Policy-based management has been proposed as a means of providing dynamic change in the behavior of applications and systems at run-time rather than through reengineering. The paper proposes a policy-enabled DiffServ architecture to provide predictable and measurable quality of service (QoS) for voice over Internet Protocol (VoIP) applications, and illustrates its performance.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1317671,no,undetermined,0
Are found defects an indicator of software correctness? An investigation in a controlled case study,"In quality assurance programs, we want indicators of software quality, especially software correctness. The number of found defects during inspection and testing are often used as the basis for indicators of software correctness. However, there is a paradox in this approach, since the remaining defects is what impacts negatively on software correctness, not the found ones. In order to investigate the validity of using found defects or other product or process metrics as indicators of software correctness, a controlled case study is launched. 57 sets of 10 different programs from the PSP course are assessed using acceptance test suites for each program. In the analysis, the number of defects found during the acceptance test are compared to the number of defects found during development, code size, share of development time spent on testing etc. It is concluded from a correlation analysis that 1) fewer defects remain in larger programs 2) more defects remain when larger share of development effort is spent on testing, and 3) no correlation exist between found defects and correctness. We interpret these observations as 1) the smaller programs do not fulfill the expected requirements 2) that large share effort spent of testing indicates a ""hacker"" approach to software development, and 3) more research is needed to elaborate this issue.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1383109,no,undetermined,0
Evaluating cognitive complexity measure with Weyuker properties,"Cognitive complexity measure is based on cognitive informatics, which in turn helps in comprehending the software characteristics. Weyuker properties must be satisfied by every complexity measure to qualify as a good and comprehensive one. An attempt has been made to evaluate cognitive complexity measure in terms of nine Weyuker properties, through examples. It has been found that eight of nine Weyuker properties have been satisfied by the cognitive weight software complexity measure and hence establishes the cognitive complexity as a well structured one.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1327464,no,undetermined,0
Distribution patterns of effort estimations,"Effort estimations within software development projects and the ability to work within these estimations are perhaps the single most important, and at the same time inadequately mastered, discipline for overall project success. This study examines some characteristics of accuracies in software development efforts and identifies patterns that can be used to increase the understanding of the effort estimation discipline as well as to improve the accuracy of effort estimations. The study complements current research by taking a more simplistic approach than usually found within mainstream research concerning effort estimations. It shows that there are useful patterns to be found as well as interesting causalities, usable to increase the understanding and effort estimation capability.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1333398,no,undetermined,0
A system's view of numerical wave propagation modeling for airborne radio communication systems,"An end-to-end simulation for an airborne communication system in the UHF range is performed; it describes the radio signal quality between an aircraft and a ground station. Effects like the installed performance behavior of antennas as well as multipath wave propagation are considered. All parts are implemented by using only commercially available software. The interfaces between each software tool are illuminated as well. All partial results are transformed to a system level simulation with MatLab. The calculated EM field strength is used to determine the bit error rate along a special flight path, which can be used to improve the data predictability for the overall system design. For the system level approach, much more understanding is given to systems engineering, and each sub aspect is given even more importance.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1331815,no,undetermined,0
Adaptive video streaming in vertical handoff: a case study,"Video streaming has become a popular form of transferring video over the Internet. With the emergence of mobile computing needs, a successful video streaming solution demands 1) uninterrupted services even with the presence of mobility and 2) adaptive video delivery according to current link properties. We study the need and evaluate the performance of adaptive video streaming in vertical handoff scenarios. We use universal seamless handoff architecture (USHA) to create a seamless handoff environment, and use the video transfer protocol (VTP) to adapt video streaming rates according to ""eligible rate estimates"". Using testbed measurements experiments, we verify the importance of service adaptation, as well as show the improvement of user-perceived video quality, via adapting video streaming in the vertical handoffs.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1331716,no,undetermined,0
System optimization with component reliability estimation uncertainty: a multi-criteria approach,"Summary & Conclusions-This paper addresses system reliability optimization when component reliability estimates are treated as random variables with estimation uncertainty. System reliability optimization algorithms generally assume that component reliability values are known exactly, i.e., they are deterministic. In practice, that is rarely the case. For risk-averse system design, the estimation uncertainty, propagated from the component estimates, may result in unacceptable estimation uncertainty at the system-level. The system design problem is thus formulated with multiple objectives: (1) to maximize the system reliability estimate, and (2) to minimize its associated variance. This formulation of the reliability optimization is new, and the resulting solutions offer a unique perspective on system design. Once formulated in this manner, standard multiple objective concepts, including Pareto optimality, were used to determine solutions. Pareto optimality is an attractive alternative for this type of problem. It provides decision-makers the flexibility to choose the best-compromise solution. Pareto optimal solutions were found by solving a series of weighted objective problems with incrementally varied weights. Several sample systems are solved to demonstrate the approach presented in this paper. The first example is a hypothetical series-parallel system, and the second example is the fault tolerant distributed system architecture for a voice recognition system. The results indicate that significantly different designs are obtained when the formulation incorporates estimation uncertainty. If decision-makers are risk averse, and wish to consider estimation uncertainty, previously available methodologies are likely to be inadequate.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1331680,no,undetermined,0
End-to-end defect modeling,"In this context, computer models can help us predict outcomes and anticipate with confidence. We can now use cause-effect modeling to drive software quality, moving our organization toward higher maturity levels. Despite missing good software quality models, many software projects successfully deliver software on time and with acceptable quality. Although researchers have devoted much attention to analyzing software projects' failures, we also need to understand why some are successful - within budget, of high quality, and on time-despite numerous challenges. Restricting software quality to defects, decisions made in successful projects must be based on some understanding of cause-effect relationships that drive defects at each stage of the process. To manage software quality by data, we need a model describing which factors drive defect introduction and removal in the life cycle, and how they do it. Once properly built and validated, a defect model enables successful anticipation. This is why it's important that the model include all variables influencing the process response to some degree.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1331312,no,undetermined,0
Measuring software product quality: a survey of ISO/IEC 9126,"To address the issues of software product quality, the Joint Technical Committee 1 of the International Organization for Standardization and International Electrotechnical Commission published a set of software product quality standards known as ISO/IEC 9126. These standards specify software product quality's characteristics and subcharacteristics and their metrics. Based on a user survey, this study of the standard helps clarity quality attributes and provides guidance for the resulting standards.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1331309,no,undetermined,0
Efficient multi-frame motion estimation algorithms for MPEG-4 AVC/JVT/H.264,"In H.264 standard, a lot of computational complexity is consumed in the encoder for motion estimation. It allows seven block sizes to perform the motion/compensation, and refers to previous five frames for searching the best motion vector to achieve lower bitrate and higher quality. Since the H.264 reference software uses the full search scheme to obtain the best performance, it spends a lot of searching time. In this paper we propose efficient searching algorithms by reuse of the motion vector information from the last reference frame. The proposed algorithms use the stored motion vectors to compose the current motion vector without performing the full search in each reference frame. Therefore, our proposed algorithms can obtain the speed up ratio 4 in average for encoding, which benefits from the prediction of the motion vector for reference frames in advance and maintain a good performance. Any fast search algorithm can be utilized to further largely reduce the computational load for the motion estimation from previous one frame.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1328852,no,undetermined,0
Direct digital synthesis: a tool for periodic wave generation (part 2),"Direct digital synthesis (DDS) is a useful tool for generating periodic waveforms. In this two-part article, the basic idea of this synthesis technique is presented and then focused on the quality of the sinewave a DDS can create, introducing the SFDR quality parameter. Next effective methods to increase the SFDR are presented through sinewave approximations, hardware schemes such as dithering and noise shaping, and an extensive list of reference. When the desired output is a digital signal, the signal's characteristics can be accurately predicted using the formulas given in this article. When the desired output is an analog signal, the reader should keep in mind that the performance of the DDS is eventually limited by the performance of the digital-to-analog converter and the follow-on analog filter. Hoping that this article would incite engineers to use DDS either in integrated circuits DDS or software-implemented DDS. From the author's experience, this technique has proven valuable when frequency resolution is the challenge, particularly when using low-cost microcontrollers.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1328096,no,undetermined,0
A window adaptive hybrid vector filter for color image restoration,"A novel window adaptive hybrid filter for the removal of different types of noise which contaminate color images is proposed. At each pixel location, the image vector is first classified into several different signal activity areas through a modified quadtree decomposition on the luminance of the input image, with only one empirical parameter required to be controlled. Then, an optimal window and weight adaptive vector filtering operation is activated for the best tradeoff between noise suppression and detail preservation. The proposed filter has demonstrated superior performance in suppressing several distinctive types of color image noise, which include Gaussian, impulse, and mixed noise. Significant improvements have been achieved in terms of standard objective measurements as well as the perceived image quality.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1326517,no,undetermined,0
A transparent and centralized performance management service for CORBA based applications,"The quest for service quality in enterprise applications is driving companies to profile their online performance. Application management tools come in handy to deliver the required diagnosis. However, distributed applications are hard to manage due to their complexity and geographical dispersion. To cope with this problem, this paper presents a Java based management solution for CORBA distributed applications. The solution combines XML, SNMP and portable interceptors to provide a nonintrusive performance management service. Components can be attached to client and server sides to monitor messages and gather data into a centralized database. A detailed analysis can then be performed to expose behavioral problems in specific parts of the application. Performance reports and charts are supplied through a Web console. A prototypical implementation was tested against two available ORB to assess functionality and interposed overhead.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1317684,no,undetermined,0
Improved quantization structures using generalized HMM modelling with application to wideband speech coding,"In this paper, a low-complexity, high-quality recursive vector quantizer based on a generalized hidden Markov model of the source is presented. Capitalizing on recent developments in vector quantization based on Gaussian mixture models, we extend previous work on HMM-based quantizers to the case of continuous vector-valued sources, and also formulate a generalization of the standard HMM. This leads us to a family of parametric source models with very flexible modelling capabilities, with which are associated low-complexity recursive quantization structures. The performance of these schemes is demonstrated for the problem of wideband speech spectrum quantization, and shown to compare favorably to existing state-of-the-art schemes.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1325947,no,undetermined,0
A collaborative operation method between new energy-type dispersed power supply and EDLC,"In this paper, the modified Euler-type moving average prediction (EMAP) model is proposed to operate a new energy type dispersed power supply system in autonomous mode. This dispersed power supply system consists of a large-scale photovoltaic system (PV) and a fuel cell, as well as an electric double-layer capacitor (EDLC). This system can meet the multi-quality electric power requirements of customers, and ensures voltage stability and uninterruptible power supply function as well. Each subsystem of this distributed power supply contributes to the above-mentioned system performance with its own excellent characteristics. Based on the collaborative operation methods by EMAP model, the required capacity of EDLC to compensate the fluctuation of both PV output and load demand is examined by the simulation using software MATLAB/Simulink, and, response characteristics of this system is confirmed with simulation by software PSIM.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1325299,no,undetermined,0
An incremental learning algorithm with confidence estimation for automated identification of NDE signals,"An incremental learning algorithm is introduced for learning new information from additional data that may later become available, after a classifier has already been trained using a previously available database. The proposed algorithm is capable of incrementally learning new information without forgetting previously acquired knowledge and without requiring access to the original database, even when new data include examples of previously unseen classes. Scenarios requiring such a learning algorithm are encountered often in nondestructive evaluation (NDE) in which large volumes of data are collected in batches over a period of time, and new defect types may become available in subsequent databases. The algorithm, named Learn++, takes advantage of synergistic generalization performance of an ensemble of classifiers in which each classifier is trained with a strategically chosen subset of the training databases that subsequently become available. The ensemble of classifiers then is combined through a weighted majority voting procedure. Learn++ is independent of the specific classifier(s) comprising the ensemble, and hence may be used with any supervised learning algorithm. The voting procedure also allows Learn++ to estimate the confidence in its own decision. We present the algorithm and its promising results on two separate ultrasonic weld inspection applications.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1324403,no,undetermined,0
"Design, implementation and performance of fault-tolerant message passing interface (MPI)","Fault tolerant MPI (FTMPI) enables fault tolerance to the MPICH, an open source GPL licensed implementation of MPI standard by Argonne National Laboratory's Mathematics and Computer Science Division. FTMPI is a transparent fault-tolerant environment, based on synchronous checkpointing and restarting mechanism. FTMPI relies on non-multithreaded single process checkpointing library to synchronously checkpoint an application process. Global replicated system controller and cluster node specific node controller monitors and controls check pointing and recovery activities of all MPI applications within the cluster. This work details the architecture to provide fault tolerance mechanism for MPI based applications running on clusters and the performance of NAS parallel benchmarks and parallelized medium range weather forecasting models, P-T80 and P-TI26. The architecture addresses the following issues also: Replicating system controller to avoid single point of failure. Ensuring consistency of checkpoint files based on distributed two phase commit protocol, and robust fault detection hierarchy.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1324026,no,undetermined,0
A framework for polysensometric multidimensional spatial visualization,"Typically any single sensor instrument suffers from physical/observation constraints. This paper discusses a generalized framework, called polymorphic visual information fusion framework (PVIF) that can enable information from multiple sensors to be fused and compared to gain broader understanding of a target of observation in multidimensional space. An automated software system supporting comparative cognition has been developed to form 3D models based on the datasets from different sensors, such as XPS and LSCM. This fusion framework not only provides an information engineering based tool to overcome the limitations of individual sensor's scope of observation but also provides a means where theoretical understanding surrounding a complex target can be mutually validated by comparative cognition about the object of interest and 3D model refinement. Some polysensometric data classification metrics are provided to measure the quality of input datasets for fusion visualization.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1323978,no,undetermined,0
Probabilistic regression suites for functional verification,"Random test generators are often used to create regression suites on-the-fly. Regression suites are commonly generated by choosing several specifications and generating a number of tests from each one, without reasoning which specification should he used and how many tests should he generated from each specification. This paper describes a technique for building high quality random regression suites. The proposed technique uses information about the probablity of each test specification covering each coverage task. This probability is used, in tun, to determine which test specifications should be included in the regression suite and how many tests should, be generated from each specification. Experimental results show that this practical technique can he used to improve the quality, and reduce the cost, of regression suites. Moreover, it enables better informed decisions regarding the size and distribution of the regression suites, and the risk involved.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1322436,no,undetermined,0
Fragment class analysis for testing of polymorphism in Java software,"Testing of polymorphism in object-oriented software may require coverage of all possible bindings of receiver classes and target methods at call sites. Tools that measure this coverage need to use class analysis to compute the coverage requirements. However, traditional whole-program class analysis cannot be used when testing incomplete programs. To solve this problem, we present a general approach for adapting whole-program class analyses to operate on program fragments. Furthermore, since analysis precision is critical for coverage tools, we provide precision measurements for several analyses by determining which of the computed coverage requirements are actually feasible for a set of subject components. Our work enables the use of whole-program class analyses for testing of polymorphism in partial programs, and identifies analyses that potentially are good candidates for use in coverage tools.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1321060,no,undetermined,0
Performance evaluation and failure rate prediction for the soft implemented error detection technique,"This paper presents two error models to evaluate safety of a software error detection method. The proposed models analyze the impact on program overhead in terms of memory code area and increased execution time when the studied error detection technique is applied. For faults affecting the processor's registers, analytic formulas are derived to estimate the failure rate before program execution. These formulas are based on probabilistic methods and use statistics of the program, which are collected during compilation. The studied error detection technique was applied to several benchmark programs and then program overhead and failure rate was estimated. Experimental results validate the estimated performances and show the effectiveness of the proposed evaluation formulas.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1319693,no,undetermined,0
Measuring the complexity of component-based system architecture,"This paper proposes a graph-based metric to evaluate complexity of the architecture by analyzing the dependencies between components of the system. Measuring the complexity is helpful during analyzing, testing, and maintaining the system. This measurement could direct the process of improvement and reengineering work. A complexity measure could also be used as a predictor of the effort that is needed to maintain the system. In component-based systems, functionalities are not performed within one component. Components communicate and share information in order to provide system functionalities. In our approach, to capture system architecture, we borrow Li's model, component dependency graph (CDG).",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1307902,no,undetermined,0
Semidefinite programming for ad hoc wireless sensor network localization,We describe an SDP relaxation based method for the position estimation problem in wireless sensor networks. The optimization problem is set up so as to minimize the error in sensor positions to fit distance measures. Observable gauges are developed to check the quality of the point estimation of sensors or to detect erroneous sensors. The performance of this technique is highly satisfactory compared to other techniques. Very few anchor nodes are required to accurately estimate the position of all the unknown nodes in a network. Also the estimation errors are minimal even when the anchor nodes are not suitably placed within the network or the distance measurements are noisy.,2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1307322,no,undetermined,0
Thinking about thinking aloud: a comparison of two verbal protocols for usability testing,"We report on an exploratory experimental comparison of two different thinking aloud approaches in a usability test that focused on navigation problems in a highly nonstandard Web site. One approach is a rigid application of Ericsson and Simon's (for original paper see Protocol Analysis: Verbal Reports as Data, MIT Press (1993)) procedure. The other is derived from Boren and Ramey's (for original paper see ibid., vol. 43, no. 3, p. 261-278 (2000)) proposal based on speech communication. The latter approach differs from the former in that the experimenter has more room for acknowledging (mm-hmm) contributions from subjects and has the possibility of asking for clarifications and offering encouragement. Comparing the verbal reports obtained with these two methods, we find that the process of thinking aloud while carrying out tasks is not affected by the type of approach that was used. The task performance does differ. More tasks were completed in the B and R condition, and subjects were less lost. Nevertheless, subjects' evaluations of the Web site quality did not differ, nor did the number of different navigation problems that were detected.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1303808,no,undetermined,0
Legacy software evaluation model for outsourced maintainer,"Outsourcing has become common practice in the software industry. Organizations routinely subcontract the maintenance of their software assets to specialized companies. A great challenge for these companies, is to rapidly evaluate the quality of the systems they will have to maintain so as to accurately estimate the amount of work they will require. To answer these concerns, we developed a framework of metrics to evaluate the complexity of a legacy software system and help an outsourcing maintainer define its contracts. This framework was defined using a well known approach in software quality, called ""goal-question-metric"". We present the goal-question-metric approach, its results, and the initial experimentation of the metrics on five real life systems in Cobol.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1281405,no,undetermined,0
Reliability and robustness assessment of diagnostic systems from warranty data,"Diagnostic systems are software-intensive built-in-test systems, which detect, isolate and indicate the failures of prime systems. The use of diagnostic systems reduces the losses due to the failures of prime systems and facilitates the subsequent correct repairs. Therefore, they have found extensive applications in industry. Without loss of generality, this paper utilizes the on-board diagnostic systems of automobiles as an illustrative example. A failed diagnostic system generates Î± or Î². Î± error incurs unnecessary warranty costs to manufacturers, while Î² error causes potential losses to customers. Therefore, the reliability and robustness of diagnostic systems are important to both manufacturers and customers. This paper presents a method for assessing the reliability and robustness of the diagnostic systems by using warranty data. We present the definitions of robustness and reliability of the diagnostic systems, and the formulae for estimating Î±, Î² and reliability. To utilize warranty data for assessment, we describe the two-dimensional (time-in-service and mileage) warranty censoring mechanism, model the reliability function of the prime systems, and devise warranty data mining strategies. The impact of Î± error on warranty cost is evaluated. Fault tree analyses for Î± and Î² errors are performed to identify the ways for reliability and robustness improvement. The method is applied to assess the reliability and robustness of an automobile on-board diagnostic system.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1285438,no,undetermined,0
Analysis of end-to-end QoS for networked virtual reality services in UMTS,"Virtual reality services may be considered a good representative of advanced services in the new-generation network. The focus of this article is to address quality of service support for VR services in the context of the UMTS QoS framework specified by the 3G standardization forum, the Third Generation Partnership Project. We propose a classification of VR services based on delivery requirements (real-time or non-real-time) and degree of interactivity that maps to existing UMTS QoS classes and service attributes. The mapping is based on matching VR service requirements to performance parameters and target values defined for UMTS applications. Test cases involving heterogeneous VR applications are defined, using as a reference a general model for VR service design and delivery. Measurements of network parameters serve to determine the end-to-end QoS requirements of the considered applications, which are in turn mapped to proposed VR service classes.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1284929,no,undetermined,0
Open design of networked power quality monitoring systems,Permanent continuous power quality monitoring is beginning to be recognized as an important aid for managing power quality. Preventive maintenance can only be initiated if such monitoring is available to detect the minor disturbances that may precede major disruptions. This paper establishes the need to encourage interoperability between power quality instruments from different vendors. It discusses the frequent problem of incompatibility between equipment that results from the inherent inflexibilities in existing designs. A new approach has been proposed to enhance interoperability through the use of open systems in their design. It is demonstrated that it is possible to achieve such open design using existing software and networking technologies. The benefits and disadvantages to both the end-users and the equipment manufacturers are also being discussed.,2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1284897,no,undetermined,0
Adding assurance to automatically generated code,"Code to estimate position and attitude of a spacecraft or aircraft belongs to the most safety-critical parts of flight software. The complex underlying mathematics and abundance of design details make it error-prone and reliable implementations costly. AutoFilter is a program synthesis tool for the automatic generation of state estimation code from compact specifications. It can automatically produce additional safety certificates which formally guarantee that each generated program individually satisfies a set of important safety policies. These safety policies (e.g., array-bounds, variable initialization) form a core of properties which are essential for high-assurance software. Here we describe the AutoFilter system and its certificate generator and compare our approach to the static analysis tool PolySpace.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1281768,no,undetermined,0
Unsupervised learning for expert-based software quality estimation,"Current software quality estimation models often involve using supervised learning methods to train a software quality classifier or a software fault prediction model. In such models, the dependent variable is a software quality measurement indicating the quality of a software module by either a risk-based class membership (e.g., whether it is fault-prone or not fault-prone) or the number of faults. In reality, such a measurement may be inaccurate, or even unavailable. In such situations, this paper advocates the use of unsupervised learning (i.e., clustering) techniques to build a software quality estimation system, with the help of a software engineering human expert. The system first clusters hundreds of software modules into a small number of coherent groups and presents the representative of each group to a software quality expert, who labels each cluster as either fault-prone or not fault-prone based on his domain knowledge as well as some data statistics (without any knowledge of the dependent variable, i.e., the software quality measurement). Our preliminary empirical results show promising potentials of this methodology in both predicting software quality and detecting potential noise in a software measurement and quality dataset.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1281739,no,undetermined,0
Reducing overfitting in genetic programming models for software quality classification,"A high-assurance system is largely dependent on the quality of its underlying software. Software quality models can provide timely estimations of software quality, allowing the detection and correction of faults prior to operations. A software metrics-based quality prediction model may depict overfitting, which occurs when a prediction model has good accuracy on the training data but relatively poor accuracy on the test data. We present an approach to address the overfitting problem in the context of software quality classification models based on genetic programming (GP). The problem has not been addressed in depth for GP-based models. The presence of overfitting in a software quality classification model affects its practical usefulness, because management is interested in good performance of the model when applied to unseen software modules, i.e., generalization performance. In the process of building GP-based software quality classification models for a high-assurance telecommunications system, we observed that the GP models were prone to overfitting. We utilize a random sampling technique to reduce overfitting in our GP models. The approach has been found by many researchers as an effective method for reducing the time of a GP run. However, in our study we utilize random to reduce overfitting with the aim of improving the generalization capability of our GP models.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1281730,no,undetermined,0
Towards the definition of a maintainability model for Web applications,"The growing diffusion of Web-based services in many and different business domains has triggered the need for new Web applications (WAs). The pressing market demand imposes very short time for the development of new WAs, and frequent modifications for existing ones. Well-defined software processes and methodologies are rarely adopted both in the development and maintenance phases. As a consequence, WAs' quality usually degrades in terms of architecture, documentation, and maintainability. Major concerns regard the difficulties in estimating costs of maintenance interventions. Thus, a strong need for methods and models to assess the maintainability of existing WAs is growing more and more. In this paper we introduce a first proposal for a WA maintainability model; the model considers those peculiarities that makes a WA different from a traditional software system and a set of metrics allowing an estimate of the maintainability is identified. Results from some initial case studies to verify the effectiveness of the proposed model are presented in the paper.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1281430,no,undetermined,0
The process of and the lessons learned from performance tuning of a product family software architecture for mobile phones,"Performance is an important nonfunctional quality attribute of a software system but not always is considered when a software is designed. Furthermore, software evolves and changes can negatively affect the performance. New requirements could introduce performance problems and the need for a different architecture design. Even if the architecture has been designed to be easy to extend and flexible enough to be modified to perform its function, a software component designed to be too general and flexible can slower the execution of the application. Performance tuning is a way to assess the characteristics of an existing software and highlight design flaws or inefficiencies. Periodical performance tuning inspections and architecture assessments can help to discover potential bottlenecks before it is too late especially when changes and requirements are added to the architecture design. In this paper a performance tuning experience of one Nokia product family architecture will be described. Assessing a product family architecture means also taking into account the performance of the entire line of products and optimizations must include or at least not penalize its members.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1281429,no,undetermined,0
SRAT-distribution voltage sags and reliability assessment tool,"Interruptions to supply and sags of distribution system voltage are the main aspects causing customer complaints. There is a need for analysis of supply reliability and voltage sag to relate system performance with network structure and equipment design parameters. This analysis can also give prediction of voltage dips, as well as relating traditional reliability and momentary outage measures to the properties of protection systems and to network impedances. Existing reliability analysis software often requires substantial training, lacks automated facilities, and suffers from data availability. Thus it requires time-consuming manual intervention for the study of large networks. A user-friendly sag and reliability assessment tool (SRAT) has been developed based on existing impedance data, protection characteristics, and a model of failure probability. The new features included in SRAT are a) efficient reliability and sag assessments for a radial network with limited loops, b) reliability evaluation associated with realistic protection and restoration schemes, c) inclusion of momentary outages in the same model as permanent outage evaluation, d) evaluation of the sag transfer through meshed subtransmission network, and e) simplified probability distribution model determined from available faults records. Examples of the application of the tools to an Australian distribution network are used to illustrate the application of this model.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1278434,no,undetermined,0
A heuristic for multi-constrained multicast routing,"In contrast to the situation that the constrained minimum Steiner tree (CMST) problem has attracted much attention in the quality of service (QoS) routing area, little work has been done on multicast routing subject to multiple additive constraints, even though the corresponding applications are obvious. We propose a heuristic, HMCMC, to solve this problem. The basic idea of HMCMC is to construct the multicast tree step by step, which is done essentially based on the latest research results on multi-constrained unicast routing. Computer simulations demonstrate that, if there is one, the proposed heuristic can find a feasible multicast tree with a fairly high probability.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1303497,no,undetermined,0
Impact of difference of feeder impedances on the performance of a static transfer switch,"Conventionally, control system of a static transfer switch (STS) is designed based on the assumption that the STS terminal voltages are in-phase. This paper investigates the impact of phase difference of the STS terminal voltages on the STS transfer time and cross current. The phase difference is assumed to be a result of the difference of the feeder impedances. The paper shows that phase differences of even up to 4.5Â°, which are accompanied by up to 5% voltage drop at the STS terminal, can noticeably increase the transfer time and the cross current magnitude. The studies are conducted on the IEEE STS-l benchmark system using the PSCAD/EMTDC software.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1278426,no,undetermined,0
The system recovery benchmark,"We describe a benchmark for measuring system recovery on a nonclustered standalone system. A system's ability to recover from an outage quickly is a critical factor in overall system availability. General purpose computer systems, such as UNIX based systems, tend to execute the same sequence or series of steps during system startup and outage recovery. Our experience has shown that these steps are consistent, reproducible and measurable, and can thus be benchmarked. Additionally, the factors that create variability in restart/recovery can be bound and represented in a meaningful way. A defined set of measurements, coupled with a specification for representing the results and system variables, provide the foundation for system recovery benchmarking.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1276577,no,undetermined,0
Analysis and evaluation of topological and application characteristics of unreliable mobile wireless ad-hoc network,"We present a study of topological characteristics of mobile wireless ad-hoc networks. The characteristics studied are connectivity, coverage, and diameter. Knowledge of topological characteristics of a network aids in the design and performance prediction of network protocols. We introduce intelligent goal-directed mobility algorithms for achieving desired topological characteristics. A simulation-based study shows that to achieve low, medium and high network QoS defined in terms of combined requirements of the three metrics, the network needs respectively 8, 16, and 40 nodes. If nodes can fail, the requirements increase to 8, 36 and 60 nodes respectively. We present a theoretical derivation of the improvement due to the mobility models and the sufficient condition for 100% connectivity and coverage. Next, we show the effect of improved topological characteristics in enhancing QoS of an application level protocol, namely, a location determination protocol called Hop-Terrain. The study shows that the error in location estimation is reduced by up to 68% with goal-directed mobility.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1276575,no,undetermined,0
Safety testing of safety critical software based on critical mission duration,"To assess the safety of software based safety critical systems, we firstly analyzed the differences between reliability and safety, then, introduced a safety model based on three-state Markov model and some safety-related metrics. For safety critical software it is common to demand that all known faults are removed. Thus an operational test for safety critical software takes the form of a specified number of test cases (or a specified critical mission duration) that must be executed unsafe-failure-free. When the previous test has been early terminated as a result of an unsafe failure, it has been proposed that the further test need to be more stringent (i.e. the number of tests that must be executed unsafe-failure-free should increase). In order to solve the problem, a safety testing method based on critical mission duration and Bayesian testing stopping rules is proposed.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1276557,no,undetermined,0
Error detection enhancement in COTS superscalar processors with event monitoring features,"Increasing use of commercial off-the-shelf (COTS) superscalar processors in industrial, embedded, and real-time systems necessitates the development of error detection mechanisms for such systems. This shows an error detection scheme called committed instructions counting (CIC) to increase error detection in such systems. The scheme uses internal performance monitoring features and an external watchdog processor (WDP). The performance monitoring features enable counting the number of committed instructions in a program. The scheme is experimentally evaluated on a 32-bit PentiumÂ® processor using software implemented fault injection (SWIFI). A total of 8181 errors were injected into the PentiumÂ® processor. The results show that the error detection coverage varies between to 90.92% and 98.41%, for different workloads.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1276552,no,undetermined,0
Application-level fault tolerance in the orbital thermal imaging spectrometer,"Systems that operate in extremely volatile environments, such as orbiting satellites, must be designed with a strong emphasis on fault tolerance. Rather than rely solely on the system hardware, it may be beneficial to entrust some of the fault handling to software at the application level, which can utilize semantic information and software communication channels to achieve fault tolerance with considerably less power and performance overhead. We show the implementation and evaluation of such a software-level approach, application-level fault tolerance and detection (ALFTD) into the orbital thermal imaging spectrometer (OTIS).",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1276551,no,undetermined,0
Towards dependable Web services,"Web services are the key technology for implementing distributed enterprise level applications such as B2B and grid computing. An important goal is to provide dependable quality guarantees for client-server interactions. Therefore, service level management (SLM) is gaining more and more significance for clients and providers of Web services. The first step to control service level agreements is a proper instrumentation of the application code in order to monitor the service performance. However, manual instrumentation of Web services is very costly and error-prone and thus not very efficient. Our goal was to develop a systematic and automated, tool-supported approach for Web services instrumentation. We present a dual approach for efficiently instrumenting Web services. It consists of instrumenting the frontend Web services platform as well as the backend services. Although the instrumentation of the Web services platform necessarily is platform-specific, we have found a general, reusable approach. On the backend-side aspect-oriented programming techniques are successfully applied to instrument backend services. We present experimental studies of performance instrumentation using the application response measurement (ARM) API and evaluate the efficiency of the monitoring enhancements. Our results point the way to systematically gain better insights into the behaviour of Web services and thus how to build more dependable Web-based applications.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1276547,no,undetermined,0
EPIC: profiling the propagation and effect of data errors in software,"We present an approach for analyzing the propagation and effect of data errors in modular software enabling the profiling of the vulnerabilities of software to find 1) the modules and signals most likely exposed to propagating errors and 2) the modules and signals which, when subjected to error, tend to cause more damage than others from a systems operation point-of-view. We discuss how to use the obtained profiles to identify where dependability structures and mechanisms will likely be the most effective, i.e., how to perform a cost-benefit analysis for dependability. A fault-injection-based method for estimation of the various measures is described and the software of a real embedded control system is profiled to show the type of results obtainable by the analysis framework.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1275294,no,undetermined,0
A design tool for large scale fault-tolerant software systems,"In order to assist software designers in the application of fault-tolerance techniques to large scale software systems, a computer-aided software design tool has been proposed and implemented that assess the criticality of the software modules contained in the system. This information assists designers in identifying weaknesses in large systems that can lead to system failures. Through analysis and modeling techniques based in graph theory, modules are assessed and rated as to the criticality of their position in the software system. Graphical representation at two levels facilitates the use of cut set analysis, which is our main focus. While the task of finding all cut sets in any graph is NP-complete, the tool intelligently applies cut set analysis by limiting the problem to provide only the information needed for meaningful analysis. In this paper, we examine the methodology and algorithms used in the implementation of this tool and consider future refinements. Although further testing is needed to assess performance on increasingly complex systems, preliminary results look promising. Given the growing demand for reliable software and the complexities involved in the design of these systems, further research in this area is indicated.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1285457,no,undetermined,0
A comparison of two safety-critical architectures using the safety related metrics,"In this paper, we introduce two safety-related metrics to evaluate a safety-critical computer-based system, and the derivations of these metrics are reviewed using Markov models. We describe two Markov architectural configurations of the system, and the comparison based on the proposed safety-related metrics is demonstrated. The comparison results confirm and conclude that one of the two architectures performs safer than the other. After the analysis, we state that the set of safety-related metrics we have derived in this paper is a good set of measurements to evaluate the safety attribute of safety-critical systems.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1285516,no,undetermined,0
ISP-operated protection of home networks with FIDRAN,"In order to fight against the increasing number of network security incidents due to mal-protected home networks permanently connected to the Internet via DSL, TV cable or similar technologies, we propose that Internet service providers (ISP) operate and manage intrusion prevention systems (IPS) which are to a large extend executed on the consumer's gateway to the Internet (e.g., DSL router). The paper analyses the requirements of ISP-operated intrusion prevention systems and presents our approach for an IPS that runs on top of an active networking environment and is automatically configured by a vulnerability scanner. We call the system FIDRAN (Flexible Intrusion Detection and Response framework for Active Networks). The system autonomously analyses the home network and correspondingly configures the IPS. Furthermore, our system detects and adjusts itself to changes in the home network (new service, new host, etc.). First performance comparisons show that our approach - while offering more flexibility and being able to support continuous updating by active networking principles - competes well with the performance of conventional intrusion prevention systems like Snort-Inline.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1286830,no,undetermined,0
Processing of abdominal ultrasound images using seed based region growing method,"There are many diseases relating to abdomen. Patients suffering by abdominal diseases will be experiencing chronic or acute abdominal pain or suspects of an abdominal mass. Abdomen has two major parts: liver and gallbladder. Gallbladder and liver diseases are very common not only in Malaysia but also all over the globe. Hundreds of patients die from such diseases every year. Doctors face difficulty in diagnosing the types of diseases and sometimes unnecessary measures like surgery has to be performed. An abdominal ultrasound image is a useful way of examining internal organs, including the liver, gallbladder, spleen and kidneys. Ultrasound is safe, radiation free, faster and cheaper. Ultrasound images themselves will not give a clear view of an affected region. In general raw ultrasound images contains lot of imbedded noises. So digital processing can improve the quality of raw ultrasound images. In this work a software tool called ultrasound processing tool (UPT) has been developed by employing the histogram equalization and region growing approach to give a clearer view of the affected regions in the abdomen. The system was tested on more than 20 cases. Here, the results of two cases are presented, one on gallbladder mass and another on liver cancer. The radiologists have reported that original ultrasound images were not at all clear enough to detect the shape and area of the affected regions and the ultrasound processing tool has provided them clear and better view of the internal details of the diseases.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1287624,no,undetermined,0
An investigation into the application of different performance prediction techniques to e-Commerce applications,Summary form only given. Predictive performance models of e-Commerce applications allows grid workload managers to provide e-Commerce clients with qualities of service (QoS) whilst making efficient use of resources. We demonstrate the use of two 'coarse-grained' modelling approaches (based on layered queuing modelling and historical performance data analysis) for predicting the performance of dynamic e-Commerce systems on heterogeneous servers. Results for a popular e-Commerce benchmark show how request response times and server throughputs can be predicted on servers with heterogeneous CPUs at different background loads. The two approaches are compared and their usefulness to grid workload management is considered.,2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1303306,no,undetermined,0
On static WCET analysis vs. run-time monitoring of execution time,"Summary form only given. Dynamic, distributed, real-time control systems control a widely varying environment, are made up of application programs that are dispersed among loosely-coupled computers, and must control the environment in a timely manner. The environment determines the number of threats; thus, it is difficult to determine the range of the workload at design time using static worst-case execution time analysis. While a system is lightly loaded, it is wasteful to reserve resources for the heaviest load. Likewise, it is also possible that the load will increase higher than the assumed worst case. A system that has a preset number of resources reserved to it is no longer guaranteed to meet its deadlines under such conditions. In order to ensure that such applications meet their real-time requirements, a mechanism is required to monitor and maintain the real-time quality of service (QoS): a QoS manager, which monitors the processing timing (latency) and resource usage of a distributed real-time system, forecasts, detects and diagnoses violations of the timing constraints, and requests more or fewer resources to maintain the desired timing characteristics. To enable better control over the system, the goals are as follows: 1) Gather detailed information about antiair warfare and air-traffic control application domains and employ it in the creation of a distributed real-time sensing and visualization testbed for air-traffic control. 2) Identify mathematical relationships among independent and dependent variables, such as performance and fault tolerance vs. resource usage, and security vs. performance. 3) Uncover new techniques for ensuring performance, fault tolerance, and security by optimizing the variables under the constraints of resource availability and user requirements.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1303089,no,undetermined,0
Automatic deployment for hierarchical network enabled servers,"Summary form only given. This article focuses on the deployment of grid infrastructures, more specifically problem solving environments (PSE) for numerical applications on the grid. Although the deployment of such an architecture may be constrained e.g., firewall, right access or security, its efficiency heavily depends on the quality of the mapping between its different components and the grid resources. This article proposes a new model based on linear programming to estimate the performance of a deployment of a hierarchical PSE. The advantages of our modeling approach are: evaluate a virtual deployment before a real deployment, provide a decision builder tool (i.e., designed to compare different architectures or add new resources) and take into account the platform scalability. Using our model, it is possible to determine the bottleneck of the platform and thus to know whether a given deployment can be improved or not. We illustrate the model by applying the results to improve performance of an existing hierarchical PSE called DIET.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1303061,no,undetermined,0
Fault-aware job scheduling for BlueGene/L systems,"Summary form only given. Large-scale systems like BlueGene/L are susceptible to a number of software and hardware failures that can affect system performance. We evaluate the effectiveness of a previously developed job scheduling algorithm for BlueGene/L in the presence of faults. We have developed two new job-scheduling algorithms considering failures while scheduling the jobs. We have also evaluated the impact of these algorithms on average bounded slowdown, average response time and system utilization, considering different levels of proactive failure prediction and prevention techniques reported in the literature. Our simulation studies show that the use of these new algorithms with even trivial fault prediction confidence or accuracy levels (as low as 10%) can significantly improve the performance of the BlueGene/L system.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1302991,no,undetermined,0
On identifying stable ways to configure systems,We consider the often error-prone process of initially building and/or reconfiguring a computer system. We formulate an optimization framework for capturing certain aspects of this system (re)configuration process. We describe offline and online algorithms that could aid operators in making decisions for how best to take actions on their computers so as to maintain the health of their systems.,2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1301358,no,undetermined,0
Assessing the robustness of self-managing computer systems under highly variable workloads,"Computer systems are becoming extremely complex due to the large number and heterogeneity of their hardware and software components, the multilayered architecture used in their design, and the unpredictable nature of their workloads. Thus, performance management becomes difficult and expensive when carried out by human beings. An approach, called self-managing computer systems, is to build into the systems the mechanisms required to self-adjust configuration parameters so that the quality of service requirements of the system are constantly met. In this paper, we evaluate the robustness of such methods when the workload exhibits high variability in terms of the interarrival time and service times of requests. Another contribution of this paper is the assessment of the use of workload forecasting techniques in the design of QoS controllers.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1301348,no,undetermined,0
Hardware - software structure for on-line power quality assessment: part I,"The main objective of the proposed work is to introduce a new concept of advanced power quality assessment. The introduced system is implemented using applications of a set of powerful software algorithms and a digital signal processor based hardware data acquisition system. The suggested scheme is mainly to construct a system for real time detection and identification of different types of power quality disturbances that produce a sudden change in the power quality levels. Moreover, a new mitigation technique through generating feedback correction signals for disturbance compensation is addressed. The performance of the suggested system is tested and verified through real test examples. The obtained results reveal that, the introduced system detects fast and accurately most of the power quality disturbance events and introduces new indicative factors estimating the performance of any supply system subjected to a set number of disturbance events.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1300913,no,undetermined,0
A model of scalable distributed network performance management,"Quality of service in IP networks necessitates the use of performance management. As Internet continues to grow exponentially, a management system should be scalable in terms of network size, speed and number of customers subscribed to value-added services. This article proposes a flexible, scalable, self-adapting model for managing large-scale distributed network. In this model, Web services framework is used to build the software architecture and XML is used to build the data exchange interface. Policy-based hierarchical event-processing mechanism presented by this paper can efficiently balance the loads and improve the flexibility of the system. The prediction algorithm adopted by this model can predict the network performance more effectively and accurately.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1300545,no,undetermined,0
Motion estimation for frame-rate reduction in H.264 transcoding,"This paper proposes a transcoding method for frame rate reduction in H.264 video coding standard. H.264 adopts various block types and multiple reference frames for motion compensation. When frames are skipped to reduce frame rates in transcoder, it is not easy to estimate optimum motion vectors and block types in H.264. A simple and effective block-adaptive motion vector resampling (BAMVR) method is proposed to estimate motion vector for motion compensation. In order to improve coding efficiency and visual quality, the rate-distortion optimization (RDO) algorithm is also combined with the BAMVR method in transcoder. In experimental results, rate-distortion performance and computational complexity of the proposed transcoder are analyzed for various video sequences. The proposed method achieves remarkable improvement in computational complexity compared to the full-motion estimation (ME) with RDO method.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1300416,no,undetermined,0
Frequency-sensitive competitive learning for scalable balanced clustering on high-dimensional hyperspheres,"Competitive learning mechanisms for clustering, in general, suffer from poor performance for very high-dimensional (>1000) data because of ""curse of dimensionality"" effects. In applications such as document clustering, it is customary to normalize the high-dimensional input vectors to unit length, and it is sometimes also desirable to obtain balanced clusters, i.e., clusters of comparable sizes. The spherical kmeans (spkmeans) algorithm, which normalizes the cluster centers as well as the inputs, has been successfully used to cluster normalized text documents in 2000+ dimensional space. Unfortunately, like regular kmeans and its soft expectation-maximization-based version, spkmeans tends to generate extremely imbalanced clusters in high-dimensional spaces when the desired number of clusters is large (tens or more). This paper first shows that the spkmeans algorithm can be derived from a certain maximum likelihood formulation using a mixture of von Mises-Fisher distributions as the generative model, and in fact, it can be considered as a batch-mode version of (normalized) competitive learning. The proposed generative model is then adapted in a principled way to yield three frequency-sensitive competitive learning variants that are applicable to static data and produced high-quality and well-balanced clusters for high-dimensional data. Like kmeans, each iteration is linear in the number of data points and in the number of clusters for all the three algorithms. A frequency-sensitive algorithm to cluster streaming data is also proposed. Experimental results on clustering of high-dimensional text data sets are provided to show the effectiveness and applicability of the proposed techniques.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1296696,no,undetermined,0
Evaluation of JPEG 2000 encoder options: human and model observer detection of variable signals in X-ray coronary angiograms,"Previous studies have evaluated the effect of the new still image compression standard JPEG 2000 using nontask based image quality metrics, i.e., peak-signal-to-noise-ratio (PSNR) for nonmedical images. In this paper, the effect of JPEG 2000 encoder options was investigated using the performance of human and model observers (nonprewhitening matched filter with an eye filter, square-window Hotelling, Laguerre-Gauss Hotelling and channelized Hotelling model observer) for clinically relevant visual tasks. Two tasks were investigated: the signal known exactly but variable task (SKEV) and the signal known statistically task (SKS). Test images consisted of real X-ray coronary angiograms with simulated filling defects (signals) inserted in one of the four simulated arteries. The signals varied in size and shape. Experimental results indicated that the dependence of task performance on the JPEG 2000 encoder options was similar for all model and human observers. Model observer performance in the more tractable and computationally economic SKEV task can be used to reliably estimate performance in the complex but clinically more realistic SKS task. JPEG 2000 encoder settings different from the default ones resulted in greatly improved model and human observer performance in the studied clinically relevant visual tasks using real angiography backgrounds.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1295081,no,undetermined,0
Low-latency mobile IP handoff for infrastructure-mode wireless LANs,"The increasing popularity of IEEE 802.11-based wireless local area networks (LANs) lends them credibility as a viable alternative to third-generation (3G) wireless technologies. Even though wireless LANs support much higher channel bandwidth than 3G networks, their network-layer handoff latency is still too high to be usable for interactive multimedia applications such as voice over IP or video streaming. Specifically, the peculiarities of commercially available IEEE 802.11b wireless LAN hardware prevent existing mobile Internet protocol (IP) implementations from achieving subsecond Mobile IP handoff latency when the wireless LANs are operating in the infrastructure mode, which is also the prevailing operating mode used in most deployed IEEE 802.11b LANs. In this paper, we propose a low-latency mobile IP handoff scheme that can reduce the handoff latency of infrastructure-mode wireless LANs to less than 100 ms, the fastest known handoff performance for such networks. The proposed scheme overcomes the inability of mobility software to sense the signal strengths of multiple-access points when operating in an infrastructure-mode wireless LAN. It expedites link-layer handoff detection and speeds up network-layer handoff by replaying cached foreign agent advertisements. The proposed scheme strictly adheres to the mobile IP standard specification, and does not require any modifications to existing mobile IP implementations. That is, the proposed mechanism is completely transparent to the existing mobile IP software installed on mobile nodes and wired nodes. As a demonstration of this technology, we show how this low-latency handoff scheme together with a wireless LAN bandwidth guarantee mechanism supports undisrupted playback of remote video streams on mobile stations that are traveling across wireless LAN segments.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1295052,no,undetermined,0
Quality-evaluation models and measurements,"Quality can determine a software product's success or failure in today's competitive market. Among the many characteristics of quality, some aspects deal directly with the functional correctness or the conformance to specifications, while others deal with usability, portability, and so on. Correctness - that is, how well software conforms to requirements and specifications - is typically the most important aspect of quality, particularly when crucial operations depend on the software. Even for market segments in which new features and usability take priority, such as software for personal use in the mass market, correctness is still a fundamental part of the users' expectations. We compare and classify quality-evaluation models, particularly those evaluating the correctness aspect of quality, and examine their data requirements to provide practical guidance for selecting appropriate models and measurements.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1293078,no,undetermined,0
The framework of a web-enabled defect tracking system,"This paper presents an evaluation and investigation of issues to implement a defect management system; a tool used to understand and predict software product quality and software process efficiency. The scope is to simplify the process of defect tracking through a web-enabled application. The system will enable project management, development, quality assurance and software engineer to track and manage problem specifically defects in the context of software project. A collaborative function is essential as this will enable users to communicate in real time mode. This system makes key defect tracking coordination and information available disregards the geographical and time factor.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1292958,no,undetermined,0
The impact of training-by-examples on inspection performance using two laboratory experiments,"Software inspection is often seen as a technique to produce quality software. It has been claimed that expertise is a key determinant in inspection performance particularly in individual detection and group meetings [Sauer, C., et al.,(2000)]. Uncertainty among reviewers during group meetings due to lack of expertise is seen as a weakness in inspection performance. One aspect of achieving expertise is through education or formal training. Recent theoretical frameworks in software inspection also support the idea of possible effects of training on inspection performance [Sauer, C., et al.,(2000)]. To investigate this further, two laboratory experiments were conducted to test the effects of training using defect examples. Our findings show conflicting results between the two experiments, indirectly highlighting the importance of an effective inspection process. The results have implications for the use of a repository of defect examples for training reviewers.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1290481,no,undetermined,0
Teaching the process of code review,"Behavioural theory predicts that interventions that improve individual reviewers' expertise also improve the performance of the group in Software Development Technical Reviews (SDTR) [C. Sauer et al.,(2000)]. This includes improvements both in individual's expertise in the review process, as well as their ability to find defects and distinguish true defects from false positives. We present findings from University training in these skills using authentic problems. The first year the course was run it was designed around actual code review sessions, the second year this was expanded to enable students to develop and trial their own generic process for document reviews. This report considers the values and shortcomings of the teaching program from an extensive analysis of the defect detection in the first year, when students were involved in a review process that was set up for them, and student feedback from the second year when students developed and analysed their own process.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1290480,no,undetermined,0
Tolerating late memory traps in dynamically scheduled processors,"In the past few years, exception support for memory functions such as virtual memory, informing memory operations, software assist for shared memory protocols, or interactions with processors in memory has been advocated in various research papers. These memory traps may occur on a miss in the cache hierarchy or on a local or remote memory access. However, contemporary, dynamically scheduled processors only support memory exceptions detected in the TLB associated with the first-level cache. They do not support memory exceptions taken deep in the memory hierarchy. In this case, memory traps may be late, in the sense that the exception condition may still be undecided when a long-latency memory instruction reaches the retirement stage. In this paper we evaluate through simulation the overhead of memory traps in dynamically scheduled processors, focusing on the added overhead incurred when a memory trap is late. We also propose some simple mechanisms to reduce this added overhead while preserving the memory consistency model. With more aggressive memory access mechanisms in the processor we observe that the overhead of all memory traps - either early or late - is increased while the lateness of a trap becomes largely tolerated so that the performance gap between early and late memory traps is greatly reduced. Additionally, because of caching effects in the memory hierarchy, the frequency of memory traps usually decreases as they are taken deeper in the memory hierarchy and their overall impact on execution times becomes negligible. We conclude that support for memory traps taken throughout the memory hierarchy could be added to dynamically scheduled processors at low hardware cost and little performance degradation.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1288548,no,undetermined,0
Infrastructure support for controlled experimentation with software testing and regression testing techniques,"Where the creation, understanding, and assessment of software testing and regression testing techniques are concerned, controlled experimentation is an indispensable research methodology. Obtaining the infrastructure necessary to support such experimentation, however, is difficult and expensive. As a result, progress in experimentation with testing techniques has been slow, and empirical data on the costs and effectiveness of techniques remains relatively scarce. To help address this problem, we have been designing and constructing infrastructure to support controlled experimentation with testing and regression testing techniques. This paper reports on the challenges faced by researchers experimenting with testing techniques, including those that inform the design of our infrastructure. The paper then describes the infrastructure that we are creating in response to these challenges, and that we are now making available to other researchers, and discusses the impact that this infrastructure has and can be expected to have.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1334894,no,undetermined,0
Using empirical testbeds to accelerate technology maturity and transition: the SCRover experience,"This paper is an experience report on a first attempt to develop and apply a new form of software: a full-service empirical testbed designed to evaluate alternative software dependability technologies, and to accelerate their maturation and transition into project use. The SCRover testbed includes not only the specifications, code, and hardware of a public safety robot, but also the package of instrumentation, scenario drivers, seeded defects, experimentation guidelines, and comparative effort and defect data needed to facilitate technology evaluation experiments. The SCRover testbed's initial operational capability has been recently applied to empirically evaluate two architecture definition languages (ADLs) and toolsets, Mae and AcmeStudio. The testbed evaluation showed (1) that the ADL-based toolsets were complementary and cost-effective to apply to mission-critical systems; (2) that the testbed was cost-effective to use by researchers; and (3) that collaboration in testbed use by researchers and the Jet Propulsion Laboratory (JPL) project users resulted in actions to accelerate technology maturity and transition into project use. The evaluation also identified a number of lessons learned for improving the SCRover testbed, and for development and application of future technology evaluation testbeds.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1334899,no,undetermined,0
Tool-supported unobtrusive evaluation of software engineering process conformance,"Software engineers face the dilemma of verifying process conformance and having the Hawthorne effect distorting the results or not doing it and as a consequence not being able to know if the proposed process is actually carried out. We addressed this issue by classifying and proposing the use of some approaches based on unobtrusive observation. These approaches include cognitive labs, remote monitoring and metrics collection. In addition, a tool supporting the application of perspective based reading (PER) for requirements inspection has been used and is also presented to exemplify metrics collection. Among other features, PBR Tool unobtrusively collects metrics in order to monitor if the reading technique is faithfully applied. Two studies evaluating the feasibility of such a tool are also reported.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1334900,no,undetermined,0
An integrated design of multipath routing with failure survivability in MPLS networks,"Multipath routing employs multiple parallel paths between the source and destination for a connection request to improve resource utilization of a network. In this paper, we provide an integrated design of multipath routing in MPLS networks. In addition, we take into account the quality of service (QoS) in carrying delay-sensitive traffic and failure survivability in the design. Path protection or restoration policies enables the network to accommodate link failures and avoid traffic loss. We evaluate the performance of the proposed schemes in terms of call blocking probability, network resource utilization and load balancing factor. The results demonstrate that the proposed integrated design framework can provide effective network failure survivability, and also achieve better load balancing and/or higher network resource utilization",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1359429,no,undetermined,0
Assessing biogenic emission impact on the ground-level ozone concentration by remote sensing and numerical model,"Emission inventory data is one of the major inputs for all air quality simulation models. Emission inventory data for the prediction of ground-level ozone concentration, grouped as point, area, mobile and biogenic sources, are a composite of all reported and estimated pollutant emission information from many organizations. Before applying air quality simulation model, the emission inventory data generally require additional processing for meeting spatial, temporal, and speciation requirements using advanced information technologies. In this study, SMOKE was setup to update the essential emission processing. The emission processing work was performed to prepare emission input for U.S. EPA's Models-3/CMAQ. The fundamental anthropogenic emission inventory commonly used in Taiwan is the TEDS 4.2 software package. However, without the proper inclusion of accurate estimation of biogenic emission, the estimation of ground-level ozone concentration may not be meaningful. With the aid of SPOT satellite image, biogenic gas emission modeling analysis can be achieved to fit in BEIS-2 in SMOKE. Improved utilization of land use identification data, based on SPOT outputs and emission factors, may be influential in support of the modeling work. During this practice, land use was identified via an integrated assessment based on both geographical information system and remote sensing technologies; and emission factors were adapted from a series of existing database in the literature. The research findings clearly indicate that the majority of biogenic VOCs emissions occurred in the mountains and farmland actually exhibit fewer impacts on ground-level ozone concentration in populated areas than the anthropogenic emissions in South Taiwan. This implies that fast economic growth ends up with sustainability issue due to overwhelming anthropogenic emissions",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1370183,no,undetermined,0
Land cover classification of SSC image: unsupervised and supervised classification using ERDAS Imagine,"NASA's Earth Sciences program is primarily focused on providing high quality data products to its science community. NASA also recognizes the need to increase its involvement with the general public, including areas of information and education. The main objective of this study is to classify the vegetation, man-made structures, and miscellaneous objects from the Satellite Image of NASA Stennis Space Center (SSC), Mississippi, and USA, by using the software, ERDAS Imagine 8.5. The ERDAS Image software performs the classification of an image for identification of terrestrial features based on the spectral analysis. For classification of the SSC image, the multispectral data was used for categorization of terrestrial objects, vegetation and shadows of the trees. These are two ways to classify pixels into different categories: Supervised and unsupervised. The classification of unsupervised data through ERDAS Image helped in identifying the terrestrial objects in the Study Image (SSC). The spectral pattern present within the data for each pixel was used as the numerical basis for categorization. The first analysis of the Image SSC involved the use of generalized Unsupervised Classification with 4 categories (Grass, Trees, Man-Made and Unknown). The result of the Unsupervised Image was used to create another image by using Supervised classification. The key difference between the two images is the ability of supervised image to decipher similar images, such as the roofs of the buildings and the shadows of the trees. Image stacking was conducted to create a fully classified image to separate shadow, grass, man-made, and trees. This poster will describe the procedures for viewing and measuring image, Computer-guided (Unsupervised) and User-guided (Supervised) Procedures will be described on image stacking to view each classification one at a time and stack them into a complete Classified Image. The application of unsupervised and supervised classification in agriculture will be discussed by giving examples of measurement of field reflectance of two classes of giant salvinia [green giant salvinia (green foliage) and senesced giant salvinia (mixture of green and brown foliage)], and invasive aquatic weed in Texas.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1369859,no,undetermined,0
Anomaly detection using the emerald nanosatellite on board expert system,"An expert system is a type of program designed to model human knowledge and expertise. This work describes the design, implementation, and testing of an onboard expert system developed for the dual spacecraft Emerald small satellite mission. This system takes advantage of Emerald's distributed computing architecture and is currently being used for on-board fault detection. The distributed computing architecture is composed of a network of PICmicro and Atmel microcontrollers linked together by an I<sup>2</sup>C serial data communication bus which also supports sensor and component integration via Dallas 1-wire and RS232 standards. The expert system software is executed by an Atmel microcontroller within Emerald's expert subsystem hardware. The human knowledge and expertise that the system simulates is contained within software ""rules"" that can be easily modified from the ground. The flexibility offered by this system allows the ground operator to add, modify, or remove logical operations on-orbit and overcomes the limitations imposed by hardwired systems. While expert systems have been used on spacecraft in the past, its role on Emerald for on-board fault-detection using threat integrals and persistence counters further demonstrates the power and versatility of such systems. Results include experimental data verifying the expert system's performance and its ability to distinguish threat levels posed by out-of-limit sensor readings. This paper describes the technical design of the aforementioned features and its use as part of the Emerald satellite mission.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1367593,no,undetermined,0
Multi-antenna testbeds for research and education in wireless communications,"Wireless communication systems present unique challenges and trade-offs at various levels of the system design process. Since a variety of performance measures are important in wireless communications, a family of testbeds becomes essential to validate the gains reported by the theory. Wireless testbeds also play a very important role in academia for training students and enabling research. In this article we discuss a classification scheme for wireless testbeds and present an example of the testbeds developed at UCLA for each of these cases. We present the unique capabilities of these testbeds, provide the results of the experiments, and discuss the role they play in an educational environment.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1367558,no,undetermined,0
Improvement of PD location in GIS [gas-insulated switchgear],"The measuring system we used consisted of two UHF sensors with a frequency band of 400 MHzâˆ?500 MHz and a digital oscilloscope with maximum 2 GS/s digital rate in its two channels. The difference in arrival times of UHF signals at two sensors can be measured to assist with locating defects in GIS. The test results in the laboratory indicate that the fault location precision depends on the digital rate of the oscilloscope. On the actual site test, if the sample rate is low, there may not be enough sample plots to recover the real pulse wavefronts, which makes it difficult to identify the arrival times at the two sensors of the first peak of the UHF waveform. In this paper, a new algorithm has been described, which applies cubic spline interpolation to increase points between the two real sample points, and improve the location precision of the time-of-flight method. And then the distance difference between the two waves can be determined by using a cross-correlation function. In addition, application software has been developed to help locate the PD source in GIS.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1364276,no,undetermined,0
Efficient calculation of resolution and covariance for penalized-likelihood reconstruction in fully 3-D SPECT,"Resolution and covariance predictors have been derived previously for penalized-likelihood estimators. These predictors can provide accurate approximations to the local resolution properties and covariance functions for tomographic systems given a good estimate of the mean measurements. Although these predictors may be evaluated iteratively, circulant approximations are often made for practical computation times. However, when numerous evaluations are made repeatedly (as in penalty design or calculation of variance images), these predictors still require large amounts of computing time. In Stayman and Fessler (2000), we discussed methods for precomputing a large portion of the predictor for shift-invariant system geometries. In this paper, we generalize the efficient procedure discussed in Stayman and Fessler (2000) to shift-variant single photon emission computed tomography (SPECT) systems. This generalization relies on a new attenuation approximation and several observations on the symmetries in SPECT systems. These new general procedures apply to both two-dimensional and fully three-dimensional (3-D) SPECT models, that may be either precomputed and stored, or written in procedural form. We demonstrate the high accuracy of the predictions based on these methods using a simulated anthropomorphic phantom and fully 3-D SPECT system. The evaluation of these predictors requires significantly less computation time than traditional prediction techniques, once the system geometry specific precomputations have been made.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1362755,no,undetermined,0
A cognitive-based mechanism for constructing software inspection teams,"Software inspection is well-known as an effective means of defect detection. Nevertheless, recent research has suggested that the technique requires further development to optimize the inspection process. As the process is inherently group-based, one approach to improving performance is to attempt to minimize the commonality within the process and the group. This work proposes an approach to add diversity into the process by using a cognitively-based team selection mechanism. The paper argues that a team with diverse information processing strategies, as defined by the selection mechanism, maximize the number of different defects discovered.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1359772,no,undetermined,0
Assessing and improving state-based class testing: a series of experiments,"This work describes an empirical investigation of the cost effectiveness of well-known state-based testing techniques for classes or clusters of classes that exhibit a state-dependent behavior. This is practically relevant as many object-oriented methodologies recommend modeling such components with statecharts which can then be used as a basis for testing. Our results, based on a series of three experiments, show that in most cases state-based techniques are not likely to be sufficient by themselves to catch most of the faults present in the code. Though useful, they need to be complemented with black-box, functional testing. We focus here on a particular technique, Category Partition, as this is the most commonly used and referenced black-box, functional testing technique. Two different oracle strategies have been applied for checking the success of test cases. One is a very precise oracle checking the concrete state of objects whereas the other one is based on the notion of state invariant (abstract states). Results show that there is a significant difference between them, both in terms of fault detection and cost. This is therefore an important choice to make that should be driven by the characteristics of the component to be tested, such as its criticality, complexity, and test budget.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1359770,no,undetermined,0
A WSLA-based monitoring system for grid service - GSMon,"The target of grid monitoring and management is to monitor services in the grid for fault detection, performance analysis, performance tuning, load balancing and scheduling. This paper introduces the design and implementation of a WSLA-based grid monitoring and management system - GSMon. In GSMon, we use a service-oriented architecture, give monitoring information of the grid service a unique format by using WSLA and develop extended modules in the OGSA container. GSMon conforms to grid service and Web service standards and uses dynamic deployment technology to solve the problem of monitoring new grid services. It is a novel infrastructure for grid monitoring and management with high flexibility and scalability.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1358072,no,undetermined,0
A replicated experiment of usage-based and checklist-based reading,"Software inspection is an effective method to detect faults in software artefacts. Several empirical studies have been performed on reading techniques, which are used in the individual preparation phase of software inspections. Besides new experiments, replications are needed to increase the body of knowledge in software inspections. We present a replication of an experiment, which compares usage-based and checklist-based reading. The results of the original experiment show that reviewers applying usage-based reading are more efficient and effective in detecting the most critical faults from a user's point of view than reviewers using checklist-based reading. We present the data of the replication together with the original experiment and compares the experiments. The main result of the replication is that it confirms the result of the original experiment. This replication strengthens the evidence that usage-based reading is an efficient reading technique.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357907,no,undetermined,0
Providing data transfer with QoS as agreement-based service,"Over the last decade, grids have become a successful tool for providing distributed environments for secure and coordinated execution of applications. The successful deployment of many realistic applications in such environments on a large scale has motivated their use in experimental science [L. C. Pearlman et al., (2004), K. Keahey et al. (2004)] where grid-based computations are used to assist in ongoing experiments. In such scenarios, quality of service (QoS) guarantees on execution as well as data transfer is desirable. The recently proposed WS-Agreement model [K. Czajkowski et al. K. Keahey et al. (2004)] provides an infrastructure within which such quality of service can be negotiated and obtained. We have designed and implemented a data transfer service that exposes an interface based on this model and defines agreements which guarantee that, within a certain confidence level, file transfer can be completed under a specified time. The data transfer service accepts a client's request for data transfer and makes an agreement with the client based on QoS metrics (such as the transfer time and confidence level with which the service can be provided). In our approach we use prediction as a base for formulating an agreement with the client, and we combine prediction and rate limiting to adoptively ensure that the agreement is met.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1358024,no,undetermined,0
On the statistical properties of the F-measure,"The F-measure - the number of distinct test cases to detect the first program failure - is an effectiveness measure for debug testing strategies. We show that for random testing with replacement, the F-measure is distributed according to the geometric distribution. A simulation study examines the distribution of two adaptive random testing methods, to study how closely their sampling distributions approximate the geometric distribution, revealing that in the worst case scenario, the sampling distribution for adaptive random testing is very similar to random testing. Our results have provided an answer to a conjecture that adaptive random testing is always a more effective alternative to random testing, with reference to the F-measure. We consider the implications of our findings for previous studies conducted in the area, and make recommendations to future studies.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357955,no,undetermined,0
Machine-learning techniques for software product quality assessment,"Integration of metrics computation in most popular computer-aided software engineering (CASE) tools is a marked tendency. Software metrics provide quantitative means to control the software development and the quality of software products. The ISO/IEC international standard (14598) on software product quality states, ""Internal metrics are of little value unless there is evidence that they are related to external quality"". Many different approaches have been proposed to build such empirical assessment models. In this work, different machine learning (ML) algorithms are explored with regard to their capacities of producing assessment/predictive models, for three quality characteristics. The predictability of each model is then evaluated and their applicability in a decision-making system is discussed.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357950,no,undetermined,0
Adaptive random testing through dynamic partitioning,"Adaptive random testing (ART) describes a family of algorithms for generating random test cases that have been experimentally demonstrated to have greater fault-detection capacity than simple random testing. We outline and demonstrate two new ART algorithms, and demonstrate experimentally that they offer similar performance advantages, with considerably lower overhead than other ART algorithms.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357947,no,undetermined,0
Software failure rate and reliability incorporating repair policies,"Reliability of a software application, its failure rate and the residual number of faults in an application are the three most important metrics that provide a quantitative assessment of the failure characteristics of an application. Typically, one of many stochastic models known as software reliability growth models (SRGMs) is used to describe the failure behavior of an application in its testing phase, and obtain an estimate of the above metrics. In order to ensure analytical tractability, SRGMs are based on an assumption of instantaneous repair and thus the estimates of the metrics obtained using SRGMs tend to be optimistic. In practice, fault repair activity consumes a nonnegligible amount of time and resources. Also, repair may be conducted according to many policies which are reflective of the schedule and budget constraints of a project. A few research efforts that have sought to incorporate repair into SRGMs are restrictive, since they consider only one of the several SRGMs, model the repair process using a constant rate, and provide an estimate of only the residual number of faults. These techniques do not address the issue of estimating application failure rate and reliability in the presence of repair. In this paper we present a generic framework which relies on the rate-based simulation technique in order to provide the capability to incorporate various repair policies into the finite failure nonhomogeneous Poisson process (NHPP) class of software reliability growth models. We also present a technique to compute the failure rate and the reliability of an application in the presence of repair. The potential of the framework to obtain quantitative estimates of the above three metrics taking into consideration different repair policies is illustrated using several scenarios.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357924,no,undetermined,0
Assessment of software measurement: an information quality study,"This paper reports on the first phase of an empirical research project concerning methods to assess the quality of the information in software measurement products. Two measurement assessment instruments are developed and deployed in order to generate two sets of analyses and conclusions. These sets will be subjected to an evaluation of their information quality in phase two of the project. One assessment instrument was based on AIMQ, a generic model of information quality. The other instrument was developed by targeting specific practices relating to software project management and identifying requirements for information support. Both assessment instruments delivered data that could be used to identify opportunities to improve measurement The generic instrument is cheap to acquire and deploy, while the targeted instrument requires more effort to build. Conclusions about the relative merits of the methods, in terms of their suitability for improvement purposes, await the results from the second phase of the project.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357917,no,undetermined,0
"Estimating effort by use case points: method, tool and case study","Use case point (UCP) method has been proposed to estimate software development effort in early phase of software project and used in a lot of software organizations. Intuitively, UCP is measured by counting the number of actors and transactions included in use case models. Several tools to support calculating UCP have been developed. However, they only extract actors and use cases and the complexity classification of them are conducted manually. We have been introducing UCP method to software projects in Hitachi Systems & Services, Ltd. To effective introduction of UCP method, we have developed an automatic use case measurement tool, called U-EST. This paper describes the idea to automatically classify the complexity of actors and use cases from use case model. We have also applied the U-EST to actual use case models and examined the difference between the value by the tool and one by the specialist. As the results, UCPs measured by the U-EST are similar to ones by the specialist.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357913,no,undetermined,0
Assessing the impact of active guidance for defect detection: a replicated experiment,"Scenario-based reading (SBR) techniques have been proposed as an alternative to checklists to support the inspectors throughout the reading process in the form of operational scenarios. Many studies have been performed to compare these techniques regarding their impact on the inspector performance. However, most of the existing studies have compared generic checklists to a set of specific reading scenarios, thus confounding the effects of two SBR key factors: separation of concerns and active guidance. In a previous work we have preliminarily conducted a repeated case study at the University of Kaiserslautern to evaluate the impact of active guidance on inspection performance. Specifically, we compared reading scenarios and focused checklists, which were both characterized as being perspective-based. The only difference between the reading techniques was the active guidance provided by the reading scenarios. We now have replicated the initial study with a controlled experiment using as subjects 43 graduate students in computer science at University of Bari. We did not find evidence that active guidance in reading techniques affects the effectiveness or the efficiency of defect detection. However, inspectors showed a better acceptance of focused checklists than reading scenarios.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357909,no,undetermined,0
Crystal BallÂ® and Design for Six Sigma,"8 In today's competitive market, businesses are adopting new practices like Design For Six Sigma (DFSS), a customer driven, structured methodology for faster-to-market, higher quality, and less costly new products and services. Monte Carlo simulation and stochastic optimization can help DFSS practitioners understand the variation inherent in a new technology, process, or product, and can be used to create and optimize potential designs. The benefits of understanding and controlling the sources of variability include reduced development costs, minimal defects, and sales driven through improved customer satisfaction. This tutorial uses Crystal Ball Professional Edition, a suite of easy-to-use MicrosoftÂ® Excel-based software, to demonstrate how stochastic simulation and optimization can be used in all five phases of DFSS to develop the design for a new compressor.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1371517,no,undetermined,0
Improving the performance of dispatching rules in semiconductor manufacturing by iterative simulation,"In this paper, we consider semiconductor manufacturing processes that can be characterized by a diverse product mix, heterogeneous parallel machines, sequence-dependent setup times, a mix of different process types, i.e. single-wafer vs. batch processes, and reentrant process flows. We use dispatching rules that require the estimation of waiting times of the jobs. Based on the lead time iteration concept of Vepsalainen and Morton (1988), we obtain good waiting time estimates by using exponential smoothing techniques. We describe a database-driven architecture that allows for an efficient implementation of the suggested approach. We present results of computational experiments for reference models of semiconductor wafer fabrication facilities. The results demonstrate that the suggested approach leads to high quality solutions.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1371544,no,undetermined,0
An infinite server queueing approach for describing software reliability growth: unified modeling and estimation framework,"In general, the software reliability models based on the nonhomogeneous Poisson processes (NHPPs) are quite popular to assess quantitatively the software reliability and its related dependability measures. Nevertheless, it is not so easy to select the best model from a huge number of candidates in the software testing phase, because the predictive performance of software reliability models strongly depends on the fault-detection data. The asymptotic trend of software fault-detection data can be explained by two kinds of NHPP models; finite fault model and infinite fault model. In other words, one needs to make a hypothesis whether the software contains a finite or infinite number of faults, in selecting the software reliability model in advance. In this article, we present an approach to treat both finite and infinite fault models in a unified modeling framework. By introducing an infinite server queueing model to describe the software debugging behavior, we show that it can involve representative NHPP models with a finite and an infinite number of faults. Further, we provide two parameter estimation methods for the unified NHPP based software reliability models from both standpoints of Bayesian and nonBayesian statistics. Numerical examples with real fault-detection data are devoted to compare the infinite server queueing model with the existing one under the same probability circumstance.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1371911,no,undetermined,0
An exploratory study of groupware support for distributed software architecture evaluation process,"Software architecture evaluation is an effective means of addressing quality related issues quite early in the software development lifecycle. Scenario-based approaches to evaluate architecture usually involve a large number of stakeholders, who need to be collocated for evaluation sessions. Collocating a large number of stakeholders is an expensive and time-consuming exercise, which may prove to be a hurdle in the wide-spread adoption of architectural evaluation practices. Drawing upon the successful introduction of groupware applications to support geographically distributed teams in software inspection, and requirements engineering disciplines, we propose the concept of distributed architectural evaluation using Internet-based collaborative technologies. This paper illustrates the methodology of a pilot study to assess the viability of a larger experiment intended to investigate the feasibility of groupware support for distributed software architecture evaluation. In addition, the results of the pilot study provide some interesting findings on the viability of groupware-supported software architectural evaluation process.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1371923,no,undetermined,0
Unit testing in practice,"Unit testing is a technique that receives a lot of criticism in terms of the amount of time that it is perceived to take and in how much it costs to perform. However it is also the most effective means to test individual software components for boundary value behavior and ensure that all code has been exercise adequately (e.g. statement, branch or MC/DC coverage). In this paper we examine the available data from three safety related software projects undertaken by Pi Technology that have made use of unit testing. Additionally we discuss the different issues that have been found applying the technique at different phases of the development and using different methods to generate those test. In particular we provide an argument that the perceived costs of unit testing may be exaggerated and that the likely benefits in terms of defect detection are actually quite high in relation to those costs.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1383101,no,undetermined,0
Development of on-line vibration condition monitoring system of hydro generators,"Mechanical vibration information is critical to diagnosing the health of a generator. Existing vibration monitoring systems have poor resistibility against the strong electromagnetic interference in field and difficult to extend. In order to solve these problems, an on-line monitoring system based on LonWorks control network has been developed. In this paper, the structure of hardware and software, the functions and characteristics of system have been described in detail. The analysis result of a vibration signal has proved that this monitoring system can detect fault of generators efficiently. The system has been employed to monitor the vibration of hydro generators operation in a water power plant in Hubei province, China.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1382338,no,undetermined,0
Applying SPC to autonomic computing,"Statistical process control (SPC) is proposed as the method to frame autonomic computing system. SPC follows a data-driven approach to characterize, evaluate, predict, and improve the system services. Perspectives that are central to process measurement including central tendency, variation, stability, capability are outlined. The principles of SPC hold that by establishing and sustaining stable levels of variability, processes will yield predictable results. SPC is explored to meet and support individual autonomic computing elements' requirement. One timetabling example illustrates how SPC discover and incorporate domain-specific knowledge, thus stabilize and optimize the application service quality. The example represents reasonable application of process control that has been demonstrated to be successful in engineering point of view.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1382283,no,undetermined,0
A hybrid genetic algorithm for tasks scheduling in heterogeneous computing systems,"Efficient application scheduling is critical for achieving high performance in heterogeneous computing systems (HCS). Because an application can be partitioned into a group of tasks and represented as a directed acyclic graph (DAG), the problem can be stated as finding a schedule for a DAG to be executed in a HCS so that the schedule length can be minimized. The tasks scheduling problem is NP-hard in general, except in a few simplified situations. In order to obtain optimal or suboptimal solutions, a large number of scheduling heuristics have been presented in the literature. Genetic algorithm (GA), as a power tool to achieve global optimal, has been successfully used in this field. This work presents a new hybrid genetic algorithm to solve the scheduling problem in HCS. It uses a direct method to encode a solution into chromosome. Topological sort of DAG is used to repair the offspring in order to avoid yielding illegal or infeasible solutions, and it also guarantees that all feasible solutions can be reached with some probability. In order to remedy the GA's weakness in fine-tuning, this paper uses a greedy strategy to improve the fitness of the individuals in crossover operator, based on Lamarckian theory in the evolution. The simulation results comparing with a typical genetic algorithm and a typical list heuristic, both from the literature, show that this algorithm produces better results in terms of both quality of solution and convergence speed.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1382217,no,undetermined,0
Robust quality management for differentiated imprecise data services,"Several applications, such as Web services and e-commerce, are operating in open environments where the workload characteristics, such as the load applied on the system and the worst-case execution times, are inaccurate or even not known in advance. This implies that transactions submitted to a real-time database cannot be subject to exact schedulability analysis given the lack of a priori knowledge of the workload. In this paper we propose an approach, based on feedback control, for managing the quality of service of real-time databases that provide imprecise and differentiated services, given inaccurate workload characteristics. For each service class, the database operator specifies the quality of service requirements by explicitly declaring the precision requirements of the data and the results of the transactions. The performance evaluation shows that our approach provides reliable quality of service even in the face of varying load and inaccurate execution time estimates.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1381313,no,undetermined,0
Improving novelty detection in short time series through RBF-DDA parameter adjustment,"Novelty detection in time series is an important problem with application in different domains. such as machine failure detection, fraud detection and auditing. We have previously proposed a method for time series novelty detection based on classification of time series windows by RBF-DDA neural networks. The paper proposes a method to be used in conjunction with this time series novelty detection method whose aim is to improve performance by adequately selecting the window size and the RBF-DDA parameter values. The method was evaluated on six real-world time series and the results obtained show that it greatly improves novelty detection performance.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1380945,no,undetermined,0
Case study of condition based health maintenance of large power transformer at Rihand substation using on-line FDD-EPT,"The constant monitoring of large, medium and small power transformers for purposes of assessing their health, operating condition while maximizing personnel resources and preserving capital is often a topic of spirited discussions at both national and international conferences. Further, the considerations of transformers being out of service for extended periods of time due to conditions that, with the proper diagnostic monitoring equipment could have preemptively detected, diagnosed and prevented catastrophic equipment failure is of prime importance in today's economic conditions. These operating conditions are becoming more serious in several locations around the world. A recent case study of PD monitoring done at the Riband substation in India is discussed here in the sequel. Additional transformers tested in India for PGCIL (Ballabgarh) and NTPC (Noida, Delhi) in 1999 will be presented using the FDD-EPT system. Demonstrations of the FDD-EPT (fault diagnostic device for electrical power transformers) system on transformers for BHEL, MP and Tata Power in Mumbai also provided encouraging results. This will further illustrate the efficacy of this system.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1380659,no,undetermined,0
Link characterization for system level CDMA performance evaluation in high mobility,"The evaluation of the performance of a cellular system using a simulation environment usually involves a link-to-system mapping based on a channel quality indication. The mapping is done using a set of link curves characterizing the link behavior, that are generated offline. In some cases, the static performance of a link is a sufficient indicator. But in high mobility, the block-fading assumption introduces inaccuracies in the link-to-system mapping. This paper proposes a dual-variable mapping technique that can be used in high-mobility conditions, using the mean and the variance of the SINR seen during a frame. Simulations show that, using this methodology, the mapping can be created independent of the velocity and the multipath profile of the channel. This mapping technique can also be used as a channel quality indication for link adaptation purposes.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1379001,no,undetermined,0
Wireless download agent for high-capacity wireless LAN based ubiquitous services,"We propose a wireless download agent which effectively controls the point at which a download starts for providing a comfortable mobile computing environment. A user can get the desired data with the wireless download agent while walking through a service area without stopping. We conducted simulations to evaluate its performance in terms of throughput, download period, and probability of successful download. Our results show that the proposed scheme very well suits the wireless download agent in high-speed wireless access systems with many users. Furthermore, we describe the use of the proposed scheme considering the randomness of the walking directions of users.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1378951,no,undetermined,0
Meter data management for the electricity market,"The quality of the meter data for billing and analysis is very important in the electricity market. The meter data is used primarily in billing & settlement for industrial customers and bulk trading partners who have direct access to data, and used secondarily in trading, transmission operations & planning, and load forecasting & scheduling who don't have direct access to data. A software system called the metering data management (MDM) system has been developed using the web service technology to support the meter usage data collection, validation, estimation, versioning, and publishing at Bonneville Power Administration (BPA), a US Federal Power Marketing Agency. One of its key features is the validation and estimation of the meter usage data based on statistical models. The paper presents the infrastructure and implementation details of MDM. It also addresses a novel approach to validating the meter data and estimating missing values in the meter data. The key performance criteria of MDM are scalability, collaboration, integration, in addition to good data acquisition and data persistence capabilities. Further more, use of the meter data with weather information for more accurate validation and estimation is also discussed",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1378673,no,undetermined,0
Quantifying the quality of object-oriented design: the factor-strategy model,"The quality of a design has a decisive impact on the quality of a software product; but due to the diversity and complexity of design properties (e.g., coupling, encapsulation), their assessment and correlation with external quality attributes (e.g., maintenance, portability) is hard. In contrast to traditional quality models that express the ""goodness"" of design in terms of a set of metrics, the novel Factor-Strategy model proposed by This work, relates explicitly the quality of a design to its conformance with a set of essential principles, rules and heuristics. This model is based on a novel mechanism, called detection strategy, that raises the abstraction level in dealing with metrics, by allowing to formulate good-design rules and heuristics in a quantifiable manner, and to detect automatically deviations from these rules. This quality model provides a twofold advantage: (i) an easier construction and understanding of the model as quality is put in connection with design principles rather than ""raw numbers""; and (ii) a direct identification of the real causes of quality flaws. We have validated the approach through a comparative analysis involving two versions of a industrial software system.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1374319,no,undetermined,0
Noise identification with the k-means algorithm,"The presence of noise in a measurement dataset can have a negative effect on the classification model built. More specifically, the noisy instances in the dataset can adversely affect the learnt hypothesis. Removal of noisy instances will improve the learnt hypothesis; thus, improving the classification accuracy of the model. A clustering-based noise detection approach using the k-means algorithm is presented. We present a new metric for measuring the potentiality (noise factor) of an instance being noisy. Based on the computed noise factor values of the instances, the clustering-based algorithm is then used to identify and eliminate p% of the instances in the dataset. These p% of instances are considered the most likely to be noisy among the instances in the dataset - the p% value is varied from 1% to 40%. The noise detection approach is investigated with respect to two case studies of software measurement data obtained from NASA software projects. The two datasets are characterized by the same thirteen software metrics and a class label that classifies the program modules as fault-prone and not fault-prone. It is shown that as more noisy instances are removed, classification accuracy of the C4>5 learner improves. This indicates that the removed instances are most likely noisy instances that attributed to poor classification accuracy.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1374211,no,undetermined,0
Using fuzzy theory for effort estimation of object-oriented software,"Estimating software effort and costs is a very important activity that includes very uncertain elements. The concepts of the fuzzy set theory has been successfully used for extending metrics such as FP and reducing human influence in the estimation process. However, when we consider object-oriented technologies, other models, such as the use case model, are used to represent the specification in the early stages of development. New metrics based on this model were proposed and the application of the fuzzy set theory in this context is also very important. This work introduces the metric FUSP (fuzzy use case size points) that allows gradual classifications in the estimation by using fuzzy numbers. Results of a study case show some advantages and limitations of the proposed metric.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1374187,no,undetermined,0
Semi-supervised learning for software quality estimation,"A software quality estimation model is often built using known software metrics and fault data obtained from program modules of previously developed releases or similar projects. Such a supervised learning approach to software quality estimation assumes that fault data is available for all the previously developed modules. Considering the various practical issues in software project development, fault data may not be available for all the software modules in the training data. More specifically, the available labeled training data is such that a supervised learning approach may not yield good software quality prediction. In contrast, a supervised classification scheme aided by unlabeled data, i.e., semisupervised learning, may yield better results. This work investigates semisupervised learning with the expectation maximization (EM) algorithm for the software quality classification problem. Case studies of software measurement data obtained from two NASA software projects, JM1 and KC2, are used in our empirical investigation. A small portion of the JM1 dataset is randomly extracted and used as the labeled data, while the remaining JM1 instances are used as unlabeled data. The performance of the semisupervised classification models built using the EM algorithm is evaluated by using the KC2 project as a test dataset. It is shown that the EM-based semisupervised learning scheme improves the predictive accuracy of the software quality classification models.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1374185,no,undetermined,0
Rotor cage fault diagnosis in induction motors based on spectral analysis of current Hilbert modulus,"Hilbert transformation is an ideal phase shifting tool in data signal processing. Being Hilbert transformed, the conjugated one of a signal is obtained. The Hilbert modulus is defined as the square of a signal and its conjugation. This work presents a method by which rotor faults of squirrel cage induction motors, such as broken rotor bars and eccentricity, can be diagnosed. The method is based on the spectral analysis of the stator current Hilbert Modulus of the induction motors. Theoretical analysis and experimental results demonstrate that has the same rotor fault detecting ability as the extended Park' vector approach. The vital advantage of the former is the smaller hardware and software spending compared with the existing ones.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1373123,no,undetermined,0
Systematic operational profile development for software components,"An operational profile is a quantification of the expected use of a system. Determining an operational profile for software is a crucial and difficult part of software reliability assessment in general and it can be even more difficult for software components. This paper presents a systematic method for deriving an operational profile for software components. The method uses both actual usage data and intended usage assumptions to derive a usage structure, usage distribution and characteristics of parameters (including relationships between parameters). A usage structure represents the flow and interaction of operation calls. Statecharts are used to model the usage structures. A usage distribution represents probabilities of the operations. The method is illustrated on two Java classes but can be applied to any software component that is accessed through an application program interface (API).",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1371957,no,undetermined,0
Empirical evaluation of orthogonality of class mutation operators,"Mutation testing is a fault-based testing technique which provides strong quality assurance. Mutation testing has a very long history for the procedural programs at unit-level testing, but the research on mutation testing of object-oriented programs is still immature. Recently, class mutation operators are proposed to detect object-oriented specific faults. However, any analysis has not been conducted on the class mutation operators. In this paper, we evaluate the orthogonality of the class mutation operators by some experiment. The experimental results show the high possibility that each class mutation operator has fault-revealing power that is not achieved by other mutation operators, i.e. orthogonal. Also, the results show that the number of mutants from the class mutation operators is small so that the cost is not so high as procedural programs.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1371955,no,undetermined,0
A controlled experiment for evaluating a metric-based reading technique for requirements inspection,"Natural language requirements documents are often verified by means of some reading technique. Some recommendations for defining a good reading technique point out that a concrete technique must not only be suitable for specific classes of defects, but also for a concrete notation in which requirements are written. Following this suggestion, we have proposed a metric-based reading (MBR) technique used for requirements inspections, whose main goal is to identify specific types of defects in use cases. The systematic approach of MBR is basically based on a set of rules as ""if the metric value is too low (or high) the presence of defects of type de fType<sub>1</sub>,...de fType<sub>n</sub> must be checked"". We hypothesised that if the reviewers know these rules, the inspection process is more effective and efficient, which means that the defects detection rate is higher and the number of defects identified per unit of time increases. But this hypotheses lacks validity if it is not empirically validated. For that reason the main goal is to describe a controlled experiment we carried out to ascertain if the usage of MBR really helps in the detection of defects in comparison with a simple checklist technique. The experiment result revealed that MBR reviewers were more effective at detecting defects than checklist reviewers, but they were not more efficient, because MBR reviewers took longer than checklist reviewers on average.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357908,no,undetermined,0
Module-order modeling using an evolutionary multi-objective optimization approach,"The problem of quality assurance is important for software systems. The extent to which software reliability improvements can be achieved is often dictated by the amount of resources available for the same. A prediction for risk-based rankings of software modules can assist in the cost-effective delegation of the limited resources. A module-order model (MOM) is used to gauge the performance of the predicted rankings. Depending on the software system under consideration, multiple software quality objectives may be desired for a MOM; e.g., the desired rankings may be such that if 20% of modules were targeted for reliability enhancements then 80% of the faults would be detected. In addition, it may also be desired that if 50% of modules were targeted then 100% of the faults would be detected. Existing works related to MOM(s) have used an underlying prediction model to obtain the rankings, implying that only the average, relative, or mean square errors are minimized. Such an approach does not provide an insight into the behavior of a MOM, the performance of which focusses on how many faults are accounted for by the given percentage of modules enhanced. We propose a methodology for building MOM (s) by implementing a multiobjective optimization with genetic programming. It facilitates the simultaneous optimization of multiple performance objectives for a MOM. Other prediction techniques, e.g., multiple linear regression and neural networks, cannot achieve multiobjective optimization for MOM(s). A case study of a high-assurance telecommunications software system is presented. The observed results show a new promise in the modeling of goal-oriented software quality estimation models.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357900,no,undetermined,0
Increasing the accuracy and reliability of analogy-based cost estimation with extensive project feature dimension weighting,"Accurate and reliable software cost estimation is a vital task in software project portfolio decisions like resource scheduling or bidding. A prominent and transparent method of supporting estimators is analogy-based cost estimation, which is based on finding similar projects in historical portfolio data. However, the various project feature dimensions used to determine project analogy represent project aspects differing widely in their relevance; they are known to have varying impact on the analogies - and in turn on the overall estimation accuracy and reliability - , which is not addressed by traditional approaches. This paper (a) proposes an improved analogy-based approach based on extensive dimension weighting, and (ii) empirically evaluates the accuracy and reliability improvements in the context of five real-world portfolio data sets. Main results are accuracy and reliability improvements for all analyzed portfolios and quality measures. Furthermore, the approach indicates a quality barrier for analogy-based estimation approaches using the same basic assumptions and quality measures.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1334902,no,undetermined,0
Estimation of software defects fix effort using neural networks,"Software defects fix effort is an important software development process metric that plays a critical role in software quality assurance. People usually like to apply parametric effort estimation techniques using historical lines of code and function points data to estimate effort of defects fixes. However, these techniques are neither efficient nor effective for a new different kind of project's fixing defects when code will be written within the context of a different project or organization. In this paper, we present a solution for estimating software defect fix effort using self-organizing neural networks.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342658,no,undetermined,0
Maintenance issues in outsourced software components,"Software outsourcing is becoming a popular alternative in the software industry, and the evidence for the importance of assuring the quality of subcontractors' contributions is found in ISO 9000-3 Std and ISO/IEC 2001. There are risks and benefits of introducing subcontractors in the framework of the project. One of the major risks is the future maintenance difficulty. Reliable maintenance is only possible if adequate measures are taken in advance during the project's development and maintenance planning phase, and documented in the maintenance contract. In this paper, we present and make a justification for a set of recommendations to make such maintenance reliable and cost-effective. We analyze the associated risks and develop a model to estimate the overall product quality metrics during the maintenance phase.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1344973,no,undetermined,0
Polygonal and polyhedral contour reconstruction in computed tomography,"This paper is about three-dimensional (3-D) reconstruction of a binary image from its X-ray tomographic data. We study the special case of a compact uniform polyhedron totally included in a uniform background and directly perform the polyhedral surface estimation. We formulate this problem as a nonlinear inverse problem using the Bayesian framework. Vertice estimation is done without using a voxel approximation of the 3-D image. It is based on the construction and optimization of a regularized criterion that accounts for surface smoothness. We investigate original deterministic local algorithms, based on the exact computation of the line projections, their update, and their derivatives with respect to the vertice coordinates. Results are first derived in the two-dimensional (2-D) case, which consists of reconstructing a 2-D object of deformable polygonal contour from its tomographic data. Then, we investigate the 3-D extension that requires technical adaptations. Simulation results illustrate the performance of polygonal and polyhedral reconstruction algorithms in terms of quality and computation time.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1344040,no,undetermined,0
Geotechnical application of borehole GPR - a case history,"Borehole GPR measurements were performed to complement the site characterization of a planned expansion of a cement plant. This expansion includes a mill and a reclaim facility adjacent to the present buildings, the whole site being located in a karstic environment. Twenty-one geotechnical exploration borings revealed that the depth to bedrock is very irregular (between 1.5 m and 18 m) and that the rocks likely have vertical alteration channels that extend many meters below the rock surface. The purpose of the GPR survey was to reveal the presence of potential cavities and to better determine the required vertical extent of the caissons of the foundations. In general, the subsurface conditions consist of a top fill layer, which is electrically conductive, a residual clay layer and a limestone bedrock. Very poor EM penetration prevented surface measurements. Hence, 100 MHz borehole antennas were used to perform single-hole reflection and cross-hole transmission measurements. Sixteen geotechnical holes were visited during the survey. All holes were surveyed in reflection mode. Nineteen tomographic panels were scanned. Velocity tomogram were obtained for all the data. Attenuation tomography was performed in fewer occasions, due to higher uncertainty in the quality of the amplitude data. Resistivity probability maps were drawn when both velocity and attenuation data were obtained. The velocity tomography calculations were constrained using velocity profiles along the borings. These profiles were obtained by inversion of the single-hole first arrival data. The velocity tomography results show that globally the area can be separated in two zones, one with an average velocity around 0.1 m/ns and one with a slower 0.09 m/ns average velocity. In addition, low velocity anomalies attributable to weathered zones appear in the tomogram. In conclusion, the GPR results revealed no cavities. Sensitive zones were located, which helped the planning and the budgeting of the caissons - onstruction.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1343415,no,undetermined,0
ISPIS: a framework supporting software inspection processes,"This paper describes ISPIS, a computational framework for supporting the software inspection process whose requirements set was derived from knowledge acquired by empirical studies. ISPIS allows the inspection of all artifact types by geographically distributed teams. Specific defect detection support is provided by the integration of external tools. A case study has shown the feasibility of using ISPIS to support real inspections",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342772,no,undetermined,0
Test-suite reduction for model based tests: effects on test quality and implications for testing,"Model checking techniques can be successfully employed as a test case generation technique to generate tests from formal models. The number of tests cases produced, however, is typically large for complex coverage criteria such as MCDC. Test-suite reduction can provide us with a smaller set of test cases that present the original coverage-often a dramatically smaller set. One potential drawback with test-suite reduction is that this might affect the quality of the test-suite in terms of fault finding. Previous empirical studies provide conflicting evidence on this issue. To further investigate the problem and determine its effect when testing formal models of software, we performed an experiment using a large case example of a flight guidance system, generated reduced test-suites for a variety of structural coverage criteria while presenting coverage, and recorded their fault finding effectiveness. Our results show that the size of the specification based test-suites can be dramatically reduced and that the fault detection of the reduced test-suites is adversely affected. In this report we describe our experiment, analyze the results, and discuss the implications for testing based on formal specifications.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342735,no,undetermined,0
A computational framework for supporting software inspections,"Software inspections improve software quality by the analysis of software artifacts, detecting their defects for removal before these artifacts are delivered to the following software life cycle activities. Some knowledge regarding software inspections have been acquired by empirical studies. However, we found no indication that computational support for the whole software inspection process using appropriately such knowledge is available. This paper describes a computational framework whose requirements set was derived from knowledge acquired by empirical studies to support software inspections. To evaluate the feasibility of such framework, two studies have been accomplished: one case study, which has shown the feasibility of using the framework to support inspections, and an experimental study that evaluated the supported software inspection planning activity. Preliminary results of this experimental study suggested that unexperienced subjects are able to plan inspections with higher defect detection effectiveness, and in less time, when using this computational framework.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342723,no,undetermined,0
WS-FIT: a tool for dependability analysis of Web services,This work provides an overview of fault injection techniques and their applicability to testing SOAP RPC based Web service systems. We also give a detailed example of the WS-FIT package and use it to detect a problem in a Web service based system.,2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342690,no,undetermined,0
A view-based control flow metric,"We present a new metric for control flow path sets based on a control flow abstraction technique and evaluate the concept in a study of a large set of artificially generated programs, using a special model of fault detection ability.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342661,no,undetermined,0
Reliability-aware co-synthesis for embedded systems,"As technology scales, transient faults due to single event upsets have emerged as a key challenge for reliable embedded system design. This work proposes a design methodology that incorporates reliability into hardware-software co-design paradigm for embedded systems. We introduce an allocation and scheduling algorithm that efficiently handles conditional execution in multi-rate embedded systems, and selectively duplicates critical tasks to detect soft errors, such that the reliability of the system is increased. The increased reliability is achieved by utilizing the otherwise idle computation resources and incurs no resource or performance penalty. The proposed algorithm is fast and efficient, and is suitable for use in the inner loop of our hardware/software co-synthesis framework, where the scheduling routine has to be invoked many times.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342457,no,undetermined,0
The necessity of assuring quality in software measurement data,"Software measurement data is often used to model software quality classification models. Related literature has focussed on developing new classification techniques and schemes with the aim of improving classification accuracy. However, the quality of software measurement data used to build such classification models plays a critical role in their accuracy and usefulness. We present empirical case studies, which demonstrate that despite using a very large number of diverse classification techniques for building software quality classification models, the classification accuracy does not show a dramatic improvement. For example, a simple lines-of-code based classification performs comparatively to some other more advanced classification techniques such as neural networks, decision trees, and case-based reasoning. Case studies of the NASA JM1 and KC2 software measurement datasets (obtained through the NASA Metrics Data Program) are presented. Some possible reasons that affect the quality of a software measurement dataset include presence of data noise, errors due to improper software data collection, exclusion of software metrics that are better representative software quality indicators, and improper recording of software fault data. This study shows, through an empirical study, that instead of searching for a classification technique that perform well for given software measurement dataset, the software quality and development teams should focus on improving the quality of the software measurement dataset.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357896,no,undetermined,0
Experimental study on QoS provisioning to heterogeneous VoIP sources in Diffserv environment,"The work presents a research activity focused on the experimental study of three different issues related to the provision of VoIP services with QoS guarantee in DiffServ networks. Firstly the study deals with the analysis of two dissimilar strategies for the setting of the parameters of a traffic control module in a DiffServ network node. Using these results, secondly the effectiveness of static and dynamic SLAs strategies for QoS provisioning in DiffServ environment is experimentally evaluated. This analysis is carried out considering aggregation of voice sources adopting two distinct codecs, i.e. G723.1 and G729. These codecs produce traffic with different statistical features (variable and constant bit rate respectively). Hence, this approach allows assessing the impact on the single sources' performance of the multiplexing of heterogeneous VoIP sources in a single class.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1341824,no,undetermined,0
Nanolab: a tool for evaluating reliability of defect-tolerant nano architectures,"As silicon manufacturing technology reaches the nanoscale, architectural designs need to accommodate the uncertainty inherent at such scales. These uncertainties are germane in the miniscule dimension of the device, quantum physical effects, reduced noise margins, system energy levels reaching computing thermal limits, manufacturing defects, aging and many other factors. Defect tolerant architectures and their reliability measures gain importance for logic and micro-architecture designs based on nano-scale substrates. Recently, a Markov random field (MRF) has been proposed as a model of computation for nanoscale logic gates. In this paper, we take this approach further by automating this computational scheme and a belief propagation algorithm. We have developed MATLAB based libraries and toolset for fundamental logic gates that can compute output probability distributions and entropies for specified input distributions. Our tool eases evaluation of reliability measures of combinational logic blocks. The effectiveness of this automation is illustrated in this paper by automatically deriving various reliability results for defect-tolerant architectures, such as triple modular redundancy (TMR), cascaded triple modular redundancy (CTMR) and multi-stage iterations of these. These results are used to analyze trade-offs between reliability and redundancy for these architectural configurations.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1339504,no,undetermined,0
The framework and algorithm of a new phasor measurement unit,"Phasor measurement unit (PMU) has the ability to identify and track the phasor values of voltage and current synchronously on a power system in real time, which is crucial to the detection of disturbances and characterization of transient swings. This paper provides an introduction to the framework of a new-type PMU, including the hardware architecture and the software structure. Then, an improved phasor computation algorithm is proposed to correct the errors occurred by the recursive DFT in the presence of frequency offset. Digital simulations and lab tests were accomplished with the new phasor-tracking algorithm. Results validated the performance of the improved phasor-tracking algorithm and its advantages over the traditional DFT algorithm.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1338097,no,undetermined,0
Efficient design diversity estimation for combinational circuits,"Redundant systems are designed using multiple copies of the same resource (e.g., a logic network or a software module) in order to increase system dependability: Design diversity has long been used to protect redundant systems against common-mode failures. The conventional notion of diversity relies on ""independent"" generation of ""different"" implementations of the same logic function. In a recent paper, we presented a metric to quantify diversity among several designs. The problem of calculating the diversity metric is NP-complete (i.e., can be of exponential complexity). In this paper, we present efficient techniques to estimate the value of the design diversity metric. For datapath designs, we have formulated very fast techniques to calculate the value of the metric by taking advantage of the regularity in the datapath structures. For general combinational logic circuits, we present an adaptive Monte-Carlo simulation technique for estimating accurate bounds on the value of the metric.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1336768,no,undetermined,0
Supporting quality of service in a non-dedicated opportunistic environment,"In This work we investigate the utilization of non-dedicated, opportunistic resources in a desktop environment to provide statistical assurances to a class of QoS sensitive, soft real-time applications. Supporting QoS in such an environment presents unique challenges: (1) soft real-time tasks must have continuous access to resources in order to deliver meaningful services. Therefore the tasks will fail if not enough idle resources are available in the system. (2) Although soft real-time tasks can be migrated from one machine to another, their QoS may be affected if there are frequent migrations. In this paper, we define two new QoS metrics (task failure rate and probability of bad migrations) to characterize these QoS failures/degradations. We also design admission control and resource recruitment algorithms to provide statistical guarantees on these metrics. Our model based simulation results show that the admission control algorithms are effective at providing the desired level of assurances, and are robust to different resource usage patterns. Our resource recruitment algorithm may need long time of observations to provide the desired guarantee. But even with moderate observations, we can reduce the probability of a bad migration from 12% to less than 4%, which is good enough for most real applications.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1336551,no,undetermined,0
Building a genetically engineerable evolvable program (GEEP) using breadth-based explicit knowledge for predicting software defects,"There has been extensive research in the area of data mining over the last decade, but relatively little research in algorithmic mining. Some researchers shun the idea of incorporating explicit knowledge with a Genetic Program environment. At best, very domain specific knowledge is hard wired into the GP modeling process. This work proposes a new approach called the Genetically Engineerable Evolvable Program (GEEP). In this approach, explicit knowledge is made available to the GP. It is considered breadth-based, in that all pieces of knowledge are independent of each other. Several experiments are performed on a NASA-based data set using established equations from other researchers in order to predict software defects. All results are statistically validated.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1336240,no,undetermined,0
Investigating the active guidance factor in reading techniques for defect detection,"Inspections are an established quality assurance technique. In order to optimize the inspection performance, different reading techniques, such as checklist-based reading and scenario-based reading have been proposed. Various experiments have been conducted to evaluate which of these techniques produces better inspection results (i.e., which finds more defects with less effort). However, results of these empirical investigations are not conclusive yet. Thus, the success factors of the different reading approaches need to be further analyzed. In this paper, we report on a preliminary empirical study that examined the influence of the active guidance factor (provided by scenario-based approaches) when inspecting requirements specification documents. First results show that active guidance is accepted with favor by inspectors and suggest that it is better suited for larger and more complex documents.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1334909,no,undetermined,0
"Finding ""early"" indicators of UML class diagrams understandability and modifiability","Given the relevant role that models obtained in the early stages play in the development of OO systems, in the recent years special attention has been paid to the quality of such models. Adhering to this fact, the main objective of this work is to obtain ""early"" indicators of UML class diagrams understandability and modifiability. These indicators will allow OO designers to improve the quality of the diagrams they model and hence contribute improving the quality of the OO systems, which are finally delivered. The empirical data were obtained through a controlled experiment and its replication we carried out for obtaining prediction models of the Understandability and Modifiability Time of UML class diagrams based on a set of metrics previously defined for UML class diagrams structural complexity and size. The obtained results, reveal that the metrics that count the number of methods (NM), the number of attributes (NA), the number of generalizations (NGen), the number of dependencies (NDEP), the maximum depth of the generalization hierarchies (MaxDIT) and the maximum height of the aggregation hierarchies (MaxHAgg) could influence the effort needed to maintain UML class diagrams.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1334908,no,undetermined,0
The application of LDPC codes in image transmission,"In this paper, we combine the adaptive modulation and coding (AMC) with LDPC (low-density parity-check) codes and apply it to image transmission under Rayleigh fading channels. The transmitter adjusts the modulation mode according to the channel state information. Through computer simulation, we get the performance of our adaptive LDPC scheme in image transmission under certain demand of BER, and we also study the impacts of channel estimation error. We can see that in image transmission, our adaptive LDPC scheme can provide an acceptable image quality, meanwhile a higher spectral efficiency.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1345933,no,undetermined,0
Modeling and identification for high-performance robot control: an RRR-robotic arm case study,"This paper explains a procedure for getting models of robot kinematics and dynamics that are appropriate for robot control design. The procedure consists of the following steps: 1) derivation of robot kinematic and dynamic models and establishing correctness of their structures; 2) experimental estimation of the model parameters; 3) model validation; and 4) identification of the remaining robot dynamics, not covered with the derived model. We give particular attention to the design of identification experiments and to online reconstruction of state coordinates, as these strongly influence the quality of the estimation process. The importance of correct friction modeling and the estimation of friction parameters are illuminated. The models of robot kinematics and dynamics can be used in model-based nonlinear control. The remaining dynamics cannot be ignored if high-performance robot operation with adequate robustness is required. The complete procedure is demonstrated for a direct-drive robotic arm with three rotational joints.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1347177,no,undetermined,0
Myrinet networks: a performance study,"As network computing become commonplace, the interconnection networks and the communication system software become critical in achieving high performance. Thus, it is essential to systematically assess the features and performance of the new networks. Recently, Myricom has introduced a two-port ""E-card"" Myrinet/PCl-X interface. In this paper, we present the basic performance of its GM2.I messaging layer, as well as a set of microbenchmarks designed to assess the quality of MPI implementation on top of GM. These microbenchmarks measure the latency, bandwidth, intra-node performance, computation/communication overlap, parameters of the LogP model, buffer reuse impact, different traffic patterns, and collective communications. We have discovered that the MPI basic performance is close to those offered at the GM. We find that the host overhead is very small in our system. The Myrinet network is shown to be sensitive to the buffer reuse patterns. However, it provides opportunities for overlapping computation with communication. The Myrinet network is able to deliver up to 2000MB/s bandwidth for the permutation patterns.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1347794,no,undetermined,0
Testing and defect tolerance: a Rent's rule based analysis and implications on nanoelectronics,"Defect tolerant architectures will be essential for building economical gigascale nanoelectronic computing systems to permit functionality in the presence of a significant number of defects. The central idea underlying a defect tolerant configurable system is to build the system out of partially perfect components, detect the defects and configure the available good resources using software. In this paper we discuss implications of defect tolerance on power area, delay and other relevant parameters for computing architectures. We present a Rent's rule based abstraction of testing for VLSI systems and evaluate the redundancy requirements for observability. It is shown that for a very high interconnect defect density, a prohibitively large number of redundant components are necessary for observability and this has adverse affect on the system performance. Through a unified framework based on a priori wire length estimation and Rent's rule we illustrate the hidden cost of supporting such an architecture.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1347850,no,undetermined,0
An evaluation of k-nearest neighbour imputation using Likert data,"Studies in many different fields of research suffer from the problem of missing data. With missing data, statistical tests will lose power, results may be biased, or analysis may not be feasible at all. There are several ways to handle the problem, for example through imputation. With imputation, missing values are replaced with estimated values according to an imputation method or model. In the k-nearest neighbour (k-NN) method, a case is imputed using values from the k most similar cases. In this paper, we present an evaluation of the k-NN method using Likert data in a software engineering context. We simulate the method with different values of k and for different percentages of missing data. Our findings indicate that it is feasible to use the k-NN method with Likert data. We suggest that a suitable value of k is approximately the square root of the number of complete cases. We also show that by relaxing the method rules with respect to selecting neighbours, the ability of the method remains high for large amounts of missing data without affecting the quality of the imputation.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357895,no,undetermined,0
A neuro-fuzzy tool for software estimation,"Accurate software estimation such as cost estimation, quality estimation and risk analysis is a major issue in software project management. We present a soft computing framework to tackle this challenging problem. We first use a preprocessing neuro-fuzzy inference system to handle the dependencies among contributing factors and decouple the effects of the contributing factors into individuals. Then we use a neuro-fuzzy bank to calibrate the parameters of contributing factors. In order to extend our framework into fields that lack of an appropriate algorithmic model of their own, we propose a default algorithmic model that can be replaced when a better model is available. Validation using industry project data shows that the framework produces good results when used to predict software cost.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357862,no,undetermined,0
"Analysis, testing and re-structuring of Web applications","The current situation in the development of Web applications is reminiscent of the early days of software systems, when quality was totally dependent on individual skills and lucky choices. In fact, Web applications are typically developed without following a formalized process model: requirements are not captured and design is not considered; developers quickly move to the implementation phase and deliver the application without testing it. Not differently from more traditional software system, however, the quality of Web applications is a complex, multidimensional attribute that involves several aspects, including correctness, reliability, maintainability, usability, accessibility, performance and conformance to standards. In this context, aim of this PhD thesis was to investigate, define and apply a variety of conceptual tools, analysis, testing and restructuring techniques able to support the quality of Web applications. The goal of analysis and testing is to assess the quality of Web applications during their development and evolution; restructuring aims at improving the quality by suitably changing their structure.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357838,no,undetermined,0
Developing a multi-objective decision approach to select source-code improving transformations,"Our previous work on improving the quality of object-oriented legacy systems through re-engineering proposed a software transformation framework based on soft-goal inter-dependency graphs (Tahvildari and Kontogiannis, 2002). We considered a class of transformations where a program is transformed into another program in the same language (source-to-source transformations) and that the two programs may differ in specific qualities such as performance and maintainability. This paper defines a decision making process that determines a list of source-code improving transformations among several applicable transformations. The decision-making process is developed on a multi-objective decision analysis technique. This type of technique is necessary as there are a number of different, and sometimes conflicting, criterion among nonfunctional requirements. For the migrant system, the proposed approach uses heuristic estimates to guide the discovery process.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357829,no,undetermined,0
Weighted least square estimation algorithm with software phase-locked loop for voltage sag compensation by SMES,"A superconducting magnetic energy storage (SMES) system is developed to protect a critical load from momentary voltage sags. This system is composed of a 0.3 MJ SMES coil, a 150 KVA IGBT-based current source inverter and a phase-shift inductor. In order to compensate the load voltage effectively whenever a source voltage sag happens, it is crucial for the signal processing algorithm to extract the fundamental components from the sample signals quickly, precisely and stably. In this paper, an estimation algorithm based on the weighted least square principle is developed to detect the positive- and negative-sequence fundamental components from the measured AC voltages and currents. A software phase-locked loop (SPLL) is applied to track the positive-sequence component of the source voltage. Simulations and experiments are carried out to demonstrate the algorithms. The results are presented.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1355430,no,undetermined,0
Power quality factor and line-disturbances measurements in three-phase systems,"A power quality meter (PQM) is presented for measuring, as a first objective, a single indicator, designated power quality factor (PQF), in the range between zero to one, which integrally reflect the power transfer quality of a general three phase network feeding unbalanced nonlinear loads. PQF definition is based on the analysis of functions in the frequency domain, separating the fundamental terms from the harmonic terms of the Fourier series. Then, quality aspects considered in the PQF definition can be calculated: a) the voltage and current harmonic levels b) the degree of unbalance and c) the phase displacement factor in the different phases at the fundamental frequency. As a second objective, the PQM has been designed for detecting, classifying and organizes power line disturbances. For monitoring power line disturbances, the PQM is configured as virtual instrument, which automatically classifies and organizes them in a database while they are being recorded. The type of disturbances includes: impulse, oscillation, sag, swell, interruption, undervoltage, overvoltage, harmonics and frequency variation. For amplitude disturbances (impulse, sag, swell, interruption, undervoltage and overvoltage), the PQM permits the measurement of parameters such as amplitude, start time and final time. Measurement of harmonic distortion allows recording and visual presentation of the spectrum of amplitudes and phases corresponding to the first 40 harmonics. Software tools use the database structure to present summaries of power disturbances and locate an event by severity or time of occurrence. Simulated measurements are included to demonstrate the versatility of the instrument.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1355328,no,undetermined,0
Who solved the optimal software release problems based on Markovian software reliability model?,"In this paper, we answer the titled question for the typical software release problems based on a Markovian software reliability model. Under the assumption that the software fault-detection process is described by a Markovian birth process with absorption, like Jelinski and Moranda model (1972), we formulate the total expected software costs with two different release policies, and compare them in terms of the cost minimization. Also, we consider the other software cost model taking account of gain in reliability. It can be concluded throughout numerical examples that the existing optimal software release policy underestimates and overestimates the real optimal software release time and its associated cost function, respectively.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1354398,no,undetermined,0
Measuring application error rates for network processors,"Faults in computer systems can occur due to a variety of reasons. In many systems, an error has a binary effect, i.e. the output is either correct or it is incorrect. However, networking applications exhibit different properties. For example, although a portion of the code behaves incorrectly due to a fault, the application can still work correctly. Integrity of a network system is often unchanged during faults. Therefore, measuring the effects of faults on the network processor applications require new measurement metrics to be developed. In this paper, we highlight essential application properties and data structures that can be used to measure the error behavior of network processors. Using these metrics, we study the error behavior of seven representative networking applications under different cache access fault probabilities.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1354210,no,undetermined,0
"Software's secret sauce: the ""-ilities"" [software quality]","If beauty is in the eye of the beholder, then quality must be as well. We live in a world where beauty to one is a complete turnoff to another. Software quality is no different. We have the developer's perspective, the end users perspective, the testers perspective, and so forth. As you can see, meeting the requirements might be different from being fit for a purpose, which can also be different from complying with rules and regulations on how to develop and deploy the software. Yet we can think of all three perspectives as ways to determine how to judge and assess software quality. These three perspectives tie directly to the persistent software attributes focus section in this issue and, consequently, to the concept of software ""-ilities"". The -ilities (or software attributes) are a collection of closely related behaviors that by themselves have little or no value to the end users but that can greatly increase a software application or system's value when added.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1353217,no,undetermined,0
Synchronization techniques for power quality instruments,"Voltage characteristics measurement in power systems requires the accurate fundamental frequency estimation and the signal synchronization, even in the presence of disturbances. In this paper, the authors have developed and tested two innovative techniques for instrument synchronization. The first is based on signal spectral analysis techniques, performed by means of chirp-Z (CZT) analysis. The second is a PLL software, based upon a time-domain coordinate transformation and an innovative phase detection technique. In order to evaluate how synchronization techniques are adversely affected by the application of a disturbing influence, experimental tests have been carried out, taking into account the requirements of the standards. The proposed techniques have been compared with standard hardware solutions. In the paper, the proposed techniques are described, experimental results are presented and the accuracy specifications are discussed.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1351327,no,undetermined,0
Collaborative multisensor network architecture based on smart Web sensors for power quality applications,"In this paper, the aim of the authors is to propose a multisensor network architecture for distributed power quality measurements, based on the use of low cost smart Web sensors and on a collaborative mobile agent mechanism that allow an easy remote access, for a generic user, to the measurement results. The network has a full decentralized processing architecture in which the sensors can collaborate in the signal analysis. A low cost smart Web sensor is also presented. It is mainly characterized by an architecture, both hardware and software, modular and adaptable to specific requirement that allows an easy integration into distributed measurement systems. Finally, in-field measurements, conducted on low voltage sub-networks, are reported for some of the main power quality disturbances.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1351319,no,undetermined,0
Instantaneous line-frequency measurement under nonstationary situations,"A virtual instrument for the measurement of instantaneous power-system-frequency is proposed. It is based on the frequency estimation of the voltage signal using three equidistant samples. An algorithm is further developed that diminishes the variance of the estimation. The procedure is applied to the case of single and three-phase networks and relative errors in the frequency estimation are obtained. Low cost hardware, consisting of compatible PC, standard data acquisition card and signal conditioning module, has been used in conjunction with a software application developed with LABVIEWâ„? Finally, measurements using single and three-phase signals, simulating severe conditions of signal quality, were performed. A variation of the frequency throughout the measurement time has been assumed, according to a sinusoidal signal of 5 Hz, within a Â±1 Hz margin. The developed tool has been proven, with worst case data and relative errors of 0.1% and 0.025% having been obtained for single and three-phase signals, respectively.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1351217,no,undetermined,0
Checkpointing for peta-scale systems: a look into the future of practical rollback-recovery,"Over the past two decades, rollback-recovery via checkpoint-restart has been used with reasonable success for long-running applications, such as scientific workloads that take from few hours to few months to complete. Currently, several commercial systems and publicly available libraries exist to support various flavors of checkpointing. Programmers typically use these systems if they are satisfactory or otherwise embed checkpointing support themselves within the application. In this paper, we project the performance and functionality of checkpointing algorithms and systems as we know them today into the future. We start by surveying the current technology roadmap and particularly how Peta-Flop capable systems may be plausibly constructed in the next few years. We consider how rollback-recovery as practiced today will fare when systems may have to be constructed out of thousands of nodes. Our projections predict that, unlike current practice, the effect of rollback-recovery may play a more prominent role in how systems may be configured to reach the desired performance level. System planners may have to devote additional resources to enable rollback-recovery and the current practice of using ""cheap commodity"" systems to form large-scale clusters may face serious obstacles. We suggest new avenues for research to react to these trends.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1350776,no,undetermined,0
Electronic circuits tests in laboratory for superimposing voltages techniques,The tests carried out in electrical and electronic laboratories allow researchers to advance in the field of superposing voltages techniques. The use of simulation software applications are mentioned as part of this study. The real measurements have been taken in two environments. The first tests were developed over the line used to feed the laboratory equipment. The second activity has been carried out in an environment more secure for researchers and without monopolizing a power line to develop the tests.,2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1348221,no,undetermined,0
Wavelet based transformer protection,"This paper presents the development of a wavelet based transformer protection algorithm. In this paper, a laboratory transformer is modelled and simulated to see the accuracy of the proposed algorithm by using ATP-EMTP. The main objective of this paper is to analyze discontinuities in current signals during phase to ground faults and phase to phase faults. The generated data is used with the MATLAB software to test the performance of the proposed technique regarding the speed of response, computational burden and reliability. maximum likelihood parameter estimation is used to interpret detail coefficients.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1348199,no,undetermined,0
Estimating the size of Web applications by using a simplified function point method,"Software size estimation is a key factor to determine the amount of time and effort needed to develop software systems, and the Web applications are no exception. In this paper a simplified way of the IFPUG (International Function Point Users Group) function points based on the simplification ideas suggested by NESMA (Netherlands Software Metrics Association) to estimate size of management information systems is presented. In an empirical study, twenty Web applications were analyzed. The estimates using the simplified method were close to the ones using the IFPUG detailed method. Based on the results, it was possible to establish a simplified method to estimate the size of Web applications according to the development characteristics of the studied company.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1348154,no,undetermined,0
Evaluation of reward analysis methods with MRMSolve 2.0,"The paper presents the 2.0 release of MRMSolve, a Markov reward model (MRM) analysis tool. This new release integrates recent research results, such as MRMs with partial reward loss, second order MRMs and their combination, with old and widely known reward analysis methods, such as the ones by DeSouza-Gail, Nabli-Sericola and Donatiello-Grassi. The paper compares the mentioned direct distribution analysis methods with each other and with the moments based reward estimation methods.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1348031,no,undetermined,0
The Effects of an ARMOR-based SIFT environment on the performance and dependability of user applications,"Few, distributed software-implemented fault tolerance (SIFT) environments have been experimentally evaluated using substantial applications to show that they protect both themselves and the applications from errors. We present an experimental evaluation of a SIFT environment used to oversee spaceborne applications as part of the Remote Exploration and Experimentation (REE) program at the Jet Propulsion Laboratory. The SIFT environment is built around a set of self-checking ARMOR processes running on different machines that provide error detection and recovery services to themselves and to the REE applications. An evaluation methodology is presented in which over 28,000 errors were injected into both the SIFT processes and two representative REE applications. The experiments were split into three groups of error injections, with each group successively stressing the SIFT error detection and recovery more than the previous group. The results show that the SIFT environment added negligible overhead to the application's execution time during failure-free runs. Correlated failures affecting a SIFT process and application process are possible, but the division of detection and recovery responsibilities in the SIFT environment allows it to recover from these multiple failure scenarios. Only 28 cases were observed in which either the application failed to start or the SIFT environment failed to recognize that the application had completed. Further investigations showed that assertions within the SIFT processes-coupled with object-based incremental checkpointing-were effective in preventing system failures by protecting dynamic data within the SIFT processes.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1274045,no,undetermined,0
Speed-accuracy tradeoff during performance of a tracking task without visual feedback,"To help people with visual impairment, especially people with severely impaired vision, access graphic information on a computer screen, we have carried out fundamental research on the effect of increasing the number of detection fields. In general, application of the parallelism concept enables information to be accessed more precisely and easily when the number of sensors is high. We have developed a ""Braille Box"" by modifying Braille cells to form a tactile stimulator array which is compatible with the fingertip. Each pin can be controlled independently so that we can change the size and type of array in order to study the tactile perception of both simple and complex graphical forms. Our results show that by applying the parallelism concept to the detection field, people with visual impairment can increase the speed of exploration of geometric forms without decreasing the level of accuracy: thus, avoiding a speed-accuracy tradeoff. Further experiments need to be done with this Braille Box in order to improve the device and help people with visual impairment access graphic information.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1273531,no,undetermined,0
Empirical analysis of safety-critical anomalies during operations,"Analysis of anomalies that occur during operations is an important means of improving the quality of current and future software. Although the benefits of anomaly analysis of operational software are widely recognized, there has been relatively little research on anomaly analysis of safety-critical systems. In particular, patterns of software anomaly data for operational, safety-critical systems are not well understood. We present the results of a pilot study using orthogonal defect classification (ODC) to analyze nearly two hundred such anomalies on seven spacecraft systems. These data show several unexpected classification patterns such as the causal role of difficulties accessing or delivering data, of hardware degradation, and of rare events. The anomalies often revealed latent software requirements that were essential for robust, correct operation of the system. The anomalies also caused changes to documentation and to operational procedures to prevent the same anomalous situations from recurring. Feedback from operational anomaly reports helped measure the accuracy of assumptions about operational profiles, identified unexpected dependencies among embedded software and their systems and environment, and indicated needed improvements to the software, the development process, and the operational procedures. The results indicate that, for long-lived, critical systems, analysis of the most severe anomalies can be a useful mechanism both for maintaining safer, deployed systems and for building safer, similar systems in the future.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1271171,no,undetermined,0
Model-based segmentation and tracking of head-and-shoulder video objects for real time multimedia services,"A statistical model-based video segmentation algorithm is presented for head-and-shoulder type video. This algorithm uses domain knowledge by abstracting the head-and-shoulder object with a blob-based statistical region model and a shape model. The object segmentation problem is then converted into a model detection and tracking problem. At the system level, a hierarchical structure is designed and spatial and temporal filters are used to improve segmentation quality. This algorithm runs in real time over a QCIF size video, and segments it into background, head and shoulder three video objects on average Pentium PC platforms. Due to its real time feature, this algorithm is appropriate for real time multimedia services such as videophone and web chat. Simulation results are offered to compare MPEG-4 performance with H.263 on segmented video objects with respects to compression efficiency, bit rate adaptation and functionality.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1223565,no,undetermined,0
Improving Chinese/English OCR performance by using MCE-based character-pair modeling and negative training,"In the past several years, we've been developing a high performance OCR engine for machine printed Chinese/ English documents. We have reported previously (1) how to use character modeling techniques based on MCE (minimum classification error) training to achieve the high recognition accuracy, and (2) how to use confidence-guided progressive search and fast match techniques to achieve the high recognition efficiency. In this paper, we present two more techniques that help reduce search errors and improve the robustness of our character recognizer. They are (1) to use MCE-trained character-pair models to avoid error-prone character-level segmentation for some trouble cases, and (2) to perform a MCE-based negative training to improve the rejection capability of the recognition models on the hypothesized garbage images during recognition process. The efficacy of the proposed techniques is confirmed by experiments in a benchmark test.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1227690,no,undetermined,0
Quantifying architectural attributes,"Summary form only given. Traditional software metrics are inapplicable to software architectures, because they require information that is not available at the architectural level, and reflect attributes that are not meaningful at this level. We briefly present architecture-relevant quality attributes, then we introduce architecture-enabled quantitative functions, and run an experiment which shows how and to what extent the latter are correlated to (hence can be used to predict) the former.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1227498,no,undetermined,0
The life cycle of a fuzzy knowledge-based classifier,"We describe the life cycle of a fuzzy knowledge-based classifier with special emphasis on one of its most neglected steps: the maintenance of its knowledge base. First, we analyze the process of underwriting Insurance applications, which is the classification problem used to illustrate the life cycle of a classifier. After discussing some design tradeoffs that must be addressed for the on-line and off-line use of a classifier, we describe the design and implementation of a fuzzy rule-based (FRB) and a fuzzy case-based (FCB) classifier. We establish a standard reference dataset (SRD), consisting of 3,000 insurance applications with their corresponding decisions. The SRD exemplifies the results achieved by an ideal, optimal classifier, and represents the target for our design. We apply evolutionary algorithms to perform an off-line optimization of the design parameters of each classifier, modifying their behavior to approximate this target. The SRD is also used as a reference for testing and performing a five-fold cross-validation of the classifiers. Finally, we focus on the monitoring and maintenance of the FRB classifier. We describe a fusion architecture that supports an off-line quality assurance process of the on-line FRB classifier. The fusion module takes the outputs of multiple classifiers, determines their degree of consensus, and compares their overall agreement with that of the FRB classifier. From this analysis, we can identify the most suitable cases to update the SRD, to audit, or to be reviewed by senior underwriters.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1226834,no,undetermined,0
Evaluating four white-box test coverage methodologies,"This paper presents an illustrative study aimed at evaluating the effectiveness of four white-box test coverage techniques for software programs. In the study, an experimental design was considered which was used to evaluate the chosen testing techniques. The evaluation criteria were determined both in terms of the ability to detect faults and the number of test cases required. Faults were seeded artificially into the program and several faulty-versions of programs (mutants) were generated taking help of mutation operators. Test case execution and coverage measurements were done with the help of two testing tools, Cantata and OCT. Separate regression models relating coverage and effectiveness (fault detection ability and number of test cases required) are developed. These models can be helpful for determining test effectiveness when the coverage levels are known in a problem domain.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1226246,no,undetermined,0
Identifying effective software metrics using genetic algorithms,"Various software metrics may be used to quantify object-oriented source code characteristics in order to assess the quality of the software. This type of software quality assessment may be viewed as a problem of classification: given a set of objects with known features (software metrics) and group labels (quality rankings), design a classifier that can predict the quality rankings of new objects using only the software metrics. We have obtained a variety of software measures for a Java application used for biomedical data analysis. A system architect has ranked the quality of the objects as low, medium-low, medium or high with respect to maintainability. A commercial program was used to parse the source code identifying 16 metrics. A genetic algorithm (GA) was implemented to determine which subset of the various software metrics gave the best match to the quality ranking specified by the expert. By selecting the optimum metrics for determining object quality, GA-based feature selection offers an insight into which software characteristics developers should try to optimize.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1226139,no,undetermined,0
Discriminatory software metric selection via a grid of interconnected multilayer perceptrons,"Software metrics quantify source code characteristics for the purpose of software quality analysis. An initial approach to the difficulty in mapping source code to quality rankings is to multiply the number of features collected. However, as the number of metrics used for analysis increases, rules of thumb for robust classification are violated, ultimately reducing confidence in the quality assessment. Thus, a metric selection method is necessary. This paper examines the ability of a grid of interconnected multilayer perceptrons to select an appropriate subset of software metrics. Local interconnections between the multilayer perceptrons, in the form of feature evolution heuristics, allow publication of discriminatory features. The combination of competitive publication of discriminatory features with a limited number of inputs leads to classifiers that conform to robust classifier design rules. This paper examines the determination of discriminatory feature subsets by a grid of multilayer perceptrons in relation to a gold standard provided by a software architect.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1226096,no,undetermined,0
A cognitive complexity metric based on category learning,"Software development is driven by software comprehension. Controlling a software development process is dependent on controlling software comprehension. Measures of factors that influence software comprehension are required in order to achieve control. The use of high-level languages results in many different kinds of lines of code that require different levels of comprehension effort. As the reader learns the set of arrangements of operators, attributes and labels particular to an application, comprehension is eased as familiar arrangements are repeated. Elements of cognition that describe the mechanics of comprehension serve as a guide to assessing comprehension demands in the understanding of programs written in high level languages. A new metric, kinds of lines of code identifier density is introduced and a case study demonstrates its application and importance. Related work is discussed.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1225966,no,undetermined,0
Nonideal battery properties and their impact on software design for wearable computers,"This paper describes nonideal properties of batteries and how these properties may impact power-performance trade offs in wearable computing. The first part of the paper details the characteristics of an ideal battery and how these characteristics are used in sizing batteries and estimating discharge times. Typical nonideal characteristics and the regions of operation where they occur are described. The paper then presents results from a first-principles, variable-load battery model, showing likely areas for exploiting battery behavior in mobile computing. The major result is that, when battery behavior is nonideal, lowering the average power or the energy per operation may not increase the amount of computation that can be completed in a battery life.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1223632,no,undetermined,0
An exploratory study of software review in practice,"The aim of this paper is to identify the key software review inputs that significantly affect review performance in practice. Five in-depth semi-structured interviews were conducted with different IT organizations. From the interviews' results, the typical issues for conducting software review include (1) selecting right reviewers to perform a defect detection task, (2) the limitation of time and resources for organizing and conducting software review (3) no standard and specific guideline to measure an effective review for different types of software artifacts (i.e. requirement, design, code and test cases). Thus the result shows that the experience (i.e. knowledge and skills) of reviewers is the most significant input influencing software review performance.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1222807,no,undetermined,0
Transparent distributed threads for Java,"Remote method invocation in Java RMI allows the flow of control to pass across local Java threads and thereby span multiple virtual machines. However, the resulting distributed threads do not strictly follow the paradigm of their local Java counterparts for at least three reasons. Firstly, the absence of a global thread identity causes problems when reentering monitors. Secondly, blocks synchronized on remote objects do not work properly. Thirdly, the thread interruption mechanism for threads executing a remote call is broken. These problems make multi-threaded distributed programming complicated and error prone. We present a two-level solution: On the library level, we extend KaRMI (Philippsen et al. (2000)), a fast replacement for RMI, with global thread identities for eliminating problems with monitor reentry. Problem with synchronization on remote objects are solved with a facility for remote monitor acquisition. Our interrupt forwarding mechanism enables the application to get full control over its distributed threads. On the language level, we integrate these extensions with JavaParty's transparent remote objects (Philippsen et al. (1997)) to get transparent distributed threads. We finally evaluate our approach with benchmarks that show costs and benefits of our overall design.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1213261,no,undetermined,0
Performance evaluation of a perceptual ringing distortion metric for digital video,"This paper evaluates a perceptual impairment measure for ringing artifacts, which are common in hybrid MC/DPCM/DCT coded video, as a predictor of the mean opinion score (MOS) obtained in the standard subjective assessment. The perceptual ringing artifacts measure is based on a vision model and a ringing distortion region segmentation algorithm, which is converted into a new perceptual ringing distortion metric (PRDM) on a scale of 0 to 5. This scale corresponds to a modified double-stimulus impairment scale variant II (DSIS-II) method. The Pearson correlation, the Spearman rank order correlation and the average absolute error are used to evaluate the performance of the PRDM compared with the subjective test data. The results show a strong correlation between the PRDM and the MOS with respect to ringing artifacts.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1221045,no,undetermined,0
Wire length prediction based clustering and its application in placement,"In this paper, we introduce a metric to evaluate proximity of connected elements in a netlist. Compared to connectivity by S. Hauck and G. Borriello (1997) and edge separability by J. Cong and S.K. Lim (2000), our metric is capable of predicting short connections more accurately. We show that the proposed metric can also predict relative wire length in multipin nets. We develop a fine-granularity clustering algorithm based on the new metric and embed it into the Fast Placer Implementation (FPI) framework by B. Hu and M. Marek-Sadowska (2003). Experimental results show that the new clustering algorithm produces better global placement results than the net absorption of Hu and M. Marek-Sadowska (2003) algorithm, connectivity of S. Hauck and G. Borriello (1997), and edge separability of J. Cong and S.K. Lim (2000) based algorithms. With the new clustering algorithm, FPI achieves up to 50% speedup compared to the latest version of Capo8.5 in http://vlsicad.ucsd.edu/Resources/SoftwareLinks/PDtools/, without placement quality losses.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1219128,no,undetermined,0
Cost driven six sigma analysis for CE software development,Increased software demands from consumers lead CE (consumer electronics) companies towards a shift from hardware-oriented to a more software-oriented industry. Many CE companies spend a tremendous amount of money on software development and post release defect fixing. We propose a modification of the traditional six sigma approach to fully analyze and measure software development cost.,2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1218877,no,undetermined,0
A novel and efficient control strategy for three-phase boost rectifiers,"To date three-phase boost rectifiers, due to their high efficiency, good current quality and low EMI emissions are widely used in industry as power factor correction (PFC) converters. Performance criteria of these converters significantly improve with increasing the switching frequency, and highly depend on the control strategy used. This paper introduces a simple and fast control strategy for three-phase boost rectifiers. The proposed method maintains the advantages of the other schemes, including high efficiency, fast dynamic response, unity power factor, sinusoidal input currents and regulated output voltage. In particular, by the introduction of a modified vector classification algorithm nonlinear function generations. Also abc-dq and dq-Î±Î² transformation carried out in conventional SVM-based strategies are avoided. Therefore, there is no need for current regulators in the d-q frame, and associated control difficulties are removed. Simulation results on PSCAD/EMTDC software program, confirm the validity of the analytical work.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1218142,no,undetermined,0
Combined wavelet domain and temporal video denoising,"We develop a new filter which combines spatially adaptive noise filtering in the wavelet domain and temporal filtering in the signal domain. For spatial filtering, we propose a new wavelet shrinkage method, which estimates how probable it is that a wavelet coefficient represents a ""signal of interest"" given its value, given the locally averaged coefficient magnitude and given the global subband statistics. The temporal filter combines a motion detector and recursive time-averaging. The results show that this combination outperforms single resolution spatio-temporal filters in terms of quantitative performance measures as well as in terms of visual quality. Even though our current implementation of the new filter does not allow real-time processing, we believe that its optimized software implementation could be used for real- or near real-time filtering.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1217940,no,undetermined,0
Detection of errors in case hardening processes brought on by cooling lubricant residues,"Life cycle of case hardened steel work pieces depends on the quality of hardening. A large influencing factor on the quality of hardening is the cleanliness of the work pieces. In manufacturing a large amount of auxiliary materials such as cooling lubricants and drawing compounds are used to ensure correct execution of cutting and forming processes. Especially the residues of cooling lubricants are carried into following processes on the surfaces of the machined parts. Stable and controlled conditions cannot be guaranteed for these subsequent processes as the residues' influence on the process performance is known insufficiently, leading to a high uncertainty and consequently high expense factor. Therefore, information is needed about the type and amount of contamination. In practice the influence of these cooling lubricants on case hardening steels is a well-known phenomenon but correlation of the residue volume and resulting hardness are not known. A short overview of the techniques to detect cooling lubricant residues will be given in this paper and a method to detect the influence of the residues on the hardening process of case hardening steels will be shown. An example will be given for case hardening steel 16MnCr5 (1.7131). The medium of contamination is ARAL SAROL 470 EP.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1217209,no,undetermined,0
Uncertainty in the output of artificial neural networks,"Analysis of the performance of artificial neural networks (ANNs) is usually based on aggregate results on a population of cases. In this paper, we analyze ANN output corresponding to the individual case. We show variability in the outputs of multiple ANNs that are trained and ""optimized"" from a common set of training cases. We predict this variability from a theoretical standpoint on the basis that multiple ANNs can be optimized to achieve similar overall performance on a population of cases, but produce different outputs for the same individual case because the ANNs use different weights. We use simulations to show that the average standard deviation in the ANN output can be two orders of magnitude higher than the standard deviation in the ANN overall performance measured by the A<sub>z</sub> value. We further show this variability using an example in mammography where the ANNs are used to classify clustered microcalcifications as malignant or benign based on image features extracted from mammograms. This variability in the ANN output is generally not recognized because a trained individual ANN becomes a deterministic model. Recognition of this variability and the deterministic view of the ANN present a fundamental contradiction. The implication of this variability to the classification task warrants additional study.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1216214,no,undetermined,0
"Low-cost, on-line software-based self-testing of embedded processor cores","A comprehensive online test strategy requires both concurrent and non-concurrent fault detection capabilities to guarantee SoCs's successful normal operation in-field at any level of its life cycle. While concurrent fault detection is mainly achieved by hardware or software redundancy, like duplication, non-concurrent fault detection, particularly useful for periodic testing, is usually achieved through hardware BIST. Software-based self-test has been recently proposed as an effective alternative to hardware-based self-test allowing at-speed testing while eliminating area, performance and power consumption overheads. In this paper we focus on the applicability of software-based self-test to non-concurrent on-line testing of embedded processor cores. Low-cost in-field testing requirements, particularly small test execution time and low power consumption guide the development of self-test routines. We show how self-test programs with a limited number of memory references and based on compact test routines provide an efficient low-cost on-line test strategy.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1214382,no,undetermined,0
Stability and volatility in the Linux kernel,"Packages are the basic units of release and reuse in software development. The contents and boundaries of packages should therefore be chosen to minimize change propagation and maximize reusability. This suggests the need for a predictive measure of stability at the package level. We observed the rates of change of packages in Linux, a large open-source software system. We compared our empirical observations to a theoretical 'stability metric' proposed by Martin. In this case, we found that Martin's metric has no predictive value.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1231215,no,undetermined,0
Business rule evolution and measures of business rule evolution,"There is an urgent industrial need to enforce the changes of business rules (BRs) to software systems quickly, reliably and economically. Unfortunately, evolving BRs in most existing software systems is both time-consuming and error-prone. In order to manage, control and improve BR evolution, it is necessary that the software evolution community comes to an understanding of the ways in which BRs are implemented and how BR evolution can be facilitated or hampered by the design of software systems. We suggest that new software metrics are needed to allow us to measure the characteristics of BR evolution and to help us to explore possible improvements in a systematic way. A suitable set of BR-related metrics help us to discover the root causes of the difficulties inherent in BR evolution, evaluate the success of proposed approaches to BR evolution and improve the BR evolution process as a whole.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1231218,no,undetermined,0
E-MAGINE: the development of an evaluation method to assess groupware applications,"This paper describes the development of the evaluation method E-MAGINE. The aim of this evaluation method is to support groups to efficiently assess the groupware applications, which fit them best. The method focuses on knowledge sharing groups and follows a modular structure. E-MAGINE is based on the Contingency Perspective in order to structurally characterize groups in their context. Another main building block of the method forms the new ISO-norm for ICT tools, ""Quality in Use."" The overall method comprises two main phases. The initial phase leads to a first level profile and provides an indication of possible mismatches between group and application. The formulation of this initial profile has the benefit of providing a clear guide for further decisions on what instruments should be applied in the final phase of the evaluation process. It is argued that E-MAGINE fulfills the demand for a more practical and efficient groupware evaluation approach.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1231400,no,undetermined,0
Observations on balancing discipline and agility,"Agile development methodologies promise higher customer satisfaction, lower defect rates, faster development times and a solution to rapidly changing requirements. Plan-driven approaches promise predictability, stability, and high assurance. However, both approaches have shortcomings that, if left unaddressed, can lead to project failure. The challenge is to balance the two approaches to take advantage of their strengths and compensate for their weaknesses. We believe this can be accomplished using a risk-based approach for structuring projects to incorporate both agile and disciplined approaches in proportion to a project's needs. We present six observations drawn from our efforts to develop such an approach. We follow those observations with some practical advice to organizations seeking to integrate agile and plan-driven methods in their development process.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1231450,no,undetermined,0
An industrialized restructuring service for legacy systems,"Summary form only given. The article presents RainCodeOnline (http://www.raincode.com/online), Belgium-based initiative to provide COBOL restructuring services through the Web. The article details a number of issues: the underlying technology; the process; the pros and cons of automated restructuring (dialects, process, perception, pricing, security, cost, performance, quality, etc.); and the perception of quality and how it can be further improved. A number of results are presented as well.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1235458,no,undetermined,0
Impact analysis and change management of UML models,"The use of Unified Modeling Language (UML) analysis/design models on large projects leads to a large number of interdependent UML diagrams. As software systems evolve, those diagrams undergo changes to, for instance, correct errors or address changes in the requirements. Those changes can in turn lead to subsequent changes to other elements in the UML diagrams. Impact analysis is then defined as the process of identifying the potential consequences (side-effects) of a change, and estimating what needs to be modified to accomplish a change. In this article, we propose a UML model-based approach to impact analysis that can be applied before any implementation of the changes, thus allowing an early decision-making and change planning process. We first verify that the UML diagrams are consistent (consistency check). Then changes between two different versions of a UML model are identified according to a change taxonomy, and model elements that are directly or indirectly impacted by those changes (i.e., may undergo changes) are determined using formally defined impact analysis rules (written with Object Constraint Language). A measure of distance between a changed element and potentially impacted elements is also proposed to prioritize the results of impact analysis according to their likelihood of occurrence. We also present a prototype tool that provides automated support for our impact analysis strategy, that we then apply on a case study to validate both the implementation and methodology.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1235428,no,undetermined,0
Application of neural networks for software quality prediction using object-oriented metrics,"The paper presents the application of neural networks in software quality estimation using object-oriented metrics. Quality estimation includes estimating reliability as well as maintainability of software. Reliability is typically measured as the number of defects. Maintenance effort can be measured as the number of lines changed per class. In this paper, two kinds of investigation are performed: predicting the number of defects in a class; and predicting the number of lines change per class. Two neural network models are used: they are Ward neural network; and General Regression neural network (GRNN). Object-oriented design metrics concerning inheritance related measures, complexity measures, cohesion measures, coupling measures and memory allocation measures are used as the independent variables. GRNN network model is found to predict more accurately than Ward network model.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1235412,no,undetermined,0
A consumer report on BDD packages,"BDD packages have matured to a state where they are often considered a commodity. Does this mean that all (publicly and commercially) available packages are equally good? Does this preclude any new developments? In this paper, we present a consumer report on 13 BDD packages and thereby try to answer these questions. We argue that there is a substantial spectrum in quality as measured by various metrics and we found that even the better packages do not always deploy the latest technology. We show how various design decisions underlying the studied packages exhibit themselves at the programming interface level, and we claim that this allows us to predict performance to a certain extent.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1232832,no,undetermined,0
One approach to the metric baselining imperative for requirements processes,"The success of development projects in customer-oriented industries depends on reliable processes for the definition and maintenance of requirements. With the sustained, severe reduction in the rush to new technology, this widely accepted fact has become increasingly evident in the networking industry. Customers now focus on high product quality as they strive for economy of operation. Enhancing product quality necessitates enhancing processes, which in turn can necessitate applying more accurate (and precise) measures. Finding process deviations and identifying patterns of product deficiencies are critical steps to achieving high quality products. We describe the application of quantitative process control (QPC) during early development phases to establish and maintain baseline distributions characterizing RMCM&T processes, and to monitor their evolutions. Metric baselining as described includes key metric identification, and data normalization, filtering, and categorization. Empirical baselining provides the statistical sensitivity to detect requirements process problems, and to support targeted identification of particular requirements-related patterns in defects.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1232767,no,undetermined,0
Assessing the maintainability benefits of design restructuring using dependency analysis,"Software developers and project managers often have to assess the quality of software design. A commonly adopted hypothesis is that a good design should cost less to maintain than a poor design. We propose a model for quantifying the quality of a design from a maintainability perspective. Based on this model, we propose a novel strategy for predicting the ""return on investment"" (ROI) for possible design restructurings using procedure level dependency analysis. We demonstrate this approach with two exploratory Java case studies. Our results show that common low level source code transformations change the system dependency structure in a beneficial way, allowing recovery of the initial refactoring investment over a number of maintenance activities.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1232477,no,undetermined,0
Using service utilization metrics to assess the structure of product line architectures,"Metrics have long been used to measure and evaluate software products and processes. Many metrics have been developed that have lead to different degrees of success. Software architecture is a discipline in which few metrics have been applied, a surprising fact given the critical role of software architecture in software development. Software product line architectures represent one area of software architecture in which we believe metrics can be of especially great use. The critical importance of the structure defined by a product line architecture requires that its properties be meaningfully assessed and that informed architectural decisions be made to guide its evolution. To begin addressing this issue, we have developed a class of closely related metrics that specifically target product line architectures. The metrics are based on the concept of service utilization and explicitly take into account the context in which individual architectural elements are placed. We define the metrics, illustrate their use, and evaluate their strengths and weaknesses through their application on three example product line architectures.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1232476,no,undetermined,0
An evaluation of checklist-based reading for entity-relationship diagrams,"Software inspections have attracted much attention over the last 25 years. It is a method for static analysis of artifacts during software development. However, few studies look at inspections of entity-relationship diagrams (ER diagrams), and in particular an evaluation of the number of defects, number of false-positives and the types of defects that are found by most reviewers. ER-diagrams are commonly used in the design of databases, which makes them an important modelling concept for many large software systems. We present a large empirical study (486 subjects) where checklists were used for an inspection. The goodness in the evaluation is judged based on the type of defects the reviewers identify and the relationship between the detection of real defects and false positives. It is concluded that checklist-based inspections of entity-relationship diagrams is worthwhile.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1232475,no,undetermined,0
An experiment family to investigate the defect detection effect of tool-support for requirements inspection,"The inspection of software products can help to find defects early in the development process and to gather valuable information on product quality. An inspection is rather resource intensive and involves several tedious tasks like navigating, sorting, or checking. Tool support is thus hoped to increase effectiveness and efficiency. However, little empirical work is available that directly compares paper-based (i.e., manual) and tool-based software inspections. Existing reports on tool support for inspection generally tend to focus on code inspections while little can be found on requirements or design inspection. We report on an experiment family: two experiments on paper-based inspection and a third experiment to empirically investigate the effect of tool support regarding defect detection effectiveness and inspection effort in an academic environment with 40 subjects. Main results of the experiment family are: (a) The effectiveness is similar for manual and tool-supported inspections; (b) the inspection effort and defect overlap decreased significantly with tool support, while (c) efficiency increased considerably with tool support.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1232474,no,undetermined,0
An analogy-based approach for predicting design stability of Java classes,"Predicting stability in object-oriented (OO) software, i.e., the ease with which a software item evolves while preserving its design, is a key feature for software maintenance. In fact, a well designed OO software must be able to evolve without violating the compatibility among versions, provided that no major requirement reshuffling occurs. Stability, like most quality factors, is a complex phenomenon and its prediction is a real challenge. We present an approach, which relies on the case-based reasoning (CBR) paradigm and thus overcomes the handicap of insufficient theoretical knowledge on stability. The approach explores structural similarities between classes, expressed as software metrics, to guess their chances of becoming unstable. In addition, our stability model binds its value to the impact of changing requirements, i.e., the degree of class responsibilities increase between versions, quantified as the stress factor. As a result, the prediction mechanism favours the stability values for classes having strong structural analogies with a given test class as well as a similar stress impact. Our predictive model is applied on a testbed made up of the classes from four major version of the Java API.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1232472,no,undetermined,0
Defining and validating metrics for navigational models,"Nowadays, several approaches for developing Web applications have been proposed in the literature. Most of them extend existing object-oriented conceptual modeling methods, incorporating new constructors in order to model the navigational structure and the content of Web applications. Such new constructors are commonly represented in a navigational model. While navigational models constitute the backbone of Web application design, their quality has a great impact on the quality of the final product, which is actually implemented and delivered. We discuss a set of metrics for navigational models that has been proposed for analyzing the quality of Web applications in terms of size and structural complexity. These metrics were defined and validated using a formal framework (DISTANCE) for software measure construction that satisfies the measurement needs of empirical software engineering research. Some experimental studies have shown that complexity affects the ability to understand and maintain conceptual models. In order to prove this, we also made a controlled experiment to observe how the proposed metrics can be used as early maintainability indicators.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1232468,no,undetermined,0
Analyzing the cost and benefit of pair programming,"We use a combination of metrics to understand, model, and evaluate the impact of pair programming on software development. Pair programming is a core technique in the hot process paradigm of extreme programming. At the expense of increased personnel cost, pair programming aims at increasing both the team productivity and the code quality as compared to conventional development. In order to evaluate pair programming, we use metrics from three different categories: process metrics such as the pair speed advantage of pair programming; product metrics such as the module breakdown structure of the software; and project context metrics such as the market pressure. The pair speed advantage is a metric tailored to pair programming and measures how much faster a pair of programmers completes programming tasks as compared to a single developer. We integrate the various metrics using an economic model for the business value of a development project. The model is based on the standard concept of net present value. If the market pressure is strong, the faster time to market of pair programming can balance the increased personnel cost. For a realistic sample project, we analyze the complex interplay between the various metrics integrated in our model. We study for which combinations of the market pressure and pair speed advantage the value of the pair programming project exceeds the value of the corresponding conventional project. When time to market is the decisive factor and programmer pairs are much faster than single developers, pair programming can increase the value of a project, but there also are realistic scenarios where the opposite is true. Such results clearly show that we must consider metrics from different categories in combination to assess the cost-benefit relation of pair programming.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1232465,no,undetermined,0
Dealing with missing software project data,"Whilst there is a general consensus that quantitative approaches are an important part of successful software project management, there has been relatively little research into many of the obstacles to data collection and analysis in the real world. One feature that characterises many of the data sets we deal with is missing or highly questionable values. Naturally this problem is not unique to software engineering, so we explore the application of two existing data imputation techniques that have been used to good effect elsewhere. In order to assess the potential value of imputation we use two industrial data sets. Both are quite problematic from an effort modelling perspective because they contain few cases, have a significant number of missing values and the projects are quite heterogeneous. We examine the quality of fit of effort models derived by stepwise regression on the raw data and data sets with values imputed by various techniques is compared. In both data sets we find that k-nearest neighbour (k-NN) and sample mean imputation (SMI) significantly improve the model fit, with k-NN giving the best results. These results are consistent with other recently published results, consequently we conclude that imputation can assist empirical software engineering.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1232464,no,undetermined,0
A ranking of software engineering measures based on expert opinion,"This research proposes a framework based on expert opinion elicitation, developed to select the software engineering measures which are the best software reliability indicators. The current research is based on the top 30 measures identified in an earlier study conducted by Lawrence Livermore National Laboratory. A set of ranking criteria and their levels were identified. The score of each measure for each ranking criterion was elicited through expert opinion and then aggregated into a single score using multiattribute utility theory. The basic aggregation scheme selected was a linear additive scheme. A comprehensive sensitivity analysis was carried out. The sensitivity analysis included: variation of the ranking criteria levels, variation of the weights, variation of the aggregation schemes. The top-ranked measures were identified. Use of these measures in each software development phase can lead to a more reliable quantitative prediction of software reliability.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1232286,no,undetermined,0
Detection and recovery techniques for database corruption,"Increasingly, for extensibility and performance, special purpose application code is being integrated with database system code. Such application code has direct access to database system buffers, and as a result, the danger of data being corrupted due to inadvertent application writes is increased. Previously proposed hardware techniques to protect from corruption require system calls, and their performance depends on details of the hardware architecture. We investigate an alternative approach which uses codewords associated with regions of data to detect corruption and to prevent corrupted data from being used by subsequent transactions. We develop several such techniques which vary in the level of protection, space overhead, performance, and impact on concurrency. These techniques are implemented in the Dali main-memory storage manager, and the performance impact of each on normal processing is evaluated. Novel techniques are developed to recover when a transaction has read corrupted data caused by a bad write and gone on to write other data in the database. These techniques use limited and relatively low-cost logging of transaction reads to trace the corruption and may also prove useful when resolving problems caused by incorrect data entry and other logical errors.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1232268,no,undetermined,0
"High quality statecharts through tailored, perspective-based inspections","In the embedded systems domain, statecharts have become an important technique to describe the dynamic behavior of a software system. In addition, statecharts are an important element of object-oriented design documents and are thus widely used in practice. However, not much is known about how to inspect them. Since their invention by Pagan in 1976, inspections proved to be an essential quality assurance technique in software engineering. Traditionally, inspections were used to detect defects in code documents, and later in requirements documents. We define a defect taxonomy for statecharts. Using this taxonomy, we present an inspection approach for inspecting statecharts, which combines existing inspection techniques with several new perspective-based scenarios. Moreover, we address the problems of inspecting large documents by using prioritized use cases in combination with perspective-based reading.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1231608,no,undetermined,0
An empirical study of an ER-model inspection meeting,"A great benefit of software inspections is that they can be applied at almost any stage of the software development life cycle. We document a large-scale experiment conducted during an entity relationship (ER) model inspection meeting. The experiment was aimed at finding empirically validated answers to the question ""which reading technique has a more efficient detection rate when searching for defects in an ER model"". Secondly, the effect of the usage of roles in a team meeting was also explored. Finally, we investigate the reviewers' ability to find defects belonging to certain defect categories. The findings showed that the participants using a checklist had a significantly higher detection rate than the ad hoc groups. Overall, the groups using roles had a lower performance than those without roles. Furthermore, the findings showed that when comparing the groups using roles to those without roles, the proportion of syntactic and semantic defects found in the number of overall defects identified did not significantly differ.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1231607,no,undetermined,0
Low-cost on-line fault detection using control flow assertions,"A control flow fault occurs when a processor fetches and executes an incorrect next instruction. Executable assertions, i.e., special instructions that check some invariant properties of a program, provide a powerful and low-cost method for on-line detection of hardware-induced control flow faults. We propose a technique called ACFC (Assertions for Control Flow Checking) that assigns an execution parity to a basic block, and uses the parity bit to detect faults. Using a graph model of a program, we classify control flow faults into skip, re-execute and multi-path faults. We derive some necessary conditions for these faults to manifest themselves as execution parity errors. To force a control flow fault to excite a parity error, the target program is instrumented with additional instructions. Special assertions are inserted to detect such parity errors. We have a developed a preprocessor that takes a C program as input and inserts ACFC assertions automatically. We have implemented a software-based fault injection tool SFIG which takes advantage of the GNU debugger. Fault injection experiments show that ACFC incurs less performance overhead (around 47%) and memory overhead (around 30%) than previous techniques, with no significant loss in fault coverage.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1214380,no,undetermined,0
The efficient bus arbitration scheme in SoC environment,"This paper presents the dynamic bus arbiter architecture for a system on chip design. The conventional bus-distribution algorithms, such as the static fixed priority and the round robin, show several defects that are bus starvation, and low system performance because of bus distribution latency in a bus cycle time. The proposed dynamic bus architecture is based on a probability bus distribution algorithm and uses an adaptive ticket value method to solve the impartiality and starvation problems. The simulation results show that the proposed algorithm reduces the buffer size of a master by 11% and decreases the bus latency of a master by 50%.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1213054,no,undetermined,0
Performance analysis of software rejuvenation,"Cluster-based systems, a combination of interconnected individual computers, have become a popular solution to build the scalable and highly available Web servers. In order to reduce system outages due to aging phenomenon, software rejuvenation, a proactive fault-tolerance strategy has been introduced into cluster systems. Compared with clusters of a flat architecture, in which all the nodes share the same functions, we model and analyze the dispatcher-worker based cluster systems, which employ prediction-based rejuvenation both on the dispatcher and the worker pool. To evaluate the effects of rejuvenation, stochastic reward net models are constructed and solved by SPNP (stochastic Petri net package). Numerical results show that prediction-based software rejuvenation can significantly increase system availability and reduce the expected job loss probability.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1236365,no,undetermined,0
System health and intrusion monitoring (SHIM): project summary,"Computer systems and networks today are vulnerable to attacks. In addition to preventive strategies, intrusion detection has been used to further improve the security of computers and networks. Nevertheless, current intrusion detection and response system can detect only known attacks and provide primitive responses. The System Health and Intrusion Monitoring (SHIM) project aims at developing techniques to monitor and assess the health of a large distributed system. SHIM can accurately detect novel attacks and provide strategic information for further correlation, assessments, and response management.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1194966,no,undetermined,0
Performance evaluation of a perceptual ringing distortion metric for digital video,"This paper evaluates a perceptual impairment measure for ringing artifacts, which are common in hybrid MC/DPCM/DCT coded video, as a predictor of the mean opinion score (MOS) obtained in the standard subjective assessment. The perceptual ringing artifacts measure is based on a vision model and a ringing distortion region segmentation algorithm, which is converted into a new perceptual ringing distortion metric (PRDM) on a scale of 0 to 5. This scale corresponds to a modified double-stimulus impairment scale variant II (DSIS-II) method. The Pearson correlation, the Spearman rank order correlation and the average absolute error are used to evaluate the performance of the PRDM compared with the subjective test data. The results show a strong correlation between the PRDM and the MOS with respect to ringing artifacts.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1199549,no,undetermined,0
A performance oriented migration framework for the grid,"At least three factors in the existing migration frameworks make them less suitable in Grid systems especially when the goal is to improve the response times for individual applications. These factors are the separate policies for suspension and migration of executing applications employed by these migration frameworks, the use of pre-defined conditions for suspension and migration and the lack of knowledge of the remaining execution time of the applications. In this paper we describe a migration framework for performance oriented Grid systems that implements tightly coupled policies for both suspension and migration of executing applications and takes into account both system load and application characteristics. The main goal of our migration framework is to improve the response times for individual applications. We also present some results that demonstrate the usefulness of our migration framework.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1199361,no,undetermined,0
A lightweight middleware protocol for ad hoc distributed object computing in ubiquitous computing environments,"Devices in ubiquitous computing environments are usually embedded, wearable, and handheld, have resource constraints, and are all connected to each other through wireless connections and other computers possibly through fixed network infrastructures, such as the Internet. These devices may form numerous webs of short-range and often low-power mobile ad hoc networks to exchange information. Distributed object computing (DOC) middleware technologies have been successful in promoting high quality and reusable distributed software for enterprise-oriented environments. In order to reap the same benefit in ubiquitous computing environments, it is important to note that the natural interactions among distributed objects in ubiquitous computing environments are quite different due to various factors, such as bandwidth constraints, unpredictable device mobility, network topology change, and context-sensitivity (or situation-awareness) of application objects. Hence, the interactions among distributed objects tend to be more spontaneous and short-lived rather than predictable and long-term. In this paper, a middleware protocol, RKF, to facilitate distributed object-based application software to interact in an ad hoc fashion in ubiquitous computing environments is presented. RKF addresses both spontaneous object discovery and context-sensitive object data exchange. Our experimental results, based on RKF's implementation and evaluation inside the object request broker of our RCSM middleware test bed, indicate that it is lightweight, has good performance, and can be easily used in PDA-like devices.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1199252,no,undetermined,0
Identifying comprehension bottlenecks using program slicing and cognitive complexity metrics,Achieving and maintaining high software quality is most dependent on how easily the software engineer least familiar with the system can understand the system's code. Understanding attributes of cognitive processes can lead to new software metrics that allow the prediction of human performance in software development and for assessing and improving the understandability of text and code. In this research we present novel metrics based on current understanding of short-term memory performance to predict the location of high frequencies of errors and to evaluate the quality of a software system. We further enhance these metrics by applying static and dynamic program slicing to provide programmers with additional guidance during software inspection and maintenance efforts.,2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1199195,no,undetermined,0
Investigating the defect detection effectiveness and cost benefit of nominal inspection teams,"Inspection is an effective but also expensive quality assurance activity to find defects early during software development. The defect detection process, team size, and staff hours invested can have a considerable impact on the defect detection effectiveness and cost-benefit of an inspection. In this paper, we use empirical data and a probabilistic model to estimate this impact for nominal (noncommunicating) inspection teams in an experiment context. Further, the analysis investigates how cutting off the inspection after a certain time frame would influence inspection performance. Main findings of the investigation are: 1) Using combinations of different reading techniques in a team is considerably more effective than using the best single technique only (regardless of the observed level of effort). 2) For optimizing the inspection performance, determining the optimal process mix in a team is more important than adding an inspector (above a certain team size) in our model. 3) A high level of defect detection effectiveness is much more costly to achieve than a moderate level since the average cost for the defects found by the inspector last added to a team increases more than linearly with growing effort investment. The work provides an initial baseline of inspection performance with regard to process diversity and effort in inspection teams. We encourage further studies on the topic of time usage with defect detection techniques and its effect on inspection effectiveness in a variety of inspection contexts to support inspection planning with limited resources.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1199069,no,undetermined,0
Auditory morphing based on an elastic perceptual distance metric in an interference-free time-frequency representation,"An elastic spectral distance measure based on a F0 adaptive pitch synchronous spectral estimation and selective elimination of periodicity interferences, that was developed for a high-quality speech modification procedure STRAIGHT [1], is introduced to provide a basis for auditory morphing. The proposed measure is implemented on a low dimensional piecewise bilinear time-frequency mapping between the target and the original speech representations. A preliminary test results of morphing emotional speech samples indicated that proposed procedure provides perceptually monotonic and high-quality interpolation and extrapolation of CD quality speech samples.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1198766,no,undetermined,0
Tests and tolerances for high-performance software-implemehted fault detection,"We describe and test a software approach to fault detection in common numerical algorithms. Such result checking or algorithm-based fault tolerance (ABFT) methods may be used, for example, to overcome single-event upsets in computational hardware or to detect errors in complex, high-efficiency implementations of the algorithms. Following earlier work, we use checksum methods to validate results returned by a numerical subroutine operating subject to unpredictable errors in data. We consider common matrix and Fourier algorithms which return results satisfying a necessary condition having a linear form; the checksum tests compliance with this condition. We discuss the theory and practice of setting numerical tolerances to separate errors caused by a fault from those inherent in finite-precision floating-point calculations. We concentrate on comprehensively defining and evaluating tests having various accuracy/computational burden tradeoffs, and we emphasize average-case algorithm behavior rather than using worst-case upper, bounds on error.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1197125,no,undetermined,0
Statistical process control to improve coding and code review,"Software process comprises activities such as estimation, planning, requirements analysis, design, coding, reviews, and testing, undertaken when creating a software product. Effective software process management involves proactively managing each of these activities. Statistical process control tools enable proactive software process management. One such tool, the control chart, can be used for managing, controlling, and improving the code review process.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1196321,no,undetermined,0
Software-based erasure codes for scalable distributed storage,"This paper presents a new class of erasure codes, Lincoln Erasure codes (LEC), applicable to large-scale distributed storage that includes thousands of disks attached to multiple networks. A high-performance software implementation that demonstrates the capability to meet these anticipated requirements is described. A framework for evaluation of candidate codes was developed to support in-depth analysis. When compared with erasure codes based on the work of Reed-Solomon and Luby (2000), tests indicate LEC has a higher throughput for encoding and decoding and lower probability of failure across a range of test conditions. Strategies are described for integration with storage-related hardware and software.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1194852,no,undetermined,0
Concurrent fault detection in a hardware implementation of the RC5 encryption algorithm,"Recent research has shown that fault diagnosis and possibly fault tolerance are important features when implementing cryptographic algorithms by means of hardware devices. In fact, some security attack procedures are based on the injection of faults. At the same time, hardware implementations of cryptographic algorithms, i.e. crypto-processors, are becoming widespread. There is however, only very limited research on implementing fault diagnosis and tolerance in crypto-algorithms. Fault diagnosis is studied for the RC5 crypto-algorithm, a recently proposed block-cipher algorithm that is suited for both software and hardware implementations. RC5 is based on a mix of arithmetic and logic operations, and is therefore a challenge for fault diagnosis. We study fault propagation in RC5, and propose and evaluate the cost/performance tradeoffs of several error detecting codes for RC5. Costs are estimated in terms of hardware overhead, and performances in terms of fault coverage. Our most important conclusion is that, despite its nonuniform nature, RC5 can be efficiently protected by using low-cost error detecting codes.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1212865,no,undetermined,0
Design of Very Lightweight Agents for reactive embedded systems,"Large-scale real-time high-performance data acquisition computing systems often require to be fault-tolerant and adaptive to changes. We consider a multi-agent system based approach to achieve these goals. This research is a part of ongoing research efforts to build a triggering and data acquisition system (known as BTeV) for particle-accelerator-based high energy physics experiments at Fermi National Laboratory. The envisioned hardware consists of pixel detectors and readout sensors embedded in the accelerator, which are connected to specialized FPGAs (field-programmable gate arrays). The FPGAs are connected to approximately 2,500 digital signal processors (DSPs). After initial filtering and processing of data by the DSPs, a farm of approximately 2,500 Linux computers are responsible for post-processing a large amount of high speed data input. To support the adaptive fault-tolerance feature, we introduce the notion of very lightweight agents (VLAs), which are designed to be adaptive but small in footprint and extremely efficient. Each digital signal processor will run a very lightweight agent along with a physics application program that collects and processes input data from the corresponding FPGA. Since VLAs can be proactive or reactive, Brooks' subsumption architecture is a good basis for the design. In this paper we present several necessary changes in the original subsumption architecture to better serve the BTeV architecture.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1194794,no,undetermined,0
Advanced analysis of dynamic neural control advisories for process optimization and parts maintenance,"This paper details an advanced set of analyses designed to drive specific process variable setpoint adjustments or maintenance actions required for cost effective process control using the Dynamic Neural Controllerâ„?(DNC) wafer-to-wafer advisories for semiconductor manufacturing advanced process control. The new analytic displays and metrics are illustrated using data obtained on a LAM 4520XL at STMicroelectronics as part of a SEMATECH SPIT beta test evaluation. The DNC represents a comprehensive modeling environment that uses as its input extensive process chamber information and history of the time since maintenance actions occurred. The DNC uses a neural network to predict multiple quality output metrics and a closed-loop risk-based optimization to maximize process quality performance while minimizing overall cost of tool operation and machine downtime. The software responds in an advisory mode on a wafer-to-wafer basis as to the optimal actions to be taken. In this paper, we present three specific instances of patterns arising during wafer processing over time that signal the process or equipment engineer to the need for corrective action: either a process setpoint adjustment or specific maintenance actions. Based on the controller's recommended corrective action set with the overall risk reduction predicted by such actions, a metric of corrective action ""urgency"" can be created. The tracking of this metric over time yields different pattern types that signify a quantified need for a specific type of corrective action. Three basic urgency patterns are found: 1. a pattern in a given maintenance action over time showing increasing urgency or ""risk reduction"" capability for the action; 2. a pattern in a process variable specific to a given recipe indicating a chronic request over time to only adjust the variable setpoint either above or below the current target; 3. a pattern in a process variable existing over all recipes processed through the chamber indicating chronic request to adjust the variable setpoint in either or both directions over time. This pattern is a pointer to the need for a maintenance action that is either corroborated by the urgency graph for that maintenance action, or if no such action has been previously take- n, a guide to the source of the equipment malfunction.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1194514,no,undetermined,0
A pragmatic approach to managing APC FDC in high volume logic production,"At Infineon Technologies APC fault detection is now implemented in many process areas in its high volume fabs. With the APC Software ""APC-Trend"" process engineers and maintenance can detect and classify anomalies in machine and process parameters and supervise them on the basis of an automated alarming system. An overview of the current usage of APC FDC at Infineon is given.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1194511,no,undetermined,0
A simple system for detection of EEG artifacts in polysomnographic recordings,"We present an efficient parametric system for automatic detection of electroencephalogram (EEG) artifacts in polysomnographic recordings. For each of the selected types of artifacts, a relevant parameter was calculated for a given epoch. If any of these parameters exceeded a threshold, the epoch was marked as an artifact. Performance of the system, evaluated on 18 overnight polysomnographic recordings, revealed concordance with decisions of human experts close to the interexpert agreement and the repeatability of expert's decisions, assessed via a double-blind test. Complete software (Matlab source code) for the presented system is freely available from the Internet at http://brain.fuw.edu.pl/artifacts.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1193788,no,undetermined,0
A tamper-resistant framework for unambiguous detection of attacks in user space using process monitors,"Replication and redundancy techniques rely on the assumption that a majority of components are always safe and voting is used to resolve any ambiguities. This assumption may be unreasonable in the context of attacks and intrusions. An intruder could compromise any number of the available copies of a service resulting in a false sense of security. The kernel based approaches have proven to be quite effective but they cause performance impacts if any code changes are in the critical path. We provide an alternate user space mechanism consisting of process monitors by which such user space daemons can be unambiguously monitored without causing serious performance impacts. A framework that claims to provide such a feature must itself be tamper-resistant to attacks. We theoretically analyze and compare some relevant schemes and show their fallibility. We propose our own framework that is based on some simple principles of graph theory and well-founded concepts in topological fault tolerance, and show that it can not only unambiguously detect any such attacks on the services but is also very hard to subvert. We also present some preliminary results as a proof of concept.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1192456,no,undetermined,0
Fast flow analysis to compute fuzzy estimates of risk levels,"In the context of software quality assessment, this paper proposes original flow analyses which propagate numerical estimates of blocking risks along an inter-procedural control flow graph (CFG) and which combine these estimates along the different CFG paths using fuzzy logic operations. Two specialized analyses can be further defined in terms of definite and possible flow analysis. The definite analysis computes the minimum blocking risk levels that statements may encounter on every path, while the possible analysis computes the highest blocking risk levels encountered by statements on at least one path. This paper presents original flow equations to compute the definite and possible blocking risk levels for statements in source code. The described fix-point algorithm presents a linear execution time and memory complexity and it is also fast in practice. The experimental context used to validate the presented approach is described and results are reported and discussed for eight publicly available systems written in C whose total size is about 300 KLOC Results show that the analyses can be used to compute, identify, and compare definite and possible blocking risks in software systems. Furthermore, programs which are known to be synchronized like ""samba"" show a relatively high level of blocking risks. On the other hand, the approach allows to identify even low levels of blocking risks as those presented by programs like ""gawk"".",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1192443,no,undetermined,0
Intelligent algorithms for QoS management in modern communication networks,"This paper discusses the application of fuzzy logic and neural network based algorithms to enhance and improve the quality of service (QOS) in modern communication networks. In the last decade, a rich variety of different networking technologies have been developed to meet the diverse and ever growing service demands for multimedia communications. Some are inherent with absolute and quantitative QoS guarantee while others provide relative and qualitative QoS. Essential measures for QoS provision and guarantee include buffer/queue control and management, bandwidth estimation and allocation, traffic flow and congestion control, etc. These are adopted across networking technologies in various forms and mechanisms. This paper intends to provide an overview of the benefits that intelligent neuro-fuzzy algorithms may bring to these commonly used QoS measures.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1191654,no,undetermined,0
A frame-level measurement apparatus for performance testing of ATM equipment,"Performance testing of asynchronous transfer mode (ATM) equipment is dealt with here. The attention is principally paid to frame-level metrics, recently proposed by the ATM Forum because of their suitability to reflect user-perceived performance better than traditional cell-level metrics. Following the suggestions of the ATM Forum, more and more network engineers and production managers are interested today in these metrics, thus increasing the need of instruments and measurement solutions appropriate to their estimation. Trying to satisfy this exigency, a new VME extension for instrumentation (VXI) based measurement apparatus is proposed in the paper. The apparatus features a suitable software, developed by the authors, which allows the evaluation of the aforementioned metrics by simply making use of common ATM analyzers; only two VXI line interfaces, capable of managing the physical and ATM layers, are, in fact, adopted. Some details concerning ATM technology and its hierarchical structure, as well as the main differences between frames, specific to the ATM adaptation layer, and cells, characterizing the underlying ATM layer, are first given. Both the hardware and software solutions of the measurement apparatus are then described in detail, paying particular attention to the measurement procedures implemented. In the end, the performance of a new ATM device is assessed through the proposed apparatus.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1191406,no,undetermined,0
High performance generalized cylinders visualization,"This paper studies the problem of real-time rendering of highly deformable generalized cylinders. Some efficient schemes for high axis curvature detection are presented, as well as an incremental non-uniform sampling process. We also show how the recent 3D card ""skinning"" feature, classical in character animation, can be derived in order to allow for very high frame-rate when rendering such generalized cylinders. Finally, an algorithm is presented, that permits the object to dynamically adapt its display process, for guaranteed frame-rate purposes. This algorithm dynamically modifies the different sampling parameters in order to achieve optimal quality visualization for a given pre-imposed frame-rate.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1199625,no,undetermined,0
"Gate oxide degradation due to plasma damage related charging while ILD cap oxide deposition - detection, localization and resolution","The article reports on the flow of detection, localization and resolution of a plasma damage related problem in a logic chip production line. The problem was observed on standard 0.25 Î¼m logic technology. The introduction and optimization of a voltage breakdown (VBD) test in ILT (in line test) routines led to the detection of an insufficient gate oxide quality. Using data-mining application software and taking into consideration the structure of the test routine, the root cause for the degradation of the gate-oxide was found to be ILD (inter-layer-dielectric) cap oxide deposition. A matrix design of experiment was used to optimize the plasma deposition process in order to minimize charging effects by paying attention to wafer uniformity and reproducibility. It is shown that the principal detractor for the quality of gate oxide was eliminated by introducing the new ILD cap oxide process.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1200927,no,undetermined,0
Fragment class analysis for testing of polymorphism in Java software,"Adequate testing of polymorphism in object-oriented software requires coverage of all possible bindings of receiver classes and target methods at call sites. Tools that measure this coverage need to use class analysis to compute the coverage requirements. However, traditional whole-program class analysis cannot be used when testing partial programs. To solve this problem, we present a general approach for adapting whole-program class analyses to operate on program fragments. Furthermore, since analysis precision is critical for coverage tools, we provide precision measurements for several analyses by determining which of the computed coverage requirements are actually feasible. Our work enables the use of whole-program class analyses for testing of polymorphism in partial programs, and identifies analyses that compute precise coverage requirements and therefore are good candidates for use in coverage tools.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1201201,no,undetermined,0
"The impact of pair programming on student performance, perception and persistence","This study examined the effectiveness of pair programming in four lecture sections of a large introductory programming course. We were particularly interested in assessing how the use of pair programming affects student performance and decisions to pursue computer science related majors. We found that students who used pair programming produced better programs, were more confident in their solutions, and enjoyed completing the assignments more than students who programmed alone. Moreover, pairing students were significantly more likely than non-pairing students to complete the course, and consequently to pass it. Among those who completed the course, pairers performed as well on the final exam as non-pairers, were significantly more likely to be registered as computer science related majors one year later, and to have taken subsequent programming courses. Our findings suggest that not only does pairing not compromise students' learning, but that it may enhance the quality of their programs and encourage them to pursue computer science degrees.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1201243,no,undetermined,0
Modeling and measurements of novel high k monolithic transformers,"This paper presents modeling and measurements of a novel monolithic transformer with high coupling k and quality factor Q characteristics. The present transformer utilizes a Z-shaped multilayer metalization to increase k without sacrificing Q. The new transformer has been fabricated using Motorola 0.18 micron copper process. A simple 2-port lumped circuit model is used to model the new design. Experimental data shows a good agreement with predicted data obtained from an HFSS software simulator. An increase of about 10% in mutual coupling and 15% in Q has been achieved. For a modest increase in k of about 5%, Q can be increased by up to 20%.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1212595,no,undetermined,0
A fast method of characterizing HTSC bulk material,"HTSC bulk material which has been developed during the last ten years is now prepared in increasing quantities and used in demonstrators or prototypes of several applications, e.g., flywheels and motors. The quality control of large amounts of samples requires a method which is fast and provides essential information about the sample quality. In particular, the critical current density and its inhomogeneity should be determined. In this contribution we report about a trapped field measurement technique using pulsed field magnetization and field detection by a Hall-array. With this arrangement the required time of measurement is in the order of a few minutes per sample. The software which controls the devices contains also an analyzing procedure for the calculation of an effective critical current density as well as parameters describing the homogeneity of the samples.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1212433,no,undetermined,0
Accurate modeling and simulation of SAW RF filters,"The popularity of wireless services and the increasing demand for higher quality, new services, and the need for higher data rates will boost the cellular terminal market. Today, third generation (3G) systems exist in many metropolitan areas. In addition, wireless LAN systems, such as Bluetooth or IEEE 802.11-based systems, are emerging. The key components in the microwave section of the mobile terminals of these systems incorporate - apart from active radio frequency integrated circuits (RFICs) and RF modules - a multitude of passive components. The most unique passive components used in the microwave section are surface acoustic wave (SAW) filters. Due to the progress of integration in the active part of the systems the component count in modern terminals is decreasing. On the other hand, the average number of SAW RF filters per cellular phone is increasing due to multi-band terminals. As a consequence, the passive components outnumber the RFICs by far in today's systems. The market is demanding smaller and smaller terminals and, thus, the size of all components has to be reduced. Further reduction of component count and required PCB area is obtained by integration of passive components and SAW devices using low-temperature co-fired ceramic (LTCC). The trend or reducing the size dramatically while keeping or even improving the performance of the RF filters requires accurate software tools for the simulation of all relevant effects and interactions. In the past it was sufficient to predict the acoustic behavior on the SAW chip, but higher operating frequencies, up to 2.5 GHz, and stringent specifications up to 6 GHz demand to account for electromagnetic (EM) effects, too. The combination of accurate acoustic simulation tools together with 2.5/3D EM simulation software packages allows to predict and optimize the performance of SAW filters and SAW-based front-end modules.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1210554,no,undetermined,0
Model checking for probability and time: from theory to practice,"Probability features increasingly often in software and hardware systems: it is used in distributed coordination and routing problems, to model fault-tolerances and performance, and to provide adaptive resource management strategies. Probabilistic model checking is an automatic procedure for establishing if a desired property holds in a probabilistic specifications such as ""leader election is eventually resolved with probability 1"", ""the chance of shutdown occurring is at most 0.01%"", and ""the probability that a message will be delivered within 30ms is at least 0.75"". A probabilistic model checker calculates the probability of a given temporal logic property being satisfied, as opposed to validity. In contrast to conventional model checkers, which rely on reachability analysis of the underlying transition system graph, probabilistic model checking additionally involves numerical solutions of linear equations and linear programming problems. This paper reports our experience with implementing PRISM (www.cs.bham.ac.uk/âˆ¼dxp/prism), a probabilistic symbolic model checker, demonstrates its usefulness in analyzing real-world probabilistic protocols, and outlines future challenges for this research direction.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1210075,no,undetermined,0
On end-to-end bandwidth analysis and measurement,"Many applications are interested in the end-to-end bandwidth of a path. Several tools have been implemented focusing on path capacity, but available bandwidth estimation is still an open problem to be investigated. We propose an end-to-end bandwidth measurement methodology with active probe based on packet pair technology. Our method can measure the path capacity in condition that the competing traffic is light and available bandwidth is get if the competing traffic is heavy. We also present a novel algorithm for filtering the measurement data.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1209059,no,undetermined,0
Quality-based auto-tuning of cell uplink load level targets in WCDMA,"The objective of this paper was to validate the feasibility of auto-tuning WCDMA cell uplink load level targets based on quality of service. The uplink cell load level was measured with received wideband total power. The quality indicators used were called blocking probability, packet queuing probability and degraded block error ratio probability. The objective was to improve performance and operability of the network with control software aiming for a specific quality of service. The load level targets in each cell were regularly adjusted with a control method in order to improve performance. The approach was validated using a dynamic WCDMA system simulator. The conducted simulations support the assumption that the uplink performance can be managed and improved by the proposed cell-based automated optimization.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1208913,no,undetermined,0
Asymptotic insensitivity of least-recently-used caching to statistical dependency,"We investigate a widely popular least-recently-used (LRU) cache replacement algorithm with semiMarkov modulated requests. SemiMarkov processes provide the flexibility for modeling strong statistical correlation, including the broadly reported long-range dependence in the World Wide Web page request patterns. When the frequency of requesting a page n is equal to the generalized Zipf's law c/n<sup>Î±</sup>, Î± > 1, our main result shows that the cache fault probability is asymptotically, for large cache sizes, the same as in the corresponding LRU system with i.i.d. requests. This appears to be the first explicit average case analysis of LRU caching with statistically dependent request sequences. The surprising insensitivity of LRU caching performance demonstrates its robustness to changes in document popularity. Furthermore, we show that the derived asymptotic result and simulation experiments are in excellent agreement, even for relatively small cache sizes. The potential of using our results in predicting the behavior of Web caches is tested using actual, strongly correlated, proxy server access traces.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1208695,no,undetermined,0
Model-based fault-adaptive control of complex dynamic systems,"<div style=""font-variant: small-caps; font-size: .9em;"">First Page of the Article</div><img class=""img-abs-container"" style=""width: 95%; border: 1px solid #808080;"" src=""/xploreAssets/images/absImages/01208147.png"" border=""0"">",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1208147,no,undetermined,0
A 2-level call admission control scheme using priority queue for decreasing new call blocking & handoff call dropping,"In order to provide a fast moving mobile host (MH) supporting multimedia applications with a consistent quality of service (QoS), an efficient call admission mechanism is in need. This paper proposes the 2-level call admission (2LCAC) scheme based on a call admission scheme using the priority to guarantee the consistent QoS for mobile multimedia applications. The 2LCAC consists of the basic call admission and advanced call admission; the former determines call admission based on bandwidth available in each cell and the latter determines call admission by utilizing delay tolerance time (DTT) and priority queue (PQueue) algorithms. In order to evaluate the performance of our scheme, we measure the metrics such as the blocking probability of new calls, dropping probability of handoff calls and bandwidth utilization. The result shows that the performance of our scheme is superior to that of existing schemes such as complete sharing policy (CSP), guard channel policy (GCP) and adaptive guard channel policy (AGCP).",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1207582,no,undetermined,0
Considering fault removal efficiency in software reliability assessment,"Software reliability growth models (SRGMs) have been developed to estimate software reliability measures such as the number of remaining faults, software failure rate, and software reliability. Issues such as imperfect debugging and the learning phenomenon of developers have been considered in these models. However, most SRGMs assume that faults detected during tests will eventually be removed. Consideration of fault removal efficiency in the existing models is limited. In practice, fault removal efficiency is usually imperfect. This paper aims to incorporate fault removal efficiency into software reliability assessment. Fault removal efficiency is a useful metric in software development practice and it helps developers to evaluate the debugging effectiveness and estimate the additional workload. In this paper, imperfect debugging is considered in the sense that new faults can be introduced into the software during debugging and the detected faults may not be removed completely. A model is proposed to integrate fault removal efficiency, failure rate, and fault introduction rate into software reliability assessment. In addition to traditional reliability measures, the proposed model can provide some useful metrics to help the development team make better decisions. Software testing data collected from real applications are utilized to illustrate the proposed model for both the descriptive and predictive power. The expected number of residual faults and software failure rate are also presented.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1206460,no,undetermined,0
Hardware architecture design for variable block size motion estimation in MPEG-4 AVC/JVT/ITU-T H.264,"Variable block size motion estimation is adopted in the new video coding standard, MPEG-4 AVC/JVT/ITU-T H.264, due to its superior performance compared to the advanced prediction mode in MPEG-4 and H.263+. In this paper, we modified the reference software in a hardware-friendly way. Our main idea is to convert the sequential processing of each 8Ã—8 sub-partition of a macro-block into parallel processing without sacrifice of video quality. Based on our algorithm, we proposed a new hardware architecture for variable block size motion estimation with full search at integer-pixel accuracy. The features of our design are 2-D processing element array with 1-D data broadcasting and 1-D partial result reuse, parallel adder tree, memory interleaving scheme, and high utilization. Simulation shows that our chip can achieve real-time applications under the operating frequency of 64.11 MHz for 720Ã—480 frame at 30 Hz with search range of [-24, +23] in horizontal direction and [-16, +15] in vertical direction, which requires the computation power of more than 50 GOPS.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1206094,no,undetermined,0
Automatic document metadata extraction using support vector machines,"Automatic metadata generation provides scalability and usability for digital libraries and their collections. Machine learning methods offer robust and adaptable automatic metadata extraction. We describe a support vector machine classification-based method for metadata extraction from header part of research papers and show that it outperforms other machine learning methods on the same task. The method first classifies each line of the header into one or more of 15 classes. An iterative convergence procedure is then used to improve the line classification by using the predicted class labels of its neighbor lines in the previous round. Further metadata extraction is done by seeking the best chunk boundaries of each line. We found that discovery and use of the structural patterns of the data and domain based word clustering can improve the metadata extraction performance. An appropriate feature normalization also greatly improves the classification performance. Our metadata extraction method was originally designed to improve the metadata extraction quality of the digital libraries Citeseer [S. Lawrence et al., (1999)] and EbizSearch [Y. Petinot et al., (2003)]. We believe it can be generalized to other digital libraries.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1204842,no,undetermined,0
Voltage flicker calculations for single-phase AC railroad electrification systems,"Rapid load variations can cause abrupt changes in the utility voltage, so-called voltage flicker. The voltage flicker may result in light flickering and, in extreme cases, damage to electronic equipment. Electrified railroads are just one example where such rapid load variation occurs as trains accelerate, decelerate, and encounter and leave grades. For balanced loads, the voltage flicker is easily determined using per-phase analysis. AC electrification system substations operating at a commercial frequency, however, are supplied from only two phases of utility three-phase transmission system. In order to calculate the voltage flicker for such an unbalanced system, symmetrical component method needs to be used. In this paper, a procedure is developed for evaluating the effects of short-time traction load variation onto utility system. Applying the symmetrical component method, voltage flicker equations are developed for loads connected to A-B, B-C, and C-A phases of a three-phase utility system. Using a specially-developed software simulating the train and electrification system performance, loads at the traction power substation transformers are calculated in one-second intervals. Subsequently, voltages at the utility busbars are calculated for each interval, and the voltage variation from interval to interval is expressed in percent. The calculated voltage flicker is then compared to the utility accepted limits. Based on this comparison, the capability of the utility power system to support the traction loads can be assessed and the suitability of the proposed line taps for the traction power substations evaluated.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1204662,no,undetermined,0
Partial lookup services,"Lookup services are used in many Internet applications to translate a key (e.g., a file name) into an associated set of entries (e.g., the location of file copies). The key lookups can often be satisfied by returning just a few entries instead of the entire set. However, current implementations of lookup services do not take advantage of this usage pattern. In this paper, we formalize the notion of a partial lookup service that explicitly supports returning a subset of the entries per lookup. We present four schemes for building a partial lookup service, and propose various metrics for evaluating the schemes. We show that a partial lookup service may have significant advantages over conventional ones in terms of space usage, fairness, fault tolerance, and other factors.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1203452,no,undetermined,0
Feedback control with queueing-theoretic prediction for relative delay guarantees in web servers,"The use of feedback control theory for performance guarantees in QoS-aware systems has gained much attention in recent years. In this paper, we investigate merging, within a single framework, the predictive power of queueing theory with the reactive power of feedback control to produce software systems with a superior ability to achieve QoS specifications in highly unpredictable environments. The approach is applied to the problem of achieving relative delay guarantees in high-performance servers. Experimental evaluation of this approach on an Apache web server shows that the combined schemes perform significantly better in terms of keeping the relative delay on target compared to feedback control or queueing prediction alone.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1203053,no,undetermined,0
Architectural level risk assessment tool based on UML specifications,"Recent evidences indicate that most faults in software systems are found in only a few of a system's components [1]. The early identification of these components allows an organization to focus on defect detection activities on high risk components, for example by optimally allocating testing resources [2], or redesigning components that are likely to cause field failures. This paper presents a prototype tool called Architecture-level Risk Assessment Tool (ARAT) based on the risk assessment methodology presented in [3]. The ARAT provides risk assessment based on measures obtained from Unified Modeling Language (UML) artifacts [4]. This tool can be used in the design phase of the software development process. It estimates dynamic metrics [5] and automatically analyzes the quality of the architecture to produce architectural-level software risk assessment [3].",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1201297,no,undetermined,0
Beyond the Personal Software Process: Metrics collection and analysis for the differently disciplined,"Pedagogues such as the Personal Software Process (PSP) shift metrics definition, collection, and analysis from the organizational level to the individual level. While case study research indicates that the PSP can provide software engineering students with empirical support for improving estimation and quality assurance, there is little evidence that many students continue to use the PSP when no longer required to do so. Our research suggests that this ""PSP adoption problem"" may be due to two problems: the high overhead of PSP-style metrics collection and analysis, and the requirement that PSP users ""context switch"" between product development and process recording. This paper overviews our initial PSP experiences, our first attempt to solve the PSP adoption problem with the LEAP system, and our current approach called Hackystat. This approach fully automates both data collection and analysis, which eliminates overhead and context switching. However, Hackystat changes the kind of metrics data that is collected, and introduces new privacy-related adoption issues of its own.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1201249,no,undetermined,0
A guaranteed quality of service wireless access scheme for CDMA networks,"Current wireless multimedia applications may require different quality-of-service (QoS) measures such as throughput, packet loss rate, delay, and delay jitter. In this paper, we propose an access scheme for CDMA networks that can provide absolute QoS guarantees for different service classes. The access scheme uses several M/D/1 queues, each representing a different service class, and allocates a transmission rate to each queue so as to satisfy the different QoS requirements. Operation in error-prone channels is enabled by a mechanism that compensates sessions, which experience poor channels. Analysis and simulation results are used to illustrate the viability of the access scheme.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1235836,no,undetermined,0
"Experimental evaluation of the variation in effectiveness for DC, FPC and MC/DC test criteria","Given a test criterion, the number of test-sets satisfying the criterion may be very large, with varying fault detection effectiveness. This paper presents an experimental evaluation of the variation in fault detection effectiveness of all the test-sets for a given control-flow test criterion and a Boolean specification. The exhaustive experimental approach complements the earlier empirical studies that adopted analysis of some test-sets using random selection techniques. Three industrially used control-flow testing criteria, decision coverage (DC), full predicate coverage (FPC) and modified condition/decision coverage (MC/DC) have been analysed against four types of faults. The Boolean specifications used were taken from a past research paper and also generated randomly. To ensure that it is the test-set size, a variation of DC, decision coverage/random (DC/R), has also been considered against FPC and MC/DC criteria. In addition, a further analysis of variation in average effectiveness with respect to number of conditions in the decision has been done. The empirical results show that the MC/DC criterion is more reliable and stable in comparison to DC, DC/R and FPC.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1237977,no,undetermined,0
A generic RTOS model for real-time systems simulation with systemC,"The main difficulties in designing real-time systems are related to time constraints: if an action is performed too late, it is considered as a fault (with different levels of criticism). Designers need to use a solution that fully supports timing constraints and enables them to simulate early on the design process a real-time system. One of the main difficulties in designing HW/SW systems resides in studying the effect of serializing tasks on processors running a real-time operating system (RTOS). In this paper, we present a generic model of RTOS based on systemC. It allows assessing real-time performances and the influence of scheduling according to RTOS properties such as scheduling policy, context-switch time and scheduling latency.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1269211,no,undetermined,0
RTOS scheduling in transaction level models,"Raising the level of abstraction in system design promises to enable faster exploration of the design space at early stages. While scheduling decision for embedded software has great impact on system performance, it's much desired that the designer can select the right scheduling algorithm at high abstraction levels so as to save him from the error-prone and time consuming task of tuning code delays or task priority assignments at the final stage of system design. In this paper we tackle this problem by introducing a RTOS model and an approach to refine any unscheduled transaction level model (TLM) to a TLM with RTOS scheduling support. The refinement process provides a useful tool to the system designer to quickly evaluate different dynamic scheduling algorithms and make the optimal choice at an early stage of system design.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1275252,no,undetermined,0
Performance of common and dedicated traffic channels for point-to-multipoint transmissions in W-CDMA,"We present a performance evaluation of two strategies for transmitting packet data to a group of multicast users over the W-CDMA air interface. In the first scheme, a single common or broadcast channel is used to transmit multicast data over the entire cell, whereas in the second scheme multiple dedicated channels are employed for transmitting to the group. We evaluate the performance of both schemes in terms of the number of users that can be supported by either scheme at a given quality of service defined in terms of a target outage probability.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1285951,no,undetermined,0
Plane-based calibration for multibeam sonar mounting angles,"Multibeam sonar systems are much more efficient than the convectional single-beam echo sounders for seafloor-mapping in hydrographic surveying. On the other hand, the operation of multibeam sonar systems needs to integrate more auxiliary sensor units. Because the world coordinates of each footprint is calculated based on the geometry of the sonar head relative to the GPS of the ship. Therefore, the resulting survey quality highly depends on the accuracy of the estimated mounting configuration of the sonar head, and other sensor units. Basically, the configuration parameters include the three Euler's angles, three linear translations and the asynchronous latency of signals between the transducer and other sensors. These parameters can not be measured directly. They can only be estimated from the post-process of the bathymetry data called patch test. Generally, the patch test requires the survey ship to follow several designated paths which are parallel, reciprocal or perpendicular to each other. Furthermore, the choice of seabed slope is also an important factor for the quality of the result. The contourplots of the seabed for the different paths are used to estimate the mounting configuration of the sonar head. In this work, we propose best-fitting a small flat patch to represent the seabed right beneath a segment of the path. A pair of patches from the two adjacent segments of reciprocal or perpendicular paths are selected for comparison. The difference between the two patches gives us an idea how the mounting parameters, i.e. the rolling, pitching and yawing angles, might be. If the parameters are accurately estimated, the two patches should be coplane. We design several semi-positive definite functions and feed back control algorithms to steer the mounting angles to search for the solutions. One more advantage of this approach is that the variation of each mounting angles as the survey undergoes can be monitored. Our preliminary study shows that our results are only 1% different from other commercial software. Further comparisons will be reported in the final paper.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1283425,no,undetermined,0
The WindSat calibration/validation plan and early results,"Summary form only given. The WindSat radiometer was launched as part of the Coriolis mission in January 2003. WindSat was designed to provide fully polarimetric passive microwave measurements globally, and in particular over the oceans for ocean surface wind vector retrieval. Due to prohibitive risk and cost associated with an end-to-end pre-launch absolute radiometer calibration (i.e. from the energy incident on the main reflector through the receiver digitized output) it was important to develop an on-orbit calibration plan that verifies instrument conformance to specification and, if necessary, derive suitable calibration coefficients or sensor algorithms that bring the instrument into specification. This is especially true for the WindSat Cal/Val, in view of the fact that it is the first fully polarimetric spaceborne radiometer. This paper will provide an overview of the WindSat Cal/Val Program. The Cal/Val plan, patterned after the very successful SSM/I Cal/Val, is designed to ensure the highest quality data products and maximize the return on the available Cal/Val resources. The Cal/Val is progressive and will take place in well-defined stages: Early Orbit Evaluation, Initial Assessment, Detailed Calibration, and EDR Validation. The approach allows us to focus efforts more quickly on issues as they are identified and ensures a logical progression of activities. At each level of the Cal/Val the examination of the instrument and algorithm errors becomes more detailed. Along with the WindSat Cal/Val structure overview, we will present the independent data sources to be used and the analysis techniques to be employed. During the Early Orbit phase, special instrument operating modes have been developed to monitor the sensor health from the time of instrument turn-on to a short time after it reaches stable operating conditions. This mode is uniquely important for the WindSat since it affords the only opportunity to examine all of the data in the full 360 degree scan- > - > and to directly assess potential field-of-view intrusion effects from the spacecraft or other sensors on-board the satellite and evaluate the spin control system by observing the statistical distribution of data as the horns scan through the calibration loads. The next phase of the WindSat Cal/Val consists of an initial assessment of all sensor and environmental data products generated by the Ground Processing Software. The primary focus of the assessment is to conduct an end-to-end review of the output files (TDR, SDR and EDR) to verify the following: proper functioning of the on-line GPS modules, instrument calibration (including Antenna Pattern Correction and Stokes Coupling) does not contain large errors, EDR algorithms provide reasonable products, and there are no major geo-iocation errors. This paper will provide a summary of the results of the Early Orbit and Initial Assessment phases of the WindSat Cal/Val Program.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1282429,no,undetermined,0
Introduction of BSS into VR-based test and simulation-with application in NDE,"VR-based test and simulation system(VTSS) is a concept put forward in the authors' previous works. In a VTSS, the test processes are interactively planned, optimized and simulated in a virtual test environment generated by computer, aiming at eventually performing tests completely in virtual test environments. To make VTSS intelligent and practical, the technique of blind source separation (BSS) is introduced into VTSS. With ultrasonic non-destructive evaluation (NDE) as the technical background, a prototype of the system has been described in this paper. BSS is adopted in VTSS to serve three purposes: defect classification, system modeling and noise reduction. The conclusion is that BSS can play a very important role in VTSS.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1281198,no,undetermined,0
Effect of feature extraction and feature selection on expression data from epithelial ovarian cancer,"Classifying the gene expression levels of normal and cancerous cells and identifying the genes most contributing to this distinction propose an alternative means of diagnosis. We have investigated the effect of feature extraction and feature selection on clustering of the expression data on two different data sets for ovarian cancer. One data set consisted of 2176 transcripts from 30 samples, nine from normal ovarian epithelial cells and 21 from cancerous ones. The other data set had 7129 transcripts coming from 27 tumor and four normal ovarian tissues. Hierarchical clustering algorithms employing complete-link, average-link and Ward's method were implemented for comparative evaluation. Principal component analysis was applied for feature extraction and resulted in 100% segregation. Feature selection was performed to identify the most distinguishing genes using CARTÂ® software. Selected features were able to cluster the data with 100% success. The results suggest that adoption of feature extraction and selection enhances the quality of clustering of gene expression data for ovarian cancer. Identification of distinguishing genes is a more complex problem that requires incorporating pathway knowledge with statistical and machine learning methods.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1280921,no,undetermined,0
Towards statistical inferences of successful prostate surgery,"Prostate cancer continues to be the leading cancer in the United States male population. The options for local therapy have proliferated and include various forms of radiation delivery, cryo-destruction, and novel forms of energy delivery as in high-intensity focused ultrasound. Surgical removal, however, remains the standard procedure for cure. Currently there are little objective parameters that are used to compare the efficiency of each form of surgical removal. As surgeons apply these different surgical approaches, a quality assessment would be most useful, not only with regard to overall comparison of one approach vs. another but also surgeon evaluation of personal surgical performance as they relate to a standard. To this end, we discuss the development of a process employing image reconstruction and analysis techniques to assess the volume and extent of extracapsular soft tissue removed with the prostate. Parameters such as the percent of capsule covered by soft tissue and where present the average depth of soft tissue coverage are assessed. A final goal is to develop software for the purpose of a quality assurance assessment for pathologists and surgeons to evaluate the adequacy/appropriateness of each surgical procedure; laparoscopic versus open perineal or retropubic prostatectomy.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1279809,no,undetermined,0
Programmable low-cost ultrasound machine,"Most commercial ultrasound machines do not allow the researcher to acquire internal data, modify the signal processing algorithms or test a new clinical application. A software-based ultrasound machine could offer the needed flexibility to modify the ultrasound processing algorithms and evaluate new ultrasound applications for clinical efficacy. We have designed an ultrasound machine in which all of the back-end processing is performed in software rather than in hardware. This programmable machine supports all the conventional ultrasound modes (e.g., B mode, color flow, M mode and spectral Doppler) without any performance degradation. The frame rates and image quality of this programmable machine were evaluated and found to be comparable to commercial ultrasound machines. Due to its programmability, we were able to improve the accuracy of flow velocity estimation by integrating a new clutter rejection filtering method into the system. The machine also supports advanced features such as 3D volume rendering in software. Our results demonstrate that a software-based ultrasound machine can provide the same functionality for clinical use as conventional ultrasound systems while offering increased flexibility at low cost.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1279456,no,undetermined,0
Early estimation of the size of VHDL projects,"The analysis of the amount of human resources required to complete a project is felt as a critical issue in any company of the electronics industry. In particular, early estimation of the effort involved in a development process is a key requirement for any cost-driven system-level design decision. In this paper, we present a methodology to predict the final size of a VHDL project on the basis of a high-level description, obtaining a significant indication about the development effort. The methodology is the composition of a number of specialized models, tailored to estimate the size of specific component types. Models were trained and tested on two disjoint and large sets of real VHDL projects. Quality-of-result indicators show that the methodology is both accurate and robust.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1275285,no,undetermined,0
First results of real-time time and frequency transfer using GPS code and carrier phase observations,"We have used code and carrier phase data from the global positioning system (GPS) satellites to estimate time differences between atomic clocks in near (<10 s) real-time. For some sites we have used data transmitted via Internet connections and TCP/IP, while for other sites data were collected in deferred time, but processed by a Kalman filter-based software as if they were available in real time. Satellite orbit and clock data of different quality have been used. The real-time estimates of time differences of the station clocks have been compared to those estimated from regular postprocessing using accurate satellite orbits and clocks from the international GPS service (IGS). First results show that the standard deviation of the differences between the real-time carrier phase-based and the postprocessing estimates of the clock time differences can be less than 100 ps for baselines of about 1000 km.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1275111,no,undetermined,0
Measurement-based admission control in UMTS multiple cell case,"In this paper, we develop an efficient call admission control (CAC) algorithm for UMTS systems. We first introduce the expressions that we developed for signal-to-interference (SIR) for both uplink and downlink, to obtain a novel CAC algorithm that takes into account, in addition to SIR constraints, the effects of mobility, coverage as well as the wired capacity in the LMTS terrestrial radio access network (UTRAN). for the uplink, and the maximal transmission power of the base station, for the downlink. As of its implementation, we investigate the measurement-based approach as a means to predict future, both handoff and new, call arrivals and thus manage different priority levels. Compared to classical CAC algorithms, our CAC mechanism achieves better performance in terms of outage probability and QoS management.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260419,no,undetermined,0
A high-level pipelined FPGA based DCT for video coding applications,"Video coding functions, such as discrete cosine transform (DCT), variable length coding and motion estimation, require a significant amount of processing power to implement in software. For high quality video or in applications where a powerful processor is not available, a hardware implementation is the solution. We propose a flexible field programmable gate array (FPGA) model, based on a high-level pipelined processor core, that can improve the performance of video coding. Furthermore, distributed arithmetic and exploitation of parallelism and bit-level pipelining are used to produce a DCT implementation on a single FPGA.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1273224,no,undetermined,0
Stability and performance of the stochastic fault tolerant control systems,"In this paper, the stability and performance of the fault tolerant control system (FTCS) are studied. The analysis is based on a stochastic framework of integrated FTCS, in which the system component failure and the fault detection and isolation (FDI) scheme are characterized by two Markovian parameters. In addition, the model uncertainties and noise/disturbance are treated in the same framework. The sufficient conditions for stochastic stability and the system performance using a stochastic integral quadratic constraint are developed. A simulation study on an example system is performed with illustrative results obtained.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1272993,no,undetermined,0
Generic signature generation tool for diagnosis and parametric estimation of multi-variable dynamical nonlinear systems,"In this paper, a graphical signature generation tool is proposed for diagnosis and parametric estimation for nonlinear systems. This tool is based on the definition of a two-dimensional graphical signature obtained by using the history of the output measurements on some moving-horizon window. The underlying idea is that if each parametric variation deforms this signature in a different way, the latter becomes a remarkable tool for diagnosis, in particular in the case of the diagnosis by human operator. The whole scheme is illustrated by a simple example.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1272692,no,undetermined,0
Low-cost power quality monitor based on a PC,"This paper presents the development of a low-cost digital system useful for power quality monitoring and power management. Voltage and current measurements are made through Hall-effect sensors connected to a standard data acquisition board, and the applications were programmed in LabVIEWâ„?, running on Windows in a regular PC. The system acquires data continuously, and stores in files the events that result from anomalies detected in the monitored power system. Several parameters related to power quality and power management can be analyzed through 6 different applications, named: ""scope and THD"", ""strip chart"", ""wave shape"", ""sags and swells"", ""classical values"" and ""p-q theory"". The acquired information can be visualized in tables and/or in charts. It is also possible to generate reports in HTML format. These reports can be sent directly to a printer, embedded in other software applications, or accessed through the Internet, using a Web browser. The potential of the developed system is shown, namely the advantages of virtual instrumentation, regarding to flexibility, cost and performance, in the scope of power quality monitoring and power management.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1267267,no,undetermined,0
Risk responsibility for supply in Latin America - the Argentinean case,"In deregulation of electricity sectors in Latin America two approaches have been used to allocate the responsibility on the electricity supply: (1) The government keeps the final responsibility on the supply. Suppliers (distribution companies or traders) do not have control on the rationing when it becomes necessary to curtail load. In such case they cannot manage the risks associated to the supply. This is the case in the markets of Brazil and Colombia. (2) The responsibility is fully transferred to suppliers. The regulatory entity supervises the quality of the supply and different types of penalties are applied when load is not supplied. This approach is currently used in Argentina, Chile and Peru. In Argentina the bilateral contracts, that are normally financial, become physical when a rationing event happens. This approach permits suppliers to have a great control on risks. Both approaches have defenders and detractors. In some cases, the conclusions on a same event have completely opposite interpretations and diagnoses. For instance, the crisis of supply in Brazil during 2002 was interpreted as a fault of the market by the defenders of the final responsibility of the state, or attributed to excess of regulation and of interference of the government by the advocates of decentralized schemes. This presentation will analyze the performance of both approaches in Latin America, assessing the diverse types of arguments used to criticize or to defend to each one of these approaches, and finally to present some conclusions on the current situation and future of the responsibility on supply and risks associated.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1267237,no,undetermined,0
Investigation of interfaces with analytical tools,"This paper focuses on advancements in three areas of analyzing interfaces, namely, acoustic microscopy for detecting damage to closely spaced interfaces, thermal imaging to detect damage and degradation of thermal interface materials and laser spallation, a relatively new concept to understand the strength of interfaces. Acoustic microscopy has been used widely in the semiconductor assembly and package area to detect delamination, cracks and voids in the package, but the resolution in the axial direction has always been a limitation of the technique. Recent advancements in acoustic waveform analysis has now allowed for detection and resolution of closely spaced interfaces such as layers within the die. Thermal imaging using infrared (IR) thermography has long been used for detection of hot spots in the die or package. With recent advancements in very high-speed IR cameras, improved pixel resolution, and sophisticated software programming, the kinetics of heat flow can now be imaged and analyzed to reveal damage or degradation of interfaces that are critical to heat transfer. The technique has been demonstrated to be useful to understand defects and degradation of thermal interface materials used to conduct heat away from the device. Laser spallation is a method that uses a short duration laser pulse to cause fracture at the weakest interface and has the ability to measure the adhesion strength of the interface. The advantage of this technique is that it can be used for fully processed die or wafers and even on packaged devices. The technique has been used to understand interfaces in devices with copper metallization and low-k dielectrics.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1261732,no,undetermined,0
Faults in grids: why are they so bad and what can be done about it?,"Computational grids have the potential to become the main execution platform for high performance and distributed applications. However, such systems are extremely complex and prone to failures. We present a survey with the grid community on which several people shared their actual experience regarding fault treatment. The survey reveals that, nowadays, users have to be highly involved in diagnosing failures, that most failures are due to configuration problems (a hint of the area's immaturity), and that solutions for dealing with failures are mainly application-dependent. Going further, we identify two main reasons for this state of affairs. First, grid components that provide high-level abstractions when working, do expose all gory details when broken. Since there are no appropriate mechanisms to deal with the complexity exposed (configuration, middleware, hardware and software issues), users need to be deeply involved in the diagnosis and correction of failures. To address this problem, one needs a way to coordinate different support teams working at the grids different levels of abstraction. Second, fault tolerance schemes today implemented on grids tolerate only crash failures. Since grids are prone to more complex failures, such those caused by heisenbugs, one needs to tolerate tougher failures. Our hope is that the very heterogeneity, that makes a grid a complex environment, can help in the creation of diverse software replicas, a strategy that can tolerate more complex failures.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1261694,no,undetermined,0
"Verification, validation, and certification of modeling and simulation applications","Certifying that a large-scale complex modeling and simulation (M&S) application can be used for a set of specific purposes is an onerous task, which involves complex evaluation processes throughout the entire M&S development life cycle. The evaluation processes consist of verification and validation activities, quality assurance, assessment of qualitative and quantitative elements, assessments by subject matter experts, and integration of disparate measurements and assessments. Planning, managing, and conducting the evaluation processes require a disciplined life-cycle approach and should not be performed in an ad hoc manner. The purpose of this tutorial paper is to present structured evaluation processes throughout the entire M&S development life cycle. Engineers, analysts, and managers can execute the evaluation processes presented herein to be able to formulate a certification decision for a large-scale complex M&S application.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1261418,no,undetermined,0
Particle swarm optimization for worst case tolerance design,"Worst case tolerance analysis is a major subtask in modern industrial electronics. Recently, the demands on industrial products like production costs or probability of failure have become more and more important in order to be competitive in business. The main key to improve the quality of electronic products is the challenge to reduce the effects of parameter variations, which can be done by robust parameter design. This paper addresses the applicability of particle swarm optimization combined with pattern search for worst case circuit design. The main advantages of this approach are the efficiency and robustness of the particle swarm optimization strategy. The method is also well suited for higher order problems, i.e. for problems with a high number of design parameters, because of the linear complexity of the pattern search algorithm.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1290245,no,undetermined,0
H-COMP: a tool for quantitative and comparative analysis of endmember identification algorithms,"Over the past years, several endmember extraction algorithms have been developed for spectral mixture analysis of hyperspectral data. Due to a lack of quantitative approaches to substantiate new algorithms, available methods have not been rigorously compared using a unified scheme. In this paper, we describe H-COMP, an IDL (Interactive Data Language)-based software toolkit for visualization and interactive analysis of results provided by endmember selection methods. The suitability of using H-COMP for assessment and comparison of endmember extraction algorithms is demonstrated in this work by a comparative analysis of three standard algorithms: Pixel Purity Index (PPI), N-FINDR, and Automated Morphological Endmember Extraction (AMEE). Simulated and real hyperspectral datasets, collected by the NASA/JPL Airborne Visible-Infrared Imaging Spectrometer (AVIRIS), are used to carry out a comparative effort, focused on the definition of reliable endmember quality metrics.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1293753,no,undetermined,0
Higher education level laboratory for real time pollution monitoring,"The present paper presents a higher education level laboratory for real time pollution monitoring used in environmental courses of the engineering universities. The main objective of this laboratory is to simulate usual applications in the field of real time measuring and monitoring of air and water pollutants. The laboratory was designed and realized by ECOSEN Ltd. Company. Real time simulators were obtained, using modules with solid state (SnO<sub>2</sub>) gas sensors. A separate module is for pH measurements in water. For data acquisition a microcontroller or a parallel interface were used. An IBM compatible PC is used for data processing and management. Dedicated software was developed in Turbo Pascal 6.0 and Visual Basic 6.0 for data acquisition, processing and management. This laboratory is included, starting from year 2000, in the university course ""Techniques of the environmental quality measurement"" held at the Environmental Engineering Faculty of the Ecological University in Bucharest. An earlier simpler version is now used by University of Civil Engineering in Bucharest.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1295589,no,undetermined,0
Reduced complexity motion estimation techniques: review and comparative study,"Because of its high computational complexity, the process of motion estimation has become a bottleneck problem in many video applications, such as mobile video terminals and software-based video codecs. This has motivated the development of a number of fast motion estimation techniques. This paper briefly reviews reduced complexity motion estimation techniques and classifies them into five categories. It then presents the results of a comparative study across all categories. The aim of this study is to provide the reader with a feel of the relative performance of the categories, with particular attention to the important trade-off between computational complexity and prediction quality.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1301858,no,undetermined,0
Efficient implementations of mobile video computations on domain-specific reconfigurable arrays,"Mobile video processing as defined in standards like MPEG-4 and H.263 contains a number of timeconsuming computations that cannot be efficiently executed on current hardware architectures. The authors recently introduced a reconfigurable SoC platform that permits a low-power, high-throughput and flexible implementation of the motion estimation and DCT algorithms. The computations are done using domainspecific reconfigurable arrays that have demonstrated up to 75% reduction in power consumption when compared to generic FPGA architecture, which makes them suitable for portable devices. This paper presents and compares different configurations of the arrays to efficiently implementing DCT and motion estimation algorithms. A number of algorithms are mapped into the various reconfigurable fabrics demonstrating the flexibility of the new reconfigurable SoC architecture and its ability to support a number of implementations having different performance characteristics.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1269064,no,undetermined,0
Time-energy design space exploration for multi-layer memory architectures,"This paper presents an exploration algorithm which examines execution time and energy consumption of a given application, while considering a parameterized memory architecture. The input to our algorithm is an application given as an annotated task graph and a specification of a multi-layer memory architecture. The algorithm produces Pareto trade-off points representing different multi-objective execution options for the whole application. Different metrics are used to estimate parameters for application-level Pareto points obtained by merging all Pareto diagrams of the tasks composing the application. We estimate application execution time although the final scheduling is not yet known. The algorithm makes it possible to trade off the quality of the results and its runtime depending on the used metrics and the number of levels in the hierarchical composition of the tasks' Pareto points. We have evaluated our algorithm on a medical image processing application and randomly generated task graphs. We have shown that our algorithm can explore huge design space and obtain (near) optimal results in terms of Pareto diagram quality.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1268867,no,undetermined,0
A cost-benefit stopping criterion for statistical testing,"Determining when to stop a statistical test is an important management decision. Several stopping criteria have been proposed, including criteria based on statistical similarity, the probability that the system has a desired reliability, and the expected cost of remaining faults. This paper proposes a new stopping criterion based on a cost-benefit analysis using the expected reliability of the system (as opposed to an estimate of the remaining faults). The expected reliability is used, along with other factors such as units deployed and expected use, to anticipate the number of failures in the field and the resulting anticipated cost of failures. Reductions in this number generated by increasing the reliability are balanced against the cost of further testing to determine when testing should be stopped.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1265715,no,undetermined,0
Towards survivable intrusion detection system,"Intrusion detection systems (IDS) are increasingly a key part of system defense, often operating under a high level of privilege to achieve their purposes. Therefore, the ability of an IDS to withstand attack is important in a production system. In this paper, we address the issue of survivable IDS. We begin by categorizing potential vulnerabilities in a generic IDS and classifying methods used to enhance IDS survivability. We then propose an efficient fault tolerance based Survivable IDS (SIDS) along with a systematic way to transform an original IDS architecture into this survivable architecture. Key components of SIDS include: a dual-functionality forward-ahead (DFFA) structure, backup communication paths, component recycling, system reconfiguration, and an anomaly detector. Use of the SIDS transformation should result in an improvement in IDS survivability at low cost.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1265702,no,undetermined,0
User-level QoS and traffic engineering for 3G wireless 1Ã—EV-DO systems,"Third-generation (3G) wireless systems such as 3G1X, 1Ã—EV-DO, and 1xEV-DV provide support for a variety of high-speed data applications. The success of these services critically relies on the capability to ensure an adequate quality of service (QoS) experience to users at an affordable price. With wireless bandwidth at a premium, traffic engineering and network planning play a vital role in addressing these challenges. We present models and techniques that we have developed for quantifying the QoS perception of 1Ã—EV-DO users generating file transfer protocol (FTP) or Web browsing sessions. We show how user-level QoS measures may be evaluated by means of a Processor-Sharing model that explicitly accounts for the throughput gains from multi-user scheduling. The model provides simple analytical formulas for key performance metrics such as response times, blocking probabilities, and throughput. Analytical models are especially useful for network deployment and in-service tuning purposes due to the intrinsic difficulties associated with simulation-based optimization approaches. Â© 2003 Lucent Technologies Inc.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6770627,no,undetermined,0
Neuronal prediction system of meteorological parameters for quality assurance of the traffic,"This work tries to be an answer to the problem of the ice formations on the road and over the taking of landing ways surfaces. As we all know, the unpredicted apparition of the ice over the roads and in the airports is the main cause of the majority human and material looses in those two places. The neuronal prediction system wants to be an alternative to the existent forecasting systems. This system will combine the last moment technique like the networks, the software (Labview, Matlab) and the most modern programming approach: neuronal networks, neuro-fuzzy systems, statistical signal processing, and the result wants to be a prediction system for the ice formation which is very precise and less expensive than the existent systems.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5731344,no,undetermined,0
A survey and measurement-based comparison of bandwidth management techniques,"This article gives a brief tutorial for bandwidth management systems; a survey of the techniques used by eight real-world systems, such as class-based queuing (CBQ), per-flow queuing (PFQ), random early detection (RED), and TCP rate control (TCR); a compact testbed with a set of methodologies to differentiate the employed techniques; and a detailed black-box evaluation. The tutorial describes the needs for the three types of policy rules: class-based bandwidth limitation, session-bandwidth guarantee, and inter/intra-class bandwidth borrowing. The survey portion investigates how the eight chosen commercial/open-source real systems enforce the three policy types. To evaluate the techniques, the designed testbed emulates real-life Internet conditions, such as many simultaneous sessions from different IPs/ports, controllable wide area network (WAN) delay and packet loss rate for each session, and different TCP source implementations. The performance metrics include accuracy of bandwidth management, fairness among sessions, robustness under Internet packet losses and different operating systems, inter/intra-class bandwidth borrowing, and voice over IP (VoIP) quality. The black-box test results demonstrate that (1) only the combination of CBQ+PFQ+TCR can solve the most difficult scenario (multiple sessions competing for the narrow 20kb/s class); (2) the TCR approach may degrade the goodput, fairness, and compatibility even under slight packet loss rates (0.5 percent); (3) without PFQ, TCR and RED have limited ability to isolate the sessions (especially for RED); (4) the G.729 VoIP quality over a 125kb/s access link becomes good only after exercising MSS-clamping to shrink the packet size of the background traffic down to 256 byte.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5341335,no,undetermined,0
Component based development for transient stability power system simulation software,"A component-based development (CBD) approach has become increasingly important in the software industry. Any software that apply the CBD will not only save time and cost through reusability of component, but also have the capability to handle the complex problems. Since CBD design is based on object-oriented programming (OOP), the components with a good quality and reusability can be created, classified and managed for future reuse. The methodology of OOP is based on the real object. The mechanism of OOP such as encapsulation, inheritance, and polymorphism are the advantages that could be used to define real objects associated with the program. This paper focused on the implementation of the CBD to power system transient stability simulation (TSS). There are many methods to solve transient stability problem, but in this paper two methods are applied to solve TSS problems, namely trapezoidal method and modified Euler method. The performance of two approaches, CBD and non CBD applications of power system transient stability simulation is assessed through tests carried out using IEEE data test systems.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1437409,no,undetermined,0
Simulation and modeling of stator flux estimator for induction motor using artificial neural network technique,"Accurate stator flux estimation for high performance induction motor drives is very important to ensure proper drive operation and stability. Unfortunately, there is some problems occurred when estimating stator flux especially at zero speed and at low frequency. Hence a simple open loop controller of pulse width modulation voltage source inverter (PWM-VSI) fed induction motor configuration is presented. By a selection of voltage model-based of stator flux estimation, a simple method using artificial neural network (ANN) technique is proposed to estimate stator flux by means of feed forward back propagation algorithm. In motor drives applications, artificial neural network has several advantages such as faster execution speed, harmonic ripple immunity and fault tolerance characteristics that will result in a significant improvement in the steady state performances. Thus, to simulate and model stator flux estimator, Matlab/Simulink software package particularly power system block set and neural network toolbox is implemented. A structure of three-layered artificial neural network technique has been applied to the proposed stator flux estimator. As a result, this technique gives good improvement in estimating stator flux which the estimated stator flux is very similar in terms of magnitude and phase angle if compared to the real stator flux.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1437408,no,undetermined,0
The effect of counting statistics on the integrity of deconvolved gamma-ray spectra,"Symetrica's spectrum-processing software improves both the spectral-resolution and sensitivity of scintillation spectrometers. This paper demonstrates the robustness of this algorithm when applied to spectra having poor statistical quality. The error in the location of the deconvolved 609keV full-energy peak, within a <sup>226</sup>Ra spectrum; was less than 2keV for a peak area containing fewer than 150 counts. For the raw data, this accuracy was only achieved for peak areas greater than 1300 counts. Processing the spectra in this way also improves the accuracy, by factor of between 2 and 2.6, with which the full-energy peak areas could be measured. This paper has also shown that, by using the processed data, a hidden <sup>137</sup>Cs source having a count rate at the level of just 2% that of a <sup>60</sup>Co masking source, was always detectable in the limited number of spectrum recorded.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1352136,no,undetermined,0
Real-time embedded system support for the BTeV level 1 Muon Trigger,"The Level 1 Muon Trigger subsystem for BTeV will be implemented using the same architectural building blocks as the BTeV Level 1 Pixel Trigger: pipelined field programmable gate arrays feeding a farm of dedicated processing elements. The muon trigger algorithm identifies candidate tracks, and is sensitive to the muon charge (sign); candidate dimuon events are identified by complementary charge track-pairs. To insure that the trigger is operating effectively, the trigger development team is actively collaborating in an independent multi-university research program for reliable, self-aware, fault adaptive behavior in real-time embedded systems (RTES). Key elements of the architecture, algorithm, performance, and engineered reliability are presented.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1351931,no,undetermined,0
The CMS high level trigger,"The High Level Trigger (HLT) system of the CMS experiment will consist of a series of reconstruction and selection algorithms designed to reduce the Level-1 trigger accept rate of 100 kHz to 100 Hz forwarded to permanent storage. The HLT operates on events assembled by an event builder collecting detector data from the CMS front-end system at full granularity and resolution. The HLT algorithms will run on a farm of commodity PCs, the filter farm, with a total expected computational power of 10<sup>6</sup> SpecInt95. The farm software, responsible for collecting, analyzing, and storing event data, consists of components from the data acquisition and the offline reconstruction domains, extended with the necessary glue components and implementation of interfaces between them. The farm is operated and monitored by the DAQ control system and must provide near-real-time feedback on the performance of the detector and the physics quality of data. In this paper, the architecture of the HLT farm is described, and the design of various software components reviewed. The status of software development is presented, with a focus on the integration issues. The physics and CPU performance of current reconstruction and selection algorithm prototypes is summarized in relation with projected parameters of the farm and taking into account the requirements of the CMS physics program. Results from a prototype test stand and plans for the deployment of the final system are finally discussed.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1351855,no,undetermined,0
Experiences in the inspection process characterization techniques,"Implementation of a disciplined engineering approach to software development requires the existence of an adequate supporting measurement & analysis system. Due to demands for increased efficiency and effectiveness of software processes, measurement models need to be created to characterize and describe the various processes usefully. The data derived from these models should then be analyzed quantitatively to assess the effects of new techniques and methodologies. In recent times, statistical and process thinking principles have led software organizations to appreciate the value of applying statistical process control techniques. As part of the journey towards SW-CMM&reg; Level 5 at the Motorola Malaysia Software Center, which the center achieved in October 2001, considerable effort was spent on exploring SPC techniques to establish process control while focusing on the quantitative process management KPA of the SW-CMMÂ®. This paper discusses the evolutionary learning experiences, results and lessons learnt by the center in establishing appropriate analysis techniques using statistical and other derivative techniques. The paper discusses the history of analysis techniques that were explored with specific focus on characterizing the inspection process. Future plans to enhance existing techniques and to broaden the scope to cover analysis of other software processes are also discussed.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1319126,no,undetermined,0
Character string predicate based automatic software test data generation,"A character string is an important element in programming. A problem that needs further research is how to automatically generate software test data for character strings. This paper presents a novel approach for automatic test data generation of program paths including character string predicates, and the effectiveness of this approach is examined on a number of programs. Each element of input variable of a character string is determined by using the gradient descent technique to perform function minimization so that the test data of character string can be dynamically generated. The experimental results illustrate that this approach is effective.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1319109,no,undetermined,0
A neuro-fuzzy model for software cost estimation,"A novel neuro-fuzzy constructive cost model (COCOMO) for software estimation is proposed. The model carries some of the desirable features of the neuro-fuzzy approach, such as learning ability and good interpretability, while maintaining the merits of the COCOMO model. Unlike the standard neural network approach, this model is easily validated by experts and capable of generalization. In addition, it allows inputs to be continuous-rating values and linguistic values, therefore avoiding the problem of similar projects having different estimated costs. Also presented in this paper is a detailed learning algorithm. The validation, using industry project data, shows that the model greatly improves the estimation accuracy in comparison with the well-known COCOMO model.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1319094,no,undetermined,0
Empirical case studies of combining software quality classification models,"The increased reliance on computer systems in the modern world has created a need for engineering reliability control of computer systems to the highest possible standards. This is especially crucial in high-assurance and mission critical systems. Software quality classification models are one of the important tools in achieving high reliability. They can be used to calibrate software metrics-based models to detect fault-prone software modules. Timely use of such models can greatly aid in detecting faults early in the life cycle of the software product. Individual classifiers (models) may be improved by using the combined decision from multiple classifiers. Several algorithms implement this concept and have been investigated. These combined learners provide the software quality modeling community with accurate, robust, and goal oriented models. This paper presents a comprehensive comparative evaluation of three combined learners, Bagging, Boosting, and Logit-Boost. We evaluated these methods with a strong and a weak learner, i.e., C4.5 and Decision Stumps, respectively. Two large-scale case studies of industrial software systems are used in our empirical investigations.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1319084,no,undetermined,0
A knowledge based tool to support industrial customers in PQ evaluations,"The work illustrates a knowledge-based software tool, aimed to support industrial customers in evaluating power quality (PQ) disturbances impact on their processes. For this evaluation the customer is provided with data and information, collected from both national and international experiences in PQ field, that allows to estimate: a) the susceptibility of the industrial process to the several typical PQ events; b) the typical costs associated to common PQ disturbances. The tool implements several procedures, each dedicated to focused evaluations of either susceptibility (SENS) or cost (COST) of PQ events.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1304631,no,undetermined,0
Addressing workload variability in architectural simulations,"The inherent variability of multithreaded commercial workloads can lead to incorrect results in architectural simulation studies. Although most architectural simulation studies ignore space variability's effects, our results demonstrate that space variability has serious implications for architectural simulation studies using multithreaded workloads. The standard solution - running long enough - does not easily apply to simulation because of its enormous slowdown. To address this problem, we propose a simulation methodology combining multiple simulations with standard statistical techniques, such as confidence intervals and hypothesis testing. This methodology greatly decreases the probability of drawing incorrect conclusions, and permits reasonable simulation times given sufficient simulation hosts.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1261392,no,undetermined,0
Unbounded system model allows robust communication infrastructure for power quality measurement and control,"A robust information infrastructure is required to collect power quality measurements and to execute corrective actions. It is based on a software architecture, designed at middleware level, that makes use of Internet protocols for communication over different media. While the middleware detects the anomalies in the communication and computation system and reacts appropriately, the application functionality is maintained through system reconfiguration or graceful degradation. Such anomalies may come from dynamic changes in the topology of the underlying communication system, or the enabling/disabling of processing nodes on the network. The added value of this approach comes from the flexibility to deal with a dynamic environment based on an unbounded system model. The paper illustrates this approach in a power quality measurement and control system with compensation based on active filters.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1259348,no,undetermined,0
An empirical comparison of two safe regression test selection techniques,"Regression test selection techniques reduce the cost of regression testing by selecting a subset of an existing test suite to use in retesting a modified program. Safe regression test selection techniques guarantee (under specific conditions) that the selected subset will not omit faults that could have been revealed by the entire suite. Many regression test selection techniques have been described in the literature. Empirical studies of some of these techniques have shown that they can be beneficial, but only a few studies have empirically compared different techniques, and fewer still have considered safe techniques. In this paper, we report the results of a comparative empirical study of implementations of two safe regression test selection techniques: DejaVu and Pytia. Our results show that, despite differences in their approaches, and despite the theoretically greater ability of DejaVu to select smaller test suites than Pythia, the two techniques often selected equivalent test suites in practice, at comparable costs. These results suggest that factors such as ease of implementation, generality, and availability of supporting tools and data may play a greater role than cost-effectiveness for practitioners choosing between these techniques.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1237978,no,undetermined,0
Accounting for false indication in a Bayesian diagnostics framework,"Accounting for the effects of test uncertainty is a significant problem in test and diagnosis. Specifically, assessment of the level of uncertainty and subsequent utilization of that assessment to improve diagnostics must be addressed. One approach, based on measurement science, is to treat the probability of a false indication (false alarm or missed detection) as the measure of uncertainty. Given the ability to determine such probabilities, a Bayesian approach to diagnosis suggests itself. In the paper, we present a mathematical derivation for false indication and apply it to the specification of Bayesian diagnosis. We draw from measurement science, reliability theory, and the theory of Bayesian networks to provide an end-to-end probabilistic treatment of the fault diagnosis problem.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1243587,no,undetermined,0
An embryonic approach to reliable digital instrumentation based on evolvable hardware,"Embryonics encompasses the capability of self-repair and self-replication in systems. This paper presents a technique based on reconfigurable hardware coupled with a novel backpropagation algorithm for reconfiguration, together referred to as evolvable hardware (EHW), for ensuring reliability in digital instrumentation. The backpropagation evolution is much faster than genetic learning techniques. It uses the dynamic restructuring capabilities of EHW to detect faults in digital systems and reconfigures the hardware to repair or adapt to the error in real-time. An example application is presented of a robust BCD to a seven-segment decoder driving a digital display. The results obtained are quite interesting and promise quick and low cost embryonic schemes for reliability in digital instrumentation.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1246539,no,undetermined,0
An efficient defect estimation method for software defect curves,Software defect curves describe the behavior of the estimate of the number of remaining software defects as software testing proceeds. They are of two possible patterns: single-trapezoidal-like curves or multiple-trapezoidal-like curves. In this paper we present some necessary and/or sufficient conditions for software defect curves of the Goel-Okumoto NHPP model. These conditions can be used to predict the effect of the detection and removal of a software defect on the variations of the estimates of the number of remaining defects. A field software reliability dataset is used to justify the trapezoidal shape of software defect curves and our theoretical analyses. The results presented in this paper may provide useful feedback information for assessing software testing progress and have potentials in the emerging area of software cybernetics that explores the interplay between software and control.,2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1245391,no,undetermined,0
Transforming quantities into qualities in assessment of software systems,"The assessment of software systems often requires to consider together qualitative and quantitative aspects. Because of the different nature, measures belong to different domains. The main problem is to aggregate such information into a derived measure able to provide an overall estimation. This problem has been traditionally solved trough the transformation of qualitative assessments into quantitative measures. Indeed, such a transformation implicitly assumes a conceptual equivalence between the terms quantitative and objective on one side, and qualitative and subjective on the other side. An alternative approach is to consider logical aggregation models, able to infer the overall evaluation based on the assessment of individual attributes. This approach requires an early transformation of quantitative measures in qualitative assessments. Such a transformation is possible trough the use of judgment functions. The aim of this paper is to introduce the judgment functions and to study their properties.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1245359,no,undetermined,0
Tolerance of control-flow testing criteria,"Effectiveness of testing criteria is the ability to detect failure in a software program. We consider not only effectiveness of some testing criterion in itself but a variance of effectiveness of different test sets satisfied the same testing criterion. We name this property ""tolerance"" of a testing criterion and show that, for practical using a criterion, a high tolerance is as well important as high effectiveness. The results of empirical evaluation of tolerance for different criteria, types of faults and decisions are presented. As well as quite simple and well-known control-flow criteria, we study more complicated criteria: full predicate coverage, modified condition/decision coverage and reinforced condition/decision coverage criteria.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1245339,no,undetermined,0
A Petri net based method for storage units estimation,"This work presents a structural methodology for computing the number of storage units (registers) in hardware/software co-design context considering timing constraints. The proposed method is based on one intermediate model-data flow net, specified by our team, that takes into account timing precedence and data-dependency. The considered hardware/software co-design framework uses Petri nets as common formalism for performing quantitative and qualitative analysis.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1244547,no,undetermined,0
A model to evaluate the economic benefits of software components development,"ABB is a multi-national corporation that is developing a new generation of products based on the concept of Industrial<sup>IT</sup>. This concept provides a common integration platform for product interoperability. As Industrial<sup>IT</sup> enabled products are developed across ABB, software reuse must be considered. Component based software development (CBSD) is an effective means to improve productivity and quality by developing reusable components. Measuring the economic benefits and performing sensitivity analyses of CBSD scenarios in the development of Industrial<sup>IT</sup> products is important to improve efficiency. This paper presents a model that allows project leaders to evaluate a variety of software development scenarios. The model is based on a goal-question-metrics (GQM) approach and was developed at the ABB corporate research laboratories.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1244479,no,undetermined,0
Online control design for QoS management,"In this paper we present an approach for QoS management that can be applied for a general class of real-time distributed computation systems. In this paper, the QoS adaptation problem is formulated based on a utility function that measures the relative performance of the system. A limited-horizon online supervisory controller is used for this purpose. The online controller explores a limited region of the state-space of the system at each time step and decides the best action accordingly. The feasibility and accuracy of the online algorithm can be assessed at design time.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1244265,no,undetermined,0
Effect of tilt angle variations in a halo implant on V<sub>th</sub> values for 0.14-Î¼m CMOS devices,"Sensitivity of critical transistor parameters to halo implant tilt angle for 0.14-Î¼m CMOS devices was investigated. V<sub>th</sub> sensitivity was found to be 3% per tilt degree. A tilt angle mismatch between two serial ion implanters used in manufacturing was detected by tracking V<sub>th</sub> performance for 0.14-Î¼m production lots. Even though individual implanters may be within tool specifications for tilt angle control (Â±0.5Â° for our specific tool type), the relative mismatch could be as large as 1Â°, and therefore, result in a V<sub>th</sub> mismatch of over 3% from nominal. The V<sub>th</sub> mismatch results are in qualitative agreement with simulation results using SUPREM and MEDICI software.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1243978,no,undetermined,0
Software architecture for diagnosing physical systems,"This paper discusses about a general software architecture suitable for the design of diagnostic systems based on logical reasoning. This type of architecture should be able to support different kinds of consistency tests and different kinds of triggering strategies. Different steps, such as data acquisition from a physical system, batch or online consistency tests, diagnostic reasoning and management of consistency tests have been isolated. Software dedicated to the diagnosis of a system of two cascading water tanks is also presented as an illustration.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1243406,no,undetermined,0
An adaptive bandwidth reservation scheme in multimedia wireless networks,"Next generation wireless networks target to provide quality of service (QoS) for multimedia applications. In this paper, the system supports two QoS criteria, i.e., the system should keep the handoff dropping probability always less than a predefined QoS bound, while maintaining the relative priorities of different traffic classes in terms of blocking probability. To achieve this goal, a dynamic multiple-threshold bandwidth reservation scheme is proposed, which is capable of granting differential priorities to different traffic class and to new ad handoff traffic for each class by dynamically adjusting bandwidth reservation thresholds. Moreover, in times of network congestion, a preventive measure by use of throttling new connection acceptance is taken. Another contribution of this paper is to generalize the concept of relative priority, hence giving the network operator more flexibility to adjust admission control policy by incorporating some dynamic factors such as offered load. The elaborate simulation is conducted to verify the performance of the scheme.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1258751,no,undetermined,0
"Software reviews, the state of the practice","A 2002 survey found that many companies use software reviews unsystematically, creating a mismatch between expected outcomes and review implementations. This suggests that many software practitioners understand basic review concepts but often fall to exploit their full potential.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1241366,no,undetermined,0
Static test compaction for multiple full-scan circuits,"Current design methodologies and methodologies for reducing test data volume and test application time for full-scan circuits allow testing of multiple circuits (or subcircuits of the same circuit) simultaneously using the same test data. We describe a static compaction procedure that accepts test sets generated independently for multiple full-scan circuits, and produces a compact test set that detects all the faults detected by the individual test sets. The resulting test set can be used for testing the circuits simultaneously using the same test data. This procedure provides an alternative to test generation procedures that perform test generation for complex circuits made up of multiple circuits. Such procedures also reduce the amount of test data and test application time required for testing all the circuits by testing them simultaneously using the same test data. However, they require consideration of a more complex circuit.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1240926,no,undetermined,0
Perf: an ongoing research project on performance evaluation,"The fast technological evolution and the convergence of fixed and mobile communication infrastructures have led to the realization of a large variety of innovative applications and services that have become an integral part of our society. Efficiency, reliability, availability and security, i.e., quality of service, are fundamental requirements for these applications. In this framework, performance evaluation and capacity planning play a key role. This paper presents an ongoing research project carried out under the FIRB Programme of the Italian Ministry of Education, Universities and Research. The project addresses basic and foundational research in performance evaluation with the objective of developing new techniques, methodologies and tools for the analysis of the complex systems and applications that drive our society.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1240636,no,undetermined,0
Assessing the dependability of OGSA middleware by fault injection,"This paper presents our research on devising a dependability assessment method for the upcoming OGSA 3.0 middleware using network level fault injection. We compare existing DCE middleware dependability testing research with the requirements of testing OGSA middleware and derive a new method and fault model. From this we have implemented an extendable fault injector framework and undertaken some proof of concept experiments with a simulated OGSA middleware system based around Apache SOAP and Apache Tomcat. We also present results from our initial experiments, which uncovered a discrepancy with our simulated OGSA system. We finally detail future research, including plans to adapt this fault injector framework from the stateless environment of a standard Web service to the stateful environment of an OGSA service.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1238079,no,undetermined,0
An experimental evaluation of correlated network partitions in the Coda distributed file system,"Experimental evaluation is an important way to assess distributed systems, and fault injection is the dominant technique in this area for the evaluation of a system's dependability. For distributed systems, network failure is an important fault model. Physical network failures often have far-reaching effects, giving rise to multiple correlated failures as seen by higher-level protocols. This paper presents an experimental evaluation, using the Loki fault injector, which provides insight into the impact that correlated network partitions have on the Coda distributed file system. In this evaluation, Loki created a network partition between two Coda file servers, during which updates were made at each server to the same replicated data volume. Upon repair of the partition, a client requested directory resolution to converge the diverging replicas. At various stages of the resolution, Loki invoked a second correlated network partition, thus allowing us to evaluate its impact on the system's correctness, performance, and availability.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1238077,no,undetermined,0
Investigating the accuracy of defect estimation models for individuals and teams based on inspection data,"Defect content estimation approaches, based on data from inspection, estimate the total number of defects in a document to evaluate the quality of the product and the development process. Objective estimation approaches require a high-quality measurement process, potentially suffer from overfitting, and may underestimate the number of defects for inspections that yield few data points. Reading techniques for inspection, which focus the attention of the inspectors on particular parts of the inspected document, may influence their subjective estimates. In this paper we consider approaches to aggregate subjective estimates of individual inspectors in a team to alleviate individual bias. We evaluate these approaches with data from an experiment in a university environment where 177 inspectors in 30 teams inspected a software requirements document. Main findings of the experiment were that reading techniques considerably influenced the accuracy of inspector estimates. Further team estimates improved both estimation accuracy and variation compared to individual estimates and one of the best empirically evaluated objective estimation approaches.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1237982,no,undetermined,0
The application of capture-recapture log-linear models to software inspections data,"Re-inspection has been deployed in industry to improve the quality of software inspections. The number of remaining defects after inspection is an important factor affecting whether to re-inspect the document or not. Models based on capture-recapture (CR) sampling techniques have been proposed to estimate the number of defects remaining in the document after inspection. Several publications have studied the robustness of some of these models using software engineering data. Unfortunately, most of the existing studies did not examine the log linear models with respect software inspection data. In order o explore the performance of the log linear models, we evaluated their performance for three person inspection teams. Furthermore, we evaluated the models using an inspection data set that was previously used to asses different CR models. Generally speaking, the study provided very promising results. According to our results, the log linear models proved to be more robust that all CR based models previously assessed for three-person inspections.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1237980,no,undetermined,0
An empirical analysis of fault persistence through software releases,"This work is based on the idea of analyzing the behavior all over the life-cycle of source files having a high number of faults at their first release. In terms of predictability, our study helps to understand if files that are faulty in their first release tend to remain faulty in later releases, and investigates the ways to assure a higher reliability to the faultiest programs, testing them carefully or lowering the complexity of their structure. The purpose of this paper is to verify empirically our hypothesis, through an experimental analysis on two different projects, and to find causes observing the structure of the faulty files. As a conclusion, we can say that the number of faults at the first release of source files is an early and significant index of its expected defect rate and reliability.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1237979,no,undetermined,0
Helmet-mounted display image quality evaluation system,"Helmet-mounted displays (HMDs) provide essential pilotage and fire control imagery information for pilots. To maintain system integrity and readiness, there is a need to develop an image quality evaluation system for HMDs. In earlier work, a framework was proposed for an HMD system called the integrated helmet and display sighting system (IHADSS), used with the U.S. Army's Apache helicopter. This paper describes prototype development and interface design and summarizes bench test findings using three IHADSS helmet display units (HDUs). The prototype consists of hardware (cameras, sensors, image capture/data acquisition cards, battery pack, HDU holder, moveable rack and handle, and computer) and software algorithms for image capture and analysis. Two cameras with different-size apertures are mounted in parallel on a rack facing an HDU holder. A handle allows users to position the HDU in front of the two cameras. The HMD test pattern is then captured. Sensors detect the position of the holder and whether the HDU is angled correctly in relation to the camera. Algorithms detect HDU features captured by the two cameras, including focus, orientation, displacement, field-of-view, and number of grayshades. Bench testing of three field-quality HDUs indicates that the image analysis algorithms are robust and able to detect the desired image features. Suggested future directions include development of a learning algorithm to automatically develop or revise feature specifications as the number of inspection samples increases.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1246558,no,undetermined,0
Co-histogram and its application in remote sensing image compression evaluation,"Peak signal-to-noise ratio (PSNR) has found its application as an evaluation metric for image coding, but in many instances it provides an inaccurate representation of the image quality. The new tool proposed in this paper is called co-histogram, which is a statistic graph generated by counting the corresponding pixel pairs of two images. For image coding evaluation, the two images are the original image and a compressed and recovered image. The graph is a two-dimensional joint probability distribution of the two images. A co-histogram shows how the pixels are distributed among combinations of two image pixel values. By means of co-histogram, we can have a visual interpretation of PSNR, and the symmetry of a co-histogram is also significant for objective evaluation of remote sensing image compression. Our experiments with two SAR images and a TM image using DCT-based JPEG and wavelet-based SPIHT coding methods perform the importance of the co-histogram symmetry.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1247210,no,undetermined,0
The design and performance of real-time Java middleware,"More than 90 percent of all microprocessors are now used for real-time and embedded applications. The behavior of these applications is often constrained by the physical world. It is therefore important to devise higher-level languages and middleware that meet conventional functional requirements, as well as dependably and productively enforce real-time constraints. We provide two contributions to the study of languages and middleware for real-time and embedded applications. We first describe the architecture of jRate, which is an open-source ahead-of-time-compiled implementation of the RTSJ middleware. We then show performance results obtained using RTJPerf, which is an open-source benchmarking suite that systematically compares the performance of RTSJ middleware implementations. We show that, while research remains to be done to make RTSJ a bullet-proof technology, the initial results are promising. The performance and predictability of JRate provides a baseline for what can be achieved by using ahead-of-time compilation. Likewise, RTJPerf enables researchers and practitioners to evaluate the pros and cons of RTSJ middleware systematically as implementations mature.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1247675,no,undetermined,0
Quality of service provision assessment for campus network,"The paper presents a methodology for assessing the quality of service (QoS) provision for a campus network. The author utilizes the Staffordshire University's network communications infrastructure (SUNCI) as a testing platform and discusses a new approach and QoS provision, by adding a component of measurement to the existing model presented by J.L. Walker (see J. Services Marketing, vol.9, no.1, p.5-14, 1995). The QoS provision is assessed in light of users' perception compared with the network traffic measurements and online monitoring reports. The users' perception of the QoS provision of a telecommunications network infrastructure is critical to the successful business management operation of any organization. The computing environment in modern campus networks is complex, employing multiple heterogeneous hardware and software technologies. In support of highly interactive user applications, QoS provision is essential to the users' ever increasing level of expectations. The paper offers a cost effective approach to assessing the QoS provision within a campus network.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1249093,no,undetermined,0
Per-prediction for PHY mode selection in OFDM communication systems,"Modern wireless communication standards like HiperLAN/2 or IEEE802.11a provide a high degree of flexibility on the physical layer (PHY) allowing the data link control (DLC) layer to choose transmission parameters with respect to the currently observed link quality. This possibility of so-called link adaptation (LA) is a key element for meeting quality of service (QoS) requirements and optimizing system performance. The decisions made by the LA algorithm are typically based on a prediction of the packet error rate (PER) implied by a certain transmit parameter setting. With most existing LA schemes, this prediction is based on the observed signal-to-noise ratio (SNR) or carrier-to-interference ratio (CIR), respectively. For frequency selective channels, however, the SNR alone does not adequately describe the channel quality. It is shown in this paper that LA schemes based on an accurate description of the momentary channel status can outperform conventional schemes with respect to throughput and reliability. Based on a novel ""indicator"" concept, schemes for obtaining an accurate prediction of the PER by evaluation of the channel transfer function are presented and evaluated by software simulation.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1258196,no,undetermined,0
Multi-million gate FPGA physical design challenges,"The recent past has seen a tremendous increase in the size of design circuits that can be implemented in a single FPGA. These large design sizes significantly impact cycle time due to design automation software runtimes and an increased number of performance based iterations. New FPGA physical design approaches need to be utilized to alleviate some of these problems. Hierarchical approaches to divide and conquer, the design, early estimation tools for design exploration, and physical optimizations are some of the key methodologies that have to be introduced in the FPGA physical design tools. This paper will investigate the loss/benefit in quality of results due to hierarchical approaches and compare and contrast some of the design automation problem formulations and solutions needed for FPGAs versus known standard cell ASIC approaches.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1257913,no,undetermined,0
A model for battery lifetime analysis for organizing applications on a pocket computer,"A battery-powered portable electronic system shuts down once the battery is discharged; therefore, it is important to take the battery behavior into account. A system designer needs an adequate high-level battery model to make battery-aware decisions targeting the maximization of the system's online lifetime. We propose such a model that allows a designer to analytically predict the battery time-to-failure for a given load. Our model also allows for a tradeoff between the accuracy and the amount of computation performed. The quality of the proposed model is evaluated using typical pocket computer applications and a detailed low-level simulation of a lithium-ion electrochemical cell. In addition, we verify the proposed model against actual measurements taken on a real lithium-ion battery.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1255477,no,undetermined,0
Estimating computation times in data intensive e-services,"A priori estimation of quality of service (QoS) levels is a significant issue in e-services since service level agreements (SLAs) need to specify and adhere to such estimates. Response time is an important metric for data intensive e-services such as data mining, data analysis and querying/information retrieval from large databases where the focus is on the time taken to present results to clients. A key component of response time in such data intensive services is the time taken to perform the computation, namely, the time taken to perform either data mining, analysis or retrieval. In this paper, we present an approach for accurately estimating the computation times of data intensive e-services.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1254471,no,undetermined,0
An extension of the behavioral theory of group performance in software development technical reviews,"In the original theory of group performance in software development technical reviews there was no consideration given to the influence of the inspection process, inspection performance, or the inspected artifact, on the structure of the theoretical model. We present an extended theoretical model, along with discussion and justification for components of the new model. These extensions include consideration of the software product quality attributes, the prescriptive processes now developed for reviews, and a more detailed consideration of the technical and nontechnical characteristics of the inspectors. We present both the structure of the model and the nature of the interactions between elements in the model. Consideration is also given to the opportunity for increased formalism in the technical review process. We show that opportunity exists to both improve industrial practice and extend research-based knowledge of the technical review process and context.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1254407,no,undetermined,0
Exploring the relationship between experience and group performance in software review,"The aim is to examine the important relationships between experience, task training and software review performance. One hundred and ninety-two volunteer university students were randomly assigned into 48 four-member groups. Subjects were required to detect defects from a design document. The main findings include (1) role experience has a positive effect on software review performance; (2) working experience in the software industry has a positive effect on software review performance; (3) task training has no significant effect on software review performance; (4) role experience has no significant effect on task training; (5) working experience in the software industry has a significant effect on task training.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1254405,no,undetermined,0
Reducing optical crosstalk in affordable systems of virtual environment,"We have implemented a scheme for elimination of depolarization artefacts in passive stereo-projection systems. These problems appear due to non-perfectness of stereo-projection equipment: depolarization of the light due to reflection from the screen or due to the passage through the screen; non-ideality of polarizing filters; mixing of left- and right-eye images in a change of relative orientation of the filters. These effects lead to strong violations in stereo-perception (known as ""ghosts""). They can be eliminated by software methods, using linear filtering of the image before its projection to the screen, based on the formula L' = L -Î±R, R' = R -Î±L , where L, R are images, destined for the left and right eye respectively, Î± is constant, defining intensity of the ghosts. This receipt, in theory leading to exact compensation of the ghosts, in practice possesses certain limitations, which allow not complete elimination but only strong suppression of the ghosts, at very careful calibration of the system. We have implemented this algorithm in VE system Avango by means of texture mappings. All necessary operations are performed by the graphics board, thus providing the real-time rendering rate. The described method considerably improves stability of stereo-perception, making high-quality performances of virtual environment possible on affordable equipment.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1253441,no,undetermined,0
RTTometer: measuring path minimum RTT with confidence,"Internet path delay is a substantial metric in determining path quality. Therefore, it is not surprising that round-trip time (RTT) plays a tangible role in several protocols and applications, such as overlay network construction protocol, peer-to-peer services, and proximity-based server redirection. Unfortunately, current RTT measurement tools report delay without any further insight about path condition. Therefore, applications usually estimate minimum RTT by sending a large number of probes to gain confidence in the measured RTT. Nevertheless, a large number of probes does not directly translate to better confidence. Usually, minimum RTT can be measured using few probes. Based on observations of path RTT presented by Z. Wang et al. (see Proc. Passive & Active Measurement Workshop - PAM'03, 2003), we develop a set of techniques, not only to measure minimum path RTT, but also to associate it with a confidence level that reveals the condition on the path during measurement. Our tool, called RTTometer, is able to provide a confidence measure associated with path RTT. Besides, given a required confidence level, RTTometer dynamically adjusts the number of probes based on a path's condition. We describe our techniques implemented in RTTometer and present our preliminary experiences using RTTometer to estimate RTT on various representative paths of the Internet.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1251234,no,undetermined,0
A Bayesian belief network for assessing the likelihood of fault content,"To predict software quality, we must consider various factors because software development consists of various activities, which the software reliability growth model (SRGM) does not consider. In this paper, we propose a model to predict the final quality of a software product by using the Bayesian belief network (BBN) model. By using the BBN, we can construct a prediction model that focuses on the structure of the software development process explicitly representing complex relationships between metrics, and handling uncertain metrics, such as residual faults in the software products. In order to evaluate the constructed model, we perform an empirical experiment based on the metrics data collected from development projects in a certain company. As a result of the empirical evaluation, we confirm that the proposed model can predict the amount of residual faults that the SRGM cannot handle.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1251044,no,undetermined,0
Optimal resource allocation for the quality control process,"Software development project employs some quality control (QC) process to detect and remove defects. The final quality of the delivered software depends on the effort spent on all the QC stages. Given a quality goal, different combinations of efforts for the different QC stages may lead to the same goal. In this paper, we address the problem of allocating resources to the different QC stages, such that the optimal quality is obtained. We propose a model for the cost of QC process and then view the resource allocation among different QC stages as an optimization problem. We solve this optimization problem using non-linear optimization technique of sequential quadratic programming. We also give examples to show how a sub-optimal resource allocation may either increase the resource requirement significantly or lower the quality of the final software.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1251028,no,undetermined,0
A comprehensive and systematic methodology for client-server class integration testing,"This article is a first attempt towards a comprehensive, systematic methodology for class interface testing in the context of client/server relationships. The proposed approach builds on and combines existing techniques. It first consists in selecting a subset of the method sequences defined for the class testing of the client class, based on an analysis of the interactions between the client and the server methods. Coupling information is then used to determine the conditions, i.e., values for parameters and data members, under which the selected client method sequences are to be executed so as to exercise the interaction. The approach is illustrated by means of an abstract example and its cost-effectiveness is evaluated through a case study.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1251027,no,undetermined,0
Detection or isolation of defects? An experimental comparison of unit testing and code inspection,"Code inspections and white-box testing have both been used for unit testing. One is a static analysis technique, the other, a dynamic one, since it is based on executing test cases. Naturally, the question arises whether one is superior to the other, or, whether either technique is better suited to detect or isolate certain types of defects. We investigated this question with an experiment with a focus on detection of the defects (failures) and isolation of the underlying sources of the defects (faults). The results indicate that there exist significant differences for some of the effects of using code inspection versus testing. White-box testing is more effective, i.e. detects significantly more defects while inspection isolates the underlying source of a larger share of the defects detected. Testers spend significantly more time, hence the difference in efficiency is smaller, and is not statistically significant. The two techniques are also shown to detect and identify different defects, hence motivating the use of a combination of methods.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1251026,no,undetermined,0
Briefing a new approach to improve the EMI immunity of DSP systems,"Hereafter, we present an approach dealing to improve the reliability of digital signal processing (DSP) systems operating in real noisy (electromagnetic interference - EMI) environments. The approach is based on the coupling of two techniques: the ""DSP-oriented signal integrity improvement"" technique deals to increase the signal-to-noise ratio (SNR) and is essentially a modification of the classic Recovery Blocks Scheme. The second technique, named ""SW-based fault handling"" aims to detect in real-time data- and control-flow faults throughout modifications of the processor C-code. When compared to conventional approaches using Fast Fourier Transform (FIT) and Hamming Code, the primary benefit of such an approach is to improve system reliability by means of a considerably low complexity, reasonably low performance degradation and, when implemented in hardware, with reduced area overhead. Aiming to illustrate the proposed approach, we present a case study for a speech recognition system, which was partially implemented in a PC microcomputer and in a COTS microcontroller. This system was tested under a home-tailored EMI environment according to the International Standard Normative IEC 61.0004-29. The obtained results indicate that the proposed approach can effectively improve the reliability of DSP systems operating in real noise (EMI) environments.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1250858,no,undetermined,0
Genetic programming-based decision trees for software quality classification,"The knowledge of the likely problematic areas of a software system is very useful for improving its overall quality. Based on such information, a more focused software testing and inspection plan can be devised. Decision trees are attractive for a software quality classification problem which predicts the quality of program modules in terms of risk-based classes. They provide a comprehensible classification model which can be directly interpreted by observing the tree-structure. A simultaneous optimization of the classification accuracy and the size of the decision tree is a difficult problem, and very few studies have addressed the issue. This paper presents an automated and simplified genetic programming (gp) based decision tree modeling technique for the software quality classification problem. Genetic programming is ideally suited for problems that require optimization of multiple criteria. The proposed technique is based on multi-objective optimization using strongly typed GP. In the context of an industrial high-assurance software system, two fitness functions are used for the optimization problem: one for minimizing the average weighted cost of misclassification, and one for controlling the size of the decision tree. The classification performances of the GP-based decision trees are compared with those based on standard GP, i.e., S-expression tree. It is shown that the GP-based decision tree technique yielded better classification models. As compared to other decision tree-based methods, such as C4.5, GP-based decision trees are more flexible and can allow optimization of performance objectives other than accuracy. Moreover, it provides a practical solution for building models in the presence of conflicting objectives, which is commonly observed in software development practice.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1250214,no,undetermined,0
Application of an attribute selection method to CBR-based software quality classification,"This study investigates the attribute selection problem for reducing the number of software metrics (program attributes) used by a case-based reasoning (CBR) software quality classification model. The metrics are selected using the Kolmogorov-Smirnov (K-S) two sample test. The ""modified expected cost of misclassification"" measure, recently proposed by our research team, is used as a performance measure to select, evaluate, and compare classification models. The attribute selection procedure presented in this paper can assist a software development organization in determining the software metrics that are better indicators of software quality. By reducing the number of software metrics to be collected during the development process, the metrics data collection task can be simplified. Moreover, reducing the number of metrics would result in reducing the computation time of a CBR model. Using an empirical case study of a real-world software system, it is shown that with a reduced number of metrics the CBR technique is capable of yielding useful software quality classification models. Moreover, their performances were better than or similar to CBR models calibrated without attribute selection.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1250169,no,undetermined,0
An intelligent early warning system for software quality improvement and project management,"One of the main reasons behind unfruitful software development projects is that it is often too late to correct the problems by the time they are detected. It clearly indicates the need for early warning about the potential risks. In this paper, we discuss an intelligent software early warning system based on fuzzy logic using an integrated set of software metrics. It helps to assess risks associated with being behind schedule, over budget, and poor quality in software development and maintenance from multiple perspectives. It handles incomplete, inaccurate, and imprecise information, and resolve conflicts in an uncertain environment in its software risk assessment using fuzzy linguistic variables, fuzzy sets, and fuzzy inference rules. Process, product, and organizational metrics are collected or computed based on solid software models. The intelligent risk assessment process consists of the following steps: fuzzification of software metrics, rule firing, derivation and aggregation of resulted risk fuzzy sets, and defuzzification of linguistic risk variables.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1250167,no,undetermined,0
Soft-error detection using control flow assertions,"Over the last few years, an increasing number of safety-critical tasks have been demanded of computer systems. In this paper, a software-based approach for developing safety-critical applications is analyzed. The technique is based on the introduction of additional executable assertions to check the correct execution of the program control flow. By applying the proposed technique, several benchmark applications have been hardened against transient errors. Fault injection campaigns have been performed to evaluate the fault detection capability of the proposed technique in comparison with state-of-the-art alternative assertion-based methods. Experimental results show that the proposed approach is far more effective than the other considered techniques in terms of fault detection capability, at the cost of a limited increase in memory requirements and in performance overhead.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1250158,no,undetermined,0
A generic method for statistical testing,"This paper addresses the problem of selecting finite test sets and automating this selection. Among these methods, some are deterministic and some are statistical. The kind of statistical testing we consider has been inspired by the work of Thevenod-Fosse and Waeselynck. There, the choice of the distribution on the input domain is guided by the structure of the program or the form of its specification. In the present paper, we describe a new generic method for performing statistical testing according to any given graphical description of the behavior of the system under test. This method can be fully automated. Its main originality is that it exploits recent results and tools in combinatorics, precisely in the area of random generation of combinatorial structures. Uniform random generation routines are used for drawing paths from the set of execution paths or traces of the system under test. Then a constraint resolution step is performed, aiming to design a set of test data that activate the generated paths. This approach applies to a number of classical coverage criteria. Moreover, we show how linear programming techniques may help to improve the quality of test, i.e. the probabilities for the elements to be covered by the test process. The paper presents the method in its generality. Then, in the last section, experimental results on applying it to structural statistical software testing are reported.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1383103,no,undetermined,0
Empirical studies of test case prioritization in a JUnit testing environment,"Test case prioritization provides a way to run test cases with the highest priority earliest. Numerous empirical studies have shown that prioritization can improve a test suite's rate of fault detection, but the extent to which these results generalize is an open question because the studies have all focused on a single procedural language, C, and a few specific types of test suites, in particular, Java and the JUnit testing framework are being used extensively in practice, and the effectiveness of prioritization techniques on Java systems tested under JUnit has not been investigated. We have therefore designed and performed a controlled experiment examining whether test case prioritization can be effective on Java programs tested under JUnit, and comparing the results to those achieved in earlier studies. Our analyses show that test case prioritization can significantly improve the rate of fault detection of JUnit test suites, but also reveal differences with respect to previous studies that can be related to the language and testing paradigm.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1383111,no,undetermined,0
Empirical evaluation of capacity estimation tools,"Bandwidth estimation is an important task because the knowledge of the bandwidth of a path is useful in a wide variety of contexts. Clients, applications and servers benefit greatly from knowing the bandwidth of a route. In this paper we describe and discuss experiments carried out to provide bandwidth estimates using five tools: bprobe, clink, nettimer pathrate, and pchar The tools are evaluated and compared according to the criteria accuracy, statistical robustness and time to estimation. We show several results for short and long network paths. The topologies tested include links which capacities range from 2 Mb/s to 1 Gb/s.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1190230,no,undetermined,0
Empirical assessment of machine learning based software defect prediction techniques,"The wide-variety of real-time software systems, including telecontrol/telepresence systems, robotic systems, and mission planning systems, can entail dynamic code synthesis based on runtime mission-specific requirements and operating conditions. This necessitates the need for dynamic dependability assessment to ensure that these systems perform as specified and not fail in catastrophic ways. One approach in achieving this is to dynamically assess the modules in the synthesized code using software defect prediction techniques. Statistical models; such as stepwise multi-linear regression models and multivariate models, and machine learning approaches, such as artificial neural networks, instance-based reasoning, Bayesian-belief networks, decision trees, and rule inductions, have been investigated for predicting software quality. However, there is still no consensus about the best predictor model for software defects. In this paper; we evaluate different predictor models on four different real-time software defect data sets. The results show that a combination of IR and instance-based learning along with the consistency-based subset evaluation technique provides a relatively better consistency in accuracy prediction compared to other models. The results also show that ""size"" and ""complexity"" metrics are not sufficient for accurately predicting real-time software defects.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544801,no,undetermined,0
Development of an on-line data quality monitor for the relativistic heavy-ion experiment ALICE,"The on-line data monitoring tool developed for the coming ALICE experiment at LHC, CERN is presented. This monitoring tool which is a part of the ALICE-DAQ software framework, written entirely in C++ language, uses standard Linux tools in conjunction with the data display and analysis package ROOT, developed at CERN. It allows checking the consistency and quality of the data and correct functioning of the various sub-detectors either at run time or during off line by playing back the recorded raw data. After discussing the functionality and performance of this package, the experience gained during the test beam periods is also summarized",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1547409,no,undetermined,0
Experiences of PD Diagnosis on MV Cables using Oscillating Voltages (OWTS),"Detecting, locating and evaluating of partial discharges (PD) in the insulating material, terminations and joints provides the opportunity for a quality control after installation and preventive detection of arising service interruption. A sophisticated evaluation is necessary between PD in several insulating materials and also in different types of terminations and joints. For a most precise evaluation of the degree and risk caused by PD it is suggested to use a test voltage shape that is preferably like the same under service conditions. Only under these requirements the typical PD parameters like inception and extinction voltage, PD level and PD pattern correspond to significant operational values. On the other hand the stress on the insulation should be limited during the diagnosis to not create irreversible damages and thereby worsening the condition of the test object. The paper introduces an oscillating wave test system (OWTS), which meets these mentioned demands well. The design of the system, its functionality and especially the operating software are made for convenient field application. Field data and experience reports will be presented and discussed. This field data serve also as good guide for the level of danger to the different insulating systems due to partial discharges",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1547150,no,undetermined,0
Research on the Intelligence High-Voltage Circuit Breaker Based on DSP,"In this paper an intelligence recloser with faults breaking and synchronizing make function based on DSP is presented. During breaking the short circuit current, the current change rate is adopted to judge whether the short circuit taking. In this paper the four reclosing possibilities are analyzed and simulated in order to find the voltage optimum inputting angle, at the same time short circuit faults are analyzed and simulated in order to diagnose and distinguish fault's kinds. Furthermore the digital filtering based on the improved Fourier transformations (IFFT) is used to estimate the optimal reclosing angle so as to weaken the instantaneous current during reclosing circuit. Based on DSP technology, the hardware circuit has been set up and the software has been programmed. And the experiments show that it is effectively to reduce the making current and obviously to improve circuit breaker's performance",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1547126,no,undetermined,0
Technology of detecting GIC in power grids &amp; its monitoring device,"The magnetic storm results in the transmission lines with geomagnetically induced current (GIC). And GIC happening in random has the frequency between 0.001 Hz~0.1 Hz, and continues from several minutes to several hours. Based on elaborating mechanism and characteristic of GIC in grids and the influence on China power grids, the article has conducted the research work of GIC monitoring technology, and has investigated the method of sampling data of GIC, the survey algorithm and the new monitoring device. The simulated test shows, the monitoring device can effectively measure GIC which is signal of quasi direct current and randomness, has advantages of having few data to be handled and needing little memory space, etc",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1546843,no,undetermined,0
Development of a portable digital radiographic system based on FOP-coupled CMOS image sensor and its performance evaluation,"As a continuation of our digital X-ray imaging sensor R&D, we have developed a cost-effective, portable, digital radiographic system based on a CMOS image sensor coupled with a fiber optic plate (FOP) and selected conventional scintillators. The imaging system consists of a commercially available CMOS image sensor of 48 &mu;m Ã— 48 Î¼m pixel size and 49.2 mm Ã— 49.3 mm active area, a FOP bundled with several millions of glass fibers of about 6 Î¼m in diameter and 3 mm in thickness, phosphor screens such as Min-R or Lanex series, a readout IC board, a GUI software, and a battery-operated X-ray generator (20-60 kV<sub>p</sub>; up to 1 mA). Here the FOP was incorporated into the imaging system to reduce the performance degradation of the CMOS sensor module caused by irradiation and also to improve image quality. In this paper, we described each imaging component of the fully-integrated portable digital radiographic system in detail, and also presented its performance analysis with experimental measurements and acquired X-ray images in terms of system response with exposure, contrast-to-noise ratio (CNR), modulation transfer function (MTF), noise power spectrum (NPS), and detective quantum efficiency (DQE).",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1546499,no,undetermined,0
Condition monitoring and fault diagnosis of electrical motors-a review,"Recently, research has picked up a fervent pace in the area of fault diagnosis of electrical machines. The manufacturers and users of these drives are now keen to include diagnostic features in the software to improve salability and reliability. Apart from locating specific harmonic components in the line current (popularly known as motor current signature analysis), other signals, such as speed, torque, noise, vibration etc., are also explored for their frequency contents. Sometimes, altogether different techniques, such as thermal measurements, chemical analysis, etc., are also employed to find out the nature and the degree of the fault. In addition, human involvement in the actual fault detection decision making is slowly being replaced by automated tools, such as expert systems, neural networks, fuzzy-logic-based systems; to name a few. It is indeed evident that this area is vast in scope. Hence, keeping in mind the need for future research, a review paper describing different types of faults and the signatures they generate and their diagnostics' schemes will not be entirely out of place. In particular, such a review helps to avoid repetition of past work and gives a bird's eye view to a new researcher in this area.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1546063,no,undetermined,0
Hierarchical behavior organization,"In most behavior-based approaches, implementing a broad set of different behavioral skills and coordinating them to achieve coherent complex behavior is an error-prone and very tedious task. Concepts for organizing reactive behavior in a hierarchical manner are rarely found in behavior-based approaches, and there is no widely accepted approach for creating such behavior hierarchies. Most applications of behavior-based concepts use only few behaviors and do not seem to scale well. Reuse of behaviors for different application scenarios or even on different robots is very rare, and the integration of behavior-based approaches with planning is unsolved. This paper discusses the design, implementation, and performance of a behavior framework that addresses some of these issues within the context of behavior-based and hybrid robot control architectures. The approach presents a step towards more systematic software engineering of behavior-based robot systems.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1545581,no,undetermined,0
Efficiently registering video into panoramic mosaics,"We present an automatic and efficient method to register and stitch thousands of video frames into a large panoramic mosaic. Our method preserves the robustness and accuracy of image stitchers that match all pairs of images while utilizing the ordering information provided by video. We reduce the cost of searching for matches between video frames by adaptively identifying key frames based on the amount of image-to-image overlap. Key frames are matched to all other key frames, but intermediate video frames are only matched to temporally neighboring key frames and intermediate frames. Image orientations can be estimated from this sparse set of matches in time quadratic to cubic in the number of key frames but only linear in the number of intermediate frames. Additionally, the matches between pairs of images are compressed by replacing measurements within small windows in the image with a single representative measurement. We show that this approach substantially reduces the time required to estimate the image orientations with minimal loss of accuracy. Finally, we demonstrate both the efficiency and quality of our results by registering several long video sequences",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544870,no,undetermined,0
A novel method for early software quality prediction based on support vector machine,"The software development process imposes major impacts on the quality of software at every development stage; therefore, a common goal of each software development phase concerns how to improve software quality. Software quality prediction thus aims to evaluate software quality level periodically and to indicate software quality problems early. In this paper, we propose a novel technique to predict software quality by adopting support vector machine (SVM) in the classification of software modules based on complexity metrics. Because only limited information of software complexity metrics is available in early software life cycle, ordinary software quality models cannot make good predictions generally. It is well known that SVM generalizes well even in high dimensional spaces under small training sample conditions. We consequently propose a SVM-based software classification model, whose characteristic is appropriate for early software quality predictions when only a small number of sample data are available. Experimental results with a medical imaging system software metrics data show that our SVM prediction model achieves better software quality prediction than some commonly used software quality prediction models",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544736,no,undetermined,0
The thin gap chambers database experience in test beam and preparations for ATLAS,"Thin gap chambers (TGCs) are used for the muon trigger system in the forward region of the LHC experiment ATLAS. The TGCs are expected to provide a trigger signal within 25 ns of the bunch spacing. About 3,600 ATLAS TGCs have been produced in Israel, Japan and China. The chambers go through a vigorous quality control program before installation. An extensive system test of the ATLAS muon spectrometer has been performed in the H8 beam line at the CERN SPS during the last few years. Three TGC stations were employed there for triggering muons in the endcap. A relational database was used for storing the conditions of the tests as well as the configuration of the system. This database has provided the detector control system with the information needed for its operation and configuration. The database is used to assist the online operation and maintenance. The same database is storing the non event condition and configuration parameters needed later for the offline and reconstruction software. A larger scale of the database has been produced to support the whole TGC system. It integrates all the production, tests and assembly information. A 1/12th model of the whole TGC system is currently in use for testing the performance of this database in configuring and condition tracking of the system. A mockup of the database was first implemented during the H8 test beams. This paper describes the database structure, its interface to other systems and its operational performance",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1547415,no,undetermined,0
Assessing the crash-failure assumption of group communication protocols,"Designing and correctly implementing group communication systems (GCSs) is notoriously difficult. Assuming that processes fail only by crashing provides a powerful means to simplify the theoretical development of these systems. When making this assumption, however, one should not forget that clean crash failures provide only a coarse approximation of the effects that errors can have in distributed systems. Ignoring such a discrepancy can lead to complex GCS-based applications that pay a large price in terms of performance overhead yet fail to deliver the promised level of dependability. This paper provides a thorough study of error effects in real systems by demonstrating an error-injection-driven design methodology, where error injection is integrated in the core steps of the design process of a robust fault-tolerant system. The methodology is demonstrated for the Fortika toolkit, a Java-based GCS. Error injection enables us to uncover subtle reliability bottlenecks both in the design of Fortika and in the implementation of Java. Based on the obtained insights, we enhance Fortika's design to reduce the identified bottlenecks. Finally, a comparison of the results obtained for Fortika with the results obtained for the OCAML-based Ensemble system in a previous work, allows us to investigate the reliability implications that the choice of the development platform (Java versus OCAML) can have",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544726,no,undetermined,0
Providing test quality feedback using static source code and automatic test suite metrics,"A classic question in software development is ""How much testing is enough?"" Aside from dynamic coverage-based metrics, there are few measures that can be used to provide guidance on the quality of an automatic test suite as development proceeds. This paper utilizes the software testing and reliability early warning (STREW) static metric suite to provide a developer with indications of changes and additions to their automated unit test suite and code for added confidence that product quality will be high. Retrospective case studies to assess the utility of using the STREW metrics as a feedback mechanism were performed in academic, open source and industrial environments. The results indicate at statistically significant levels the ability of the STREW metrics to provide feedback on important attributes of an automatic test suite and corresponding code",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544724,no,undetermined,0
Safety analysis of software product lines using state-based modeling,"The analysis and management of variations (such as optional features) are central to the development of safety-critical, software product lines. However, the difficulty of managing variations, and the potential interactions among them, across an entire product line currently hinders safety analysis in such systems. The work described here contributes to a solution by integrating safety analysis of a product line with model-based development. This approach provides a structured way to construct a state-based model of a product line having significant, safety-related variations. The process described here uses and extends previous work on product-line software fault tree analysis to explore hazard-prone variation points. The process then uses scenario-guided executions to exercise the state model over the variations as a means of validating the product-line safety properties. Using an available tool, relationships between behavioral variations and potentially hazardous states are systematically explored and mitigation steps are identified. The paper uses a product line of embedded medical devices to demonstrate and evaluate the process and results",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544718,no,undetermined,0
A software-based concurrent error detection technique for power PC processor-based embedded systems,"This paper presents a behavior-based error detection technique called control flow checking using branch trace exceptions for powerPC processors family (CFCBTE). This technique is based on the branch trace exception feature available in the powerPC processors family for debugging purposes. This technique traces the target addresses of program branches at run-time and compares them with reference target addresses to detect possible violations caused by transient faults. The reference target addresses are derived by a preprocessor from the source program. The proposed technique is experimentally evaluated on a 32-bit powerPC microcontroller using software implemented fault injection (SWIFI). The results show that this technique detects about 91% of the injected control flow errors. The memory overhead is 39.16% on average, and the performance overhead varies between 110% and 304% depending on the workload used. This technique does not modify the program source code.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544525,no,undetermined,0
Modeling and analysis of high-availability routing software,"With the explosion in the use of Internet, there is an obvious trend that critical applications based on IP network (E-transaction, etc.) are increasing dramatically. Therefore, it is required to build high performance routers (HPRs) with high availability (HA). Though researches on HA router hardware are fairly mature, investigations on HA routing software are still on-going. This paper systematically presents possible implementation approaches on HA routing software and classifies them into three catalogs: protocol extension, state synchronization and packet duplication. This paper innovatively models and analyzes these HA routing software schemes in continuous-time Markov chains (CTMC). Based on the modeling, this paper also introduces numerical results and evaluates these different HA approaches.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544220,no,undetermined,0
Energy-efficient self-adapting online linear forecasting for wireless sensor network applications,"New energy-efficient linear forecasting methods are proposed for various sensor network applications, including in-network data aggregation and mining. The proposed methods are designed to minimize the number of trend changes for a given application-specified forecast quality metric. They also self-adjust the model parameters, the slope and the intercept, based on the forecast errors observed via measurements. As a result, they incur O(1) space and time overheads, a critical advantage for resource-limited wireless sensors. An extensive simulation study based on real-world and synthetic time-series data shows that the proposed methods reduce the number of trend changes by 20%~50% over the existing well-known methods for a given forecast quality metric. That is, they are more predictive than the others with the same forecast quality metric",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1542822,no,undetermined,0
A predictive QoS control strategy for wireless sensor networks,"The number of active sensors in a wireless sensor network has been proposed as a measure, albeit limited, for quality of service (QoS) for it dictates the spatial resolution of the sensed parameters. In very large sensor network applications, the number of sensor nodes deployed may exceed the number required to provide the desired resolution. Herein we propose a method, dubbed predictive QoS control (PQC), to manage the number of active sensors in such an over-deployed network. The strategy is shown to obtain near lifetime and variance performance in comparison to a Bernoulli benchmark, with the added benefit of not requiring the network to know the total number of sensors available. This benefit is especially relevant in networks where sensors are prone to failure due to not only energy exhaustion but also environmental factors and/or those networks where nodes are replenished over time. The method also has advantages in that only transmitting sensors need to listen for QoS control information and thus enabling inactive sensors to operate at extremely low power levels",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1542810,no,undetermined,0
Determining how much software assurance is enough? A value-based approach,"A classical problem facing many software projects is how to determine when to stop testing and release the product for use. On the one hand, we have found that risk analysis helps to address such ""how much is enough?"" questions, by balancing the risk exposure of doing too little with the risk exposure of doing too much. In some cases, it is difficult to quantify the relative probabilities and sizes of loss in order to provide practical approaches for determining a risk-balanced ""sweet spot"" operating point. However, we have found some particular project situations in which tradeoff analysis helps to address such questions. In this paper, we provide a quantitative approach based on the COCOMO II cost estimation model and the COQUALMO qualify estimation model. We also provide examples of its use under the differing value profiles characterizing early startups, routine business operations, and high-finance operations in marketplace competition situation. We also show how the model and approach can assess the relative payoff of value-based testing compared to value-neutral testing based on some empirical results. Furthermore, we propose a way to perform cost/schedule/reliability tradeoff analysis using COCOMO II to determine the appropriate software assurance level in order to finish the project on time or within budget.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541826,no,undetermined,0
The GNAM monitoring system and the OHP histogram presenter for ATLAS,"ATLAS is one of the four experiments under construction along the Large Hadron Collider at CERN. During the 2004 combined test beam, the GNAM monitoring system and the OHP histogram presenter were widely used to assess both the hardware setup and the data quality. GNAM is a modular framework where detector specific code can be easily plugged in to obtain online low-level monitoring applications. It is based on the monitoring tools provided by the ATLAS trigger and data acquisition (TDAQ) software, OHP is a histogram presenter, capable to perform both as a configurable display and as a browser. From OHP, requests to execute simple interactive operations (such as reset, rebin or update) on histograms, can be sent to GNAM",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1547412,no,undetermined,0
Benchmarks and implementation of the ALICE high level trigger,"The ALICE high level trigger combines and processes the full information from all major detectors in a large computer cluster. Data rate reduction is achieved by reducing the event rate by selecting interesting events (software trigger) and by reducing the event size by selecting sub-events and by advanced data compression. Reconstruction chains for the barrel detectors and the forward muon spectrometer have been benchmarked. The HLT receives a replica of the raw data via the standard ALICE DDL link into a custom PCI receiver card (HLT-RORC). These boards also provide a FPGA co-processor for data-intensive tasks of pattern recognition. Some of the pattern recognition algorithms (cluster finder, Hough transformation) have been re-designed in VHDL to be executed in the Virtex-4 FPGA on the HLT-RORC. HLT prototypes were operated during the beam tests of the TPC and TRD detectors. The input and output interfaces to DAQ and the data flow inside of HLT were successfully tested. A full-scale prototype of the dimuon-HLT achieved the expected data flow performance. This system was finally embedded in a GRID-like system of several distributed clusters demonstrating the scalability and fault-tolerance of the HLT",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1547465,no,undetermined,0
Coverage metrics for Continuous Function Charts,"Continuous Function Charts are a diagrammatical language for the specification of mixed discrete-continuous embedded systems, similar to the languages of Matlab/Simulink, and often used in the domain of transportation systems. Both control and data flows are explicitly specified when atomic units of computation are composed. The obvious way to assess the quality of integration test suites is to compute known coverage metrics for the generated code. This production code does not exhibit those structures that would make it amenable to ""relevant"" coverage measurements. We define a translation scheme that results in structures relevant for such measurements, apply coverage criteria for both control and dataflows at the level of composition of atomic computational units, and argue for their usefulness on the grounds of detected errors.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1383123,no,undetermined,0
Main effects screening: a distributed continuous quality assurance process for monitoring performance degradation in evolving software systems,"Developers of highly configurable performance-intensive software systems often use a type of in-house performance-oriented ""regression testing"" to ensure that their modifications have not adversely affected their software's performance across its large configuration space. Unfortunately, time and resource constraints often limit developers to in-house testing of a small number of configurations and unreliable extrapolation from these results to the entire configuration space, which allows many performance bottlenecks and sources of QoS degradation to escape detection until systems are fielded. To improve performance assessment of evolving systems across large configuration spaces, we have developed a distributed continuous quality assurance (DCQA) process called main effects screening that uses in-the-field resources to execute formally designed experiments to help reduce the configuration space, thereby allowing developers to perform more targeted in-house QA. We have evaluated this process via several feasibility studies on several large, widely-used performance-intensive software systems. Our results indicate that main effects screening can detect key sources of performance degradation in large-scale systems with significantly less effort than conventional techniques.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1553572,no,undetermined,0
Capturing processor architectures from protocol processing applications: a case study,"We present a case study in finding optimized processor architectures for a given protocol processing application. The process involves application analysis, hardware/software partitioning and optimization, and evaluation of design quality through simulations, estimations and synthesis. The case study was targeted at processing key IPv6 routing functions at 200 MHz using 0.18 Î¼m CMOS technology. A comparison to an implementation on a commercial processor revealed that the captured architectures provided similar or better performance. Especially checksum calculation was efficient in the captured architectures.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1559810,no,undetermined,0
Validation and verification of prognostic and health management technologies,"Impact Technologies and the Georgia Institute of Technology are developing a Web-based software application that will provide JSF (F-35) system suppliers with a comprehensive set of PHM verification and validation (V&V) resources which will include: standards and definitions, V&V metrics for detection, diagnosis, and prognosis, access to costly seeded fault data sets and example implementations, a collaborative user forum for the exchange of information, and an automated tool for impartially evaluating the performance and effectiveness of PHM technologies. This paper presents the development of the prototype software product to illustrate the feasibility of the techniques, methodologies, and approaches needed to verify and validate PHM capabilities. A team of JSF system suppliers has been assembled to contribute, provide feedback and make recommendations to the product under development. The approach being pursued for assessing the overall PHM system accuracy is to quantify the associated uncertainties at each of the individual levels of a PHM system, and build up the accumulated inaccuracies as information is processed through the PHM architecture",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1559699,no,undetermined,0
Complexity signatures for system health monitoring,"The ability to assess risk in complex systems is one of the fundamental challenges facing the aerospace industry in general, and NASA in particular. First, such an ability allows for quantifiable trade-offs during the design stage of a mission. Second, it allows the monitoring of die health of the system while in operation. Because many of the difficulties in complex systems arise from the interactions among the subsystems, system health monitoring cannot solely focus on the health of those subsystems. Instead system level signatures that encapsulate the complex system interactions are needed. In this work, we present the entropy-scale (ES) and entropy-resolution (ER) system-level signatures that are both computationally tractable and encapsulate many of the salient characteristics of a system. These signatures are based on the change of entropy as a system is observed across different resolutions and scales. We demonstrate the use of the ES and ER signatures on artificial data streams and simple dynamical systems and show that they allow the unambiguous clustering of many types of systems, and therefore are good indicators of system health. We then show how these signatures can be applied to graphical data as well as data strings by using a simple ""graph-walking"" method. This method extracts a data stream from a graphical system representation (e.g., fault tree, software call graph) that conserves the properties of the graph. Finally we apply these signatures to analysis of software packages, and show that they provide significantly better correlation with risk markers than many standard metrics. These results indicate that proper system level signatures, coupled with detailed component-level analysis enable the automatic detection of potentially hazardous subsystem interactions in complex systems before they lead to system deterioration or failures",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1559687,no,undetermined,0
Comparison of class test integration ordering strategies,"This paper discusses the approaches used for integration testing of object oriented applications that have been modeled in unified modeling language. It explains integration testing and the test design patterns used far integration testing. Integration problems arise due to lack of interoperability among different components. Strategies for class test order for integration testing have been discussed in this paper, as integration test order specifies the estimated cost for developing stubs' and drivers. Semantics for UML diagrams enable the dependency analysis of components for integration testing. Techniques far replacing classes by stubs for breaking cycles among them have been discussed to enable stepwise integration testing.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1558922,no,undetermined,0
A rejuvenation methodology of cluster recovery,"While traditional security mechanisms rely on preventive controls and those are very limited in surviving malicious attacks, we propose a novel approach of the security issue to cluster recovery. In this paper, we present the cluster recovery model with a software rejuvenation methodology, which is applicable in security field. We propose two formal approaches, stochastic and Markov decision process. And we estimate the possibility of surviving under unknown attacks. The basic idea of rejuvenation is to investigate the consequences for the exact respond time in face of attacks and rejuvenating the running software/service, refresh its internal state, and resume or restart it. These actions deter the intruder's progress, prevent from more serious damages and provide time to perform more detailed analysis.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1558539,no,undetermined,0
Strategy for mutation testing using genetic algorithms,"In this paper, we propose a model to reveal faults and kill mutant using genetic algorithms. The model first instruments the source and mutant program and divides in small units. Instead of checking the entire program, it tries to find fault in each unit or kills each mutant unit. If any unit survives, the new test data is generated using genetic algorithm with special fitness function. The output of each test for each unit is recorded to detect the faulty unit. In this strategy, the source program and the mutant are instrumented in such a way that the input and output behavior of each unit can be traced. A checker module is used to compare and trace the output of each unit. A complete architecture of the model is proposed in the paper",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1557156,no,undetermined,0
Predictor models in software engineering (PROMISE),"<div style=""font-variant: small-caps; font-size: .9em;"">First Page of the Article</div><img class=""img-abs-container"" style=""width: 95%; border: 1px solid #808080;"" src=""/xploreAssets/images/absImages/01553662.png"" border=""0"">",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1553662,no,undetermined,0
Is mutation an appropriate tool for testing experiments? [software testing],"The empirical assessment of test techniques plays an important role in software testing research. One common practice is to instrument faults, either manually or by using mutation operators. The latter allows the systematic, repeatable seeding of large numbers of faults; however, we do not know whether empirical results obtained this way lead to valid, representative conclusions. This paper investigates this important question based on a number of programs with comprehensive pools of test cases and known faults. It is concluded that, based on the data available thus far, the use of mutation operators is yielding trustworthy results (generated mutants are similar to real faults). Mutants appear however to be different from hand-seeded faults that seem to be harder to detect than real faults.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1553583,no,undetermined,0
Predictors of customer perceived software quality,"Predicting software quality as perceived by a customer may allow an organization to adjust deployment to meet the quality expectations of its customers, to allocate the appropriate amount of maintenance resources, and to direct quality improvement efforts to maximize the return on investment. However, customer perceived quality may be affected not simply by the software content and the development process, but also by a number of other factors including deployment issues, amount of usage, software platform, and hardware configurations. We predict customer perceived quality as measured by various service interactions, including software defect reports, requests for assistance, and field technician dispatches using the afore mentioned and other factors for a large telecommunications software system. We employ the non-intrusive data gathering technique of using existing data captured in automated project monitoring and tracking systems as well as customer support and tracking systems. We find that the effects of deployment schedule, hardware configurations, and software platform can increase the probability of observing a software failure by more than 20 times. Furthermore, we find that the factors affect all quality measures in a similar fashion. Our approach can be applied at other organizations, and we suggest methods to independently validate and replicate our results.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1553565,no,undetermined,0
Performance of the ATLAS SCT readout system,"The ATLAS semiconductor tracker (SCT) together with the pixel and the transition radiation detectors form the tracking system of the ATLAS experiment at LHC. It consists of 20,000 single-sided silicon microstrip sensors assembled back-to-back into modules mounted on four concentric barrels and two end-cap detectors formed by nine disks each. The SCT module production and testing has finished while the macro-assembly is well under way. After an overview of the layout and the operating environment of the SCT, a description of the readout electronics design and operation requirements is given. The quality control procedure and the DAQ software for assuring the electrical functionality of hybrids and modules are discussed. The focus is on the electrical performance results obtained during the assembly and testing of the end-cap SCT modules",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1547482,no,undetermined,0
How software can help or hinder human decision making (and vice-versa),"Summary form only given. Developments in computing offer experts in many fields specialised support for decision making under uncertainty. However, the impact of these technologies remains controversial. In particular, it is not clear how advice of variable quality from a computer may affect human decision makers. Here the author reviews research showing strikingly diverse effects of computer support on expert decision-making. Decisions support can both systematically improve or damaged the performance of decision makers in subtle ways depending on the decision maker's skills, variation in the difficulty of individual decisions and the reliability of advice from the support tool. In clinical trials decision support technologies are often assessed in terms of their average effects. However this methodology overlooks the possibility of differential effects on decisions of varying difficulty, on decision makers of varying competence, of computer advice of varying accuracy and of possible interactions among these variables. Research that has teased apart aggregated clinical trial data to investigate these possibilities has discovered that computer support was less useful for - and sometimes hindered - professional experts who were relatively good at difficult decisions without support; at the same time the same computer support tool helped those experts who were less good at relatively easy decisions without support. Moreover, inappropriate advice from the support tool could bias decision makers' decisions and, predictably, depending on the type of case, improve or harm the decisions.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1553537,no,undetermined,0
Numerical software quality control in object oriented development,"This paper proposes new method to predict the number of the remaining bugs at the delivery inspection applied to every iteration of OOD, object oriented development. Our method consists of two parts. The first one estimates the number of the remaining bugs by applying the Gompertz curve. The second one uses the interval estimation called OOQP, object oriented quality probe. The basic idea of OOQP is to randomly extract a relatively small number of test cases, usually 10 to 20% of the entire test cases, and to execute them in the actual operation environment. From the test result of OOQP, we can efficiently predict the number of the remaining bugs by the interval estimation. The premier problem of OOQP is that OOD is imposed to use the system design specification document whose contents, like UML, tend to be ambiguous. Our estimation method works well at a matrix-typed organization where a QA team and a development team collaboratively work together to improve the software quality.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1551146,no,undetermined,0
A novel tuneable low-intensity adversarial attack,"Currently, denial of service (DoS) attacks remain amongst the most critical threats to Internet applications. The goal of the attacker in a DoS attack is to overwhelm a shared resource by sending a large amount of traffic thus, rendering the resource unavailable to other legitimate users. In this paper, we expose a novel contrasting category of attacks that is aimed at exploiting the adaptive behavior exhibited by several network and system protocols such as TCP. The goal of the attacker in this case is not to entirely disable the service but to inflict sufficient degradation to the service quality experienced by legitimate users. An important property of these attacks is the fact that the desired adversarial impact can be achieved by using an non-suspicious low-rate attack stream, which can easily evade detection. Further by tuning various parameters of the attack traffic stream, the attacker can inflict varying degrees of service degradation and at the same time making it extremely difficult for the victim to detect attacker presence. Our simulation based experiments validate our observations and demonstrate that an attacker can significantly degrade the performance of the TCP flows by inducing lowrate attack traffic which is co-ordinated to exploit the congestion control behavior of TCP",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1550965,no,undetermined,0
Delay-Centric Link Quality Aware OLSR,"This paper introduces a delay-centric link quality aware routing protocol, LQOLSR (link quality aware optimized link state routing). The LQOLSR chooses find fast and high quality routes in mobile ad hoc networks (MANET). LQOLSR predicts a packet transmission delay according to multiple transmission rates in IEEE 802.11 and selects the fastest route from source to destination by estimating relative transmission delay between nodes. We implement a LQOLSR protocol by modifying the basic OLSR (optimized link state routing) protocol. We evaluate and analyze the performance in a real testbed established in an office building",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1550948,no,undetermined,0
Outage probability lower bound in CDMA systems with lognormal-shadowed multipath Rayleigh-faded and noise-corrupted links,"The outage probability is one of the common metrics used in performance evaluation of cellular networks. In this paper, we derive a lower bound on the outage probability in CDMA systems where the communication links are disturbed by co-channel interference as well as additive noise. Each link is assumed to be faded according to both a lognormal distribution and a multipath Rayleigh distribution where the former represents the effect of shadowing while the latter represents the effect of short-term fading. The obtained lower bound is given in terms of a single-fold integral that can be easily computed using any modern software package. We present numerical results for the derived bound and compare them with the outage probability obtained by means of Monte Carlo simulations. Based on our results, we conclude that the proposed bound is relatively tight in a wide range of situations, particularly, in the case of small to moderate number of interferers and small to moderate shadowing standard deviation values.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1549474,no,undetermined,0
A QoS provision multipolling mechanism for IEEE 802.11e standard,"With flexibility and mobility, wireless local area network (WLAN) has rapidly become one of the most emergent computer research fields. It attracts significant interests both in academic and industry communities. For applying a higher quality of service (QoS) to network applications, the 802.11e Task Group has deployed hybrid coordination function (HCF) to improve the original IEEE 802.11 medium access control (MAC) protocol. Nevertheless, how to choose the right MAC parameters and QoS mechanism so as to achieve a predictable performance still remains unsolved. In this paper, we propose a polling access control scheme, which in its PCF mode applies non-preemptive priority in order to transfer voice packets more efficiently. The voice traffic characterized by packet rate of voice source and the maximum tolerable jitter (packet delay variation) is forecasted. We record the scheduling results in a queue, with which AP (access point) can poll and then enable mobile users to communicate with their opposite sites. This occurrence also solves the problem that some voice packets do not suit QoS in IEEE 802.11e standard with multipolling. During the time-gap while transmitting no voice packets, the scheme changes to DCF mode to transfer data packets. Furthermore we simulate and analyze the performance of the scheme in a WLAN environment. The experimental results show that our approach can dramatically improve the quality of network service.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1549441,no,undetermined,0
Metrics for ontologies,"The success of the semantic Web has been linked with the use of ontologies on the semantic Web. Given the important role of ontologies on the semantic Web, the need for domain ontology development and management are increasingly more and more important to most kinds of knowledge-driven applications. More and more these ontologies are being used for information exchange. Information exchange technology should foster knowledge exchange by providing tools to automatically assess the characteristics and quality of an ontology. The scarcity of theoretically and empirically validated measures for ontologies has motivated our investigation. From this investigation a suite of quality metrics have been developed and implemented as a plug-in to the ontology editor Protege so that any ontology specified in a standard Web ontology language such as RDFS or OWL may have a quality assessment analysis performed.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1548577,no,undetermined,0
FUMSâ„?artificial intelligence technologies including fuzzy logic for automatic decision making,"Advances in sensing technologies and aircraft data acquisition systems have resulted in generating huge aircraft data sets, which can potentially offer significant improvements in aircraft management, affordability, availability, airworthiness and performance (MAAAP). In order to realise these potential benefits, there is a growing need for automatically trending/mining these data and fusing the data into information and decisions that can lead to MAAAP improvements. Smiths has worked closely with the UK Ministry of Defence (MOD) to evolve Flight and Usage Management Software (FUMSâ„? to address this need. FUMSâ„?provides a single fusion and decision support platform for helicopters, aeroplanes and engines. FUMSâ„?tools have operated on existing aircraft data to provide an affordable framework for developing and verifying diagnostic, prognostic and life management approaches. Whilst FUMSâ„?provides automatic analysis and trend capabilities, it fuses the condition indicators (CIs) generated by aircraft health and usage monitoring systems (HUMS) into decisions that can increase fault detection rates and reduce false alarm rates. This paper reports on a number of decision-making processes including logic, Bayesian belief networks and fuzzy logic. The investigation presented in this paper has indicated that decision-making based on logic and fuzzy logic can offer verifiable techniques. The paper also shows how Smiths has successfully applied fuzzy logic to the Chinook HUMS CIs. Fuzzy logic has also been applied to detect sensor problems causing long-term data corruptions.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1548501,no,undetermined,0
Quality vs. quantity: comparing evaluation methods in a usability-focused software architecture modification task,"A controlled experiment was performed to assess the usefulness of portions of a usability-supporting architectural pattern (USAP) in modifying the design of software architectures to support a specific usability concern. Results showed that participants using a complete USAP produced modified designs of significantly higher quality than participants using only a usability scenario. Comparison of solution quality ratings with a quantitative measure of responsibilities considered in the solution showed positive correlation between the measures. Implications for software development are that usability concerns can be included at architecture design time, and that USAPs can significantly help software architects to produce better designs to address usability concerns. Implications for empirical software engineering are that validated quantitative measures of software architecture quality may potentially be substituted for costly and often elusive expert assessment.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541823,no,undetermined,0
Comparison of various methods for handling incomplete data in software engineering databases,"Increasing the awareness of how missing data affects software predictive accuracy has led to increasing numbers of missing data techniques (MDTs). This paper investigates the robustness and accuracy of eight popular techniques for tolerating incomplete training and test data using tree-based models. MDTs were compared by artificially simulating different proportions, patterns, and mechanisms of missing data. A 4-way repeated measures design was employed to analyze the data. The simulation results suggest important differences. Listwise deletion is substantially inferior while multiple imputation (MI) represents a superior approach to handling missing data. Decision tree single imputation and surrogate variables splitting are more severely impacted by missing values distributed among all attributes. MI should be used if the data contain many missing values. If few values are missing, any of the MDTs might be considered. Choice of technique should be guided by pattern and mechanisms of missing data.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541819,no,undetermined,0
Exploratory testing: a multiple case study,"Exploratory testing (ET) - simultaneous learning, test design, and test execution - is an applied practice in industry but lacks research. We present the current knowledge of ET based on existing literature and interviews with seven practitioners in three companies. Our interview data shows that the main reasons for using ET in the companies were the difficulties in designing test cases for complicated functionality and the need for testing from the end user's viewpoint. The perceived benefits of ET include the versatility of testing and the ability to quickly form an overall picture of system quality. We found some support for the claimed high defect detection efficiency of ET. The biggest shortcoming of ET was managing test coverage. Further quantitative research on the efficiency and effectiveness of ET is needed. To help focus ET efforts and help control test coverage, we must study planning, controlling and tracking ET.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541817,no,undetermined,0
An empirical comparison of test suite reduction techniques for user-session-based testing of Web applications,"Automated cost-effective test strategies are needed to provide reliable, secure, and usable Web applications. As a software maintainer updates an application, test cases must accurately reflect usage to expose faults that users are most likely to encounter. User-session-based testing is an automated approach to enhancing an initial test suite with real user data, enabling additional testing during maintenance as well as adding test data that represents usage as operational profiles evolve. Test suite reduction techniques are critical to the cost effectiveness of user-session-based testing because a key issue is the cost of collecting, analyzing, and replaying the large number of test cases generated from user-session data. We performed an empirical study comparing the test suite size, program coverage, fault detection capability, and costs of three requirements-based reduction techniques and three variations of concept analysis reduction applied to two Web applications. The statistical analysis of our results indicates that concept analysis-based reduction is a cost-effective alternative to requirements-based approaches.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510153,no,undetermined,0
Fault tolerant XGFT network on chip for multi processor system on chip circuits,"This paper presents a fault-tolerant eXtended Generalized Fat Tree (XGFT) Network-On-Chip (NOC) implemented with a new fault-diagnosis-and-repair (FDAR) system. The FDAR system is able to locate faults and reconfigure switch nodes in such a way that the network can route packets correctly despite the faults. This paper presents how the FDAR finds the faults and reconfigures the switches. Simulation results are used for showing that faulty XGFTs could also achieve good performance, if the FDAR is used. This is possible if deterministic routing is used in faulty parts of the XGFTs and adaptive Turn-Back (TB) routing is used in faultless parts of the network for ensuring good performance and Quality-of-Service (QoS). The XGFT is also equipped with parity bit checks for detecting bit errors from the packets.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1515723,no,undetermined,0
Performance analysis of random access channel in OFDMA systems,"The random access channel (RACH) in OFDMA systems is an uplink contention-based transport channel that is mainly used for subscriber stations to make a resource request to base stations. In this paper we focus on analyzing the performance of RACH in OFDMA systems such that the successful transmission probability, correctly detectable probability and throughput of RACH are analyzed. We also choose an access mechanism with binary exponential backoff delay procedure similar to that in IEEE 802.11. Based on the mechanism, we derive the delay and the blocking probability of RACH in OFDMA systems.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1515514,no,undetermined,0
Two approaches for the improvement in testability of communication protocols,"Protocols have grown larger and more complex with the advent of computer and communication technologies. As a result, the task of conformance testing of protocol implementation has also become more complex. The study of design for testability (DFT) is a research area in which researchers investigate design principles that will help to overcome the ever increasing complexity of testing distributed systems. Testability metrics are essential for evaluating and comparing designs. In a previous paper, we introduce a new metric for testability of communication protocols, based on the detection probability of a default. We demonstrate the usefulness of the metric for identifying faults there are more difficult to detect. In this paper, we present two approaches for improved testing of a protocol implementation once those faults that are difficult to detect are identified.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1515464,no,undetermined,0
A reflective practice of automated and manual code reviews for a studio project,"In this paper, the target of code review is project management system (PMS), developed by a studio project in a software engineering master's program, and the focus is on finding defects not only in view of development standards, i.e., design rule and naming rule, but also in view of quality attributes of PMS, i.e., performance and security. From the review results, a few lessons are learned. First, defects which had not been found in the test stage of PMS development could be detected in this code review. These are hidden defects that affect system quality and that are difficult to find in the test. If the defects found in this code review had been fixed before the test stage of PMS development, productivity and quality enhancement of the project would have been improved. Second, manual review takes much longer than an automated one. In this code review, general check items were checked by automation tool, while project-specific ones were checked by manual method. If project-specific check items could also be checked by automation tool, code review and verification work after fixing the defects would be conducted very efficiently. Reflecting on this idea, an evolution model of code review is studied, which eventually seeks fully automated review as an optimized code review.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1515372,no,undetermined,0
Best ANN structures for fault location in single-and double-circuit transmission lines,"The great development in computing power has allowed the implementation of artificial neural networks (ANNs) in the most diverse fields of technology. This paper shows how diverse ANN structures can be applied to the processes of fault classification and fault location in overhead two-terminal transmission lines, with single and double circuit. The existence of a large group of valid ANN structures guarantees the applicability of ANNs in the fault classification and location processes. The selection of the best ANN structures for each process has been carried out by means of a software tool called SARENEUR.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1514483,no,undetermined,0
Analysis of a method for improving video quality in the Internet with inclusion of redundant essential video data,"This paper presents a variant of a method for combating the burst-loss problem of multicast video packets. This variant uses redundant essential video data to offer biased protection towards all critical video information from loss, especially from contiguous loss when a burst loss of multiple consecutive packets occurs. Simulation results indicate that inclusion of redundant critical video data improves performance of our previously proposed method for solving the burst-loss problem. Furthermore, probability models, created for evaluating the effectiveness of the proposed method and its variant, give results which are consistent with those obtained from simulations.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1512871,no,undetermined,0
Ontology-based structured cosine similarity in document summarization: with applications to mobile audio-based knowledge management,"Development of algorithms for automated text categorization in massive text document sets is an important research area of data mining and knowledge discovery. Most of the text-clustering methods were grounded in the term-based measurement of distance or similarity, ignoring the structure of the documents. In this paper, we present a novel method named structured cosine similarity (SCS) that furnishes document clustering with a new way of modeling on document summarization, considering the structure of the documents so as to improve the performance of document clustering in terms of quality, stability, and efficiency. This study was motivated by the problem of clustering speech documents (of no rich document features) attained from the wireless experience oral sharing conducted by mobile workforce of enterprises, fulfilling audio-based knowledge management. In other words, this problem aims to facilitate knowledge acquisition and sharing by speech. The evaluations also show fairly promising results on our method of structured cosine similarity.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510776,no,undetermined,0
Facilitating the implementation and evolution of business rules,"Many software systems implement, amongst other things, a collection of business rules. However, the process of evolving the business rules associated with a system is both time consuming and error prone. In this paper, we propose a novel approach to facilitating business rule evolution through capturing information to assist with the evolution of rules at the point of implementation. We analyse the process of rule evolution, in order to determine the information that must be captured. Our approach allows programmers to implement rules by embedding them into application programs (giving the required performance and genericity), while still easing the problems of evolution.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510156,no,undetermined,0
Call stack coverage for test suite reduction,"Test suite reduction is an important test maintenance activity that attempts to reduce the size of a test suite with respect to some criteria. Emerging trends in software development such as component reuse, multi-language implementations, and stringent performance requirements present new challenges for existing reduction techniques that may limit their applicability. A test suite reduction technique that is not affected by these challenges is presented; it is based on dynamically generated language-independent information that can be collected with little run-time overhead. Specifically, test cases from the suite being reduced are executed on the application under test and the call stacks produced during execution are recorded. These call stacks are then used as a coverage requirement in a test suite reduction algorithm. Results of experiments on test suites for the space antenna-steering application show significant reduction in test suite size at the cost of a moderate loss in fault detection effectiveness.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510148,no,undetermined,0
2005 International Symposium on Empirical Software Engineering (IEEE Cat. No. 05EX1213),The following topics are dealt with: software project management; software maintenance; software testing; software metrics; software quality; software process improvement; software requirements; software performance evaluation; software reusability; software cost estimation; empirical analysis.,2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541800,no,undetermined,0
Optimizing test to reduce maintenance,"A software package evolves in time through various maintenance release steps whose effectiveness depends mainly on the number of faults left in the modules. Software testing is one of the most demanding and crucial phases to discover and reduce faults. In real environment, time available to test a software release is a given finite quantity. The purpose of this paper is to identify a criterion to estimate an efficient time repartition among software modules to enhance fault location in testing phase and to reduce corrective maintenance. The fundamental idea is to relate testing time to predicted risk level of the modules in the release under test. In our previous work we analyzed several kinds of risk prediction factors and their relationship with faults; moreover, we thoroughly investigated the behavior of faults on each module through releases to find significant fault proneness tendencies. Starting from these two lines of analysis, in this paper we propose a new approach to optimize the use of available testing time in a software release. We tuned and tested our hypotheses on a large industrial environment.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510141,no,undetermined,0
The top ten list: dynamic fault prediction,"To remain competitive in the fast paced world of software development, managers must optimize the usage of their limited resources to deliver quality products on time and within budget. In this paper, we present an approach (the top ten list) which highlights to managers the ten most susceptible subsystems (directories) to have a fault. Managers can focus testing resources to the subsystems suggested by the list. The list is updated dynamically as the development of the system progresses. We present heuristics to create the top ten list and develop techniques to measure the performance of these heuristics. To validate our work, we apply our presented approach to six large open source projects (three operating systems: NetBSD, FreeBSD, OpenBSD; a window manager: KDE; an office productivity suite: KOffice; and a database management system: Postgres). Furthermore, we examine the benefits of increasing the size of the top ten list and study its performance.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510122,no,undetermined,0
Power transmission control using distributed max-flow,Existing maximum flow algorithms use one processor for all calculations or one processor per vertex in a graph to calculate the maximum possible flow through a graph's vertices. This is not suitable for practical implementation. We extend the max-flow work of Goldberg and Tarjan to a distributed algorithm to calculate maximum flow where the number of processors is less than the number of vertices in a graph. Our algorithm is applied to maximizing electrical flow within a power network where the power grid is modeled as a graph. Error detection measures are included to detect problems in a simulated power network. We show that our algorithm is successful in executing quickly enough to prevent catastrophic power outages.,2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510028,no,undetermined,0
Software Test Selection Patterns and Elusive Bugs,"Traditional white and black box testing methods are effective in revealing many kinds of defects, but the more elusive bugs slip past them. Model-based testing incorporates additional application concepts in the selection of tests, which may provide more refined bug detection, but does not go far enough. Test selection patterns identify defect-oriented contexts in a program. They also identify suggested tests for risks associated with a specified context. A context and its risks is a kind of conceptual trap designed to corner a bug. The suggested tests will find the bug if it has been caught in the trap.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1509993,no,undetermined,0
Iterative Metamorphic Testing,"An enhanced version of metamorphic testing, namely n-iterative metamorphic testing, is proposed to systematically exploit more information out of metamorphic tests by applying metamorphic relations in a chain style. A contrastive case study, conducted within an integrated testing environment MTest, shows that n-iterative metamorphic testing exceeds metamorphic testing and special case testing in terms of their fault detection capabilities. Another advantage of n-iterative metamorphic testing is its high efficiency in test case generation.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1509992,no,undetermined,0
A seamless handoff approach of mobile IP based on dual-link,"Mobile IP protocol solves the problem of mobility support for hosts connected to Internet anytime and anywhere, and makes the mobility transparent to the higher layer applications. But the handoff latency in Mobile IP affects the quality of communication. This paper proposes a seamless handoff approach based on dual-link and link layer trigger, using information from link layer to predict and trigger the dual-link handoff. During the handoff, MN keeps one link connected with the current network while it handoff another link to the new network. In this paper we develop a model system based on this approach and provide experiments to evaluate the performance of this approach. The experimental results show that this approach can ensure the seamless handoff of Mobile IP.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1509638,no,undetermined,0
Understanding of estimation accuracy in software development projects,"Over the past decades large investments in software engineering research and development have been made by academia and the software industry. These efforts have produced considerably insight in the complex domain of software development, and have paid off in the shape of the improved tools, languages, methodologies and techniques. However, a recent review of estimation survey documents that less progress has been made in the area of estimation performance. This is a major concern for the software industry, as lack of estimation performance often causes budget overruns, delays, lost contracts or poor quality software. Because of these rather dramatic consequences, there is a high demand for more research on the topic of effort estimation in software development projects. That demand motivated this PhD. The thesis is written at the Estimation Group at Simula Research Laboratory in Oslo, Norway. The work is supervised by professor Magne Jorgensen, and is part of the SPIKE (Software Process Improvement based on Knowledge and Experience) project which is funded by the Norwegian Research Council",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1509320,no,undetermined,0
Ensemble imputation methods for missing software engineering data,"One primary concern of software engineering is prediction accuracy. We use datasets to build and validate prediction systems of software development effort, for example. However it is not uncommon for datasets to contain missing values. When using machine learning techniques to build such prediction systems, handling of incomplete data is an important issue for classifier learning since missing values in either training or test set or in both sets can affect prediction accuracy. Many works in machine learning and statistics have shown that combining (ensemble) individual classifiers is an effective technique for improving accuracy of classification. The ensemble strategy is investigated in the context of incomplete data and software prediction. An ensemble Bayesian multiple imputation and nearest neighbour single imputation method, BAMINNSI, is proposed that constructs ensembles based on two imputation methods. Strong results on two benchmark industrial datasets using decision trees support the method",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1509308,no,undetermined,0
Accelerating molecular dynamics simulations with configurable circuits,"Molecular dynamics (MD) is of central importance to computational chemistry. Here we show that MD can be implemented efficiently on a COTS FPGA board, and that speed-ups from 31Ã— to 88Ã— over a PC implementation can be obtained. Although the amount of speed-up depends on the stability required, 46Ã— can be obtained with virtually no detriment, and the upper end of the range is apparently viable in many cases. We sketch our FPGA implementations and describe the effects of precision on the trade-off between performance and quality of the MD simulation.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1515767,no,undetermined,0
Snort offloader: a reconfigurable hardware NIDS filter,Software-based network intrusion detection systems (NIDS) often fail to keep up with high-speed network links. In this paper an FPGA-based pre-filter is presented that reduces the amount of traffic sent to a software-based NIDS for inspection. Simulations using real network traces and the Snort rule set show that a pre-filter can reduce up to 90% of network traffic that would have otherwise been processed by Snort software. The projected performance enables a computer to perform real-time intrusion detection of malicious content passing over a 10 Gbps network using FPGA hardware that operates with 10 Gbps of throughput and software that needs only to operate with 1 Gbps of throughput.,2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1515770,no,undetermined,0
An iterative hardware Gaussian noise generator,"The quality of generated Gaussian noise samples plays a crucial role when evaluating the bit error rate performance of communication systems. This paper presents a new approach for the field-programmable gate array (FPGA) realization of a high-quality Gaussian noise generator (GNG). The datapath of the GNG can be configured differently based on the required accuracy of the Gaussian probability density function (PDF). Since the GNG is often most conveniently implemented on the same FPGA as the design under evaluation, the area efficiency of the proposed GNG is important. For a particular configuration, the proposed design utilizes only 3% of the configurable slices and two on-chip block memories of a Virtex XC2V4000-6 FPGA to generate Gaussian samples within up to Â±6.55Î´, where Î´ is the standard deviation, and can operate at up to 132 MHz.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1517373,no,undetermined,0
Composition assessment metrics for CBSE,"The objective of this paper is the formal definition of composition assessment metrics for CBSE, using an extension of the CORBA component model metamodel as the ontology for describing component assemblies. The method used is the representation of a component assembly as an instantiation of the extended CORBA component model metamodel. The resulting meta-objects diagram can then be traversed using object constraint language clauses. These clauses are a formal and executable definition of the metrics that can be used to assess quality attributes from the assembly and its constituent components. The result is the formal definition of context-dependent metrics that cover the different composition mechanisms provided by the CORBA component model and can be used to compare alternative component assemblies; a metamodel extension to capture the topology of component assemblies. The conclusion is that providing a formal and executable definition of metrics for CORBA component assemblies is an enabling precondition to allow for independent scrutiny of such metrics which is, in turn, essential to increase practitioners confidence on predictable quality attributes.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1517732,no,undetermined,0
Patterns and tools for achieving predictability and performance with real time Java,"The real-time specification for Java (RTSJ) offers the predictable memory management needed for real-time applications, while maintaining Java 's advantages of portability and ease of use. RTSJ's scoped memory allows object lifetimes to be controlled in groups, rather than individually as in C++. While easier than individual object lifetime management, scoped memory adds programming complexity from strict rules governing memory access across scopes. Moreover, memory leaks can potentially create jitter and reduce performance. To manage the complexities of RTSJ's scoped memory, we developed patterns and tools for RTZen, a real-time CORBA Object Request Broker (ORB). We describe four new patterns that enable communication and coordination across scope boundaries, an otherwise difficult task in RTSJ. We then present IsoLeak, a runtime debugging tool that visualizes the scoped hierarchies of complex applications and locates memory leaks. Our empirical results show that RTZen is highly predictable and has acceptable performance. RTZen therefore demonstrates that the use of patterns and tools like IsoLeak can help applications meet the stringent QoS requirements of DRE applications, while supporting safer, easier, cheaper and faster development in real-time Java.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541088,no,undetermined,0
Spectral analysis for the rotor defects diagnosis of an induction machine,"The main goal of this paper is to develop a method permitting to determine the nature of the rotor defects thanks to the spectral analysis. The development growing that the equipment of measurement (spectral analyzer) and the software of digital signal processing had made possible the diagnosis of the electric machine defects. Motor current signature analysis (MCSA) is used for the detection of the electrical and the mechanical faults of an induction machine to identify rotor bar faults. Also, the calculation of machine inductances (with and without rotor defects) is carried out by the tools of software MATLAB before the beginning of simulation under Software SIMULINK. Simulation and experimental results are presented to confirm the validity of proposed approach.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1531655,no,undetermined,0
Interlaced to progressive scan conversion Using fuzzy edge-based line average algorithm,"De-interlacing methods realize the interlaced to progressive conversion required in many applications. Among them, intra-field methods are widely used for their good trade off between performance and computational cost. In particular, the ELA algorithm is well-known for its advantages in reconstructing the edges of the images, although it degrades the image quality where the edges are not clear. The algorithm proposed in this paper uses a simple fuzzy system which models heuristic rules to improve the KLA rules. It can be implemented easily in software and hardware since the increase in computational cost is very low. Simulation results are included to illustrate the advantages of the proposed fuzzy ELA algorithm in de-interlacing non noisy and noisy images.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1531624,no,undetermined,0
Auxiliary voltage sag ride-through system for adjustable-speed drives,"Voltage sags and harmonics are two very important power quality problems. Adjustable-speed drives (ASD) trip due to voltage sags, interfering with production and resulting into financial losses. Harmonics can cause, among others, overheating and noise in the motors. In order to improve performance and reliability of ASD, this paper presents an auxiliary system that combines two sub-circuits that provide ride-through capabilities to critical ASD loads during balanced and unbalanced voltage sags and line current total harmonic distortion (THD) improvement using a third harmonic injection method. A supervised Adaline algorithm is used for voltage sag detection to activate a thyristor controlled rectifier to keep the DC-bus voltage within acceptable limits. A detailed description of the operating principle is presented. The theoretical analysis is verified by digital simulation using PSIM software. Implementation details and experimental results on a 12.5 kW prototype are also presented",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1531380,no,undetermined,0
High level extraction of SoC architectural information from generic C algorithmic descriptions,"The complexity of nowadays, algorithms in terms of number of lines of codes and cross-relations among processing algorithms that are activated by specific input signals, goes far beyond what the designer can reasonably grasp from the ""pencil and paper"" analysis of the (software) specifications. Moreover, depending on the implementation goal different measures and metrics are required at different steps of the implementation methodology or design flow of SoC. The process of extracting the desired measures needs to be supported by appropriate automatic tools, since code rewriting, at each design stage, may result resource consuming and error prone. This paper presents an integrated tool for automatic analysis capable of producing complexity results based on rich and customizable metrics. The tool is based on a C virtual machine that allows extracting from any C program execution the operations and data-flow information, according to the defined metrics. The tool capabilities include the simulation of virtual memory architectures.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1530961,no,undetermined,0
QoS-aware replanning of composite Web services,"Run-time service discovery and late-binding constitute some of the most challenging issues of service-oriented software engineering. For late-binding to be effective in the case of composite services, a QoS-aware composition mechanism is needed. This means determining the set of services that, once composed, not only will perform the required functionality, but also will best contribute to achieve the level of QoS promised in service level agreements (SLAs). However, QoS-aware composition relies on estimated QoS values and workflow execution paths previously obtained using a monitoring mechanism. At run-time, the actual QoS values may deviate from the estimations, or the execution path may not be the one foreseen. These changes could increase the risk of breaking SLAs and obtaining a poor QoS. Such a risk could be avoided by replanning the service bindings of the workflow slice still to be executed. This paper proposes an approach to trigger and perform composite service replanning during execution. An evaluation has been performed simulating execution and replanning on a set of composite service workflows.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1530790,no,undetermined,0
Availability modeling for reliable routing software,"With the explosion in the use of Internet, there is an obvious trend that IP-based critical applications (E-Transaction, etc.) are increasing dramatically. Therefore, it is required to build high performance routers (HPRs) with high availability (HA). Though researches on HA router hardware are fairly mature, investigations on HA routing software are still at the preliminary stage. This paper systematically presents possible implementation approaches on HA routing software and classifies them into three categories: protocol extension, state synchronization and packet duplication. This paper innovatively models and analyzes these HA routing software schemes in continuous-time Markov chains (CTMC). Based on the modeling, this paper also introduces numerical results and evaluates these different HA approaches.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1530661,no,undetermined,0
A fast and efficient partial distortion search algorithm for block motion estimation,"Under the prerequisite of assuring image quality close to full search algorithm (FS), a fast and efficient partial distortion search algorithm for block motion estimation based on predictive motion vector field (PMVPDS) is proposed in order to reduce the computational complexity of existent partial distortion algorithm. PMVPDS takes advantages of predictive motion vector field technique, half-stop technique and spiral scanning path to find matching block quickly. In addition, controllable partial distortion criterion (CPDC) is used in the calculation of block distortion to speed up further. Experiment results show that, compared with the normalized partial distortion search algorithm (NPDS) and the progressive partial distortion search algorithm (PPDS), PMVPDS provides a significant speedup and achieves better PSNR performance.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1528123,no,undetermined,0
Improving after-the-fact tracing and mapping: supporting software quality predictions,"The software engineering industry undertakes many activities that require generating and using mappings. Companies develop knowledge bases to capture corporate expertise and possibly proprietary information. Software developers build traceability matrices to demonstrate that their designs satisfy the requirements. Proposal managers map customers' statements of work to individual sections of companies' proposals to prove compliance. Systems engineers authoring interface specifications record design rationales as they make relevant decisions. We developed an approach to tracing and mapping that aims to use fully automated information retrieval techniques, and we implemented our approach in a tool called RETRO (requirements tracing on target).",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1524912,no,undetermined,0
An Efficient and Practical Defense Method Against DDoS Attack at the Source-End,"Distributed Denial-of-Service (DDoS) attack is one of the most serious threats to the Internet. Detecting DDoS at the source-end has many advantages over defense at the victim-end and intermediate-network. One of the main problems for source-end methods is the performance degradation brought by these methods, which discourages Internet service providers (ISPs) to deploy the defense system. We propose an efficient detection approach, which only requires limited fixed-length memory and low computation overhead but provides satisfying detection results. The low cost of defense is expected to attract more ISPs to join the defense. The experiments results show our approach is efficient and feasible for defense at the source-end",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1524303,no,undetermined,0
Pre-layout physical connectivity prediction with application in clustering-based placement,"In this paper, we introduce a structural metric, logic contraction, for pre-layout physical connectivity prediction. For a given set of nodes forming a cluster in a netlist, we can predict their proximity in the final layout based on the logic contraction value of the cluster. We demonstrate a very good correlation of our pre-layout measure with the post-layout physical distances between those nodes. We show an application of the logic contraction to circuit clustering. We compare our seed-growth clustering algorithm with the existing efficient clustering techniques. Experimental results demonstrate the effectiveness of our new clustering method.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1524126,no,undetermined,0
Prototype stopping rules in software development projects,"Custom software development projects under product specification uncertainty can be subject to wide variations in performance (duration, cost, and quality) depending on how the uncertainty is resolved. A prototyping strategy has been chosen in many cases to mitigate specification risk and improve performance. The study reported here sought to illuminate the case where duration was the highest priority project constraint, a feature that has often called for a concurrent development process. Unlike concurrent engineering, however, the process in this paper was a sequential, three-phase approach including an optional, up-front prototyping phase, a nominal-duration construction phase, and a variable length rework phase that grew with the arrival of specification modifications. The source of uncertainty was the modification arrival time; the management control point was the amount of time spent engaged in prototyping activities. Results showed that in situations where the modification arrival rate was sufficiently faster during prototyping than during construction, a minimal-duration choice was available. The model also returned the solution to perform no prototyping in cases where arrival rates were nearly equivalent between phases, or when the rework cost associated with modifications was consistently low.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1522700,no,undetermined,0
Motion analysis of the international and national rank squash players,"In this paper, we present a study on squash player work-rate during the squash matches of two different quality levels. To assess work-rate, the measurement of certain parameters of player motion is needed. The computer vision based software application was used to automatically obtain player motion data from the digitized video recordings of 22 squash matches. The matches were played on two quality levels - international and Slovene national players. We present the results of work-rate comparison between these two groups of players based on game duration and distance covered by the players. We found that the players on the international quality level on average cover significantly larger distances, which is partially caused by longer average game durations.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1521312,no,undetermined,0
Towards Software Quality Economics for Defect-Detection Techniques,"There are various ways to evaluate defect-detection techniques. However, for a comprehensive evaluation the only possibility is to reduce all influencing factors to costs. There are already some models and metrics for the cost of quality that can be used in that context. The existing metrics for the effectiveness and efficiency of defect-detection techniques and experiences with them are combined with cost metrics to allow a more fine-grained estimation of costs and a comprehensive evaluation of defect-detection techniques. The current model is most suitable for directly comparing concrete applications of different techniques",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1521215,no,undetermined,0
The Quantitative Safety Assessment for Safety-Critical Software,"The software fault failure rate bound is discussed and generalized for different reliability growth models. The fault introduction during testing and the fault removal efficiency are modeled to relax the two common assumptions made in software reliability models. Three approaches are introduced for the fault content estimation, and thus they are applied to software coverage estimation. A three-state non-homogenous Markov model is constructed for software safety assessment. The two most important metrics for safety assessment, steady state safety and MTTUF, are estimated using the three-state Markov model. A case study is conducted to verify the theory proposed in the paper",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1521203,no,undetermined,0
Code Normal Forms,"Because of their strong economic impact, complexity and maintainability are among the most widely used terms in software engineering. But, they are also among the most weakly understood. A multitude of software metrics attempts to analyze complexity and a proliferation of different definitions of maintainability can be found in text books and corporate quality guide lines. The trouble is that none of these approaches provides a reliable basis for objectively assessing the ability of a software system to absorb future changes. In contrast to this, relational database theory has successfully solved very similar difficulties through normal forms. In this paper, we transfer the idea of normal forms to code. The approach taken is to introduce semantic dependencies as a foundation for the definition of code normal form criteria",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1521198,no,undetermined,0
Automated detection of performance regressions: the mono experience,"Engineering a large software project involves tracking the impact of development and maintenance changes on the software performance. An approach for tracking the impact is regression benchmarking, which involves automated benchmarking and evaluation of performance at regular intervals. Regression benchmarking must tackle the nondeterminism inherent to contemporary computer systems and execution environments and the impact of the nondeterminism on the results. On the example of a fully automated regression benchmarking environment for the mono open-source project, we show how the problems associated with nondeterminism can be tackled using statistical methods.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1521132,no,undetermined,0
Application of General Perception-Based QoS Model to Find Providers' Responsibilities. Case Study: User Perceived Web Service Performance.,"This paper presents a comprehensive model intended to analyze quality of service in telecommunications services and its causes. Although many works have been published in this area, both from a technical viewpoint as well as taking into consideration subjective concerns, they have not resulted in a unique methodology to assess the experienced quality. While most of the studies consider the quality of service only from a technical and end-to-end point of view, we try to analyze quality of service as a general gauge of final users' satisfaction. The proposed model allows us to estimate the quality experienced by end users, while offering detailed results regarding the responsibility of the different agents involved in the service provision. Once we overview the most significant elements of the model, an in-depth analytical study is detailed. Finally, we illustrate a practical study for Web browsing service in order to validate the theoretical model",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1559914,no,undetermined,0
The improvement on the measuring precision of detecting fault composite insulators by using electric field mapping,"According to the statistics of the fault composite insulators, most of the defects take place at the high voltage (HV) end of the insulators. At present, the minimum defect length detected possibly at HV end of the insulators is about 7 cm by using electric field mapping device, which could not indicate the defects located between the last shed and the HV electrode. Therefore, it is important to improve the measuring precision that is suitable for indicating the defects less than 7 cm in inspecting the fault composite insulators based on electric field mapping device. In order to enhance the measuring precision of the device, we analyzed the electric field distribution along with an insulator by using the commercial software ANSYS. We found that a 5 cm defect can be found if we collect two to three electric field data between the two sheds. Therefore, we added a photoelectric cell array to trigger the device for collecting more data between the two sheds. The tests were conducted in our laboratory by using our new device. The results from our experiments show that the sensitivity of detecting the defects is increased and our new device can indicate the defects less than 5 cm at the HV end without grading rings.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1560677,no,undetermined,0
Generalized performance management of multi-class real-time imprecise data services,"The intricacy of real-time data service management increases mainly due to the emergence of applications operating in open and unpredictable environments, increases in software complexity, and need for performance guarantees. In this paper we propose an approach for managing the quality of service of real-time databases that provide imprecise and differentiated services, and that operate in unpredictable environments. Transactions are classified into service classes according to their level of importance. Transactions within each service class are further classified into subclasses based on their quality of service requirements. In this way transactions are explicitly differentiated according to their importance and quality of service requests. The performance evaluation shows that during overloads the most important transactions are guaranteed to meet their deadlines and that reliable quality of service is provided even in the face of varying load and execution time estimation errors",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1563094,no,undetermined,0
Bayesian networks modeling for software inspection effectiveness,"Software inspection has been broadly accepted as a cost effective approach for defect removal during the whole software development lifecycle. To keep inspection under control, it is essential to measure its effectiveness. As human-oriented activity, inspection effectiveness is due to many uncertain factors that make such study a challenging task. Bayesian networks modeling is a powerful approach for the reasoning under uncertainty and it can describe inspection procedure well. With this framework, some extensions have been explored in this paper. The number of remaining defects in the software is proposed to be incorporated into the framework, with expectation to provide more information on the dynamic changing status of the software. In addition, a different approach is adopted to elicit the prior belief of related probability distributions for the network. Sensitivity analysis is developed with the model to locate the important factors to inspection effectiveness.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607500,no,undetermined,0
Prostatectomy Evaluation using 3D Visualization and Quantitation,"Prostate cancer is a disease with a long natural history. Differences in survival outcomes as indicators of inappropriate surgery would take decades to appear. Therefore, the evaluation of the excised specimen according to defined parameters provides a more reasonable and timely assessment of surgical quality. There are currently a number of very different surgical approaches. Some uniform guidelines and quality assessment measuring readily available parameters would be desirable to establish a standard for comparison of surgical approaches and for individual surgical performance. In this paper, we present a novel methodology to objectively quantify the assessment process utilizing a 3D reconstructed model for the prostate gland. To this end, we discuss the development of a process employing image reconstruction and analysis techniques to assess the percent of capsule covered by soft tissue. A final goal is to develop software for the purpose of a quality assurance assessment for pathologists and surgeons to evaluate the adequacy/appropriateness of each surgical procedure; laparoscopic versus open perineal or retropubic prostatectomy. Results from applying this technique are presented and discussed",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1615637,no,undetermined,0
Work in Progress - Computer Software for Predicting Steadiness of the Students,"The paper presents a study which identifies a series of factors influencing students' steadiness in their option for engineering training and the final aim is to elaborate an IT system for monitoring the quality of educational offer. This aim is reached through a research developed in three stages. Only the first and the second stages were described here. The last one is in work. So, the first stage is materialized in elaborating and validating a questionnaire structured on three dimensions: finding the expectations, diagnosis of initial motivation for initiating students in engineering, specifying identity information and elements of personal history from educational student's experience. The sample is randomly chosen and the students from the research group belong to Technical University ""Gh. Asachi"" Iassy, Romania, attending first, second and third year of study. The second stage of the scientific research establishes the relations between the identified expectations, initial motivation of students for engineering training and personal history in educational area on the one hand and students' educational performance on the other hand. Afterwards, the results of the first two stages represents the starting point for planning computer software to predict the steadiness of students in their professional choice",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1612274,no,undetermined,0
A Simulation Task to Assess Students â€?Design Process Skill,"Research has shown that the quality of one's design process is an important ingredient in expertise. Assessing design process skill typically requires a performance assessment in which students are observed (either directly or by videotape) completing a design and assessed using an objective scoring system. This is impractical in course-based assessment. As an alternative, we developed a computer-based simulation task, in which the student respondent ""watches"" a team develop a design (in this instance a software design) and makes recommendations as to how they should proceed. The specific issues assessed by the simulation were drawn from the research literature. For each issue the student is asked to describe, in words, what the team should do next and then asked to choose among alternatives that the ""team"" has generated. Thus, the task can be scored qualitatively and quantitatively. The paper describes the task and its uses in course-based assessment",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1612150,no,undetermined,0
Work in progress - Measuring the ROI<inf>time</inf>for Static Analysis,"Static analysis is one method that offers potential for reducing errors in delivered software. Static analysis is currently used to discover buffer overflows and mathematical errors, as well as verifying compliance with documented programming standards. Static analysis is routinely used in safety critical software applications within the avionics and automotive industries. Outside of these applications, static analysis is not a routinely taught method for software development. This paper intends to provide a quantitative measure for evaluating the effectiveness of static analysis as well as presenting results from an academic environment",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1612031,no,undetermined,0
Managing a project course using Extreme Programming,"Shippensburg University offers an upper division project course in which the students use a variant of Extreme Programming (XP) including: the Planning Game, the Iteration Planning Game, test driven development, stand-up meetings and pair programming. We start the course with two weeks of controlled lab exercises designed to teach the students about test driven development in JUnit/Eclipse and designing for testability (with the humble dialog box design pattern) while practicing pair programming. The rest of our semester is spent in three four-week iterations developing a product for a customer. Our teams are generally large (14-16 students) so that the projects can be large enough to motivate the use of configuration management and defect tracking tools. The requirement of pair programming limits the amount of project work the students can do outside of class, so class time is spent on the projects and teaching is on-demand individual mentoring with lectures/labs inserted as necessary. One significant challenge in managing this course is tracking individual responsibilities and activities to ensure that all of the students are fully engaged in the project. To accomplish this, we have modified the story and task cards from XP to provide feedback to the students and track individual performance against goals as part of the students' grades. The resulting course has been well received by the students. This paper will describe this course in more detail and assess its effect on students' software engineering background through students' feedback and code metrics",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1611948,no,undetermined,0
Enhancing Internet robustness against malicious flows using active queue management,"Attackers can easily modify the TCP control protocols of host computers to inject the malicious flows to the Internet. Including DDoS and worm attack flows, these malicious flows are unresponsive to the congestion control mechanism which is necessary to the equilibrium of the whole Internet. In this paper, a new scheme against the large scale malicious flows is proposed based on the principles of TCP congestion control. The kernel is to implement a new scheduling algorithm named as CCU (compare and control unresponsive flows) which is one sort of active queue management (AQM). According to the unresponsive characteristic of malicious flows, CCU algorithm relies on the two processes of malicious flows - detection and punishment. The elastics control mechanism of unresponsive flows benefits the AQM with the high performance and enhances the Internet robustness against malicious flows. The network resource can be regulated for the basic quality of service (QoS) demands of legal users. The experiments prove that CCU can detect and restrain responsive flows more accurately compared to other AQM algorithms.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1609918,no,undetermined,0
An embedded adaptive live video transmission system over GPRS/CDMA network,"The provision of video communication over wireless links is a challenging task because there is difficulty to get better video quality guarantee for video transmission over the current wireless networks include GPRS or CDMA. In this paper, we developed a embedded adaptive live video streaming transmission system on GPRS/CDMA network based on MPEG4 video compression standard. This system presents a performance enhancement for real-time video transmission by employing an adaptive sender rate control scheme, which estimates the available channel bandwidth between the sender and receiver, then adjusts the output rate of encoder side using R-D framework and frame-skip control mechanism according to the estimated time-varying channel bandwidth.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1609867,no,undetermined,0
Reliability prediction and assessment of fielded software based on multiple change-point models,"In this paper, we investigate some techniques for reliability prediction and assessment of fielded software. We first review how several existing software reliability growth models based on non-homogeneous Poisson processes (NHPPs) can be readily derived based on a unified theory for NHPP models. Furthermore, based on the unified theory, we can incorporate the concept of multiple change-points into software reliability modeling. Some models are proposed and discussed under both ideal and imperfect debugging conditions. A numerical example by using real software failure data is presented in detail and the result shows that the proposed models can provide fairly good capability to predict software operational reliability.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607540,no,undetermined,0
Predicting software suitability using a Bayesian belief network,"The ability to reliably predict the end quality of software under development presents a significant advantage for a development team. It provides an opportunity to address high risk components earlier in the development life cycle, when their impact is minimized. This research proposes a model that captures the evolution of the quality of a software product, and provides reliable forecasts of the end quality of the software being developed in terms of product suitability. Development team skill, software process maturity, and software problem complexity are hypothesized as driving factors of software product quality. The cause-effect relationships between these factors and the elements of software suitability are modeled using Bayesian belief networks, a machine learning method. This research presents a Bayesian network for software quality, and the techniques used to quantify the factors that influence and represent software quality. The developed model is found to be effective in predicting the end product quality of small-scale software development efforts.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607435,no,undetermined,0
Dynamic characterization study of flip chip ball grid array (FCBGA) on peripheral component interconnect (PCI) board application,"This paper outlines and discusses the new mechanical characterization metrologies applied on PCI board envelope. 'The dynamic responses of PCI board were monitored and characterized using accelerometer and strain gauges. PCI board performances were analyzed to differentiate its high risk areas through analysis of board strain responses to solder joint crack. Board ""strain states"" analysis methodology was introduced to provide immediate accurate board bending modes and deflection associated with experimental results. Using this methodology, it eases the board bend mode analysis which can capture the board strain performance limit at the same time. In addition, high speed camera (HSC) tool was incorporated into the evaluation to understand the boards bend history under shock test. This allows better view of the bending moment and matching to defect locations for corrective action implementation. Detailed failure analysis mapping of solder joint crack percentages was successfully gathered to support those findings. Key influences, such as thermal/mechanical enabling preload masses and shock input profiles on solder joint crack severity were conducted as well to understand the potential risk modulators for SJR performance. Furthermore, commercial simulation software analysis tool was applied to correlate the board's bend modes and predict the high risk solder joint location; which is important for product enabling solutions design. As a result, a system level stiffener solution was designed. Hence, with this characterization and validation concept, a practical stiffener solution for PCI application was validated through a special case study to improve the board SJR performance in its use condition.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1598243,no,undetermined,0
Personal software process (PSP) assistant,"The personal software process (PSP) is a process and performance improvement method aimed at individual software engineers. The use of PSP has been shown to result in benefits such as improved estimation accuracy and reduced defect density of individuals. However, the experience of our institute and of several others is that recording various size and defect data can be onerous, which in, turn can lead to adoption and data quality problems. This paper describes a system that we have developed that performs automatic size and defect recording, aside from providing facilities for viewing and editing the usual PSP logs and reports. Moreover, the system automatically classifies and ranks defects, and then consolidates schedules and defect lists of individual developers into a schedule and defect library for the developers' team.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607210,no,undetermined,0
Early stage software reliability and design assessment,"In early developmental stages of software, failure data is not available to determine the reliability of software, but design assessment is a must in this stage. We propose a model based on reliability block diagram (RBD) for representing real-world problems and an algorithm for analysis of these models in early phase of software development. We have named this technique early reliability analysis technique (ERAT). We have performed several simulations on randomly generated software models to compute reliabilities and coupling parameters. The simulation result shows that reliabilities are good quality indicator and coupling can be correlated with system reliability and can be used for system design assessment.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607202,no,undetermined,0
An integrated solution for testing and analyzing Java applications in an industrial setting,"Testing a large-scale, real-life commercial software application is a very challenging task due to the constant changes in the software, the involvement of multiple programmers and testers, and a large amount of code. Integrating testing with development can help find program bugs at an earlier stage and hence reduce the overall cost. In this paper, we report our experience on how to apply eXVantage (a tool suite for code coverage testing, debugging, performance profiling, etc.) to a large, complex Java application at the implementation and unit testing phases in Avaya. Our results suggest that programmers and testers can benefit from using eXVantage to monitor the testing process, gain confidence on the quality of their software, detect bugs which are otherwise difficult to reveal, and identify performance bottlenecks in terms of which part of code is most frequently executed.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607197,no,undetermined,0
Identifying error proneness in path strata with genetic algorithms,"In earlier work we have demonstrated that GA can successfully identify error prone paths that have been weighted according to our weighting scheme. In this paper we investigate whether the depth of strata in the software affects the performance of the GA. Our experiments show that the GA performance changes throughout the paths. It performs better in the upper, less in the middle and best in the lower layer of the paths. Although various methods have been applied for detecting and reducing errors in software, little research has been done into partitioning a system into smaller, error prone domains for software quality assurance. To identify error proneness in software paths is important because by identifying them, they can be given priority in code inspections or testing. Our experiments observe to what extent the GA identifies errors seeded into paths using several error seeding strategies. We have compared our GA performance with random path selection.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607181,no,undetermined,0
Client profiling for QoS-based Web service recommendation,"Quality of service (QoS) is one of the important factors when choosing a Web service (WS) provided by a particular WS provider. In this paper we focus on performance, which is one of the important QoS attributes. Performance can be evaluated from the client-side as well as the server-side. As clients are connected through heterogeneous network environments, different clients can experience different performance although they are connected to the same WS provider. Through client grouping and profiling, performance experienced by prospective clients can be estimated based on other clients' historical performance. In this paper we present the common factors for client grouping and profiling, and present the results of the experiments we carried out to understand the effect of these factors on the performance experienced by WS clients. The experimental results show the importance of client grouping and profiling for WS recommendation and composition.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607140,no,undetermined,0
A new Phase Locked Loop Strategy for Power Quality Instruments Synchronisation,"Power quality instrumentation requires the accurate fundamental frequency estimation and the signal synchronization, even in presence of disturbances. In the paper the authors present an innovative synchronization technique, based on a single phase software PLL. To evaluate how the synchronization technique is adversely affected by the application of stationary and transient disturbing influences, appropriate testing conditions have been developed, taking into account the requirements of the in-force standards. In the paper the proposed technique is described and PLL performances in presence of stationary and transient disturbances are presented",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1604276,no,undetermined,0
Spare Line Borrowing Technique for Distributed Memory Cores in SoC,"In this paper, a new architecture of distributed embedded memory cores for SoC is proposed and an effective memory repair method by using the proposed spare line borrowing (software-driven reconfiguration) technique is investigated. It is known that faulty cells in memory core show spatial locality, also known as fault clustering. This physical phenomenon tends to occur more often as deep submicron technology advances due to defects that span multiple circuit elements and sophisticated circuit design. The combination of new architecture & repair method proposed in this paper ensures fault tolerance enhancement in SoC, especially in case of fault clustering. This fault tolerance enhancement is obtained through optimal redundancy utilization: spare redundancy in a fault-resistant memory core is used to fix the fault in a fault-prone memory core. The effect of spare line borrowing technique on the reliability of distributed memory cores is analyzed through modeling and extensive parametric simulation",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1604065,no,undetermined,0
Software tools to facilitate advanced network operation,"Economical trends force electricity companies to maximize the utilization of their networks. As a result margins decrease. Within these decreased margins the quality of supply must improve. Disturbances have to be avoided and, if they occur, solved as quickly as possible. This increases the working stress for the network operators. This paper describes the development and application of four software tools that facilitate and simplify the work of network operators. They concern fault location, safe coupling of MV networks, optimal transmission network operation and pseudo state estimation. These tools are based on standard load flow and short circuit analysis methods, where new additions have been made. All applications communicate with various existing applications and devices. They combine on-line measured values with power system database information. The software is equipped with a user friendly interface for grid operators. The applications were developed in cooperation between power utility Nuon and software developer phase to phase and are already in use or close to implementation",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1600590,no,undetermined,0
Development of a DSP-Based Power Quality Monitoring Instrumentfor Real-Time Detection of Power Disturbances,"Current trends of power quality monitoring instruments are based on digital signal processors (DSP), which are used to record waveforms and harmonics and comes with software for collecting data and viewing monitoring results. The variations in the DSP based instruments are in the way algorithms that are developed for processing the real-time power quality waveforms. At present, all of the available power quality monitoring instruments are not capable of troubleshooting and diagnosing power quality problems. Therefore, a DSP-based power quality monitoring instrument is proposed for real-time disturbance recognition and source detection. The proposed instrument uses the Texas Instruments TMS320C6711DSP starter kit with a TI ADS8364EVM analog digital converter mounted on the daughter card. The instrument architecture and the software implementation are discussed in the paper. Preliminary experimental results displaying the fast Fourier transform analysis of the real-time voltage signals are included",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1619703,no,undetermined,0
Voltage Sag Mitigation using NAS Battery-based Standby Power Supply,"Advances in industrial automation and manufacturing systems led to the utilization of advanced systems comprise of computers, adjustable speed drives and various industrial control systems that are sensitive to power quality problem, mainly voltage sags. Voltage sag is the reduction of the voltage at a user position with duration of between one cycle and a few seconds caused by motor starting, short circuits and fast reclosing of circuit breakers that may cause expensive shutdowns to manufacturers. This paper proposes a Standby Power Supply (SPS) configuration using Sodium Sulfur (NAS) battery for voltage sag mitigation. An electrical battery model is developed for the NAS battery. Simulation results of the electrical battery model are compared and validated experimentally. In addition, voltage sag detection and power conversion system (PCS) model of the system is also described. Using the proposed NAS-SPS configuration system, power quality mitigation is simulated to observe its performance in protecting an important load. The simulation work described in this paper is carried out using EMTDC/PSCAD software tool. The research work was undertaken by TNB Research and Universiti Tenaga Nasional in collaboration with Tokyo Electric Power Company (TEPCO) in a study on the use of NAS battery-based SPS.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1619891,no,undetermined,0
Change Propagation for Assessing Design Quality of Software Architectures,"The study of software architectures is gaining importance due to its role in various aspects of software engineering such as product line engineering, component based software engineering and other emerging paradigms. With the increasing emphasis on design patterns, the traditional practice of ad-hoc software construction is slowly shifting towards pattern-oriented development. Various architectural attributes like error propagation, change propagation, and requirements propagation, provide a wealth of information about software architectures. In this paper, we show that change propagation probability (CP) is helpful and effective in assessing the design quality of software architectures. We study two different architectures (one that employs patterns versus one that does not) for the same application. We also analyze and compare change propagation metric with respect to other coupling-based metrics.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1620112,no,undetermined,0
Simulation of partial discharge propagation and location in Abetti winding based on structural data,"Power transformer monitoring as a reliable tool for maintaining purposes of this valuable asset of power systems has always comprised partial discharge offline measurements and online monitoring. The reason lies in non-destructive feature of PD monitoring. Partial discharge monitoring helps to detect incipient insulation faults and prevent insulation failure of power transformers. This paper introduces a software package developed based on structural data of power transformer and discusses the results of the simulation on Abetti winding, which might be considered as a basic layer winding. A hybrid model is used to model the transformer winding, which has been developed by first author. Firstly, winding is modeled by ladder network method to determine model parameters and then multi-conductor transmission line model is utilized to work out voltage and current vectors and study partial discharge propagation as well as its localization. Utilized method of modeling makes it possible to simulate a transformer winding over a frequency range from a few hundred kHz to a few tens of MHz. The results take advantage of accurate modeling method and provide a reasonable interpretation as to PD propagation and location studies",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1627196,no,undetermined,0
Medical device software standards,"Much medical device software is safety-related, and therefore needs to have high integrity (in other words its probability of failure has to be low.) There is a consensus that if you want to develop high-integrity software, you need a quality system. This is because software is a complex product that is easy to change and difficult to test, and the management system that handles these issues must include such quality system elements as: detailed traceable specifications, disciplined processes, planned verification and validation and a comprehensive configuration management and change control system. It is also agreed that software quality management systems need specific processes which are different from and additional to more general quality management systems such as that required by EN 13485. Historically, ISO 9000-3 Part 3: Guidelines for the application of ISO 9001:1994 to the development, supply, installation and maintenance of computer software states these additional processes very clearly, but is not mandatory. (This is now ISO 90003.) In both Europe and the USA there is therefore a gap in both regulations and standards for Medical Devices. There is no comprehensive requirement specifically for software development methods. In Europe, IEC 60601-1 Medical electrical equipment Part 1: General requirements for safety and essential performance, has specific requirements for software in section 14 Programmable Electrical Medical Systems (PEMS). This requires (at a fairly abstract level) some basic processes and documents, and includes an invocation of the risk management process of ISO 14971 Medical devices Application of risk management to medical devices In the US, there is an FDA regulation requiring Good Manufacturing Practice, with guidance on software development methods (strangely entitled Software Validat",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5679198,no,undetermined,0
A Case Study: GQM and TSP in a Software Engineering Capstone Project,"This paper presents a case study, describing the use of a hybrid version of the team software process (TSP) in a capstone software engineering project. A mandatory subset of TSP scripts and reporting mechanisms were required, primarily for estimating the size and duration of tasks and for tracking project status against the project plan. These were supplemented by metrics and additional processes developed by students. Metrics were identified using the goal-question-metric (GQM) process and used to evaluate the effectiveness of project management roles assigned to each member of the project team. TSP processes and specific TSP forms are identified as evidence of learning outcome attainment. The approach allowed for student creativity and flexibility and limited the perceived overhead associated with use of the complete TSP. Students felt that the experience enabled them to further develop and demonstrate teamwork and leadership skills. However, limited success was seen with respect to defect tracking, risk management, and process improvement. The case study demonstrates that the approach can be used to assess learning outcome attainment and highlights for students the significance of software engineering project management",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4698926,no,undetermined,0
Resource mapping and scheduling for heterogeneous network processor systems,"Task to resource mapping problems are encountered during (i) hardware-software co-design and (ii) performance optimization of Network Processor systems. The goal of the first problem is to find the task to resource mapping that minimizes the design cost subject to all design constraints. The goal of the second problem is to find the mapping that maximizes the performance, subject to all architectural constraints. To meet the design goals in performance, it may be necessary to allow multiple packets to be inside the system at any given instance of time and this may give rise to the resource contention between packets. In this paper, a Randomized Rounding (RR) based solution is presented for the task to resource mapping and scheduling problem. We also proposed two techniques to detect and eliminate the resource contention. We evaluate the efficacy of our RR approach through extensive simulation. The simulation results demonstrate that this approach produces near optimal solutions in almost all instances of the problem in a fraction of time needed to find the optimal solution. The quality of the solution produced by this approach is also better than often used list scheduling algorithm for task to resource mapping problem. Finally, we demonstrate with a case study, the results of a Network Processor design and scheduling problem using our techniques.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4675262,no,undetermined,0
On-line detection of stator winding faults in controlled induction machine drives,"The operation of induction machines with fast switching power electric devices puts additional stress on the stator windings what leads to an increased probability of machine faults. These faults can cause considerable damage and repair costs and - if not detected in an early stage - may end up in a total destruction of the machine. To reduce maintenance and repair costs many methods have been developed and presented in literature for an early detection of machine faults. This paper gives an overview of todaypsilas detection techniques and divides them into three major groups according to their underlying methodology. The focus will be on methods which are applicable to todaypsilas inverter-fed machines. In that case and especially if operated under controlled mode, the behavior of the machine with respect to the fault is different than for grid supply. This behavior is discussed and suitable approaches for fault detection are presented. Which method is eventually to choose, will depend on the application and the available sensors as well as hard- and software resources, always considering that the additional effort for the fault detection algorithm has to be kept as low as possible. The applicability of the presented fault detection techniques are also confirmed with practical measurements.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4662507,no,undetermined,0
PD diagnosis on medium voltage cables with Oscillating Voltage (OWTS),"Detecting, locating and evaluating of partial discharges (PD) in the insulating material, terminations and joints provides the opportunity for a quality control after installation and preventive detection of arising service interruption. A sophisticated evaluation is necessary between PD in several insulating materials and also in different types of terminations and joints. For a most precise evaluation of the degree and risk caused by PD it is suggested to use a test voltage shape that is preferably like the same under service conditions. Only under these requirements the typical PD parameters like inception and extinction voltage, PD level and PD pattern correspond to significant operational values. On the other hand the stress on the insulation should be limited during the diagnosis to not create irreversible damages and thereby worsening the condition of the test object. The paper introduces an Oscillating Wave Test System (OWTS), which meets these mentioned demands well. The design of the system, its functionality and especially the operating software are made for convenient field application. Field data and experience reports will be presented and discussed. This field data serve also as good guide for the level of danger to the different insulating systems due to partial discharges.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4524798,no,undetermined,0
Analysis of voltage dips in power system - case of study,"The knowledge about the occurrence of voltage dips in a network can be obtained through the use of power quality monitors or by means of prediction methods. For assessment of the number of voltage dips, the method of fault positions could be used. The method was applied to a real 110 kV grid. The common software for short-circuit analysis was used for calculation of remaining voltage in the network during the faults. All fault types were taken into account. The expected number of dips and distribution of remaining voltage in the chosen substation, and the exposed areas for this substation are calculated. The results were compared with the outcome of the voltage dip monitoring. Although the comparison shows a good correspondence, the accuracy of assessment could be very influenced by accuracy of the failure statistics.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4524616,no,undetermined,0
Failure's Identification for Electromechanical Systems with Induction Motor,"In this paper the new approach for guaranteed state estimation for electromechanical systems with induction motor is implemented for failure's identification in such systems. Approach is based on matrix comparison systems method, and a comparison with some other estimation methods was realized in package MATLAB. The experimental unit made for practical approbation of identification algorithms is presented with received experimental data. The new approach (matrix comparison systems method) for guaranteed state estimation of an induction motor (IM) and detect faults was presented in [1, 2]. State estimation is based on a discrete-time varying linear model of the IM [3, 4] with uncertainties and perturbations, which belong to known bounded sets with no hypothesis on their distribution inside these sets. The approach uses the measurement results. Recursive and explicit algorithm is presented and illustrated by example with real 1M parameters, realized in program package MATLAB.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4493241,no,undetermined,0
Automatic Generation of Test Sets for SBST of Microprocessor IP Cores,"Higher integration densities, smaller feature lengths, and other technology advances, as well as architectural evolution, have made microprocessor cores exceptionally complex. Currently, Software-Based Self-Test (SBST) is becoming an attractive test solution since it guarantees high fault coverage figures, runs at-speed, and matches core test requirements while exploiting low-cost ATEs. However, automatically generating test programs is still an open problem. This paper presents a novel approach for test program generation, that couples evolutionary techniques with hardware acceleration. The methodology was evaluated targeting a 5-stage pipelined processor implementing a SPARCv8 micro-processor core.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4286835,no,undetermined,0
A Constraint-Based Solution for On-Line Testing of Processors Embedded in Real-Time Applications,"Software-based self-test has been proposed as a low-cost strategy for on-line periodic testing of embedded processors. In this paper, we show that structural test programs composed only by regular deterministic self-test routines may be unfeasible in a real-time embedded platform. Hence, we propose a method to consciously select a set of test routines from different test approaches to compose a test program for an embedded processor. The proposed method not only ensures the periodical execution of the test, but also considers the optimization of memory and real-time requirements of the application, which are important constraints in embedded systems. Experimental results for a Java processor running real-time tasks demonstrate the effectiveness of the proposed solution",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4286834,no,undetermined,0
On the Effect of Ontologies on Quality of Web Applications,"The semantic Web can be seen as means to improve qualitative characteristics of Web applications. Ontologies play a key role in the semantic Web and, therefore, are expected to have a profound effect on quality of a Web application. We apply the Quint2 model to predict an impact of an ontology on a number of quality dimensions of a Web application. We estimate that an ontology is likely to significantly improve the functionality and maintainability dimensions. The usability dimension is affected to a lesser extent. We explain the expected increase of quality with improved effectiveness of development process caused primarily by application domain ontologies",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4145935,no,undetermined,0
Mobility Prediction with Direction Tracking on Dynamic Source Routing,"Mobility prediction is one the most efficient techniques used to calculate lifetime estimation of wireless links (link lifetime-LLT) for a more reliable path selection. To incorporate a mobility prediction model into ad hoc routing protocols, an updating scheme should be used to maintain prediction accuracy especially for those source initiated on-demand routing protocols where LLT values are continuously changing after the route discovery phase. In this paper, a light-weight direction tracking scheme is introduced to enhance performance of the mobility prediction based routing by reassuring the accuracy of LLT used for path selection process. The simulation results show an improvement in terms of average packet latency with minimum impact on number of control messages utilized.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4084877,no,undetermined,0
Harmonic calculation software for industrial applications with adjustable speed drives,This paper describes the evaluation of a new harmonic calculation software. By using a combination of a pre-stored database and new interpolation techniques the software can very fast provide the harmonic data on real applications. The harmonic results obtained with this software have acceptable precision even with limited input data. The evaluation concludes here that this approach is very practical compared to other advanced harmonic analysis methods. The results are supported by comparisons of calculations and measurements given in an industrial application,2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1665680,no,undetermined,0
On-demand overlay networking of collaborative applications,"We propose a new overlay network, called Generic Identifier Network (GIN), for collaborative nodes to share objects with transactions across affiliated organizations by merging the organizational local namespaces upon mutual agreement. Using local namespaces instead of a global namespace can avoid excessive dissemination of organizational information, reduce maintenance costs, and improve robustness against external security attacks. GIN can forward a query with an O(1) latency stretch with high probability and achieve high performance. In the absence of a complete distance map, its heuristic algorithms for self configuration are scalable and efficient. Routing tables are maintained using soft-state mechanisms for fault tolerance and adapting to performance updates of network distances. Thus, GIN has significant new advantages for building an efficient and scalable distributed hash table for modern collaborative applications across organizations",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1651211,no,undetermined,0
Collaborative sensing using uncontrolled mobile devices,"This paper considers how uncontrolled mobiles can be used to collaboratively accomplish sensing tasks. Uncontrolled mobiles are mobile devices whose movements cannot be easily controlled for the purpose of achieving a task. Examples include sensors mounted on mobile vehicles of people to monitor air quality and to detect potential airborne nuclear, biological, or chemical agents. We describe an approach for using uncontrolled mobile devices for collaborative sensing. Considering the potentially large number of mobile sensors that may be required to monitor a large geographical area such as a city, a key issue is how to achieve a proper balance between performance and costs. We present analytical results on the rate of information reporting by uncontrolled mobile sensors needed to cover a given geographical area. We also present results from testbed implementations to demonstrate the feasibility of using existing low-cost software technologies and platforms with existing standard protocols for information reporting and retrieval to support a large system of uncontrolled mobile sensors",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1651206,no,undetermined,0
Statistical similarity search applied to content-based video copy detection,"Content-based copy detection (CBCD) is one of the emerging multimedia applications for which there is a need of a concerted effort from the database community and the computer vision community. Recent methods based on interest points and local fingerprints have been proposed to perform robust CBCD of images and video. They include two steps: the search of similar fingerprints in the database and a voting strategy that merges all the local results in order to perform a global decision. In most image or video retrieval systems, the search of similar features in the database is performed by a geometrical query in a multidimensional index structure. Recently, the paradigm of approximate knearest neighbors query has shown that trading quality for time can be widely profitable in that context. In this paper, we introduce a new approximate search paradigm, called Statistical Similarity Search (S3), dedicated to local fingerprints and we describe the original indexing structure we have developped to compute efficiently the corresponding queries. The key-point relates to the distribution of the relevant fingerprints around a query. Since a video query can result from (a combination of ) more or less transformations of an original one, we modelize the distribution of the distorsion vector between a referenced fingerprint and a candidate one. Experimental results show that these statistical queries allow high performance gains compared to classical e-range queries. By studying the influence of this approximate search on a complete CBCD scheme based on local video fingerprints, we also show that trading quality for time during the search does not degrade seriously the global robustness of the system, even with very large databases including more than 20,000 hours of video.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1647776,no,undetermined,0
UVSD: software for detection of color underwater features,"Underwater Video Spot Detector (UVSD) is a software package designed to analyze underwater video for continuous spatial measurements (path traveled, distance to the bottom, roughness of the surface etc.) Laser beams of known geometry are often used in underwater imagery to estimate the distance to the bottom. This estimation is based on the manual detection of laser spots which is labor intensive and time consuming so usually only a few frames can be processed this way. This allows for spatial measurements on single frames (distance to the bottom, size of objects on the sea-bottom), but not for the whole video transect. We propose algorithms and a software package implementing them for the semi-automatic detection of laser spots throughout a video which can significantly increase the effectiveness of spatial measurements. The algorithm for spot detection is based on the support vector machines approach to artificial intelligence. The user is only required to specify on certain frames the points he or she thinks are laser dots (to train an SVM model), and then this model is used by the program to detect the laser dots on the rest of the video. As a result the precise (precision is only limited by quality of the video) spatial scale is set up for every frame. This can be used to improve video mosaics of the sea-bottom. The temporal correlation between spot movements changes and their shape provides the information about sediment roughness. Simultaneous spot movements indicate changing distance to the bottom; while uncorrelated changes indicate small local bumps. UVSD can be applied to quickly identify and quantify seafloor habitat patches, help visualize habitats and benthic organisms within large-scale landscapes, and estimate transect length and area surveyed along video transects.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1640089,no,undetermined,0
An Improved Algorithm for Deadlock Detection and Resolution in Mobile Agent Systems,"Mobile agent systems have been proved that are the best paradigm for distributed applications. They have potential advantages to provide a convenient, efficient and high performance distributed applications. Many solutions for problems in distributed systems such as deadlock detection rely on assumptions such as data location and message passing mechanism and static network topology that could not be applied for mobile agent systems. In this paper an improved distributed deadlock detection and resolution algorithm is proposed. The algorithm is based on Ashfield et. al. process. There are some cases in which original algorithm detects false deadlock or does not detect global deadlocks. The proposed algorithm eliminates the original algorithm deficiencies and improves its performance. It also minimizes the detection agent travels through the communication network. Also it has a major impact on improving performance of the mobile agent systems",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1631606,no,undetermined,0
A new wavelet-based method for detection of high impedance faults,"Detecting high impedance faults is one of the challenging issues for electrical engineers. Over-current relays can only detect some of the high impedance faults. Distance relays are unable to detect faults with impedance over 100 Omega. In this paper, by using an accurate model for high impedance faults, a new wavelet-based method is presented. The proposed method, which employs a 3 level neural network system, can successfully differentiate high impedance faults from other transients. The paper also thoroughly analyzes the effect of choice of mother wavelet on the detection performance. Simulation results which are carried out using PSCAD/EMTDC software are summarized",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1600519,no,undetermined,0
Simulation tools for damping in high frequency resonators,"This paper presents the development of HiQLab, a simulation tool to compute the effect of damping in high frequency resonators. Existing simulation tools allow designers to compute resonant frequencies but few tools provide estimates of damping, which is crucial in evaluating the performance of such devices. In the current code, two damping mechanisms: thermoelastic damping and anchor loss, have been implemented. Thermoelastic damping results from irreversible heat flow due to mechanically-driven temperature gradients, while anchor loss occurs when high-frequency mechanical waves radiate away from the resonator and into the substrate. Our finite-element simulation tool discretizes PDE models of both phenomena, and evaluates the quality factor (Q), a measure of damping in the system, with specialized eigencomputations and model reduction techniques. The core functions of the tool are written in C++ for performance. Interfaces are in Lua and MATLAB, which give users access to powerful visualization and pre- and postprocessing capabilities",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1597708,no,undetermined,0
A quantitative supplement to the definition of software quality,"This paper proposes a new quantitative definition for software quality. The definition is based on the Taguchi philosophy for assessing and improving the quality of manufacturing processes. The Taguchi approach, originally developed for manufacturing processes, define quality in terms of ""loss imparted to society "" by a product after delivery of the product to the end user. To facilitate the use of the Taguchi definition, several ""loss functions"" have been developed. These loss functions allow quality to be quantitatively measured in monetary values (e.g. US dollars). To illustrate the application of the Taguchi definition to a software product, examples that utilize some of the loss functions are presented. The proposed definition of software quality shows good correlation to other popular qualitative and quantitative definitions for software quality.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1563182,no,undetermined,0
A quality of service mechanism for IEEE 802.11 wireless network based on service differentiation,"This paper introduces an analytical model for wireless local area network with priority schemes based service differentiation. This model can predict station performance by access stations' number and traffic type before wireless channel condition changed. Then a new algorithm, DTCWF (dynamic tuning of contention window with fairness), is proposed to modify protocol options to limit end to end delay and loss rate of high priority traffic and maximize throughput of other traffics. Simulations validate this model and the comparison between DTCWF, DCF, and EDCA shows that our algorithm can improve quality of service for real-time traffic.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1566913,no,undetermined,0
Principles of timing anomalies in superscalar processors,"The counter-intuitive timing behavior of certain features in superscalar processors that cause severe problems for existing worst-case execution time analysis (WCET) methods is called timing anomalies. In this paper, we identify structural sources potentially causing timing anomalies in superscalar pipelines. We provide examples for cases where timing anomalies can arise in much simpler hardware architectures than commonly supposed (i.e., even in hardware containing only in-orderfunctional units). We elaborate the general principle behind timing anomalies and propose a general criterion (resource allocation criterion) that provides a necessary (but not sufficient) condition for the occurrence of timing anomalies in a processor. This principle allows to state the absence of timing anomalies for a specific combination of hardware and software and thus forms a solid theoretic foundation for the time-predictable execution of real-time software on complex processor hardware.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1579148,no,undetermined,0
A method of generating massive virtual clients and model-based performance test,"Testing the performance of a server that handles massive connections requires to generate massive virtual client connections and to model realistic traffic. In this paper, we propose a novel approach to generate massive virtual clients and realistic traffic. Our approach exploits the Windows I/O completion port (IOCP), which is the Windows NT operating system support for developing a scalable, high throughput server, and model-based testing scenarios. We describe implementation details of the proposed approach. Through analysis and experiments, we prove that the proposed method can predict and evaluate performance data more accurately in cost-effective way.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1579142,no,undetermined,0
Cost and response time simulation for Web-based applications on mobile channels,"When considering the addition of a mobile presentation channel to an existing Web-based application, a key question that has to be answered even before development begins is how the mobile channel's characteristics will impact the user experience and the cost of using the application. If either of these factors is outside acceptable limits, economical considerations may forbid adding the channels, even if it would be feasible from a purely technical perspective. Both of these factors depend considerably on two metrics: The time required to transmit data over the mobile network, and the volume transmitted. The PETTICOAT method presented in this paper uses the dialog flow model and Web server log files of an existing application to identify typical interaction sequences and to compile volume statistics, which are then run through a tool that simulates the volume and time that would be incurred by executing the interaction sequences on a mobile channel. From the simulated volume and time data, we can then calculate the cost of accessing the application on a mobile channel.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1579123,no,undetermined,0
A Distributed Architecture for Network Performance Measurement and Evaluation System,"Quality of Service (QoS) has now become a central issue in network design and application. This article describes a distributed architecture for linking geographically distributed network performance measurement and evaluation system (NPMES) based on in-depth research on the key techniques of network QoS. The prototype implementation of NPMES and the performance evaluation criteria based on performance aggregation are carefully introduced. Our contribution is presenting a new performance evaluation criteria based on performance aggregation to access QoS. Experiment results indicate the scalable NPMES can do the real-time detection on network performance of ISPs from the end userÂ’s perspective. The performance aggregation approach is a bran-new idea, and of a definite practicability.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1578959,no,undetermined,0
Formal verification of dead code elimination in Isabelle/HOL,"Correct compilers are a vital precondition to ensure software correctness. Optimizations are the most error-prone phases in compilers. In this paper, we formally verify dead code elimination (DCE) within the theorem prover Isabelle/HOL. DCE is a popular optimization in compilers which is typically performed on the intermediate representation. In our work, we reformulate the algorithm for DCE so that it is applicable to static single assignment (SSA) form which is a state of the art intermediate representation in modern compilers, thereby showing that DCE is significantly simpler on SSA form than on classical intermediate representations. Moreover, we formally prove our algorithm correct within the theorem prover Isabelle/HOL. Our program equivalence criterion used in this proof is based on bisimulation and, hence, captures also the case of non-termination adequately. Finally we report on our implementation of this verified DCE algorithm in the industrial-strength scale compiler system.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1575909,no,undetermined,0
Design of a software distributed shared memory system using an MPI communication layer,"We designed and implemented a software distributed shared memory (DSM) system, SCASH-MPI, by using MPI as the communication layer of the SCASH DSM. With MPI as the communication layer, we could use high-speed networks with several clusters and high portability. Furthermore, SCASH-MPI can use high-speed networks with MPI, which is the most commonly available communication library. On the other hand, existing software DSM systems usually use a dedicated communication layer, TCP, or UDP-Ethernet. SCASH-MPI avoids the need for a large amount of pin-down memory for shared memory use that has limited the applications of the original SCASH. In SCASH-MPI, a thread is created to support remote memory communication using MPI. An experiment on a 4-node Itanium cluster showed that the Laplace Solver benchmark using SCASH-MPI achieves a performance comparable to the original SCASH. Performance degradation is only 6.3% in the NPB BT benchmark Class B test. In SCASH-MPI, page transfer does not start until a page fault is detected. To hide the latency of page transmission, we implemented a prefetch function. The latency in BT Class B was reduced by 64% when the prefetch function was used.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1575830,no,undetermined,0
Robust resource allocations in parallel computing systems: model and heuristics,"The resources in parallel computer systems (including heterogeneous clusters) should be allocated to the computational applications in a way that maximizes some system performance measure. However, allocation decisions and associated performance prediction are often based on estimated values of application and system parameters. The actual values of these parameters may differ from the estimates; for example, the estimates may represent only average values, the models used to generate the estimates may have limited accuracy, and there may be changes in the environment. Thus, an important research problem is the development of resource management strategies that can guarantee a particular system performance given such uncertainties. To address this problem, we have designed a model for deriving the degree of robustness of a resource allocation-the maximum amount of collective uncertainty in system parameters within which a user-specified level of system performance (QoS) can be guaranteed. The model is presented and we demonstrate its ability to select the most robust resource allocation from among those that otherwise perform similarly (based oh the primary performance criterion). The model's use in allocation heuristics is also demonstrated. This model is applicable to different types of computing and communication environments, including parallel, distributed, cluster; grid, Internet, embedded, and wireless.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1575796,no,undetermined,0
Performance evaluation of agent-based material handling systems using simulation techniques,"The increasing influence of global economy is changing the conventional approach to managing manufacturing companies. Real-time reaction to changes in shop-floor operations, quick and quality response in satisfying customer requests, and reconfigurability in both hardware equipment and software modules, are already viewed as essential characteristics for next generation manufacturing systems. Part of a larger research that employs agent-based modeling techniques in manufacturing planning and control, this work proposes an agent-based material handling system and contrasts the centralized and decentralized scheduling approaches for allocation of material handling operations to the available resources in the system. To justify the use of the decentralized agent-based approach and assess its performance compared to conventional scheduling systems, a series of validation tests and a simulation study are carried out. As illustrated by the preliminary results obtained in the simulation study the decentralized agent-based approach can give good feasible solutions in a short amount of time.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1574354,no,undetermined,0
The physical topology discovery for switched Ethernet based on connections reasoning technique,"Accurate and up-to-date knowledge of topology serves as the base of a number of network management functions, such as performance monitoring and evaluation, fault detection and location, resource allocation and etc. Firstly the main achievements in topology discovery for LAN are introduced and the defects of those methods are pointed out in this paper. Then the basic theories for connections reasoning technique (CRT) based on the predication logic are proposed. This technique translates the topology discovery into a math problem of logic reasoning, so that topology of LAN can be studied by mathematic tools. A new algorithm for topology discovery based on this mechanism is proposed in this paper. Compared with current discovery algorithms, this method excels in: 1) making use of AFT more effectively in topology discovery, so the whole topology can be build up by just a part of AFTs; and 2) naturally resolving the problem of topology discovery for multiple subnet switched domain.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1566795,no,undetermined,0
Reduced parallel anode readout for 256 ch flat panel PMT,"SPECT and PET need a good pixel identification to obtain high quality images. This is why new generations of PSPMT with anode array with smaller step have been developed till the recent flat panel PMT H9500 with 2"" square area, 256 anodes array, 3 mm individual pitch. The realization of electronic chains with so high number channels demands dedicated electronics readout with high cost and high management difficulty. In this work we propose a new method of anode number reduction in parallel readout limiting the total chain number to 64. Starting from the evidence that event charge distribution is always contained inside a portion of the FP PMT anodic plane, we assume that only a quarter of the anodes are involved in the detection. Our approach consists on virtually dividing 16times16 anodes in 4 arrays with 64 anodes per each. Once the charge distribution is collected by the 4 anodic planes, each individual anodic charge is projected on one plane. Physically, each i,j-element of one quadrant is associated to the corresponding i,j-element of the other three matrices. In terms of hardware, it is simply realized connecting one to each other the set of four i,j-anodes of the 4 quadrants. In this work an image reconstruction software has been developed and tested by measured charge distribution collected by 256 anodes Hamamatsu FP PMT. The final results, in term of spatial resolution and position linearity, are in agreement with ones collected by the total number of anodes",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1596951,no,undetermined,0
Determining inspection cost-effectiveness by combining project data and expert opinion,"There is a general agreement among software engineering practitioners that software inspections are an important technique to achieve high software quality at a reasonable cost. However, there are many ways to perform such inspections and many factors that affect their cost-effectiveness. It is therefore important to be able to estimate this cost-effectiveness in order to monitor it, improve it, and convince developers and management that the technology and related investments are worth while. This work proposes a rigorous but practical way to do so. In particular, a meaningful model to measure cost-effectiveness is proposed and a method to determine cost-effectiveness by combining project data and expert opinion is described. To demonstrate the feasibility of the proposed approach, the results of a large-scale industrial case study are presented and an initial validation is performed.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1566608,no,undetermined,0
"TRUSS: a reliable, scalable server architecture","Traditional techniques that mainframes use to increase reliability -special hardware or custom software - are incompatible with commodity server requirements. The Total Reliability Using Scalable Servers (TRUSS) architecture, developed at Carnegie Mellon, aims to bring reliability to commodity servers. TRUSS features a distributed shared-memory (DSM) multiprocessor that incorporates computation and memory storage redundancy to detect and recover from any single point of transient or permanent failure. Because its underlying DSM architecture presents the familiar shared-memory programming model, TRUSS requires no changes to existing applications and only minor modifications to the operating system to support error recovery.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1566557,no,undetermined,0
An experimental study of soft errors in microprocessors,"The issue of soft errors is an important emerging concern in the design and implementation of future microprocessors. The authors examine the impact of soft errors on two different microarchitectures: a DLX processor for embedded applications and a high-performance alpha processor. The results contrast impact of soft errors on combinational and sequential logic, identify the most vulnerable units, and assess soft error impact on the application.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1566554,no,undetermined,0
Estimating the costs of a reengineering project,"Accurate estimation of project costs is an essential prerequisite to making a reengineering project. Existing systems are usually reengineered because it is cheaper to reengineer them than to redevelop or to replace them. However, to make this decision, management must know what the reengineering will cost. This contribution describes an eight step tool supported process for calculating the time and the costs required to reengineer an existing system. The process is derived from the author's 20 year experience in estimating reengineering projects and has been validated by several real life field experiments in which it has been refined and calibrated",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1566151,no,undetermined,0
Congestion targeted reduction of quality of service DDoS attacking and defense scheme in mobile ad hoc networks,"In the multimedia applications over the mobile ad hoc networks, the goodput and delay performance of UDP packets is very sensitive to the congestion targeted DDoS attacking. In this paper, we analyze this type of attacking in details for the first time to our knowledge. We figure out the principles of the attacking from the analysis of network capacity and classify the attacking into four categories: pulsing attacking, round robin attacking, self-whisper attacking and flooding attacking. The defense schemes are also proposed, which includes the detection of RTS/CTS packets, signal interference frequency and retransmission time and response stage with ECN marking mechanism. Through the ns2 simulation, the basic attacking scenario is discussed and the results show that the attacking leads to the great jitter of the goodput and delay in pulsing attacking mode. The delay increasing (up to 110 times in five attacking flows) and goodput decreasing (to 77.42% in 5 attacking flows) is obvious with the addition of attacking flows.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1565905,no,undetermined,0
Automated testing of multiple redundant electrohydraulic servo-actuators and their test sets,"Testing is an important activity in the design and development of any aircraft. Testing processes can be automated to reduce the diagnostic times and to improve accuracy, which results in faster detection of faults. Thus, higher throughput for testing the unit under test (UUT) is achieved. This paper is developed for automated testing of high performance, multiple redundant electro-hydraulic servo-actuators and their test sets through software with the help of data acquisition cards. Actuator test set (ATS) is special to type test equipment (STTE) used for testing the servo-actuators of a typical fly-by-wire (FBW) aircraft. At first, the testing procedure of ATS is automated before testing the servo-actuators by interfacing personal computer (PC) with ATS through digital to analog (D/A) and analog to digital (A/D) converters and then implementing pre-flight built-in-test (PBIT) algorithm on the servo-actuators through ATS which makes the evaluation and analysis of the performance of the servo-actuators easy and simple.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1563387,no,undetermined,0
Design of SPICE experience factory model for accumulation and utilization of process assessment experience,"With growing interest in software process improvement (SPI), many companies are introducing international process models and standards. SPICE is most widely used process assessment model in the SPI work today. In the process of introducing and applying SPICE, practical experiences contribute to enhancing the project performance. The experience helps people to make decisions under uncertainty, and to find better compromises. This paper suggests a SPICE experience factory (SEF) model to use SPICE assessment experience. For this, we collected SPICE assessment results which were conducted in Korea from 1999 to 2004. The collected data does not only contain rating information but also specifies strengths and improvement point for each assessed company and its process. To use this assessment result more efficiently, root words were derived from each result items. And root words were classified into four: 1) measurement, 2) work product, 3) process performance, and 4) process definition and deployment. Database was designed and constructed to save all analyzed data in forms of root words. Database also was designed to efficiently search information the organization needs by strength/improvement point, or root word for each level. This paper describes procedures of SEF model and presents methods to utilize it. By using the proposed SEF model, even organizations which plan to undergo SPICE assessment for the first time can establish the optimal improvement strategies.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1563185,no,undetermined,0
Design of opportunity tree framework for effective process improvement based on quantitative project performance,"Nowadays IT industry drives to improve the software process on marketing and financial benefits. For efficient process improvement, work performance should be enhanced in line with organization's vision by identifying weakness for improvement and risks with process assessment results and then mapping them in the software development environment. According to organization's vision, plans should be developed for marketing and financial strategic objectives. For each plan, improvement strategies should be developed for each work performance unit such as quality, delivery, cycle time, and waste. Process attributes in each unit should be identified and improvement methods shall be determined for them. In order to suggest a PPM (project performing measure) model to quantitatively measure organization's project performing capability and make an optimal decision for process improvement, this paper statistically analyzes SPICE assessment results of 2,392 weakness for improvement by process for 49 appraisals and 476 processes which were assessed through KASPA (Korea Association of Software Process Assessors) from 1999 to 2004 and then makes SEF (SPICE experience factory). It also presents scores on project performing capability and improvement effects by level, and presents weakness for improvement by priority in the performance unit by level. And finally, this paper suggests an OTF (opportunity tree framework) model to show optimal process improvement strategies.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1563184,no,undetermined,0
A data mining-based framework for grid workflow management,"In this paper we investigate on the exploitation of data mining techniques to analyze data coming from the enactment of workflow-based processes in a service-oriented grid infrastructure. The extracted knowledge allows users to better comprehend the behavior of the enacted processes, and can be profitably exploited to provide advanced support to several phases in the life-cycle of workflow processes, including (re-)design, matchmaking, scheduling and performance monitoring. To this purpose, we focus on recent data mining techniques specifically aimed at enabling refined analyzes of workflow executions. Moreover, we introduce a comprehensive system architecture that supports the management of grid workflows by fully taking advantage of such mining techniques.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1579156,no,undetermined,0
Towards a metamorphic testing methodology for service-oriented software applications,"Testing applications in service-oriented architecture (SOA) environments needs to deal with issues like the unknown communication partners until the service discovery, the imprecise black-box information of software components, and the potential existence of non-identical implementations of the same service. In this paper, we exploit the benefits of the SOA environments and metamorphic testing (MT) to alleviate the issues. We propose an MT-oriented testing methodology in this paper. It formulates metamorphic services to encapsulate services as well as the implementations of metamorphic relations. Test cases for the unit test phase is proposed to generate follow-up test cases for the integration test phase. The metamorphic services invoke relevant services to execute test cases and use their metamorphic relations to detect failures. It has potentials to shift the testing effort from the construction of the integration test sets to the development of metamorphic relations.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1579174,no,undetermined,0
Energy/power estimation for LDPC decoders in software radio systems,"Low-density parity-check (LDPC) codes are advanced error-correcting codes with performance approaching the Shannon limit. Although many LDPC code decoding algorithms have been proposed, no detailed comparison in energy/power consumption had ever been reported. This paper presents the performance and energy/power consumption trade-off for different LDPC decoding algorithms and proposes an efficient method to estimate energy consumption. We also propose a joint power management scheme for transmitter and receiver to save receiver energy while maintaining same communication quality by properly delivering more transmit power.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1579837,no,undetermined,0
Improved Predictive Current Controlled PWM for Single-Phase Grid-Connected Voltage Source Inverters,"Inverter-based distributed generators (DG) must meet the power quality requirements set by interconnection standards such as IEEE Standard 1547 before DGs are allowed to interconnect with existing electric power systems. The power quality is highly dependent on the control strategies of the DG inverters. Traditional predictive current controller can precisely control the load current with low distortions, however, has a poor performance under component parameter variations. An improved predictive current controller has been developed by the authors for single-phase grid-connected voltage source inverters (VSI). Aiming to overcoming the drawbacks of the traditional predictive controller, a scheme for improving the robustness of inverter system is proposed along with a dual-timer control strategy and a software phase-lock-loop (PLL). The controller is designed not only to minimize the control error introduced by the control delay but also to provide a faster response for over-current protection. The simulation and experiment results show that the improved predictive controller has a superior performance to the traditional predictive controller, particularly under parameter variations. The single-phase grid-connected VSI implemented with the proposed predictive controller has shown very low current THD in both laboratory tests and in field operation of a small wind turbine system",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1581629,no,undetermined,0
The ATLAS SCT: from end-cap module assembly to detector operation,"The semiconductor tracker (SCT) forms part of the ATLAS inner detector and comprises 4088 silicon microstrip detector modules. It will measure charged particle tracks produced in the 14 TeV proton-proton collisions at the LHC by providing four space point measurements within a pseudorapidity range of |eta|<2.5. The design and assembly of the 2000 recently completed modules for the end-caps of the SCT are reported here. The procedures used for their quality assurance are described and the main results presented, showing a low loss rate during assembly of only 7%, half the allowed contingency. The 2004 combined test beam is then used to illustrate how the DAQ and offline software are being prepared for ATLAS operation",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1596442,no,undetermined,0
The detector control system of the LHCb RICH detector,"The LHCb experiment at the Large Hadron Collider (LHC) is dedicated to the study of b-quark properties. A key element of the LHCb detector is particle identification, a task performed by the ring imaging Cherenkov (RICH) subsystem. Efficient particle identification over the full momentum range of 1 to 100 GeV/c requires an extensive system of detector control and monitoring. The RICH detector control system (DCS) monitors environmental parameters which directly affect RICH performance, such as temperature, pressure and humidity. Quality of the RICH radiator gas is monitored through the DCS using ultrasound and Fabry-Perot techniques. The DCS also monitors and controls low and high voltages of the RICH photodetector and the readout electronics systems, as well as providing overall detector safety. Monitoring of the detector alignment will be performed with a laser and CCD system. Starting from the physics requirements, the RICH DCS hardware devices and software tools are described. An account is then given of system integration into the overall LHCb DCS framework",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1596368,no,undetermined,0
The use of optimal filters to track parameters of performance models,"Autonomic computer systems react to changes in the system, including failures, load changes, and changed user behaviour. Autonomic control may be based on a performance model of the system and the software, which implies that the model should track changes in the system. A substantial theory of optimal tracking filters has a successful history of application to track parameters while integrating data from a variety of sources, an issue which is also relevant in performance modeling. This work applies extended Kalman filtering to track the parameters of a simple queueing network model, in response to a step change in the parameters. The response of the filter is affected by the way performance measurements are taken, and by the observability of the parameters.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1595783,no,undetermined,0
Uncertainty in optical measurement applications: a case study,"The uncertainty related to a measurement is at least as important as the measurement itself. Apart from being able to determine intervals of confidence around the final result within which the true measurement value is expected to lie at a certain level of confidence, the rigorous treatment of uncertainty throughout an algorithm allows to increase its robustness against disturbing influences and to judge an its applicability to a given task. This paper addresses the propagation of uncertainty within a quality control application using image based sensors. Simulations and real-world results are provided to show the applicability of the proposed application",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1594610,no,undetermined,0
Novel usage of wavelets as basis in RDNN for telecommunication applications,The paper explores the use of wavelet basis recurrent dynamic neural network (RDNN) to improve the estimation of reliability growth of communication network's software. The presented RDNN handles noise contaminated data and provides enhanced speed and performance of the system as compared with alternate approaches. Nonlinearity of the system is represented by proper selection of the wavelet function. The integrated defect tracking model parameters and the data are fed to the RDNN and the network is trained for optimizing the defect tracking model performance. The designed system will assist the service release management in obtaining more effective risk reduction. Using wavelet basis for RDNN requires only 10 iterations compared to 200 iterations with hump as basis function. It also reduces the maximum percentage error from 88% to 7.69% in the expected outputs. This also improves the telecommunication system defect tracking and network deployment,2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1594500,no,undetermined,0
Implementation of a digital receiver for DS-CDMA communication systems using HW/SW codesign,"An optimized partitioning algorithm for HW/SW codesign is described and applied to the implementation of a digital RAKE receiver for DS-CDMA on an heterogeneous (hardware/software) platform. The development of this partitioning algorithm aims to gather quality features that have already proved effective when applied separately, but had not been used together yet. The partitioning algorithm is applied to a flexible RAKE architecture able to adapt its number of demodulation fingers to each propagation environment. This RAKE detector uses a serial and iterative computation of the required processing in order to jointly optimize performance and computational power",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1594169,no,undetermined,0
VRM: a failure-aware grid resource management system,"For resource management in grid environments, advance reservations turned out to be very useful and hence are supported by a variety of grid toolkits. However, failure recovery for such systems has not yet received the attention it deserves. In this paper, we address the problem of remapping reservations to other resources, when the originally selected resource fails. Instead of dealing with jobs already running, which usually means checkpointing and migration, our focus is on jobs that are scheduled on the failed resource for a specific future period of time but not started yet. The most critical factor when solving this problem is the estimation of the downtime. We avoid the drawbacks of under- or overestimating the downtime by a dynamic load-based approach that is evaluated by extensive simulations in a grid environment and shows superior performance compared to estimation-based approaches.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1592576,no,undetermined,0
Spectral gamma detectors for hand-held radioisotope identification devices (RIDs) for nuclear security applications,"The in-situ identification of the isotope causing a radiation alarm at a border crossing is essential for a quick resolution of the case. Commonly, hand-held radioisotope identification devices (RIDs) are used to address this task. Although a number of such devices are commercially available, there is still a gap between the requirements of the users and what is actually available. Two components are essential for the optimal performance of such devices: i) The quality of the raw data-the gamma spectra, and ii) isotope identification software matching the specifics of a certain gamma detector. In this paper, we investigate new spectral gamma detectors for potential use in hand-held radioisotope identification devices. The standard NaI detector is compared with new scintillation and room temperature semiconductor detectors. The following spectral gamma detectors were included into the study: NaI(Tl), LaCl<sub>3</sub>(Ce), <sup>6</sup>LiI(Eu), CdWO<sub>4</sub>, hemispheric and coplanar CdZnTe detectors. In the paper basic spectrometric properties are measured and compared in view of their use in RIDs",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1589330,no,undetermined,0
New approach for selfish nodes detection in mobile ad hoc networks,"A mobile ad hoc network (MANET) is a temporary infrastructureless network, formed by a set of mobile hosts that dynamically establish their own network on the fly without relying on any central administration. Mobile hosts used in MANET have to ensure the services that were ensured by the powerful fixed infrastructure in traditional networks, the packet forwarding is one of these services. The resource limitation of nodes used in MANET, particularly in energy supply, along with the multi-hop nature of this network may cause new phenomena which do not exist in traditional networks. To save its energy a node may behave selfishly and uses the forwarding service of other nodes without correctly forwarding packets for them. This deviation from the correct behavior represents a potential threat against the quality of service (QoS), as well as the service availability, one of the most important security requirements. Some solutions have been recently proposed, but almost all these solutions rely on the watchdog technique as stated in S. Marti et al. (2000) in their monitoring components, which suffers from many problems. In this paper we propose an approach to mitigate some of these problems, and we assess its performance by simulation.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1588323,no,undetermined,0
Performance Model Building of Pervasive Computing,"Performance model building is essential to predict the ability of an application to satisfy given levels of performance or to support the search for viable alternatives. Using automated methods of model building is becoming of increasing interest to software developers who have neither the skills nor the time to do it manually. This is particularly relevant in pervasive computing, where the large number of software and hardware components requires models of so large a size that using traditional manual methods of model building would be error prone and time consuming. This paper deals with an automated method to build performance models of pervasive computing applications, which require the integration of multiple technologies, including software layers, hardware platforms and wired/wireless networks. The considered performance models are of extended queueing network (EQN) type. The method is based on a procedure that receives as input the UML model of the application to yield as output the complete EQN model, which can then be evaluated by use of any evaluation tool.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1587694,no,undetermined,0
Application of a Robust and Efficient ICP Algorithm for Fitting a Deformable 3D Human Torso Model to Noisy Data,"We investigate the use of an iterative closest point (ICP) algorithm in the alignment of a point distribution model (PDM) of 3D human female torsos to sample female torso data. An approximate k-d tree procedure for efficient ICP is tested to assess whether it improves the speed of the alignment process. The use of different error norms, namely Lâ‚?and Lâ‚? are compared to ascertain if either offers an advantage in terms of convergence and in the quality of the final fit when the sample data is clean, noisy or has some data missing. It is found that the performance of the ICP algorithm used is improved in both speed of convergence and accuracy of fit through the combined use of an approximate and exact k-d tree search procedure and with the minimisation of the Lâ‚?norm even when up to 50% of the data is noisy or up to 25% is missing. We demonstrate the use of this algorithm in providing, via a fitted torso PDM, smooth surfaces for noisy torso data and valid data points for torsos with missing data.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1587664,no,undetermined,0
The case for outsourcing DFT,"The author discusses about outsourcing analog/mixed-signal DFT. At present we still lack a ""SAF"" metric for measuring analog IC fault coverage, as most analog faults that are found by testing are of a parametric variety, and can not be measured or scored (as in the SAF coverage grade) by using Boolean techniques. To analyze analog and mixed-signal (A/MS) logic for testability, one has to know what the analog failures are that need to be detected, what the capability of the test equipment will be for these measurements, what the error or repeatability will be, and what the trade off is going to be between increased test accuracy and test time",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1584134,no,undetermined,0
Computational intelligence based testing for semiconductor measurement systems,"This paper describes a computational intelligence-based software configuration implemented on semiconductor automatic test equipment (ATE) and how we can improve our design (e.g. memory test chip) based on such a method. The purpose of this unique software configuration incorporating neural network, genetic-algorithm and other artificial intelligence technologies is to enhance ATE capability and efficiency by providing an intelligent interface for a variety of functions that are controlled or monitored by the software. This includes automated and user directed control of the ATE and a diagnostic strategy to streamline test sequences and specific combinations of test conditions through the use of advanced diagnostic strategies. Such methods can achieve greater accuracy in failure diagnosis and fault prediction; and improve confidence in circuit performance testing that result in the determination of a DUT (device under test) status",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1584056,no,undetermined,0
JTAG-based vector and chain management for system test,"We present an embedded boundary-scan test vector management solution that ensures the correct version of test vectors is applied to the unit under test (UUT). This new vector management approach leverages the system-level boundary-scan multi-drop architecture employed in some high availability electronic systems. Compared to previous methods that do not use system-level boundary-scan resources for vector management, this new approach ensures that the correct boundary-scan vectors are retrieved from the UUT without affecting its operation and requires minimal UUT functionality to execute. The performance and effectiveness of this technique is shown using an actual design example that has been realized in a new product",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1584041,no,undetermined,0
Asymptotic Performance of a Multichart CUSUM Test Under False Alarm Probability Constraint,"Traditionally the false alarm rate in change point detection problems is measured by the mean time to false detection (or between false alarms). The large values of the mean time to false alarm, however, do not generally guarantee small values of the false alarm probability in a fixed time interval for any possible location of this interval. In this paper we consider a multichannel (multi-population) change point detection problem under a non-traditional false alarm probability constraint, which is desirable for a variety of applications. It is shown that in the multichart CUSUM test this constraint is easy to control. Furthermore, the proposed multichart CUSUM test is shown to be uniformly asymptotically optimal when the false alarm probability is small: it minimizes an average detection delay, or more generally, any positive moment of the stopping time distribution for any point of change.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1582175,no,undetermined,0
Improved Current Controller Based on SVPWM for Three-phase Grid-connected Voltage Source Inverters,"Space vector pulse-width modulation (SVPWM) has been widely employed for the current control of three-phase voltage source inverters (VSI). However, in grid-connected distributed generation (DG) systems, SVPWM also brings drawbacks to current controllers, such as the compromised output current due to the grid harmonic disturbance and nonlinearity of the system, the lack of inherent over-current protection, etc. In this paper, an improved current controller has been developed for three-phase grid-connected VSIs. A grid harmonic feed-forward compensation method is employed to depress the grid disturbance. Based on a dual-timer sampling scheme, a software predictor and filter is proposed for both the current feedback loop and grid voltage feed-forward loop to eliminate the system nonlinearity due to the control delay. Both simulation and experimental results have verified the superior performance of the proposed current controller",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1582047,no,undetermined,0
Frequency-Adjustable Positive Sequence Detector for Power Conditioning Applications,"This paper proposes a novel and simple positive sequence detector (PSD), which is inherently self-adjustable to fundamental frequency deviations by means of a software-based PLL (phase locked loop). Since the proposed positive sequence detector is not based on Fortescue's classical decomposition and no special input filtering is needed, its dynamic response may be as fast as one fundamental cycle. The digital PLL ensures that the positive sequence components can be calculated even under distorted waveform conditions and fundamental frequency deviations. For the purpose of validating the proposed models, the positive sequence detector has been implemented in a PC-based power quality monitor and experimental results illustrate its good performance. The PSD algorithm has also been evaluated in the control loop of a series active filter and simulation results demonstrate its effectiveness in a closed-loop system. Moreover, considering single-phase applications, this paper also proposes a general single-phase PLL and a fundamental wave detector (FWD) immune to frequency variations and waveform distortions",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1581895,no,undetermined,0
Experiences from conducting semi-structured interviews in empirical software engineering research,"Many phenomena related to software development are qualitative in nature. Relevant measures of such phenomena are often collected using semi-structured interviews. Such interviews involve high costs, and the quality of the collected data is related to how the interviews are conducted. Careful planning and conducting of the interviews are therefore necessary, and experiences from interview studies in software engineering should consequently be collected and analyzed to provide advice to other researchers. We have brought together experiences from 12 software engineering studies, in which a total of 280 interviews were conducted. Four areas were particularly challenging when planning and conducting these interviews; estimating the necessary effort, ensuring that the interviewer had the needed skills, ensuring good interaction between interviewer and interviewees, and using the appropriate tools and project artifacts. The paper gives advice on how to handle these areas and suggests what information about the interviews should be included when reporting studies where interviews have been used in data collection. Knowledge from other disciplines is included. By sharing experience, knowledge about the accomplishments of software engineering interviews is increased and hence, measures of high quality can be achieved",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1509301,no,undetermined,0
An industrial case study of implementing and validating defect classification for process improvement and quality management,"Defect measurement plays a crucial role when assessing quality assurance processes such as inspections and testing. To systematically combine these processes in the context of an integrated quality assurance strategy, measurement must provide empirical evidence on how effective these processes are and which types of defects are detected by which quality assurance process. Typically, defect classification schemes, such as ODC or the Hewlett-Packard scheme, are used to measure defects for this purpose. However, we found it difficult to transfer existing schemes to an embedded software context, where specific document- and defect types have to be considered. This paper presents an approach to define, introduce, and validate a customized defect classification scheme that considers the specifics of an industrial environment. The core of the approach is to combine the software engineering know-how of measurement experts and the domain know-how of developers. In addition to the approach, we present the results and experiences of using the approach in an industrial setting. The results indicate that our approach results in a defect classification scheme that allows classifying defects with good reliability, that allows identifying process improvement actions, and that can serve as a baseline for evaluating the impact of process improvements",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1509297,no,undetermined,0
Assessing the impact of coupling on the understandability and modifiability of OCL expressions within UML/OCL combined models,"Diagram-based UML notation is limited in its expressiveness thus producing a model that would be severely underspecified. The flaws in the limitation of the UML diagrams are solved by specifying UML/OCL combined models, OCL being an essential add-on to the UML diagrams. Aware of the importance of building precise models, the main goal of this paper is to carefully describe a family of experiments we have undertaken to ascertain whether any relationship exists between object coupling (defined through metrics related to navigations and collection operations) and two maintainability sub-characteristics: understandability and modifiability of OCL expressions. If such a relationship exists, we will have found early indicators of the understandability and modifiability of OCL expressions. Even though the results obtained show empirical evidence that such a relationship exists, they must be considered as preliminaries. Further validation is needed to be performed to strengthen the conclusions and external validity",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1509292,no,undetermined,0
The real-time computing model for a network based control system,"This paper studies a network based real-time control system, and proposes to model this system as a periodic real-time computing system. With efficient scheduling algorithms and software fault-tolerance deadline mechanism, this model proves that the system can meet its task timing constraints while tolerating system faults. The simulation study shows that in cases with high failure probability, the lower priority tasks suffer a lot in completion rate. In cases with low failure probability, this algorithm works well with both high priority and lower priority task. This conclusion suggests that an Internet based control system should manage to keep the failure rate to the minimum to achieve a good system performance.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1468843,no,undetermined,0
Finding predictors of field defects for open source software systems in commonly available data sources: a case study of OpenBSD,"Open source software systems are important components of many business software applications. Field defect predictions for open source software systems may allow organizations to make informed decisions regarding open source software components. In this paper, we remotely measure and analyze predictors (metrics available before release) mined from established data sources (the code repository and the request tracking system) as well as a novel source of data (mailing list archives) for nine releases of OpenBSD. First, we attempt to predict field defects by extending a software reliability model fitted to development defects. We find this approach to be infeasible, which motivates examining metrics-based field defect prediction. Then, we evaluate 139 predictors using established statistical methods: Kendall's rank correlation, Pearson's rank correlation, and forward AIC model selection. The metrics we collect include product metrics, development metrics, deployment and usage metrics, and software and hardware configurations metrics. We find the number of messages to the technical discussion mailing list during the development period (a deployment and usage metric captured from mailing list archives) to be the best predictor of field defects. Our work identifies predictors of field defects in commonly available data sources for open source software systems and is a step towards metrics-based field defect prediction for quantitatively-based decision making regarding open source software components",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1509310,yes,undetermined,0
Enabling SMT for real-time embedded systems,"In order to deal with real time constraints, current embedded processors are usually simple in-order processors with no speculation capabilities to ensure that execution times of applications are predictable. However, embedded systems require ever more compute power and the trend is that they will become as complex as current high performance systems. SMTs are viable candidates for future high performance embedded processors, because of their good cost/performance trade-off. However, current SMTs exhibit unpredictable performance. Hence, the SMT hardware needs to be adapted in order to meet real time constraints. This paper is a first step toward the use of high performance SMT processors in future real time systems. We present a novel collaboration between OS and SMT processors that entails that the OS exercises control over how resources are shared inside the processor. We illustrate this collaboration by a mechanism in which the OS cooperates with the SMT hardware to guarantee that a given thread runs at a specific speed, enabling SMT for real-time systems.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7079809,no,undetermined,0
Evolving legacy systems through a multi-objective decision process,"Our previous work on improving the quality of object-oriented legacy systems includes: i) devising a quality-driven re-engineering framework (L. Tahvildari et al., 2003); ii) proposing a software transformation framework based on soft-goal interdependency graphs to enhance quality (L. Tahvildari and K. Kontogiannis, 2002); and iii) investigating the usage of metrics for detecting potential design flaws (L. Tahvildari and K. Kontogiannis, 2004). This paper defines a decision making process that determines a list of source-code improving transformations among several applicable transformations. The decision-making process is developed on a multi-objective decision analysis technique. This type of technique is necessary as there are a number of different, and sometimes conflicting, criterion among non-functional requirements. For the migrant system, the proposed approach uses heuristic estimates to guide the discovery process",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1613372,no,undetermined,0
Bandwidth adaptive multimedia streaming for PDA applications over WLAN environment,"Network management and bandwidth adaptation are important areas for wireless multimedia streaming applications. We have developed the managed wireless network with bandwidth adaptive media streaming technique for IEEE 802.11x applications. The unique contributions of this project are: (1) a real-time detection of wireless bandwidth change via XML/Java based SNMP system, (2) dynamic adaptation of streaming media to optimize the quality, and (3) coordination and performance optimization among SNMP server, media streaming server, wireless AP points, and wireless clients. Based on this technique, we are able to deliver wireless streaming services adapted to the bandwidth change. The SNMP server, media-streaming server, and wireless AP points are coupled and operated in a well-coordinated fashion. The software implementation was written in the combination of Java, XML and C#. The bandwidth adaptation is user transparent while the media streaming settings are modified on the fly. The experiments are conducted and they confirmed our design.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1571876,no,undetermined,0
A Case for Clumsy Packet Processors,"Hardware faults can occur in any computer system. Although faults cannot be tolerated for most systems (e.g., servers or desktop processors), many applications (e.g., networking applications) provide robustness in software. However, processors do not utilize this resiliency, i.e., regardless of the application at hand, a processor is expected to operate completely fault-free. In this paper, we will question this traditional approach of complete correctness and investigate possible performance and energy optimizations when this correctness constraint is released. We first develop a realistic model that estimates the change in the fault rates according to the clock frequency of the cache. Then, we present a scheme that dynamically adjusts the clock frequency of the data caches to achieve the desired optimization goal, e.g., reduced energy or reduced access latency. Finally, we present simulation results investigating the optimal operation frequency of the data caches, where reliability is compromised in exchange of reduced energy and increased performance. Our simulation results indicate that the clock frequency of the data caches can be increased as much as 4 times without incurring a major penalty on the reliability. This also results in 41% reduction in the energy consumed in the data caches and a 24% reduction in the energy-delay-fallibility product.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1550990,no,undetermined,0
Supporting architecture evaluation process with collaborative applications,"The software architecture community has proposed several approaches to assess the capability of a system's architecture with respect to desired quality attributes (such as maintainability and performance). Scenario-based approaches are considered effective and mature. However, these methods heavily rely on face-to-face meetings, which are expensive and time consuming. Encouraged by the successful adoption of Internet-based technologies for several meeting based activities, we have been developing an approach to support architecture evaluation using Web-based collaborative applications, in this paper, we present a preliminary framework for conducting architecture evaluation in a distributed arrangement. We identify some supportive technologies and their expected benefits. We also present some of the initial findings of a research program designed to assess the effectiveness of the proposed idea. Findings of this study provide some support for distributed architecture evaluation.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1492919,no,undetermined,0
An implementation of a general regression network on FPGA with direct Matlab link,"Neural networks play a key role in many electronic applications, we can find them from industrial control applications to predictive models. They are mainly implemented as software entities because they require a great amount of complex mathematical operations. With the increasing power and capabilities of current FPGAs, now it is possible to translate them into hardware. This hardware implementations increase both the speed and usefulness of this neural networks. This paper presents a hardware implementation of a particular neural network, the general regression neural network. This network is able to approximate functions and it is used in control, prediction, fault diagnosis, engine management and many others. The paper describes an implementation of this neural network using different hardware platforms and using different implementation for each hardware target. This paper also presents an integrated development environment to produce the final hardware description in VHDL code from the Matlab generated neural network. The paper also describes a simulation scheme to test if the assumptions made to increase the performance of the network have a negative impact on the precision for the particular implementation.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1490722,no,undetermined,0
Effects of low power electronics & computer equipment on power quality at distribution grid-measurements and forecast,"The paper discusses power quality issues in case of simultaneous connection of a large number of low power nonlinear loads, like personal computers (PC), TV receivers, audio and video devices and similar electronic devices to the grid. Such consumers are at one-hand sensitive devices that require a stable and high quality of supply, but on the other hand they are sources of harmonics. Operation of a single PC, as a representative of such loads, is modeled using Matlab-Simulink software and verified by measurement performed in laboratory. The simultaneous operation of large number of PC-s is investigated by measurement on real site. The measurements are performed at several location connected to the grids with large number of PC type consumers (Computer Center) and other low power loads (highly populated residential area). The results are analyzed and compared with IEEE Standard 519 limits. In order to get the forecast of possible future situation, the developed model has been used for prediction of the ""worst"" case i.e. the highest possible harmonic level. It is shown that single PC is a significant source of harmonics. If a large number of PCs is connected to the grid, than they may present a serious threat to power quality, i.e. they can be a possible source of some negative effects.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1490140,no,undetermined,0
Spectral gamma detectors for hand-held radioisotope identification devices (RIDs) for nuclear security applications,"The in-situ identification of the isotope causing a radiation alarm at a border monitor is essential for a quick resolution of the case. Commonly, hand-held radioisotope identification devices (RIDs) are used to solve this task. Although a number of such devices are commercially available, we see still a gap between the requirements of the users and what is actually available. Two components are essential for the optimal performance of such devices: i) the quality of the raw data - the gamma spectra, and ii) isotope identification software matching the specifics of a certain gamma detector. In this paper, we investigate new spectral gamma detectors for potential use in hand-held radioisotope identification devices. The standard NaI detector is compared with new scintillation and room temperature semiconductor detectors. The following spectral gamma detectors were included into the study: NaI(Tl), LaCl<sub>3</sub>(Ce), <sup>6</sup>LiI(Eu), CdWO<sub>4 </sub>, hemispheric and coplanar CZT detectors. In the paper basic spectrometric properties are measured and compared",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1466839,no,undetermined,0
A ratio sensitive image quality metric,"Many applications require a fast quality measure for digitally coded images, such as the mobile multimedia communication. Although the widely used peak signal-noise ratio (PSNR) has low computation cost, it fails to predict structured errors that dominate in digital images. On the other hand, human visual system (HVS) based metrics can improve the prediction accuracy. However, they are computationally complex and time consuming. This paper proposes a very simple image quality measure defined by a ratio based mathematical formulation that attempts to simulate the scaling of the human visual sensation to brightness. The current results demonstrate that the novel image quality metric predicts the subjective ratings better and has lower computation cost than the PSNR. As an alternative to the PSNR, the proposed metric can be used for on-line or real-time picture quality assessment applications.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1439076,no,undetermined,0
PET reconstruction with system matrix derived from point source measurements,"The quality of images reconstructed by statistical iterative methods depends on an accurate model of the relationship between image space and projection space through the system matrix. A method of acquiring the system matrix on the CPS Innovations the HiRez scanner was developed. The system matrix was derived by positioning the point source in the scanner field of view and processing the response in projection space. Such responses include geometrical and detection physics components of the system matrix. The response is parameterized to correct point source location and to smooth projection noise. Special attention was paid to span concepts of HiRez scanner. The projection operator for iterative reconstruction was constructed, taking into account estimated response parameters. The computer generated and acquired data were used to compare reconstruction obtained by the HiRez standard software and produced by better modeling. Results showed that the better resolution and noise property can be achieved.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1462759,no,undetermined,0
Development and performances of a dental digital radiographic system using a high resolution CCD image sensor,"Dental digital radiographic (DDR) system using a high resolution charge-coupled device (CCD) imaging sensor was developed and the performances of this system for dental clinic imaging was evaluated. In order to determine the performances of the system, the modulation transfer function (MTF), the signal to noise ratio according to X-ray exposure, the dose reduction effects and imaging quality of the system were investigated. This system consists of a CCD imaging sensor (pixel size: 22 mum) to detect X-ray, an electrical signal processing circuit and a graphical user interface software to display the images and diagnosis. The MTF was obtained from a Fourier transform of the line spread function (LSF), which was itself derived from the edge spread function (ESF) of a sharp edge image acquired. The spatial resolution of the system was measured at a 10% contrast in terms of the corresponding MTF value and the distance between the X-ray source and the CCD image sensor was fixed at 20 cm. The best image quality obtained at the exposure conditions of 60 kVp, 7 mA and 0.05 sec. At this time, the signal to noise ratio and X-ray dose were 23 and 41% (194 muGy) of a film-based method (468 muGy). The spatial resolution of this system, the result of MTF, was approximately 12 line pairs per mm at the 0.05 exposure time. Based on the results, the developed DDR system using a CCD imaging sensor could be suitably applied for intra-oral radiographic imaging because of its low dose, real time acquisition, no chemical processing, image storage and retrieval etc.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1462556,no,undetermined,0
Implementation of DSP based relaying with particular reference to effect of STATCOM on transmission line protection,"The presence of flexible AC transmission system (FACTS) devices in the loop significantly affects the apparent resistance and reactance seen by a distance relay, as FACTS devices respond to changes in the power system configuration. The objective of this paper is to investigate the effect of mid-point compensation of STATic synchronous COMpensator (STATCOM) on the performance of impedance distance relay under normal load and fault conditions and to implement the adaptive distance-relaying scheme for transmission line protection. From the simulation studies carried out in PSCAD software and using analytical calculations, it is identified that there is a need for the distance relay to adapt to the new settings in its characteristic for the detection of fault within the zone of protection of transmission line. Apparent impedance is simulated for different loading and line to ground (L-G) fault conditions at different locations on the power system network. The proposed adaptive distance relay scheme has been implemented on TMS320C50 Digital signal processor (DSP) system.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1460217,no,undetermined,0
A software phase locked loop for unbalanced and distorted utility conditions,"A software phase locked loop (PLL) for custom power devices is described in this paper. The PLL uses the previous and current samples of the 3-phase utility voltages and separates them into the sequence components. The phase angle and frequency of the estimated positive sequence component are tracked by the first two stages of the PLL. The third stage estimates the parameter Î”Î¸ which is required by the first stage. Simulations of the performance of the PLL have been presented, as well as the performance of a dynamic voltage restorer incorporating the proposed PLL. The proposed PLL performs well under unbalanced and distorted utility conditions. It tracks the utility phase angle and frequency smoothly and also extracts the sequence components of the utility voltages. A DVR incorporating the PLL provides voltage support to sensitive loads and also acts as a harmonic filter.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1460157,no,undetermined,0
A data collection scheme for reliability evaluation and assessment-a practical case in Iran,"Data collection is an essential element of reliability assessment and many utilities throughout the world have established comprehensive procedures for assessing the performance of their electric power systems. Data collection is also a constituent part of quantitative power system reliability assessment in which system past performance and prediction of future performance are evaluated. This paper presents an overview of the Iran electric power system data collection scheme and the procedure to its reliability analysis. The scheme contains both equipment reliability data collection procedure and structure of reliability assessment. The former constitutes generation, transmission and distribution equipment data. The latter contains past performance and predictive future performance of the Iran power system. The benefits of this powerful data base within an environment of change and uncertainty will help utilities to keep down cost, while meeting the multiple challenges of providing high degrees of reliability and power quality of electrical energy.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1460147,no,undetermined,0
PD diagnosis on medium voltage cables with oscillating voltage (OWTS),"Detecting, locating and evaluating of partial discharges (PD) in the insulating material, terminations and joints provides the opportunity for a quality control after installation and preventive detection of arising service interruption. A sophisticated evaluation is necessary between PD in several insulating materials and also in different types of terminations and joints. For a most precise evaluation of the degree and risk caused by PD it is suggested to use a test voltage shape that is preferably like the same under service conditions. Only under these requirements the typical PD parameters like inception and extinction voltage, PD level and PD pattern correspond to significant operational values. On the other hand the stress on the insulation should be limited during the diagnosis to not create irreversible damages and thereby worsening the condition of the test object. The paper introduces an oscillating wave test system (OWTS), which meets these mentioned demands well. The design of the system, its functionality and especially the operating software are made for convenient field application. Field data and experience reports was presented and discussed. This field data serve also as good guide for the level of danger to the different insulating systems due to partial discharges.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1460088,no,undetermined,0
"Bluespec System Verilog: efficient, correct RTL from high level specifications","Bluespec System Verilog is an EDL toolset for ASIC and FPGA design offering significantly higher productivity via a radically different approach to high-level synthesis. Many other attempts at high-level synthesis have tried to move the design language towards a more software-like specification of the behavior of the intended hardware. By means of code samples, demonstrations and measured results, we illustrate how Bluespec System Verilog, in an environment familiar to hardware designers, can significantly improve productivity without compromising generated hardware quality.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1459818,no,undetermined,0
Room environment monitoring system from PDA terminal,"A room environment monitoring and control system, via a PDA terminal, was designed and fabricated. An RF wireless sensor module with several air quality monitoring sensors was developed for an indoor environment monitoring system in home networking. The module has the capacity for various kinds of sensors such as humidity sensor, temperature sensor, CO<sub>2</sub> sensor, flying dust sensor, etc. The developed module is very convenient for installation on the wall of a room or office, and the sensors can be easily replaced due to a well-designed module structure and RF connection method. To reduce the system cost, only one RF transmission block was used for sensor signal transmission to an 8051 microcontroller board using a time sharing method. In this home networking system, various indoor environmental parameters could be monitored in real time from the sensor module. Indoor vision was transferred to the client PC or PDA from a surveillance camera installed indoors or at a desired site. A Web server using Oracle DB was used for saving the vision data from the Web-camera and various data from the wireless sensor module.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1439105,no,undetermined,0
Measurement of harmonics/inter-harmonics of time-varying frequencies,"A novel method of extraction and measurement of individual harmonics of a signal with time-varying frequency is presented. The proposed method is based on a nonlinear, adaptive mechanism. Compared with the well-established techniques such as DFT, the proposed method offers (i) higher degree of accuracy, (ii) structural/performance robustness, and (iii) frequency-adaptivity. The structural simplicity of the algorithm renders it suitable for both software and hardware implementations. The limitation of the proposed method as compared with DFT-based methods is its slower transient response. Based on simulation studies, performance of the method is presented and its accuracy and response time are compared with a DFT-based method.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1375072,no,undetermined,0
High-impedance fault detection using discrete wavelet transform and frequency range and RMS conversion,"High-impedance faults (HIFs) are faults which are difficult to detect by overcurrent protection relays. Various pattern recognition techniques have been suggested, including the use of wavelet transform . However this method cannot indicate the physical properties of output coefficients using the wavelet transform. We propose to use the Discrete Wavelet Transform (DWT) as well as frequency range and rms conversion to apply a pattern recognition based detection algorithm for electric distribution high impedance fault detection. The aim is to recognize the converted rms voltage and current values caused by arcs usually associated with HIF. The analysis using discrete wavelet transform (DWT) with the conversion yields measurement voltages and currents which are fed to a classifier for pattern recognition. The classifier is based on the algorithm using nearest neighbor rule approach. It is proposed that this method can function as a decision support software package for HIF identification which could be installed in an alarm system.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1375120,no,undetermined,0
Channel and source considerations of a bit-rate reduction technique for a possible wireless communications system's performance enhancement,"In wireless commercial and military communications systems, where bandwidth is at a premium, robust low-bit-rate speech coders are essential. They operate at fix bit rates and those bit rates cannot be altered without major modifications in the vocoder design. A novel approach to vocoders, in order to reduce the bit rate required to transmit speech signal, is proposed. While traditional low-bit-rate vocoders code original input speech, the proposed procedure operates on the time-scale modified signal. The proposed method offers any bit rate from 2400 b/s to downwards without modifying the principle vocoder structure, which is the new NATO standard, Stanag 4591, Mixed Excitation Linear Prediction (MELP) vocoder. We consider the application of transmitting MELP-encoded speech over noisy communication channels by applying different modulation techniques, after time-scale compression is applied. Three different time-scale modification algorithms have been evaluated and waveform similarity overlap and add (WSOLA) algorithm has been selected for time-scale modification purposes. Computer simulation results, both source and channel, are presented in terms of objective speech quality metrics and informal subjective listening tests. Design parameters such as codec complexity and delay are also investigated. Simulation results lead to a possible wireless communications system, whose performance might be enhanced by using the spared bits offered by the procedure.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1381428,no,undetermined,0
An Alternative to Technology Readiness Levels for Non-Developmental Item (NDI) Software,"Within the Department of Defense, Technology Readiness Levels (TRLs) are increasingly used as a tool in assessing program risk. While there is considerable evidence to support the utility of using TRLs as part of an overall risk assessment, some characteristics of TRLs limit their applicability to software products, especially Non-Developmental Item (NDI) software including Commercial-Off-The-Shelf, Government-Off-The-Shelf, and Open Source Software. These limitations take four principle forms: 1) ""blurring-together"" various aspects of NDI technology/product readiness; 2) the absence of some important readiness attributes; 3) NDI product ""decay;"" and 4) no recognition of the temporal nature of system development and acquisition context. This paper briefly explores these issues, and describes an alternate methodology which combines the desirable aspects of TRLs with additional readiness attributes, and defines an evaluation framework which is easily understandable, extensible, and applicable across the full spectrum of NDI software.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1385892,no,undetermined,0
Quality metric for approximating subjective evaluation of 3-D objects,"Many factors, such as the number of vertices and the resolution of texture, can affect the display quality of three-dimensional (3-D) objects. When the resources of a graphics system are not sufficient to render the ideal image, degradation is inevitable. It is, therefore, important to study how individual factors will affect the overall quality, and how the degradation can be controlled given limited resources. In this paper, the essential factors determining the display quality are reviewed. We then integrate two important ones, resolution of texture and resolution of wireframe, and use them in our model as a perceptual metric. We assess this metric using statistical data collected from a 3-D quality evaluation experiment. The statistical model and the methodology to assess the display quality metric are discussed. A preliminary study of the reliability of the estimates is also described. The contribution of this paper lies in: 1) determining the relative importance of wireframe versus texture resolution in perceptual quality evaluation and 2) proposing an experimental strategy for verifying and fitting a quantitative model that estimates 3-D perceptual quality. The proposed quantitative method is found to fit closely to subjective ratings by human observers based on preliminary experimental results.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1407900,no,undetermined,0
Application fault tolerance with Armor middleware,"Many current approaches to software-implemented fault tolerance (SIFT) rely on process replication, which is often prohibitively expensive for practical use due to its high performance overhead and cost. The adaptive reconfigurable mobile objects of reliability (Armor) middleware architecture offers a scalable low-overhead way to provide high-dependability services to applications. It uses coordinated multithreaded processes to manage redundant resources across interconnected nodes, detect errors in user applications and infrastructural components, and provide failure recovery. The authors describe the experiences and lessons learned in deploying Armor in several diverse fields.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1405971,no,undetermined,0
Experimental VoIP capacity measurements for 802.11b WLANs,"There is an increasing interest in supporting voice over IP (VoIP) applications over wireless local area networks (WLANs). To provide quality of service guarantee to voice traffic, an admission control mechanism must be administered to avoid over loading the network. In this paper, we present an experimental evaluation of VoIP capacity in WLANs under various data and voice loads. The obtained capacity estimates can be used in admission control engine to make an admission or a rejection decision. In this paper, we identify a suitable evaluation metric to quantify the performance of a voice call. The metric accounts for packet losses and packet delays for each voice flow. We then define voice capacity in a WLAN and present experimental capacity measurements under various background data traffic loads. The experimental capacity measurements are immediately useful for WLAN hotspot providers and enterprise WLAN architects.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1405182,no,undetermined,0
Reducing Corrective Maintenance Effort Considering Module's History,"A software package evolves in time through various maintenance release steps whose effectiveness depends mainly on the number of faults left in the modules. The testing phase is therefore critical to discover these faults. The purpose of this paper is to show a criterion to estimate an optimal repartition of available testing time among software modules in a maintenance release. In order to achieve this objective we have used fault prediction techniques based both on classical complexity metrics and an additional, innovative factor related to the moduleÂ’s age in terms of release. This method can actually diminish corrective maintenance effort, while assuring a high reliability for the delivered software.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1402136,no,undetermined,0
A Tool for Static and Dynamic Model Extraction and Impact Analysis,"Planning changes is often an imprecise task and implementing changes is often time consuming and error prone. One reason for these problems is inadequate support for efficient analysis of the impacts of the performed changes. The paper presents a technique, and associated tool, that uses a mixed static and dynamic model extraction for supporting the analysis of the impacts of changes.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1402130,no,undetermined,0
ADAMS Re-Trace: A Traceability Recovery Tool,"We present the traceability recovery tool developed in the ADAMS artefact management system. The tool is based on an Information Retrieval technique, namely Latent Semantic Indexing and aims at supporting the software engineer in the identification of the traceability links between artefacts of different types. We also present a case study involving seven student projects which represented an ideal workbench for the tool. The results emphasise the benefits provided by the tool in terms of new traceability links discovered, in addition to the links manually traced by the software engineer. Moreover, the tool was also helpful in identifying cases of lack of similarity between artefacts manually traced by the software engineer, thus revealing inconsistencies in the usage of domain terms in these artefacts. This information is valuable to assess the quality of the produced artefacts.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1402112,no,undetermined,0
SWIFT: software implemented fault tolerance,"To improve performance and reduce power, processor designers employ advances that shrink feature sizes, lower voltage levels, reduce noise margins, and increase clock rates. However, these advances make processors more susceptible to transient faults that can affect correctness. While reliable systems typically employ hardware techniques to address soft-errors, software techniques can provide a lower-cost and more flexible alternative. This paper presents a novel, software-only, transient-fault-detection technique, called SWIFT. SWIFT efficiently manages redundancy by reclaiming unused instruction-level resources present during the execution of most programs. SWIFT also provides a high level of protection and performance with an enhanced control-flow checking mechanism. We evaluate an implementation of SWIFT on an Itanium 2 which demonstrates exceptional fault coverage with a reasonable performance cost. Compared to the best known single-threaded approach utilizing an ECC memory system, SWIFT demonstrates a 51% average speedup.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1402092,no,undetermined,0
Detecting indirect coupling,"Coupling is considered by many to be an important concept in measuring design quality There is still much to be learned about which aspects of coupling affect design quality or other external attributes of software. Much of the existing work concentrates on direct coupling, that is, forms of coupling that exists between entities that are directly related to each other. A form of coupling that has so far received little attention is indirect coupling, that is, coupling between entities that are not directly related. What little discussion there is in the literature suggests that any form of indirect coupling is simple the transitive closure of a form of direct coupling. We demonstrate that this is not the case, that there are forms of indirect coupling that cannot be represented in this way and suggest ways to measure it. We present a tool that identifies a particular form of indirect coupling that is integrated in the Eclipse IDE.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1402016,no,undetermined,0
Evaluation of effects of pair work on quality of designs,"Quality is a key issue in the development of software products. Although the literature acknowledges the importance of the design phase of software lifecycle and the effects of the design process and intermediate products on the final product, little progress has been achieved in addressing the quality of designs. This is partly due to difficulties associated in defining quality attributes with precision and measurement of the many different types and styles of design products, as well as problems with assessing the methodologies utilized in the design process. In this research we report on an empirical investigation that we conducted to examine and evaluate quality attributes of design products created through a process of pair-design and solo-design. The process of pair-design methodology involves pair programming principles where two people work together and periodically switch between the roles of driver and navigator. The evaluation of the quality of design products was based on ISO/IEC 9126 standards. Our results show some mixed findings about the effects of pair work on the quality of design products.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1402003,no,undetermined,0
"Analysis, fast algorithm, and VLSI architecture design for H.264/AVC intra frame coder","Intra prediction with rate-distortion constrained mode decision is the most important technology in H.264/AVC intra frame coder, which is competitive with the latest image coding standard JPEG2000, in terms of both coding performance and computational complexity. The predictor generation engine for intra prediction and the transform engine for mode decision are critical because the operations require a lot of memory access and occupy 80% of the computation time of the entire intra compression process. A low cost general purpose processor cannot process these operations in real time. In this paper, we proposed two solutions for platform-based design of H.264/AVC intra frame coder. One solution is a software implementation targeted at low-end applications. Context-based decimation of unlikely candidates, subsampling of matching operations, bit-width truncation to reduce the computations, and interleaved full-search/partial-search strategy to stop the error propagation and to maintain the image quality, are proposed and combined as our fast algorithm. Experimental results show that our method can reduce 60% of the computation used for intra prediction and mode decision while keeping the peak signal-to-noise ratio degradation less than 0.3 dB. The other solution is a hardware accelerator targeted at high-end applications. After comprehensive analysis of instructions and exploration of parallelism, we proposed our system architecture with four-parallel intra prediction and mode decision to enhance the processing capability. Hadamard-based mode decision is modified as discrete cosine transform-based version to reduce 40% of memory access. Two-stage macroblock pipelining is also proposed to double the processing speed and hardware utilization. The other features of our design are reconfigurable predictor generator supporting all of the 13 intra prediction modes, parallel multitransform and inverse transform engine, and CAVLC bitstream engine. A prototype chip is fabricated with TSMC 0.25-Î¼m CMOS 1P5M technology. Simulation results show that our implementation can process 16 mega-pixels (4096Ã—4096) within 1 s, or namely 720Ã—480 4:2:0 30 Hz video in real time, at the operating frequency of 54 MHz. The transistor count is 429 K, and the core - size is only 1.855Ã—1.885 mm<sup>2</sup>.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1397780,no,undetermined,0
Estimation of complex permittivity of arbitrary shape and size dielectric samples using cavity measurement technique at microwave frequencies,"In this paper, a simple cavity measurement technique is presented to estimate the complex permittivity of arbitrary shape and size dielectric samples. Measured shift in resonant frequency and change in quality factor due to the dielectric sample loading in the cavity is compared with simulated values obtained using the finite-element method software high frequency structure simulator and matched using the Newton-Raphson method to estimate the complex permittivity of a arbitrary shape and size dielectric sample. Complex permittivity of Teflon (PTFE) and an MgO-SiC composite is estimated in the S-band using this method for four different samples of varying size and shapes. The result for Teflon shows a good agreement with the previously published data, and for the MgO-SiC composite, the estimated real and imaginary parts of permittivity for four different shape and size samples are within 10%, proving the usefulness of the method. This method is particularly suitable for estimation of complex permittivity of high-loss materials",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1393217,no,undetermined,0
Model-based performance risk analysis,"Performance is a nonfunctional software attribute that plays a crucial role in wide application domains spreading from safety-critical systems to e-commerce applications. Software risk can be quantified as a combination of the probability that a software system may fail and the severity of the damages caused by the failure. In this paper, we devise a methodology for estimation of performance-based risk factor, which originates from violations, of performance requirements, (namely, performance failures). The methodology elaborates annotated UML diagrams to estimate the performance failure probability and combines it with the failure severity estimate which is obtained using the functional failure analysis. We are thus able to determine risky scenarios as well as risky software components, and the analysis feedback can be used to improve the software design. We illustrate the methodology on an e-commerce case study using step-by step approach, and then provide a brief description of a case study based on large real system.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1392717,no,undetermined,0
An extended Chi2 algorithm for discretization of real value attributes,"The variable precision rough sets (VPRS) model is a powerful tool for data mining, as it has been widely applied to acquire knowledge. Despite its diverse applications in many domains, the VPRS model unfortunately cannot be applied to real-world classification tasks involving continuous attributes. This requires a discretization method to preprocess the data. Discretization is an effective technique to deal with continuous attributes for data mining, especially for the classification problem. The modified Chi2 algorithm is one of the modifications to the Chi2 algorithm, replacing the inconsistency check in the Chi2 algorithm by using the quality of approximation, coined from the rough sets theory (RST), in which it takes into account the effect of degrees of freedom. However, the classification with a controlled degree of uncertainty, or a misclassification error, is outside the realm of RST. This algorithm also ignores the effect of variance in the two merged intervals. In this study, we propose a new algorithm, named the extended Chi2 algorithm, to overcome these two drawbacks. By running the software of See5, our proposed algorithm possesses a better performance than the original and modified Chi2 algorithms.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1388252,no,undetermined,0
Quantifying software architectures: an analysis of change propagation probabilities,"Summary form only given. Software architectures are an emerging discipline in software engineering as they play a central role in many modern software development paradigms. Quantifying software architectures is an important research agenda, as it allows software architects to subjectively assess quality attributes and rationalize architecture-related decisions. In this paper, we discuss the attribute of change propagation probability, which reflects the likelihood that a change that arises in one component of the architecture propagates (i.e. mandates changes) to other components.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1387113,no,undetermined,0
A technique based on the OMG metamodel and OCL for the definition of object-oriented metrics applied to UML models,"Summary form only given. The development of wide software systems is an activity that consumes great quantities of time and resources. Even with the increase of the automation in the software development activities, the resources stay scarce. As a consequence of this, there is a big interest in the metrics of software due to its potential for a better, more efficient use of the resources. Tools help and assist in the planning and in the estimation of the complexity of the applications to develop during different stage of a process. We describe a technique to define metrics using the OMG (Object Management Group) standard specification. The semantic in each metrics is specified formally with OCL (Object Constraint Language) based on OMG metamodels. For their concrete specification, we establish a series of steps that allows define uniformly each metric in the different RUP models. Furthermore, the present paper shows the use of metrics defined using this technique and the relation between data obtained from the application of the metrics to thirteen object oriented systems. They encompass projects from the capture of requirements until its implementation, showing the metrics applied in different stage of the deployment.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1387107,no,undetermined,0
Pseudo dynamic metrics [software metrics],"Summary form only given. Software metrics have become an integral part of software development and are used during every phase of the software development life cycle. Research in the area of software metrics tends to focus predominantly on static metrics that are obtained by static analysis of the software artifact. But software quality attributes such as performance and reliability depend on the dynamic behavior of the software artifact. Estimating software quality attributes based on dynamic metrics for the software system are more accurate and realistic. The research presented in this paper attempts to narrow the gap between static metrics and dynamic metrics, and lay the foundation for a more systematic approach to estimate the dynamic behavior of a software system early in the software development cycle. Focusing on coupling metrics, we present an empirical study to analyze the relationship between static and dynamic coupling metrics and propose the concept of pseudo dynamic metrics to estimate the dynamic behavior early in the software development lifecycle.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1387106,no,undetermined,0
Reusability metrics for software components,"Summary form only given. Assessing the reusability, adaptability, compose-ability and flexibility of software components is more and more of a necessity due to the growing popularity of component based software development (CBSD). Even if there are some metrics defined for the reusability of object-oriented software (OOS), they cannot be used for CBSD because these metrics require analysis of source code. The aim of this paper is to study the adaptability and compose-ability of software components, both qualitatively and quantitatively. We propose metrics and a mathematical model for the above-mentioned characteristics of software components. The interface characterization is the starting point of our evaluation. The adaptability of a component is discussed in conjunction with the complexity of its interface. The compose-ability metric defined for database components is extended for general software components. We also propose a metric for the complexity and adaptability of the problem solved by a component, based on its use cases. The number of alternate flows from the use case narrative is considered as a measurement for the complexity of the problem solved by a component. This was our starting point in developing a set of metrics for evaluating components functionality-wise. The main advantage of defining these metrics is the possibility to measure adaptability, reusability and quality of software components, and therefore to identify the most effective reuse strategy.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1387023,no,undetermined,0
A median based interpolation algorithm for deinterlacing,"In this paper, state-of-the-art interpolation algorithms for deinterlacing within a single frame are investigated. Based on Chen's directional measurements and on the median filter, a novel interpolation algorithm for deinterlacing is proposed. By efficiently estimating the diagonal and vertical directional correlations of the neighboring pixels, the proposed method performs better than existing techniques on different images and video sequences, for both subjective and objective measurements. Additionally, the proposed method has a simple structure with low computation complexity, which therefore makes it simple to implement in hardware.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1439083,no,undetermined,0
An emotional decision making help provision approach to distributed fault tolerance in MAS,"Fault is inevitable especially in MASS (multi-agent system) because of their distributed nature. This paper introduces a new approach for fault tolerance using help provision and emotional decision-making. Tasks that are split into criticality-assigned real-time subtasks according to their precedence graph are distributed among specialized agents with different skills. If a fault occurs for an agent in a way that it cannot continue its task, the agent requests help from the others with the same skill to redo or continue its task. Requested agents would help the faulty agent based on their nervousness on their own tasks compared to his task. It Is also possible for an agent to discover death of another agent, which has accepted one of his tasks, by polling. An implementation using JADE platform is presented in this paper and the results are reported.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1438671,no,undetermined,0
Variance expressions for software reliability growth models,"<div style=""font-variant: small-caps; font-size: .9em;"">First Page of the Article</div><img class=""img-abs-container"" style=""width: 95%; border: 1px solid #808080;"" src=""/xploreAssets/images/absImages/01408434.png"" border=""0"">",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1408434,no,undetermined,0
Applications of fuzzy-logic-wavelet-based techniques for transformers inrush currents identification and power systems faults classification,"The advent of wavelet transforms (WTs) and fuzzy-inference mechanisms (FIMs) with the ability of the first to focus on system transients using short data windows and of the second to map complex and nonlinear power system configurations provide an excellent tool for high speed digital relaying. This work presents a new approach to real-time fault classification in power transmission systems, and identification of power transformers magnetising inrush currents using fuzzy-logic-based multicriteria approach Omar A.S. Youssef [2004, 2003] with a wavelet-based preprocessor stage Omar A.S. Youssef [2003, 2001]. Three inputs, which are functions of the three line currents, are utilised to detect fault types such as LG, LL, LLG as well as magnetising inrush currents. The technique is based on utilising the low-frequency components generated during fault conditions on the power system and/or magnetising inrush currents. These components are extracted using an online wavelet-based preprocessor stage with data window of 16 samples (based on 1.0 kHz sampling rate and 50 Hz power frequency). Generated data from the simulation of an 330 Î”/33Y kV, step-down transformer connected to a 330 kV model power system using EMTP software were used by the MATLAB program to test the performance of the technique as to its speed of response, computational burden and reliability. Results are shown and they indicate that this approach can be used as an effective tool for high-speed digital relaying, and that computational burden is much simpler than the recently postulated fault classification.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1397423,no,undetermined,0
Dynamic load balancing performance in cellular networks with multiple traffic types,"Several multimedia applications are being introduced to cellular networks. Since the quality of service (QoS) requirements such as bandwidth for different services might be different, the analysis of conventional multimedia cellular networks has been done using multi-dimensional Markov-chains in previous works. In these analyses, it is assumed that a call request will be blocked if the number of available channels is not sufficient to support the service. However, it has been shown in previous works that the call blocking rate can be reduced significantly, if a dynamic load balancing scheme is employed. In this paper, we develop an analytical framework for the analysis of dynamic load balancing schemes with multiple traffic types. To illustrate the impact of dynamic load balancing on the performance, we study the integrated cellular and ad hoc relay (iCAR) system. Our results show that with a proper amount of load balancing capability (i.e., load balancing channels), the call blocking probability for all traffic types can be reduced significantly.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1404713,no,undetermined,0
Parametric imaging in dynamic susceptibility contrast MRI-phantom and in vivo studies,"Possibility of quantitative perfusion imaging with DSC MRI is still under discussion. In this work, the quantitative related parameters are analyzed and DSC-MRI limitations are discussed. It includes investigation of measurement procedures/conditions as well as parametric image synthesis methodology. The set of phantoms was constructed and used to inspect the role of Gd-DTPA concentration estimation by EPI measurements, the influence of partial volume effect on concentration estimation, the role of a phantom and its pipes orientation, etc. Additionally, parametric image synthesis methodology was investigated by analysis of influence of a bolus dispersion, bolus arrival time, and other signal parameters on an image quality. As a conclusion testing software package is proposed.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1403357,no,undetermined,0
Consolidating software tools for DNA microarray design and manufacturing,"As the human genome project progresses and some microbial and eukaryotic genomes are recognized, a novel technology, DNA microarray (also called gene chip, biochip, gene microarray, and DNA chip) technology, has attracted increasing number of biologists, bioengineers and computer scientists recently. This technology promises to monitor the whole genome at once, so that researchers can study the whole genome on the global level and have a better picture of the expressions among millions of genes simultaneously. Today, it is widely used in many fields - disease diagnosis, gene classification, gene regulatory network, and drug discovery. We present a concatenated software solution for the entire DNA array flow exploring all steps of a consolidated software tool. The proposed software tool has been tested on Herpes B virus as well as simulated data. Our experiments show that the genomic data follow the pattern predicted by simulated data although the number of border conflicts (quality of the DNA array design) is several times smaller than for simulated data. We also report a trade-off between the number of border conflicts and the running time for several proposed algorithmic techniques employed in the physical design of DNA arrays.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1403119,no,undetermined,0
The use of impedance measurement as an effective method of validating the integrity of VRLA battery production,The response of batteries to an injection of AC current to give an indication of battery state-of-health has been well established and extensively reported over a number years but it has been used largely as a means of assessing the condition of batteries in service. In this paper the use of impedance measurement as a quality assurance procedure during manufacture will be described. There are a number of commercially available meters that are used for monitoring in field operations but they have not been developed for use in a manufacturing environment. After extensive laboratory testing a method specifically designed for impedance measurement system at the end of manufacturing lines in order to assure higher product integrity was devised and validated. A special testing station was designed for this purpose and includes battery conditioning prior to the test and sophisticated software driven data analysis in order to increase the effectiveness and reliability of the measurement. The paper reports the analysis of the data collected over two years of monitoring of the entire production with the impedance testing station at the end of the production line. Data comparison with earlier production shows an increase in the number of batteries scrapped internally and correspondingly a reduction in the number of defective products reaching distribution centres and also the field. The accumulated experience has also been very helpful in getting better information about the effect of the various parameters that affect the measured impedance value and this will assist in improving the reliability of impedance measurements in field service.,2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1401531,no,undetermined,0
A real-time network simulation application for multimedia over IP,"This paper details a secure voice over IP (SVoIP) development tool, the network simulation application (Netsim), which provides real-time network performance implementation of quality of service (QoS) statistics in live real-time data transmission over packet networks. This application is used to implement QoS statistics including packet loss and inter-arrival delay jitter. Netsim is written in Visual C++ using MFC for MS Windows environments. The program acts as a transparent gateway for a SVoIP server/client pair connected via IP networks. The user specifies QoS parameters such as mean delay, standard deviation of delay, unconditional loss probability, conditional loss probability, etc. Netsim initiates and accepts connection, controls packet flow, records all packet sending / arrival times, generates a data log, and performs statistical calculations.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1399567,no,undetermined,0
Image-quality assessment in optical tomography,"Modern medical imaging systems often rely on complicated hardware and sophisticated algorithms to produce useful digital images. It is essential that the imaging hardware and any reconstruction algorithms used are optimized, enabling radiologists to make the best decisions and quantify a patient's health status. Optimization of the hardware often entails determining the physical design of the system, such as the locations of detectors in optical tomography or the design of the collimator in SPECT systems. For software or reconstruction algorithm optimization one is often determining the values of regularization parameters or the number of iterations in an iterative algorithm. In this paper, we present an overview of many approaches to measuring task performance as a means to optimize imaging systems and algorithms. Much of the work in this area has taken place in the areas of nuclear-medicine and X-ray imaging. The purpose of this paper is to present some of the task-based measures of image quality that are directly applicable to optical tomography.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1398827,no,undetermined,0
Medical software control quality using the 3D Mojette projector,"The goal of this paper is to provide a tool that allows to assess a set of 2D projection data from a 3D object which can be either real or synthetic data. However, the generated 2D projection set is not sampled onto a classic orthogonal grid but uses a regular grid depending on the discrete angle of projection and the 3D orthogonal grid. This allows a representation of the set of projections that can easily be described in spline spaces. The subsequent projections set is used after an interpolation scheme to compare (in the projection space) the adequation between the original dataset and the obtained reconstruction. These measures are performed from a 3D multiresolution stack in the case of 3D PET projector. Its direct use for the digital radiography reprojection control quality assessment is finally exposed.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1398668,no,undetermined,0
Applications of fuzzy inference mechanisms to power system relaying,"Most transmission line protective schemes are based on deterministic computations on a well defined model of the system to be protected. This results in difficulty because of the complexity of the system model, the lack of knowledge of its parameters, the great number of information to be processed, and the difficulty in taking into consideration any system variation as the rules are fixed. The application of fuzzy logic for exploring complex, nonlinear systems, diagnosis systems and other expert systems, particularly when there is no simple mathematical model to be performed, provides a very powerful and attractive solution to classification problems. In this paper, a feasibility study on the application of different fuzzy reasoning mechanisms to power system relaying algorithms is conducted. Those mechanisms are namely, Mamdani's mechanism, Larsen's mechanism, Takagi-Sugeno's mechanism, and Tsukamoto mechanism. A comparative analysis on the application of these fuzzy inference mechanisms to a novel fault detection and phase selection technique on EHV transmission lines is reported. The proposed scheme utilises only the phase angle between two line currents for the decision making part of the scheme. A sample three-phase power system was simulated using the EMTP software. An online wavelet-based preprocessor stage is used with data window of 10 samples (based on 4.5 kHz sampling rate and 50 Hz power frequency). The performance of the proposed model was extensively tested in each case of fuzzy inference mechanism using the MATLAB software. The advantages and disadvantages of each mechanism are reported and compared together. Some of the test results are included in this paper.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1397443,no,undetermined,0
Regression benchmarking with simple middleware benchmarks,"The paper introduces the concept of regression benchmarking as a variant of regression testing focused at detecting performance regressions. Applying the regression benchmarking in the area of middleware development, the paper explains how regression benchmarking differs from middleware benchmarking in general. On a real-world example of TAO, the paper shows why the existing benchmarks do not give results sufficient for regression benchmarking, and proposes techniques for detecting performance regressions using simple benchmarks.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1395179,no,undetermined,0
Using software tools and metrics to produce better quality test software,"Automatic test equipment (ATE) software is often written by test equipment engineers without professional software training. This may lead to poor designs and an excessive number of defects. The Naval Surface Warfare Center (NSWC), Corona Division, as the US Navy's recognized authority on test equipment assessment, has reviewed a large number of test software programs. As an aid in the review process, various software tools have been used such as PC-lint<sup>â„?/sup> or Understand for C++<sup>â„?/sup>. This paper focus on software tools for C compilers since C is the most error prone language in use today. The McCabe cyclomatic complexity metric and the Halstead complexity measures are just two of the ways to measure ""software quality"". Applying the best practices of industry including coding standards, software tools, configuration management and other practices produce better quality code in less time. Good quality code would also be easier to write, understand, maintain and upgrade.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1436853,no,undetermined,0
Admission control in deadline-based network resource management,"In our deadline-based network resource management framework, each application data unit (ADU) is characterized by a (size, deadline) pair, which can be used to convey the resource and QoS requirements of the ADU. Specifically, the ADU's bandwidth requirement can be implicitly estimated by the ratio: size / (deadline - current time), and the time at which the ADU should be delivered is specified by the deadline. The ADU deadline is mapped onto deadlines at the network layer, which are carried by packets and used by routers for channel scheduling. In an earlier work, we have shown that deadline-based channel scheduling achieves good performance in terms of the percentage of ADUs that are delivered on-time. However, when a network is under heavy load, congestion may occur, and deadline-based scheduling alone may not be sufficient to prevent performance degradation. The level of congestion can be reduced by admission control. In this paper, two application-layer admission control algorithms are developed. The performance of these two algorithms is evaluated by simulation.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1394951,no,undetermined,0
COSMAD: a Scilab toolbox for output-only modal analysis and diagnosis of vibrating structures,"Modal analysis of vibrating structures is a usual technique for design and monitoring in many industrial sectors: car manufacturing, aerospace, civil structures. We present COSMAD a software environment for in-operation situation without any measured or controlled input. COSMAD is an identification and detection Scilab toolbox. It covers modal identification with visual inspection of the results via a GUI or fully automated modal identification and monitoring. The toolbox offers a complete package for signal visualization, filtering and down-sampling, the tracking of frequency and damping over time, mode-shape visualization, MAC computation, etc. The detection part is also a comprehensive toolbox for modal diagnosis and physical localization of faulty components",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1393861,no,undetermined,0
Model gain scheduling and reporting for ethylene plant on-line optimizer,"An ethylene plant is one of the largest chemical plants. As such, there are frequent changes in hydrocarbon feed mix, individual feed quality and demand for its olefin products. This makes it difficult to operate an ethylene plant in an optimal way. Showa Denko K.K. (SDK) has applied on-line optimization software packages of Honeywell Process Solutions to its ethylene plant. The software packages consist of large-scale model predictive control (MPC) controllers and optimization systems. As a function of the software, furnace yield model gains of MPC are scheduled by using a rigorous first principle model to cope with the strong non-linearity of the reactions in the furnace. In addition, non-linearity of consumption of fuel and steam is important, especially when user demand or storage tank room is limited and productions rates are kept constant. In that case, the direction of optimization of fuel and steam users could be wrong if non-linearity is ignored. The software, however, cannot cope with that non-linearity. Therefore, the achievable performance of the original software is limited. To cope with non-linearity, SDK developed a function of gain scheduling for utility models. As a result, both throughput and efficiency were improved more than expected. Since the on-line optimizer copes with many constraints and checks benefits, it is difficult for operators to intuitively understand the optimization results. Therefore, SDK also developed user interfaces and daily reporting systems for the on-line optimizer. The user interfaces indicate the operating condition and the bottlenecks in the ethylene plant every minute. Thus, operators can recognize the operation determined by the on-line optimizer. In daily reports, process trends of bottleneck points and manipulated variables of the on-line optimizer are reported. The daily reports are automatically generated and sent to members in associated sections by e-mail. With the report, the operation of the ethylene plant can be monitored and fine-tuning of the on-line optimizer can be performed.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1387516,no,undetermined,0
What do you mean my board test stinks?,"Board level testing account for the functionality and performance of all the device placed on the board, and also account for how the devices are assembled on to the board and how the device interact with one another. Three areas of board and system level testing are structural testing, functional testing and parametric testing. Structural testing focuses on the assembly process given above. Parametric tests have well defined metrics like bit error rate and jitter. Functional testing are targeted very less in order to target the defects.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1387431,no,undetermined,0
Image fusion for a digital camera application based on wavelet domain hidden Markov models,"The traditional image fusion for a digital camera application may not be satisfactory to classify pixels in the source image by the statistical techniques. In this paper, we present a technique, based on wavelet domain hidden Markov models (HMMs) and max-likelihood estimation. The method presented here consists of deciding the quality of pixels in source images directly from the statistical techniques to overcome the shift-variant of inverse wavelet transform. We have studied several possibilities including all energy methods that are utilized in the standard image fusion for digital camera application and discuss the difference to our new method. The new framework uses two trained HMMs to decide if the wavelet coefficients of source images are in-focus or out-focus, and then judges the quality of pixels in the source images directly to overcome the shift-variant of inverse wavelet transform. The two trained HMMs are obtained separately from an in-focus image and a out-focus image using EM algorithm. We used the method to merge two images in which have two clocks with different focus and obtain the best fusion results in preserving edge information and avoiding shift-variant.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1384603,no,undetermined,0
Empirical study of session-based workload and reliability for Web servers,"The growing availability of Internet access has led to significant increase in the use of World Wide Web. If we are to design dependable Web-based systems that deal effectively with the increasing number of clients and highly variable workload, it is important to be able to describe the Web workload and errors accurately. In this paper we focus on the detailed empirical analysis of the session-based workload and reliability based on the data extracted from actual Web logs often Web servers. First, we address the data collection process and describe the methods for extraction of workload and error data from Web log files. Then, we introduce and analyze several intra-session and inter-session metrics that collectively describe Web workload in terms of user sessions. Furthermore, we analyze Web error characteristics and estimate the request-based and session-based reliability of Web servers. Finally, we identify the invariants of the Web workload and reliability that apply through all data sets considered. The results presented in this paper show that session-based workload and reliability are better indicators of the users perception of the Web quality than the request-based metrics and provide more useful measures for tuning and maintaining of the Web servers.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1383135,no,undetermined,0
From test count to code coverage using the Lognormal Failure Rate,"When testing software, both effort and delay costs are related to the number of tests developed and executed. However the benefits of testing are related to coverage achieved and defects discovered. We establish a novel relationship between test costs and the benefits, specifically between the quantity of testing and test coverage, based on the Lognormal Failure Rate model. We perform a detailed study of how code coverage increases as a function of number of test cases executed against the 29 files of an application termed SHARPE. Distinct, known coverage patterns are available for 735 tests against this application. By simulating execution of those tests in randomized sequences we determined the average empirical coverage as a function of number of test cases executed for SHARPE as a whole and for each of its individual files. Prior research suggests the branching nature of software causes code coverage to grow as the Laplace transform of the lognormal. We use the empirical SHARPE coverage data to validate the lognormal hypothesis and the derivation of the coverage growth model. The SHARPE data provides initial insights into how the parameters of the lognormal are related to program file characteristics, how the properties of the files combine into properties of the whole, and how data quantity and quality affects parameter estimation.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1383126,no,undetermined,0
Multiple profile evaluation using a single test suite in random testing,"Using an integral formulation of random testing for regular continuous input, we present an approximate solution that enables highly accurate estimations of the results for arbitrary operational profiles, using strategically chosen input in a single test suite. The current approach, building on the Lagrange interpolation and Gaussian integration theorems, thus solves the longstanding issue of the necessity of unique test suites for fundamentally different operational profiles in random testing. Empirical comparisons are furthermore performed for a total of 27 profiles on five seeded faults in a numerical routine and a straightforward modulus fault. Using a high enough expansion in the approximation, the resulting failure frequencies and variances became statistically identical with the ""ordinary"" evaluations in all cases. It thus becomes possible to eliminate any extra test cases and still keep full accuracy when changing between operational profiles.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1383125,no,undetermined,0
Modeling and optimization of heterogeneous wireless LAN,"A sophisticated approach to automated prediction for the optimal layout and quantity of WLAN access points to achieve the desired network parameters is introduced. The algorithms were implemented in Web-based software for RF planning of a complete wireless local area system with multiple radio frequency technologies (IEEE 802.11a, 802.11b/g, and Bluetooth) based on a mesh topology. The implemented optimization method based on evolution strategies offers easy specification of various network aspects and QoS requirements (hotspots of various technologies, proffered areas near Ethernet ports and power outlets, throughput, number of concurrent users, etc.). The first few months of operation and usage in many real-world scenarios have shown good performance and appropriate accuracy. The application is also able to model site-specific coverage and the capacity of the wireless network using semi-empirical propagation models.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1404919,no,undetermined,0
Reliable and valid measures of threat detection performance in X-ray screening,"Over the last decades, airport security technology has evolved remarkably. This is especially evident when state-of-the-art detection systems are concerned. However, such systems are only as effective as the personnel who operate them. Reliable and valid measures of screener detection performance are important for risk analysis, screener certification and competency assessment, as well as for measuring quality performance and effectiveness of training systems. In many of these applications the hit rate is used in order to measure detection performance. However, measures based on signal detection theory have gained popularity in recent years, for example in the analysis of data from threat image projection (TIP) or computer based training (CBT) systems. In this study, computer-based tests were used to measure detection performance for improvised explosive devices (IEDs). These tests were conducted before and after training with an individually adaptive CBT system. The following measures were calculated: pHit, d', Î”m, Az, A' p(c)<sub>max</sub>. All measures correlated well, but ROC curve analysis suggests that ""nonparametric"" measures are more valid to measure detection performance for IEDs. More specifically, we found systematic deviations in the ROC curves that are consistent with two-state low threshold theory of R.D. Luce (1963). These results have to be further studied and the question rises if similar results could be obtained for other X-ray screening data. In any case, it is recommended to use A' in addition to d' in practical applications such as certification, threat image projection and CBT rather than the hit rate alone.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1405409,no,undetermined,0
Upward looking ice profiler sonar instruments for ice thickness and topography measurements,"Scientific and engineering studies in polar and marginal ice zones require detailed information on sea ice thickness and topography. Until recently, vertical ice dimension data have been largely inferred from aerial and satellite remote-sensing sensors. The capabilities of these sensors are still very limited for establishing accurate ice thicknesses and do not address details of ice topography. Alternative under-ice measurement methodologies continue to be major sources of accurate sea ice thickness and topography data for basic ice-covered ocean studies and, increasingly, for addressing important navigation, offshore structure design/safety, and climate change issues. Upward-looking sonar (ULS) methods characteristically provide under-ice topography data with high horizontal and vertical spatial resolution. Originally, the great bulk of data of this type was acquired from ULS sensors mounted on polar-traversing submarines during the cold war era. Unfortunately, much of the collected information was, and remains, hard to access. Consequently, the development of sea-floor based moored upward looking sonar (ULS) instrumentation, or ice profilers, over the past decade has begun to yield large, high quality, databases on ice undersurface topography and ice draft/thickness for scientific, engineering and operational users. Recent applications of such data include regional oceanographic studies, force-on-structure analyses, real-time ice jam detection, and tactical AUV operations. Over 100 deployments of moored and AUV-mounted ice profiler sonars, associated with an overall data recovery rate of 95%, are briefly reviewed. Prospective new applications of the technology will be presented and related to likely directions of future developments in profiler hardware and software.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1406369,no,undetermined,0
Teaming assessment: is there a connection between process and product?,"It is reasonable to suspect that team process influences the way students work, the quality of their learning and the excellence of their product. This study addresses the relations between team process variables on the one hand, and behaviors and outcomes, on the other. We measured teaming skill, project behavior and performance, and project product grades. We found that knowledge of team process predicts team behavior, but that knowledge alone does not predict performance on the project. Second, both effort and team skills, as assessed by peers, were related to performance. Third, team skills did not correlate with the students' effort. This pattern of results suggests that instructors should address issues of teaming and of effort separately. It also suggests that peer ratings of teammates tap aspects of team behavior relevant to project performance, whereas declarative knowledge of team process does not.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1408762,no,undetermined,0
Smooth ergodic hidden Markov model and its applications in text to speech systems,"In text-to-speech systems, the accuracy of information extraction from text is crucial in producing high quality synthesized speech. In this paper, a new scheme for converting text into its equivalent phonetic spelling is proposed and developed. This method has many advantages over its predecessors and it can complement many other text to speech converting systems in order to get improved performance.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1434043,no,undetermined,0
A mathematical morphological method to thin edge detection in dark region,"The performance of image segmentation depends on the output quality of the edge detection process. Typical edge detecting method is based on detecting pixels in an image with high gradient values, and then applies a global threshold value to extract the edge points of the image. By these methods, some detected edge points may not belong to the edge and some thin edge points in dark regions of the image are being eliminated. These eliminated edges may be with important features of the image. This paper proposes a new mathematical morphological edge-detecting algorithm based on the morphological residue transformation derived from dilation operation to detect and preserve the thin edges. Moreover, this work adopts five bipolar oriented edge masks to prune the miss detected edge points. The experimental results show that the proposed algorithm is successfully to preserve the thin edges in the dark regions.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1433746,no,undetermined,0
Transmission line model influence on fault diagnosis,"Artificial neural networks have been used to develop software applied to fault identification and classification in transmission lines with satisfactory results. The input data to the neural network are the sampled values of voltage and current waveforms. The values proceed from the digital fault recorders, which monitor the transmission lines and make the data available in their analog channels. It is extremely important, for the learning process of the neural network, to build databases that represent the fault scenarios properly. The aim of this paper is to evaluate the influence of transmission line models on fault diagnosis, using constant and frequency-dependent parameters.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1432415,no,undetermined,0
Generating multiple noise elimination filters with the ensemble-partitioning filter,"We present the ensemble-partitioning filter which is a generalization of some common filtering techniques developed in the literature. Filtering the training dataset, i.e., removing noisy data, can be used to improve the accuracy of the induced data mining learners. Tuning the few parameters of the ensemble-partitioning filter allows filtering a given data mining problem appropriately. For example, it is possible to specialize the ensemble-partitioning filter into the classification, ensemble, multiple-partitioning, or iterative-partitioning filter. The predictions of the filtering experts are then utilized such that if an instance is misclassified by a certain number of experts or learners, it is identified as noisy. The conservativeness of the ensemble-partitioning filter depends on the filtering level and the number of filtering iterations. A case study of software metrics data from a high assurance software project analyzes the similarities between the filters obtained from the specialization of the ensemble-partitioning filter. We show that over 25% of the time, the filters at different levels of conservativeness agree on labeling instances as noisy. In addition, the classification filter has the lowest agreement with the other filters.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431489,no,undetermined,0
Rule-based noise detection for software measurement data,"The quality of training data is an important issue for classification problems, such as classifying program modules into the fault-prone and not fault-prone groups. The removal of noisy instances will improve data quality, and consequently, performance of the classification model. We present an attractive rule-based noise detection approach, which detects noisy instances based on Boolean rules generated from the measurement data. The proposed approach is evaluated by injecting artificial noise into a clean or noise-free software measurement dataset. The clean dataset is extracted from software measurement data of a NASA software project developed for realtime predictions. The simulated noise is injected into the attributes of the dataset at different noise levels. The number of attributes subjected to noise is also varied for the given dataset. We compare our approach to a classification filter, which considers and eliminates misclassified instances as noisy data. It is shown that for the different noise levels, the proposed approach has better efficiency in detecting noisy instances than the C4.5-based classification filter. In addition, the noise detection performance of our approach increases very rapidly with an increase in the number of attributes corrupted.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431478,no,undetermined,0
Instruction level test methodology for CPU core software-based self-testing,"TIS (S. Shamshiri et al., 2004) is an instruction level methodology for CPU core self-testing that enhances the instruction set of a CPU with test instructions. Since the functionality of test instructions is the same as the NOP instruction, NOP instructions can be replaced with test instructions so that online testing can be done with no performance penalty. TIS tests different parts of the CPU and detects stuck-at faults. This method can be employed in offline and online testing of all kinds of processors. Hardware-oriented implementation of TIS was proposed previously (S. Shamshiri et al., 2004) that tests just the combinational units of the processor. Contributions of this paper are first, a software-based approach that reduces the hardware overhead to a reasonable size and second, testing the sequential parts of the processor besides the combinational parts. Both hardware and software oriented approaches are implemented on a pipelined CPU core and their area overheads are compared. To demonstrate the appropriateness of the TIS test technique, several programs are executed and fault coverage results are presented.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431227,no,undetermined,0
Testability analysis of reactive software,"This paper is about testability analysis for reactive software. We describe an application of the SATAN method, which allows testability of data-flow designs to be measured, to analyze testability of the source code of reactive software, such as avionics software. We first propose the transformation of the source code generated from data-flow designs into the static single assignment (SSA) form; then we describe the algorithm to automatically translate the SSA form into a testability model. Thus, analyzing the testability model can allow the detection of the software parts which induce a testability weakness.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1428409,no,undetermined,0
Implementation of a sensor fault reconstruction scheme on an inverted pendulum,"This paper presents a robust sensor fault reconstruction scheme, using an unknown input observer, applied to an inverted pendulum. The scheme is adapted from existing work in the literature. A suitable interface between the pendulum and a computer enabled the application. Very good results were obtained.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1426856,no,undetermined,0
Fault detection in model predictive controller,"Real-time monitoring and maintaining model predictive controller (MPC) is becoming an important issue with its wide implementation in the industries. In this paper, a measure is proposed to detect faults in MFCs by comparing the performance of the actual controller with the performance of the ideal controller. The ideal controller is derived from the dynamic matrix control (DMC) in an ideal work situation and treated as a measure benchmark. A detection index based on the comparison is proposed to detect the state change of the target controller. This measure is illustrated through the implementation for a water tank process.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1426773,no,undetermined,0
Multimedia coding using adaptive regions of interest,"The key attributes of neural processing essential to adaptive multimedia processing are presented. The objective is to show why neural networks are a core technology for efficient representation for image information. It is demonstrated how neural network technology gives a solution for extracting a segmentation mask in adaptive region-of-interest (ROI) video coding. The algorithm uses a two-layer neural network architecture that classifies video frames in ROI and non-ROI areas, also being able to adapt its performance automatically to scene changes. The algorithm is incorporated in motion-compensated discrete cosine transform (MC-DCT) based coding schemes, optimally allocating more bits to ROI than to non-ROI areas, achieving better image quality, as well as signal-to-noise ratio improvements compared to standard MPEG MC-DCT encoders.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1416537,no,undetermined,0
Incorporating imperfect debugging into software fault processes,"For the traditional SRGMs, it is assumed that a detected fault is immediately removed and is perfectly repaired with no new faults being introduced. In reality, it is impossible to remove all faults from the fault correction process and have a fault-free effect on the software development environment. In order to relax this perfect debugging assumption, we introduce the possibility of imperfect debugging phenomenon. Furthermore, most of the traditional SRGMs have focused on the failure detection process. Consideration of fault correction process in the existing models is limited. However, to achieve desired level of software quality, it is very important to apply powerful technologies for removing the errors in the fault correction process. Therefore, we divide these processes into different two nonhomogeneous Poisson processes (NHPPs). Moreover, these models are considered to be more practical to depict the fault-removal phenomenon in software development.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1414597,no,undetermined,0
Fast multiple reference frame selection method for motion estimation in JVT/H.264,"The three main reasons why the new H.264 (MPEG-4 AVC) video coding standard has a significant performance better than the other standards are the adoption of variable block sizes, multiple reference frames, and the consideration of rate distortion optimization within the codec. However, these features incur a considerable increase in encoder complexity. As for the multiple reference frames motion estimation, the increased computation is in proportion to the number of searched reference frames. In this paper, a fast multi-frame selection method is proposed for H.264 video coding. The proposed scheme can efficiently determine the best reference frame from the allowed five reference frames. Simulation results show that the speed of the proposed method is over two times faster than that of the original scheme adopted in JVT reference software JM73 while keeping the similar video quality and bit-rate.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1412835,no,undetermined,0
GXP : An Interactive Shell for the Grid Environment,"We describe GXP, a shell for distributed multi-cluster environments. With GXP, users can quickly submit a command to many nodes simultaneously (approximately 600 milliseconds on over 300 nodes spread across five local-area networks). It therefore brings an interactive and instantaneous response to many cluster/network operations, such as trouble diagnosis, parallel program invocation, installation and deployment, testing and debugging, monitoring, and dead process cleanup. It features (1) a very fast parallel (simultaneous) command submission, (2) parallel pipes (pipes between local command and all parallel commands), and (3) a flexible and efficient method to interactively select a subset of nodes to execute subsequent commands on. It is very easy to start using GXP, because it is designed not to require cumbersome per-node setup and installation and to depend only on a very small number of pre-installed tools and nothing else. We describe how GXP achieves these features and demonstrate through examples how they make many otherwise boring and error-prone tasks simple, efficient, and fun",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1410681,no,undetermined,0
SUMMARY: efficiently summarizing transactions for clustering,"Frequent itemset mining was initially proposed and has been studied extensively in the context of association rule mining. In recent years, several studies have also extended its application to the transaction (or document) classification and clustering. However, most of the frequent-itemset based clustering algorithms need to first mine a large intermediate set of frequent itemsets in order to identify a subset of the most promising ones that can be used for clustering. In this paper, we study how to directly find a subset of high quality frequent itemsets that can be used as a concise summary of the transaction database and to cluster the categorical data. By exploring some properties of the subset of itemsets that we are interested in, we proposed several search space pruning methods and designed an efficient algorithm called SUMMARY. Our empirical results have shown that SUMMARY runs very fast even when the minimum support is extremely low and scales very well with respect to the database size, and surprisingly, as a pure frequent itemset mining algorithm, it is very effective in clustering the categorical data and summarizing the dense transaction databases.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1410290,no,undetermined,0
Perceptron-Based Branch Confidence Estimation,"Pipeline gating has been proposed for reducing wasted speculative execution due to branch mispredictions. As processors become deeper or wider, pipeline gating becomes more important because the amount of wasted speculative execution increases. The quality of pipeline gating relies heavily on the branch confidence estimator used. Not much work has been done on branch confidence estimators since the initial work [6]. We show the accuracy and coverage characteristics of the initial proposals do not sufficiently reduce mis-speculative execution on future deep pipeline processors. In this paper, we present a new, perceptron-based, branch confidence estimator, which is twice as accurate as the current best-known method and achieves reasonable mispredicted branch coverage. Further, the output of our predictor is multi-valued, which enables us to classify branches further as ""strongly low confident"" and ""weakly low confident"". We reverse the predictions of ""strongly low confident"" branches and apply pipeline gating to the ""weakly low confident"" branches. This combination of pipeline gating and branch reversal provides a spectrum of interesting design options ranging from significantly reducing total execution for only a small performance loss, to lower but still significant reductions in total execution, without any performance loss.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1410083,no,undetermined,0
Automated comprehensive assessment and visualization of voltage sag performance,"This paper describes modular software for the automated assessment and visualization of voltage sag performance. The software allows in-depth analysis of voltage sag performance of the individual buses, the performance of the entire network at different voltage levels and inside the industry facility. The prediction and characterization of voltage sag, identifying the area of vulnerability and the area affected by the fault, and the propagation of voltage sags can be done automatically taking into account different fault statistics for symmetrical and asymmetrical faults, and different fault distributions. The module also considers the protection system and effects of its failure on the duration of voltage sags. The software capabilities are demonstrated on a generic distribution network and the results of the module are presented in the graphical and tabular form using specially developed graphical user interface (GUI).",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1409352,no,undetermined,0
Estimating user-perceived Web QoS by means of passive measurement,"To enforce SLA management, SLA metrics and quality assessment method for network services or application services are required. In this paper two metrics for Web application service intrinsic performance, average round-trip time and delivery speed are defined taking both implementation mechanism of Web applications and user access behavior into account. In three cases, Web traffic traces for a specific Web site were recorded at client-side to calculate above two metrics; meanwhile the user-perceived service qualities of the Web site were evaluated subjectively. Linear regress analysis of the observation data shows that user-perceived quality of Web application service can be objectively estimated by a linear regression function that uses the two metrics as independent variables.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1409185,no,undetermined,0
Addressing the performance of two software reliability modeling methods,"<div style=""font-variant: small-caps; font-size: .9em;"">First Page of the Article</div><img class=""img-abs-container"" style=""width: 95%; border: 1px solid #808080;"" src=""/xploreAssets/images/absImages/01408378.png"" border=""0"">",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1408378,no,undetermined,0
Embedded system engineering using C/C++ based design methodologies,This paper analyzes and compares the effectiveness of various system level design methodologies in assessing performance of embedded computing systems from the earliest stages of the design flow. The different methodologies are illustrated and evaluated by applying them to the design of an aircraft pressurization system (APS). The APS is mapped on a heterogeneous hardware/software platform consisting of two ASICs and a microcontroller. The results demonstrate the high impact of computer aided design (CAD) tools on design time and quality.,2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1409904,no,undetermined,0
"Software, performance and resource utilisation metrics for context-aware mobile applications","As mobile applications become more pervasive, the need for assessing their quality, particularly in terms of efficiency (i.e., performance and resource utilisation), increases. Although there is a rich body of research and practice in developing metrics for traditional software, there has been little study on how these relate to mobile context-aware applications. Therefore, this paper defines and empirically evaluates metrics to capture software, resource utilisation and performance attributes, for the purpose of modelling their impact in context-aware mobile applications. To begin, a critical analysis of the problem domain identifies a number of specific software, resource utilisation and performance attributes. For each attribute, a concrete metric and technique of measurement is defined. A series of hypotheses are then proposed, and tested empirically using linear correlation analysis. The results support the hypotheses thus demonstrating the impact of software code attributes on the efficiency of mobile applications. As such, a more formal model in the form of mathematical equations is proposed in order to facilitate runtime decisions regarding the efficient placement of mobile objects in a context-aware mobile application framework. Finally, a preliminary empirical evaluation of the model is carried out using a typical application and an existing mobile application framework",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1509290,no,undetermined,0
Multispectral fingerprint biometrics,"A novel fingerprint sensor is described that combines a multispectral imager (MSI) with a conventional optical fingerprint sensor. The goal of this combination is a fingerprint sensor with improved usability and security relative to standard technology. The conventional sensor that was used in this research is a commercially available system based on total internal reflectance (TIR). It was modified to accommodate an MSI sensor in such a way that both MSI and TIR images are able to be collected when a user places his/her finger on the sensor platen. The MSI data were preprocessed to enhance fingerprint features. Both the preprocessed MSI images and the TIR images were then passed to a commercial fingerprint software package for minutiae detection and matching. A multiperson study was conducted to test the relative performance characteristics of the two types of finger data under typical office conditions. Results demonstrated that the TIR sensor performance was degraded by a large number of poor quality fingerprint images, likely due to a large percentage of samples taken on people with notably dry skin. The corresponding MSI data showed no such degradation and produced significantly better results. A selective combination of both modalities is shown to offer the potential of further performance improvements.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1495928,no,undetermined,0
A 32-bit COTS-based fault-tolerant embedded system,"This paper presents a 32-bit fault-tolerant (FT) embedded system based on commercial off-the-shelf (COTS) processors. This embedded system uses two 32-bit PentiumÂ® processors with master/checker (M/C) configuration and an external watchdog processor (WDP) for implementing a behavioral-based error detection scheme called committed instructions counting (CIC). The experimental evaluation was performed using both power-supply disturbance (PSD) and software-implemented fault injection (SWIFI) methods. A total of 9000 faults have been injected into the embedded system to measure the coverage of error detection mechanisms, i.e., the checker processor and the CIC scheme. The results show that the M/C configuration is not enough for this system and the CIC scheme could cover the limitation of the M/C configuration.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1498161,no,undetermined,0
Pattern recognition based tools enabling autonomic computing.,"Fault detection is one of the important constituents of fault tolerance, which in turn defines the dependability of autonomic computing. In presented work several pattern recognition tools were investigated in application to early fault detection. The optimal margin classifier technique was utilized to detect the abnormal behavior of software processes. The comparison with the performance of the quadratic classifiers is reported. The optimal margin classifiers were also implemented to the fault detection in hardware components. The impulse parameter probing technique was introduced to mitigate intermittent and transient fault problems. The pattern recognition framework of analysis of responses to a controlled component perturbation yielded promising results",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1498079,no,undetermined,0
Mining Logs Files for Computing System Management,"With advancement in science and technology, computing systems become increasingly more difficult to monitor, manage and maintain. Traditional approaches to system management have been largely based on domain experts through a knowledge acquisition process to translate domain knowledge into operating rules and policies. This has been experienced as a cumbersome, labor intensive, and error prone process. There is thus a pressing need for automatic and efficient approaches to monitor and manage complex computing systems. A popular approach to system management is based on analyzing system log files. However, several new aspects of the system log data have been less emphasized in existing analysis methods and posed several challenges. The aspects include disparate formats and relatively short text messages in data reporting, asynchronous data collection, and temporal characteristics in data representation. First, a typical computing system contains different devices with different software components, possibly from different providers. These various components have multiple ways to report events, conditions, errors and alerts. The heterogeneity and inconsistency of log formats make it difficult to automate problem determination. To perform automated analysis, we need to categorize the text messages with disparate formats into common situations. Second, text messages in the log files are relatively short with a large vocabulary size. Third, each text message usually contains a timestamp. The temporal characteristics provide additional context information of the messages and can be used to facilitate data analysis. In this paper, we apply text mining to automatically categorize the messages into a set of common categories, and propose two approaches of incorporating temporal information to improve the categorization performance",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1498077,no,undetermined,0
Combining Visualization and Statistical Analysis to Improve Operator Confidence and Efficiency for Failure Detection and Localization,"Web applications suffer from software and configuration faults that lower their availability. Recovering from failure is dominated by the time interval between when these faults appear and when they are detected by site operators. We introduce a set of tools that augment the ability of operators to perceive the presence of failure: an automatic anomaly detector scours HTTP access logs to find changes in user behavior that are indicative of site failures, and a visualizer helps operators rapidly detect and diagnose problems. Visualization addresses a key question of autonomic computing of how to win operators' confidence so that new tools will be embraced. Evaluation performed using HTTP logs from Ebates.com demonstrates that these tools can enhance the detection of failure as well as shorten detection time. Our approach is application-generic and can be applied to any Web application without the need for instrumentation",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1498055,no,undetermined,0
Towards Autonomic Virtual Applications in the In-VIGO System,"Grid environments enable users to share nondedicated resources that lack performance guarantees. This paper describes the design of application-centric middleware components to automatically recover from failures and dynamically adapt to grid environments with changing resource availabilities, improving fault-tolerance and performance. The key components of the application-centric approach are a global per-application execution history and an autonomic component that tracks the performance of a job on a grid resource against predictions based on the application execution history, to guide rescheduling decisions. Performance models of unmodified applications built using their execution history are used to predict failure as well as poor performance. A prototype of the proposed approach, an autonomic virtual application manager (AVAM), has been implemented in the context of the In-VIGO grid environment and its effectiveness has been evaluated for applications that generate CPU-intensive jobs with relatively short execution times (ranging from tens of seconds to less than an hour) on resources with highly variable loads - a workload generated by typical educational usage scenarios of In-VIGO-like grid environments. A memory-based learning algorithm is used to build the performance models for CPU-intensive applications that are used to predict the need for rescheduling. Results show that In-VIGO jobs managed by the AVAM consistently meet their execution deadlines under varying load conditions and gracefully recover from unexpected failures",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1498049,no,undetermined,0
Efficient algorithms and software for detection of full-length LTR retrotransposons,"LTR retrotransposons constitute one of the most abundant classes of repetitive elements in eukaryotic genomes. In this paper, we present a new algorithm for detection of full-length LTR retrotransposons in genomic sequences. The algorithm identifies regions in a genomic sequence that show structural characteristics of LTR retrotransposons. Three key components distinguish our algorithm from that of current software-(i) a novel method that preprocesses the entire genomic sequence in linear time and produces high quality pairs of LTR candidates in running time that is constant per pair, (ii) a thorough alignment-based evaluation of candidate pairs to ensure high quality prediction, and (Hi) a robust parameter set encompassing both structural constraints and quality controls providing users with a high degree of flexibility. Validation of both our serial and parallel implementations of the algorithm against the yeast genome indicates both superior quality and performance results when compared to existing software.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1498006,no,undetermined,0
TCAM-based distributed parallel packet classification algorithm with range-matching solution,"Packet classification (PC) has been a critical data path function for many emerging networking applications. An interesting approach is the use of TCAM to achieve deterministic, high speed PC However, apart from high cost and power consumption, due to slow growing clock rate for memory technology in general, PC based on the traditional single TCAM solution has difficulty to keep up with fast growing line rates. Moreover, the TCAM storage efficiency is largely affected by the need to support rules with ranges, or range matching. In this paper, a distributed TCAM scheme that exploits chip-level-parallelism is proposed to greatly improve the PC throughput. This scheme seamlessly integrates with a range encoding scheme, which not only solves the range matching problem but also ensures a balanced high throughput performance. Using commercially available TCAM chips, the proposed scheme achieves PC performance of more than 100 million packets per second (Mpps), matching OC768 (40 Gbps) line rate.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1497900,no,undetermined,0
A method for studying partial discharges location and propagation within power transformer winding based on the structural data,"Power transformer inner insulation system is a very critical component. Its degradation may pose apparatus to fail while in service. On the other hand, experimental experiences prove that partial discharges are a major source of insulation failure in power transformers. If the deterioration of the insulation system caused by PD activity can be detected at an early stage, preventive maintenance measures may be taken. Because of the complex structure of the transformer, accurate PD location is difficult and is one of the challenges power utilities are faced with. This problem comes to be vital in open access systems. In this paper a theory for locating partial discharge and its propagation along the winding is proposed, which is based on structural data of a transformer. The lumped element winding model is constructed. Quasi-static condition is applied and each turn of the winding is considered as a segment. Then an algorithm is developed to use the constructed matrices for PD location. A software package in Visual Basic environment has been developed. This paper introduces the background theory and utilized techniques.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1496196,no,undetermined,0
Fault prediction of boilers with fuzzy mathematics and RBF neural network,"How to predict potential faults of a boiler in an efficient and scientific way is very important. A lot of comprehensive research has been done, and promising results have been obtained, especially regarding the application of intelligent software. Still there are a lot of problems to be studied. It combines fuzzy mathematics with. RBF neural network in an intuition and natural way. Thus a new method is proposed for the prediction of the potential faults of a coal-fired boiler. The new method traces the development trend of related operation and state variables. The new method has been tested on a simulation machine. And its predicted results were compared with those of traditional statistical results. It is found that the new method has a good performance.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1495278,no,undetermined,0
A feasible schedulability analysis for fault-tolerant hard real-time systems,"Hard real-time systems require predictable performance despite the occurrence of failures. In this paper, the authors proposed a new fault-tolerant priority assignment algorithm based on worst-case response time schedulability analysis for fault-tolerant hard real-time system. This algorithm can be used, together with the schedulability analysis, to effectively improve system fault resilience when the two traditional fault-tolerant priority assignment policies cannot improve system fault resilience. Also, a fault-tolerant priority configuration search algorithm for the proposed analysis was presented.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1467898,no,undetermined,0
Real-time detection and containment of network attacks using QoS regulation,"In this paper, we present a network measurement mechanism that can detect and mitigate attacks and anomalous traffic in real-time using QoS regulation. The detection method rapidly pursues the dynamics of the network on the basis of correlation properties of the network protocols. By observing the proportion occupied by each traffic protocol and correlating it to that of previous states of traffic, it can be possible to determine whether the current traffic is behaving normally. When abnormalities are detected, our mechanism allows aggregated resource regulation of each protocol's traffic. The trace-driven results show that the rate-based regulation of traffic characterized by protocol classes is a feasible vehicle for mitigating the impact of network attacks on end servers.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1494367,no,undetermined,0
The role of traffic forecasting in QoS routing - a case study of time-dependent routing,"QoS routing solutions can be classified into two categories, state-dependent and time-dependent, according to their awareness of the future traffic demand in the network. Compared with representative state-dependent routing algorithms, a time-dependent variation of WSP - TDWSP - is proposed in this paper to study the role of traffic forecasting in QoS routing, by customizing itself for a range of traffic demands. Our simulation results confirm the feasibility of traffic forecasting in the context of QoS routing, which empowers TDWSP to achieve better routing performance and to overcome QoS routing difficulties, even though completely accurate traffic prediction is not required. The case study involving TDWSP further reveals that even a static forecast can remain effective over a large area in the solvable traffic demand space, if the network topology and the peak traffic value are given. Thus, the role of traffic forecasting in QoS routing becomes more prominent.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1494351,no,undetermined,0
Some schedulers to achieve proportional junk rate differentiation,"The queueing delay or loss rate is usually used as the performance metric for the real-time multimedia applications. In this paper, to provide more suitable quality of services (QoS) requirements of some applications, we propose a new performance metric, junk rate, where junk is the packet which queueing delay exceeds its time threshold. The model of proportional junk rate differentiation is also constructed to provide the predictable and controllable ratio of junk rates for different classes. Three schedulers, namely, proportional junk rate scheduler with infinite memory, proportional junk rate scheduler with memory M, average junk distance scheduler, are proposed to achieve this model. Simulation results show that three schedulers actually yield this model well.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1493831,no,undetermined,0
Fast motion estimation algorithm based on predictive line diamond search technology,"Motion estimation (ME) is one of the most time-consuming parts in video encoding systems, and significantly affects the output quality of an encoded sequence. In this paper, a new algorithm is presented, referred to as the predictive line diamond search (PLDS), which is a gradient descent search combined with a new search strategy, namely, one-dimensional line search (1DLS). 1DLS assists a local search around an initial search center with a low computational complexity. After performing 1DLS, a small diamond search pattern and a more compact line search pattern are adaptively used according to the results of the previous search. Our experimental results show that, compared with the previous techniques, the proposed algorithm has lower computational complexity and provides better prediction performance, especially for fast or complex motion sequences.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1493463,no,undetermined,0
Discerning user-perceived media stream quality through application-layer measurements,"The design of access networks for proper support of multimedia applications requires an understanding of how the conditions of the underlying network (packet loss and delays, for instance) affect the performance of a media stream. In particular, network congestion can affect the user-perceived quality of a media stream. By choosing metrics that indicate and/or predict the quality ranking that a user would assign to a media stream, we can deduce the performance of a media stream without polling users directly. We describe a measurement mechanism utilizing objective measurements taken from a media player application that strongly correlate with user rankings of stream quality. Experimental results demonstrate the viability of the chosen metrics as predictors or indicators of user quality rankings, and suggest a new mechanism for evaluating the present and future quality of a media stream.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1489940,no,undetermined,0
Real-time energy market design and operations challenges at the ISO New England,"On March 1, 2003, ISO New England launched its LMP based energy market for both real-time and day-ahead. Under this SMD implementation, the day-ahead market is a financial forward market, which does not enforce obligation on the physical generating resources to provide energy, while the real-time market involves the dispatch of physical resources to serve the real-time load in a secure and reliable manner. In this paper an over view of the real-time energy market in ISO New England is presented. Also, some operations and design challenges of the energy market was discussed.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1489477,no,undetermined,0
Evaluation and performance comparison of power swing detection algorithms,"This paper presents performance comparisons and evaluations of several power swing detection algorithms. Any sudden change in the configuration or the loading of an electrical network causes power swing between the load concentrations of the network. In order to prevent the distance protection from tripping during such conditions, a power swing blocking is utilized. The decreasing impedance, the VcosÏ† and the superimposed currents methods are conventional algorithms for power swing detection. In this paper, the behavior of these algorithms is evaluated. Different conditions have been generated by EMTDC software and voltage and current waveforms have been given to the power swing detectors. Operation of the power swing detectors has been investigated for different conditions and the operation of different algorithms are compared.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1489280,no,undetermined,0
Modeling the effects of 1 MeV electron radiation in gallium-arsenide solar cells using SILVACOÂ® virtual wafer fabrication software,The ALTAS device simulator from Silvaco International has the potential for predicting the effects of electron radiation in solar cells by modeling material defects. A GaAs solar cell was simulated in ATLAS and compared to an actual cell with radiation defects identified using deep level transient spectroscopy techniques (DLTS). The solar cells were compared for various fluence levels of 1 MeV electron radiation and showed an average of less than three percent difference between experimental and simulated cell output characteristics. These results demonstrate that ATLAS software can be a viable tool for predicting solar cell degradation due to electron radiation.,2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1488249,no,undetermined,0
Software based in-system memory test for highly available systems,"In this paper we describe a software based in-system memory test that is capable of testing system memory in both offline and online environments. A technique to transparently ""steal"" a chunk of memory from the system for running tests and then inserting it back for normal application's use is proposed. Factors like system memory architecture that needs to be considered while adapting any conventional memory testing algorithm for in-system testing are also discussed. Implementation of the proposed techniques can significantly improve the system's ability to proactively detect and manage functional faults in memory. An extension of the methodology described is expected to be applicable for in-system testing of other system components (like processor) as well.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1498209,no,undetermined,0
The impact of institutional forces on software metrics programs,"Software metrics programs are an important part of a software organization's productivity and quality initiatives as precursors to process-based improvement programs. Like other innovative practices, the implementation of metrics programs is prone to influences from the greater institutional environment the organization exists in. In this paper, we study the influence of both external and internal institutional forces on the assimilation of metrics programs in software organizations. We use previous case-based research in software metrics programs as well as prior work in institutional theory in proposing a model of metrics implementation. The theoretical model is tested on data collected through a survey from 214 metrics managers in defense-related and commercial software organizations. Our results show that external institutions, such as customers and competitors, and internal institutions, such as managers, directly influence the extent to which organizations change their internal work-processes around metrics programs. Additionally, the adaptation of work-processes leads to increased use of metrics programs in decision-making within the organization. Our research informs managers about the importance of management support and institutions in metrics programs adaptation. In addition, managers may note that the continued use of metrics information in decision-making is contingent on adapting the organization's work-processes around the metrics program. Without these investments in metrics program adaptation, the true business value in implementing metrics and software process improvement is not realized.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1498772,no,undetermined,0
A game model for selection of purchasing bids in consideration of fuzzy values,"A number of efficiency-based vendor selection and negotiation models have been developed to deal with multiple attributes including price, quality and delivery performance which is treated as important bid attributes. But some alternative vendor's preferences of attributes are difficult of quantitative analysis, such as trust, reliability, and courtesy of the vendor, which are considered to be crucial issues of recent reaches in vendor evaluation. This paper proposes a buyer-seller game model that has distinct advantages over existing methods for bid selection and negotiation, the fuzzy indexes are used to evaluate those attributes which are difficult of quantitative analysis, we propose a new method that expands Talluri (2002) and Joe Zhu (2004)'s method and allows which the elements composing problems are given by fuzzy numerical values, An important outcome of assessing relative efficiencies within a group of decision making units (DMUs) in fuzzy data envelopment analysis is a set of virtual multipliers or weights accorded to each (input or output) factor taken into account. In this paper, by assessing upper bounds on factor weights and compacting the resulted intervals, a CSW is determined. Since resulted efficiencies by the proposed CSW are fuzzy numbers rather than crisp values, it is more informative for decision maker.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1499472,no,undetermined,0
Optimized distributed delivery of continuous-media documents over unreliable communication links,"Video-on-demand (VoD) applications place very high requirements on the delivery medium. High-quality services should provide for a timely delivery of the data-stream to the clients plus a minimum of playback disturbances. The major contributions of this paper are that it proposes a multiserver, multi-installment (MSMI) solution approach (sending the document in several installments from each server) to the delivery problem and achieves a minimization of the client waiting time, also referred to as the access time (AT) or start-up latency in the literature. By using multiple spatially distributed servers, we are able to exploit slow connections that would otherwise prevent the deployment of video-on-demand-like services, to offer such services in an optimal manner. Additionally, the delivery and playback schedule that is computed by our approach is loss-aware in the sense that it is flexible enough to accommodate packet losses without interrupts. The mathematical framework presented covers both computation and optimization problems associated with the delivery schedule, offering a complete set of guidelines for designing MSMI VoD services. The optimizations presented include the ordering of the servers and determining the number of installments based on the packet-loss probabilities of the communication links. Our analysis guarantees the validity of a delivery schedule recommended by the system by providing a percentage of confidence for an uninterrupted playback at the client site. This, in a way, quantifies the degree of quality of service rendered by the system and the MSMI strategy proposed. The paper is concluded by a rigorous simulation study that showcases the substantial advantages of the proposed approach and explores how optimization of the schedule parameters affects performance.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1501809,no,undetermined,0
11th IEEE International Software Metrics Symposium - Cover,To following topics are dealt with: software metrics; cost estimation; software architectures; measurement-based decision making; quality assurance; product assessment; process improvement; education; open source software projects; data mining; software repositories; defect analysis and testing; and function points,2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1509273,no,undetermined,0
A BIST approach for testing FPGAs using JBITS,"This paper explores the built-in self test (BIST) concepts to test the configurable logic blocks (CLBs) of static RAM (SRAM) based FPGAs using Java Bits (JBits). The proposed technique detects and diagnoses single and multiple stuck-at faults in the CLBs while significantly reducing the time taken to perform the testing. Previous BIST approaches for testing FPGAs use traditional CAD tools which lack control over configurable resources, resulting in the design being placed on the hardware in a different way than intended by the designer. In this paper, the design of the logic BIST architecture is done using JBits 2.8 software for Xilinx Virtex family of devices. The test requires seven configurations and two test sessions to test the CLBs. The time taken to generate the entire BIST logic in both the sessions is approximately 77 seconds as compared with several minutes to hours in traditional design flow.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1508546,no,undetermined,0
Scheduling tasks with Markov-chain based constraints,"Markov-chain (MC) based constraints have been shown to be an effective QoS measure for a class of real-time systems, particularly those arising from control applications. Scheduling tasks with MC constraints introduces new challenges because these constraints require not only specific task finishing patterns but also certain task completion probability. Multiple tasks with different MC constraints competing for the same resource further complicates the problem. In this paper, we study the problem of scheduling multiple tasks with different MC constraints. We present two scheduling approaches which (i) lead to improvements in ""overall"" system performance, and (ii) allow the system to achieve graceful degradation as system load increases. The two scheduling approaches differ in their complexities and performances. We have implemented our scheduling algorithms in the QNX real-time operating system environment and used the setup for several realistic control tasks. Data collected from the experiments as well as simulation all show that our new scheduling algorithms outperform algorithms designed for window-based constraints as well as previous algorithms designed for handling MC constraints.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1508457,no,undetermined,0
Business performance management system for CRM and sales execution,"In 2004 the IBM Telesales organization launched a new customer segmentation process to improve profits, revenue growth and customer satisfaction. The challenges were to automatically monitor customer segment status to ensure results are in line with segment targets, and to automatically generate high-quality predictive analytical models to improve customer segmentation rules and management over time. This paper describes a software solution that combines business performance management with data mining techniques to provide a powerful combination of performance monitoring and proactive customer management in support of the new telesales business processes.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1508389,no,undetermined,0
Measuring the functionality of online stores,"This paper shows the need of a framework which can be used to measure the functionality delivered by electronic commerce (e-commerce) systems. Such a framework would be helpful in areas such as cost prediction, effort estimation, and so on. The paper goes on to propose such a framework, based on the established methods of function points and object points.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1508087,no,undetermined,0
Survivability of a distributed multi-agent application - a performance control perspective,"Distributed multi-agent systems (DMAS) such as supply chains functioning in highly dynamic environments need to achieve maximum overall utility during operation. The utility from maintaining performance is an important component of their survivability. This utility is often met by identifying trade-offs between quality of service and performance. To adaptively choose the operational settings for better utility, we propose an autonomous and scalable queueing theory based methodology to control the performance of a hierarchical network of distributed agents. By formulating the MAS as an open queueing network with multiple classes of traffic we evaluate the performance and subsequently the utility, from which we identify the control alternative for a localized, multi-tier zone. When the problem scales, another larger queueing network could be composed using zones as building blocks. This method advocates the systematic specification of the DMAS's attributes to aid realtime translation of the DMAS into a queueing network. We prototype our framework in Cougaar and verify our results.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1507044,no,undetermined,0
2005 IEEE International Conference on Automation Science and Engineering (CASE) (IEEE Cat. No.05EX1181),The following topics were dealt with: laboratory automation; yeast cell automated lifetime analysis; rapid prototyping systems; production process and its quality measurement; production management; fish breeding optimization; patient treatment; control theory; robots; dynamic shipment planning; machining; collaborative diagnostics; industrial process; manufacturing systems; multi-agent systems; supply chain management; impulse resistance measurement instrument; and production fault detection.,2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1506731,no,undetermined,0
Empirical case studies in attribute noise detection,"The problem of determining the noisiest attribute(s) from a set of domain-specific attributes is of practical importance to domain experts and the data mining community. Data noise is generally of two types: attribute noise and mislabeling errors (class noise). For a given domain-specific dataset, attributes that contain a significant amount of noise can have a detrimental impact on the success of a data mining initiative, e.g., reducing the predictive ability of a classifier in a supervised learning task. Techniques that provide information about the noise quality of an attribute are useful tools for a data mining practitioner when performing analysis on a dataset or scrutinizing the data collection processes. Our technique for detecting noisy attributes uses an algorithm that we recently proposed for the detection of instances with attribute noise. This paper presents case studies that confirm our recent work done on detecting noisy attributes and further validates that our technique is indeed able to detect attributes that contain noise.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1506475,no,undetermined,0
A comparative study on software reuse metrics and economic models from a traceability perspective,"A fundamental task when employing software reuse is evaluating its impacts by measuring the relation of reused and developed software, the cost for obtaining reuse and the cost avoided by reusing software during development and maintenance. Different reuse related metrics exist in the literature, varying from strictly code-based metrics, aiming to measure the amount of code reused in a product, to more elaborate cost-based metrics and models, aiming to measure the costs involved in reuse programs and to evaluate the impacts of reuse in software development. Although reuse is commonly claimed to benefit maintenance, the traceability problem is still neglected on the reuse metrics arena, despite its great impacts on software maintenance. Reuse metrics may be used as important support tools for dealing with the traceability between reused assets and their clients. The goal of this work is to evaluate the current state of the art on the reuse metrics area with special emphasis on code-based metrics, building on previous surveys with further analysis and considerations on the applicability of such metrics to reuse traceability.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1506452,no,undetermined,0
"Hydratools, a MATLABÂ® based data processing package for Sontek Hydra data","The U.S. Geological Survey (USGS) has developed a set of MATLAB tools to process and convert data collected by Sontek Hydra instruments to netCDF, which is a format used by the USGS to process and archive oceanographic time-series data. The USGS makes high-resolution current measurements within 1.5 meters of the bottom. These data are used in combination with other instrument data from sediment transport studies to develop sediment transport models. Instrument manufacturers provide software which outputs unique binary data formats. Multiple data formats are cumbersome. The USGS solution is to translate data streams into a common data format: netCDF. The Hydratools toolbox is written to create netCDF format files following EPIC conventions, complete with embedded metadata. Data are accepted from both the ADV and the PCADP. The toolbox will detect and remove bad data, substitute other sources of heading and tilt measurements if necessary, apply ambiguity corrections, calculate statistics, return information about data quality, and organize metadata. Standardized processing and archiving makes these data more easily and routinely accessible locally and over the Internet. In addition, documentation of the techniques used in the toolbox provides a baseline reference for others utilizing the data.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1506360,no,undetermined,0
Development of ANN-based virtual fault detector for Wheatstone bridge-oriented transducers,This paper reports on the development of a new artificial neural network-based virtual fault detector (VFD) for detection and identification of faults in DAS-connected Wheatstone bridge-oriented transducers of a computer-based measurement system. Experimental results show that the implemented VFD is convenient for fusing intelligence into such systems in a user-interactive manner. The performance of the proposed VFD is examined experimentally to detect seven frequently occurring faults automatically in such transducers. The presented technique used an artificial neural network-based two-class pattern classification network with hard-limit perceptrons to fulfill the function of an efficient residual generator component of the proposed VFD. The proposed soft residual generator detects and identifies various transducer faults in collaboration with a virtual instrument software-based inbuilt algorithm. An example application is also presented to demonstrate the use of implemented VFD practically for detecting and diagnosing faults in a pressure transducer having semiconductor strain gauges connected in a Wheatstone bridge configuration. The results obtained in the example application with this strategy are promising.,2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1504767,no,undetermined,0
Analyzing gene expression time-courses,"Measuring gene expression over time can provide important insights into basic cellular processes. Identifying groups of genes with similar expression time-courses is a crucial first step in the analysis. As biologically relevant groups frequently overlap, due to genes having several distinct roles in those cellular processes, this is a difficult problem for classical clustering methods. We use a mixture model to circumvent this principal problem, with hidden Markov models (HMMs) as effective and flexible components. We show that the ensuing estimation problem can be addressed with additional labeled data partially supervised learning of mixtures - through a modification of the expectation-maximization (EM) algorithm. Good starting points for the mixture estimation are obtained through a modification to Bayesian model merging, which allows us to learn a collection of initial HMMs. We infer groups from mixtures with a simple information-theoretic decoding heuristic, which quantifies the level of ambiguity in group assignment. The effectiveness is shown with high-quality annotation data. As the HMMs we propose capture asynchronous behavior by design, the groups we find are also asynchronous. Synchronous subgroups are obtained from a novel algorithm based on Viterbi paths. We show the suitability of our HMM mixture approach on biological and simulated data and through the favorable comparison with previous approaches. A software implementing the method is freely available under the GPL from http://ghmm.org/gql.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1504683,no,undetermined,0
Software reliability prediction and analysis during operational use,"In reality, the fault detection (or correction) phenomenon and software reliability estimation in the operational phase are different from that in the testing phase. The fault removal continues at a slower rate during the operational phase. In this paper, we will investigate some techniques for software reliability prediction and measurement in the operational phase. We first review how some software reliability growth models (SRGMs) based on non-homogeneous Poisson processes (NHPPs) can be readily derived based on a unified theory for NHPP models. Under this general framework, we can not only verify some conventional SRGMs but also derive some new SRGMs that can be used for software reliability measurement in the operational phase. That is, based on the unified theory, we can incorporate the concept of multiple changepoints into software reliability modeling. We can formularize and simulate the fault detection process in the operational phase. Some numerical illustrations based on real software failure data are also presented. The experimental results show that the proposed models can easily reflect the possible changes of fault detection process and offer quantitative analysis on software failure behavior in field operation.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1503132,no,undetermined,0
How to produce better quality test software,"LabWindows/CVI is a popular C compiler for writing automated test equipment (ATE) test software. Since C was designed as a portable assembly language, it uses many low-level machine operations that tend to be error prone, even for the professional programmer. Test equipment engineers also tend to underestimate the effort required to write high-quality software. Quality software has very few defects and is easy to write and maintain. The examples used in this article are for the C programming language, but the principles also apply to most other programming languages. Most of the tools mentioned work with both C and C++ software.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1502445,no,undetermined,0
A study on the application of digital cameras to derive audio information from the TVs on trains,"Digital camera can help people take videos anytime with ease. For this reason, application of digital video camera's technology is the hottest topic right now. Due to the fact that the TV on the train is in a public place it can't give audio information. We were trying to find a way to get the audio information by second time (ST) detection video only. ""Second time"" (ST) means the user uses their digital camera to take video from the original video. We propose a new way to use digital watermarking by embedding some pointer symbols into the source video, so passengers can use this kind of video to detect high quality audio in ST situations. In order to show other possible application for this process in the future, in this study we also use VB programming to design simulation software ""DDAS"" (directly detection audio system). The software was designed to handle uncompressed AVl files. We tested this software using ST video. In the Experiments of this software successfully detected audio information and audio file types covering AVI, MPEG, MIDI and WAV file type. According to the questionnaires from the users, DDAS system's output audio file was given a 3.8 MOS level which shows the system has enough audio embedded ability. From the questionnaires, we also found ST video's screen size affects the detected audio's MOS quality. We found the best MOS quality was located in 1:1âˆ?:4/3 screen size. We also use this kind of technology for study and we found that using multimedia improves the student's grades and the grades about 6 average grades.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1502362,no,undetermined,0
Application-specific worst case corners using response surfaces and statistical models,"Integrated circuits (ICs) must be robust to manufacturing variations. Circuit simulation at a set of worst case corners is a computationally efficient method for verifying the robustness of a design. This paper presents a new statistical methodology to determine the worst case corners for a set of circuit performances. The proposed methodology first estimates response surfaces for circuit performances as quadratic functions of the process parameters with known statistical distributions. These response surfaces are then used to extract the worst case corners in the process parameter space as the points where the circuit performances are at their minimum/maximum values corresponding to a specified tolerance level. Corners in the process parameter space close to each other are clustered to reduce their number, which reduces the number of simulations required for design verification. The novel concept of a relaxation coefficient to ensure that the corners capture the minimum/maximum values of all the circuit performances at the desired tolerance level is also introduced. The corners are realistic since they are derived from the multivariate statistical distribution of the process parameters at the desired tolerance level. The methodology is demonstrated with examples showing extraction of corners from digital standard cells and also the corners for analog/radio frequency (RF) blocks found in typical communication ICs.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1501902,no,undetermined,0
P-RnaPredict-a parallel evolutionary algorithm for RNA folding: effects of pseudorandom number quality,"This paper presents a fully parallel version of RnaPredict, a genetic algorithm (GA) for RNA secondary structure prediction. The research presented here builds on previous work and examines the impact of three different pseudorandom number generators (PRNGs) on the GA's performance. The three generators tested are the C standard library PRNG RAND, a parallelized multiplicative congruential generator (MCG), and a parallelized Mersenne Twister (MT). A fully parallel version of RnaPredict using the Message Passing Interface (MPI) was implemented on a 128-node Beowulf cluster. The PRNG comparison tests were performed with known structures whose sequences are 118, 122, 468, 543, and 556 nucleotides in length. The effects of the PRNGs are investigated and the predicted structures are compared to known structures. Results indicate that P-RnaPredict demonstrated good prediction accuracy, particularly so for shorter sequences.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1501839,no,undetermined,0
Inconsistency measurement of software requirements specifications: an ontology-based approach,"Management of requirements inconsistency is key to the development of complex trustworthy software system, and precise measurement is precondition for the management of requirements inconsistency properly. But at present, although there are a lot of work on the detection of requirements inconsistency, most of them are limited in treating requirements inconsistency according to heuristic rules, we still lacks of promising method for handling requirements inconsistency properly. Based on an abstract requirements refinement process model, this paper takes domain ontology as infrastructure for the refinement of software requirements, the aim of which is to get requirements descriptions that are comparable. Thus we can measure requirements inconsistency based on tangent plane of requirements refinement tree, after we have detected inconsistent relations of leaf nodes at semantic level.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1467922,no,undetermined,0
A cost-efficient server architecture for real-time credit-control,"The importance of mobile and electronic commerce results in much attention given to credit-control systems. There are high non-functional requirements on such systems, e.g. high availability and reliability. These requirements can be met by changing the architecture of the credit-control system. In this paper the authors suggested a new architecture and a number of alternative implementations of credit-control server. By quantifying the availability, reliability, performance and cost the designers enabled to make better trade-off decisions. The new architecture was compared with the current, ""state-of-the-art"" solution. Finally, suggestions for the system developers concerning the choice of an appropriate architecture implementation variant were presented.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1467897,no,undetermined,0
Encoder-Assisted Adaptive Video Frame Interpolation,"<div style=""font-variant: small-caps; font-size: .9em;"">First Page of the Article</div><img class=""img-abs-container"" style=""width: 95%; border: 1px solid #808080;"" src=""/xploreAssets/images/absImages/01415413.png"" border=""0"">",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1415413,no,undetermined,0
Customizing event ordering middleware for component-based systems,"The stringent performance requirements of distributed realtime embedded systems often require highly optimized implementations of middleware services. Performing such optimizations manually can be tedious and error-prone. This paper proposes a model-driven approach to generate customized implementations of event ordering services in the context of component based systems. Our approach is accompanied by a number of tools to automate the customization. Given an application App, an event ordering service Order and a middleware platform P, we provide tools to analyze high-level specifications of App to extract information relevant to event ordering and to use the extracted application information to obtain a customized service, Order(App), with respect to the application usage.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1420992,no,undetermined,0
Advanced suppression of stochastic pulse shaped partial discharge disturbances,"Digital partial discharge (PD) diagnosis and testing systems are state of the art for performing quality assurance or fault identification on high voltage apparatus as well as commissioning tests. However their on-site application is a rather difficult task as PD information is generally superimposed with electromagnetic disturbances. These disturbances affect negatively all known PD evaluation systems. Especially the detection and suppression of stochastically distributed pulse shaped disturbances is a major problem. To determine such disturbances fast machine intelligent recognition systems are being developed. Three different strategies based on digital signal processing are discussed in this paper, while focusing on time resolved neural signal recognition. The system investigated more closely is currently able to distinguish between PD pulses and disturbances with the disturbances values being ten times higher than the peak values of the PD pulses. Therefore a measuring system acquires the input data in the VHF range (20-100 MHz). The discrimination of the pulses is performed in real time in time domain using fast neural network hardware. With that signal recognition system a noise reassessed phase resolved pulse sequence (PRPS) data set in the CIGRE data format is generated that can be the input source of most PD evaluation software. Optionally a noise reassessed analogue data stream can be generated that is suitable for any conventional PD measuring system.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1430396,no,undetermined,0
Managing a relational database with intelligent agents,"A prototype relational database system was developed that has indexing capability, which threads into data acquisition and analysis programs used by a wide range of researchers. To streamline the user interface and table design, free-formatted table entries were used as descriptors for experiments. This approach potentially could increase data entry errors, compromising system index and retrieval capabilities. A methodology of integrating intelligent agents with the relational database was developed to cleanse and improve the data quality for search and retrieval. An intelligent agent was designed using JACK<sup>â„?/sup> (Agent Oriented Software Group) and integrated with an Oracle-based relational database. The system was tested by triggering agent corrective measures and was found to improve the quality of the data entries. Wider testing protocols and metrics for assessing its performance are subjects for future studies. This methodology for designing intelligent-based database systems should be useful in developing robust large-scale database systems.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1428468,no,undetermined,0
Design Aspects for Wide-Area Monitoring and Control Systems,"This paper discusses the basic design and special applications of wide-area monitoring and control systems, which complement classical protection systems and Supervisory Control and Data Acquisition/Energy Management System applications. Systemwide installed phasor measurement units send their measured data to a central computer, where snapshots of the dynamic system behavior are made available online. This new quality of system information opens up a wide range of new applications to assess and actively maintain system's stability in case of voltage, angle or frequency instability, thermal overload, and oscillations. Recent developed algorithms and their design for these application areas are introduced. With practical examples, the benefits in terms of system security are shown.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1428012,no,undetermined,0
Opportunistic file transfer over a fading channel under energy and delay constraints,"We consider transmission control (rate and power) strategies for transferring a fixed-size file (finite number of bits) over fading channels under constraints on both transmit energy and transmission delay. The goal is to maximize the probability of successfully transferring the entire file over a time-varying wireless channel modeled as a finite-state Markov process. We study two implementations regarding the delay constraints: an average delay constraint and a strict delay constraint. We also investigate the performance degradation caused by the imperfect (delayed or erroneous) channel knowledge. The resulting optimal policies are shown to be a function of the channel-state information (CSI), the residual battery energy, and the number of residual information bits in the transmit buffer. It is observed that the probability of successful file transfer increases significantly when the CSI is exploited opportunistically. When the perfect instantaneous CSI is available at the transmitter, the faster channel variations increase the success probability under delay constraints. In addition, when considering the power expenditure in the pilot for channel estimation, the optimal policy shows that the transmitter should use the pilot only if there is sufficient energy left for packet transfer; otherwise, a channel-independent policy should be used.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1425747,no,undetermined,0
High-abstraction level complexity analysis and memory architecture simulations of multimedia algorithms,"An appropriate complexity analysis stage is the first and fundamental step for any methodology aiming at the implementation of today's (complex) multimedia algorithms. Such a stage may have different final implementation goals such as defining a new architecture dedicated to the specific multimedia standard under study, or defining an optimal instruction set for a selected processor architecture, or to guide the software optimization process in terms of control-flow and data-flow optimization targeting a specific architecture. The complexity of nowadays multimedia standards, in terms of number of lines of codes and cross-relations among processing algorithms that are activated by specific input signals, goes far beyond what the designer can reasonably grasp from the ""pencil and paper"" analysis of the (software) specifications. Moreover, depending on the implementation goal different measures and metrics are required at different steps of the implementation methodology or design flow. The process of extracting the desired measures needs to be supported by appropriate automatic tools, since code rewriting, at each design stage, may result resource consuming and error prone. This paper reviews the state of the art of complexity analysis methodologies oriented to the design of multimedia systems and presents an integrated tool for automatic analysis capable of producing complexity results based on rich and customizable metrics. The tool is based on a C virtual machine that allows extracting from any C program execution the operations and data-flow information, according to the defined metrics. The tool capabilities include the simulation of virtual memory architectures. This paper shows some examples of complexity analysis results that can be yielded with the tool and presents how the tools can be used at different stages of implementation methodologies.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1425531,no,undetermined,0
Theoretical consideration of adaptive fault tolerance architecture for open distributed object-oriented computing,"In distributed object-oriented computing, a complex set of requirements has to be considered for maintaining the required level of reliability of objects to give higher level of performance. The authors proposed a mechanism to analyze the complexity of these underlying environments and design a dynamically reconfigurable architecture according to the changes of the underlying environment. The replication should have a suitable architecture for the adaptable fault tolerance so that it can handle the different situations of the underlying system before the fault occurs. The system can provide the required level of reliability by measuring the reliability of the underlying environment and then either adjusting the replication degree or migrating the object appropriately. It is also possible to improve the reliability with shifting to a suitable replication protocol adaptively. However, there should be a mechanism to overcome the problems when ""replication protocol transition period"" exists. This will also find a client-server group communication protocol that communicates with objects under both different environmental conditions and different replication protocols.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1423726,no,undetermined,0
Multilevel-converter-based VSC transmission operating under fault AC conditions,"A study of a floating-capacitor (FC) multilevel-converter-based VSC transmission system operating under unbalanced AC conditions is presented. The control strategy is based on the use of two controllers, i.e. a main controller, which is implemented in the synchronous d-q frame without involving positive and negative sequence decomposition, and an auxiliary controller, which is implemented in the negative sequence d-q frame with the negative sequence current extracted. Automatic power balancing during AC fault is achieved without communication between the two converters by automatic power modulation on the detection of abnormal DC voltages. The impact of unbalanced floating capacitor voltages of the FC converter on power devices is described. A software-based method, which adds square waves whose amplitudes vary with the capacitor voltage errors to the nominal modulation signals for fast capacitor voltage balancing during faults, is proposed. Simulations on a 300 kV DC, 300 MW VSC transmission system based on a four-level FC converter show good performance of the proposed control strategy during unbalanced conditions caused by single-phase to ground fault.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1421136,no,undetermined,0
Do SQA programs work - CMM works. a meta analysis,"Many software development professionals and managers of software development organizations are not fully convinced in the profitability of investments for the advancement of SQA systems. The results included in each of the articles we found, cannot lead to general conclusions on the impact of investments in upgrading an SQA system. Our meta analysis was based on CMM level transition (CMMLT) analysis of available publications and was for the seven most common performance metric. The CMMLT analysis is applicable for combined analysis of empirical data from many sources. Each record in our meta analysis database is calculated as ""after-before ratio"", which is nearly free of the studied organization's characteristics. Because the CMM guidelines and SQA requirement are similar, we claim that the results for CMM programs are also applicable to investments in SQA systems. The extensive database of over 1,800 projects from a variety of 19 information sources leading to the meta analysis results - proved that investments in CMM programs and similarly in SQA systems contribute to software development performance.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1421069,no,undetermined,0
Tool-based configuration of real-time CORBA middleware for embedded systems,"Real-time CORBA is a middleware standard that has demonstrated successes in developing distributed, realtime, and embedded (DRE) systems. Customizing real-time CORBA for an application can considerably reduce the size of the middleware and improve its performance. However, customizing middleware is an error-prone task and requires deep knowledge of the CORBA standard as well as the middleware design. This paper presents ZEN-kit, a graphical tool for customizing RTZen (an RTSJ-based implementation of real-time CORBA). This customization is achieved through modularizing the middleware so that features may be inserted or removed based on the DRE application requirements. This paper presents three main contributions: 1) it describes how real-time CORBA features can be modularized and configured in RTZen using components and aspects, 2) it provides a configuration strategy to customize real-time middleware to achieve low-footprint ORBs, and 3) it presents ZEN-kit, a graphical tool for composing customized real-time middleware.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1420990,no,undetermined,0
On a method for mending time to failure distributions,"Many software reliability growth models assume that the time to next failure may be infinite; i.e., there is a chance that no failure will occur at all. For most software products this is too good to be true even after the testing phase. Moreover, if a non-zero probability is assigned to an infinite time to failure, metrics like the mean time to failure do not exist. In this paper, we try to answer several questions: Under what condition does a model permit an infinite time to next failure? Why do all non-homogeneous Poisson process (NHPP) models of the finite failures category share this property? And is there any transformation mending the time to failure distributions? Indeed, such a transformation exists; it leads to a new family of NHPP models. We also show how the distribution function of the time to first failure can be used for unifying finite failures and infinite failures NHPP models.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1467830,no,undetermined,0
Understanding perceptual distortion in MPEG scalable audio coding,"In this paper, we study coding artifacts in MPEG-compressed scalable audio. Specifically, we consider the MPEG advanced audio coder (AAC) using bit slice scalable arithmetic coding (BSAC) as implemented in the MPEG-4 reference software. First we perform human subjective testing using the comparison category rating (CCR) approach, quantitatively comparing the performance of scalable BSAC with the nonscaled TwinVQ and AAC algorithms. This testing indicates that scalable BSAC performs very poorly relative to TwinVQ at the lowest bitrate considered (16 kb/s) largely because of an annoying and seemingly random mid-range tonal signal that is superimposed onto the desired output. In order to better understand and quantify the distortion introduced into compressed audio at low bit rates, we apply two analysis techniques: Reng bifrequency probing and time-frequency decomposition. Using Reng probing, we conclude that aliasing is most likely not the cause of the annoying tonal signal; instead, time-frequency or spectrogram analysis indicates that its cause is most likely suboptimal bit allocation. Finally, we describe the energy equalization quality metric (EEQM) for predicting the relative perceptual performance of the different coding algorithms and compare its predictive ability with that of ITU Recommendation ITU-R BS.1387-1.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1420376,no,undetermined,0
"A model-driven performance analysis framework for distributed, performance-sensitive software systems","Large-scale, distributed, performance-sensitive software (DPSS) systems from the basis of mission- and often safety-critical applications. DPSS systems comprise of many independent artifacts, such as network/bus interconnects, many coordinated local and remote endsystems, and multiple layers of software. DPSS systems demand multiple, simultaneous predictable performance requirements such as end-to-end latencies, throughput, reliability and security, while also requiring the ability to control and adapt operating characteristics for applications with respect to such features as time, quantity of information, and quality of service (QoS) properties including predictability, controllability, and adaptability of operating characteristics for applications with respect to such features as time, quantity of information, accuracy, confidence and synchronization. All these issues become highly volatile in large-scale DPSS systems due to the dynamic interplay of the many interconnected parts that are often constructed from smaller parts.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1420143,no,undetermined,0
Optimizing checkpoint sizes in the C3 system,"The running times of many computational science applications are much longer than the mean-time-between-failures (MTBF) of current high-performance computing platforms. To run to completion, such applications must tolerate hardware failures. Checkpoint-and-rest art (CPR) is the most commonly used scheme for accomplishing this - the state of the computation is saved periodically on stable storage, and when a hardware failure is detected, the computation is restarted from the most recently saved state. Most automatic CPR, schemes in the literature can be classified as system-level checkpointing schemes because they take core-dump style snapshots of the computational state when all the processes are blocked at global barriers in the program. Unfortunately, a system that implements this style of checkpointing is tied to a particular platform amd cannot optimize the checkpointing process using application-specific knowledge. We are exploring an alternative called automatic application-level checkpointing. In our approach, programs are transformed by a pre-processor so that they become self-checkpointing and self-rest art able on any platform. In this paper, we evaluate a mechanism that utilizes application knowledge to minimize the amount of information saved in a checkpoint.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1420141,no,undetermined,0
Resource Allocation for Periodic Applications in a Shipboard Environment,"Providing efficient workload management is an important issue for a large-scale heterogeneous distributed computing environment where a set of periodic applications is executed. The considered distributed system is expected to operate in an environment where the input workload is likely to change unpredictably, possibly invalidating a resource allocation that was based on the initial workload estimate. The tasks consist of multiple application strings, each made up of an ordered sequence of applications. There are quality of service (QoS) constraints that must be satisfied for each string. This work addresses the problem of finding a robust initial allocation of resources to application strings that is able to absorb some level of unknown input workload increase without rescheduling. An allocation feasibility analysis is presented followed by four heuristics for finding a near-optimal allocation of resources. The performance of the proposed heuristics is evaluated and compared using simulation. The proposed heuristics also are compared to a mathematically derived upper bound.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1419957,no,undetermined,0
A Cost-Effective Main Memory Organization for Future Servers,"Today, the amount of main memory in mid-range servers is pushing practical limits with as much as 192 GB memory in a 24 processor system. Further, with the onset of multi-threaded, multi-core processor chips, it is likely that the number of memory chips per processor chip will start to increase, making DRAM cost and size an even larger burden. We investigate in this paper an alternative main memory organization - a two-level noninclusive memory hierarchy - where the second level is substantially slower than the first level, with the aim of reducing total system cost and spatial requirements of servers of today and the future. We quantitatively investigate how big and how slow the second level can be. Surprisingly, we find that only 30% of the entire memory resources typically needed must be accessed at DRAM speed whereas the rest can be accessed at a speed that is an order of magnitude slower with a negligible (1.2% on average) performance impact. We also present a cost-effective implementation of how to manage such a hierarchy and how it can bring down memory cost by leveraging memory compression and sharing of memory resources among servers.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1419865,no,undetermined,0
Self-Managing Sensor-Based Middleware for Performance Monitoring and Data Integration in Grids,"This paper describes a sensor-based middleware for performance monitoring and data integration in the Grid that is capable of self-management. The middleware unifies both system and application monitoring in a single system, storing various types of monitoring and performance data in decentralized storages, and providing a uniform interface to access that data. We have developed event-driven and demand-driven sensors to support rule-based monitoring and data integration. Grid service-based operations and TCP-based data delivery are exploited to balance tradeoffs between interoperability, flexibility and performance. Peer-to-peer features have been integrated into the middleware, enabling self-managing capabilities, supporting group-based and automatic data discovery, data query and subscription of performance and monitoring data.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1419830,no,undetermined,0
Software-synchronized all-optical sampling for fiber communication systems,"This paper describes a software-synchronized all-optical sampling system that presents synchronous eye diagrams and data patterns as well as calculates accurate Q values without requiring clock recovery. A synchronization algorithm is presented that calculates the offset frequency between the data bit rate and the sampling rate, and as a result, synchronous eye diagrams can be presented. The algorithm is shown to be robust toward poor signal quality and adds less than 100-fs timing drift to the eye diagrams. An extension of the software synchronization algorithm makes it possible to automatically find the pattern length of a periodic data pattern in a data signal. As a result, individual pulses can be investigated and detrimental effects present on the data signal can be identified. Noise averaging can also be applied. To measure accurate Q values without clock recovery, a high sampling rate is required in order to establish the noise statistics of the measured signal before any timing drift occurs. This paper presents a system with a 100-MHz sampling rate that measures accurate Q values at bit rates as high as 160 Gb/s. The high bandwidth of the optical sampling system also contributes to sampling more noise, which in turn results in lower Q values compared with conventional electrical sampling with a lower bandwidth. A theory that estimates the optically sampled Q values as a function of the sampling gate width is proposed and experimentally verified.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1417004,no,undetermined,0
Dynamic routing in translucent WDM optical networks: the intradomain case,"Translucent wavelength-division multiplexing optical networks use sparse placement of regenerators to overcome physical impairments and wavelength contention introduced by fully transparent networks, and achieve a performance close to fully opaque networks at a much less cost. In previous studies, we addressed the placement of regenerators based on static schemes, allowing for only a limited number of regenerators at fixed locations. This paper furthers those studies by proposing a dynamic resource allocation and dynamic routing scheme to operate translucent networks. This scheme is realized through dynamically sharing regeneration resources, including transmitters, receivers, and electronic interfaces, between regeneration and access functions under a multidomain hierarchical translucent network model. An intradomain routing algorithm, which takes into consideration optical-layer constraints as well as dynamic allocation of regeneration resources, is developed to address the problem of translucent dynamic routing in a single routing domain. Network performance in terms of blocking probability, resource utilization, and running times under different resource allocation and routing schemes is measured through simulation experiments.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1416990,no,undetermined,0
Pro-active Page Replacement for Scientific Applications: A Characterization,"Paging policies implemented by today's operating systems cause scientific applications to exhibit poor performance, when the application's working set does not fit in main memory. This has been typically attributed to the sub-optimal performance of LRU-like virtual-memory replacement algorithms. On one end of the spectrum, researchers in the past have proposed fully automated compiler-based techniques that provide crucial information on future access patterns (reuse-distances, release hints etc) of an application that can be exploited by the operating system to make intelligent prefetching and replacement decisions. Static techniques like the aforementioned can be quite accurate, but require that the source code be available and analyzable. At the other end of the spectrum, researchers have also proposed pure system-level algorithmic innovations to improve the performance of LRU-like algorithms, some of which are only interesting from the theoretical sense and may not really be implementable. Instead, in this paper we explore the possibility of tracking application's runtime behavior in the operating system, and find that there are several useful characteristics in the virtual memory behavior that can be anticipated and used to pro-actively manage physical memory usage. Specifically, we show that LRU-like replacement algorithms hold onto pages long after they outlive their usefulness and propose a new replacement algorithm that exploits the predictability of the application's page-fault patterns to reduce the number of page-faults. Our results demonstrate that such techniques can reduce page-faults by as much as 78% over both LRU and EELRU that is considered to be one of the state-of-the-art algorithms towards addressing the performance shortcomings of LRU. Further, we also present an implementable replacement algorithm within the operating system, that performs considerably better than the Linux kernel's replacement algorithm",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1430579,no,undetermined,0
Load balancing of services with server initiated connections,"The growth of on-line Internet services using wireless mobile handsets has increased the demand for scalable and dependable wireless services. The systems hosting such services face high quality-of-service (QoS) requirements in terms of quick response time and high availability. With growing traffic and loads using wireless applications, the servers and networks hosting these applications need to handle larger loads. Hence, there is a need to host such services on multiple servers to distribute the processing and communications tasks and balance the load across various similar entities. Load balancing is especially important for servers facilitating hosting of various wireless applications where it is difficult to predict the load delivered to a server. A load-balancer (LB) is a node that accepts all requests from external entities and directs them to internal nodes for processing based on their processing capabilities and current load patterns. We present the problem of load balancing among multiple servers, each having server initiated connections with other network entities. Challenges involved in balancing loads arising from such connections are presented and some practical solutions are proposed. As a case study, the architectures of load balancing schemes on a set of SMS (short message service) gateway servers is presented, along with deployment strategies. Performance and scalability issues are also highlighted for different possible solutions.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431343,no,undetermined,0
Design and evaluation of hybrid fault-detection systems,"As chip densities and clock rates increase, processors are becoming more susceptible to transient faults that can affect program correctness. Up to now, system designers have primarily considered hardware-only and software-only fault-detection mechanisms to identify and mitigate the deleterious effects of transient faults. These two fault-detection systems, however, are extremes in the design space, representing sharp trade-offs between hardware cost, reliability, and performance. In this paper, we identify hybrid hardware/software fault-detection mechanisms as promising alternatives to hardware-only and software-only systems. These hybrid systems offer designers more options to fit their reliability needs within their hardware and performance budgets. We propose and evaluate CRAFT, a suite of three such hybrid techniques, to illustrate the potential of the hybrid approach. For fair, quantitative comparisons among hardware, software, and hybrid systems, we introduce a new metric, mean work to failure, which is able to compare systems for which machine instructions do not represent a constant unit of work. Additionally, we present a new simulation framework which rapidly assesses reliability and does not depend on manual identification of failure modes. Our evaluation illustrates that CRAFT, and hybrid techniques in general, offer attractive options in the fault-detection design space.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431553,no,undetermined,0
Modeling and analysis of non-functional requirements as aspects in a UML based architecture design,"The problem of effectively designing and analyzing software system to meet its nonfunctional requirements such as performance, security, and adaptability is critical to the system's success. The significant benefits of such work include detecting and removing defects earlier, reducing development time and cost while improving the quality. The formal design analysis framework (FDAF) is an aspect-oriented approach that supports the design and analysis of non-functional requirements for distributed, real-time systems. In the FDAF, nonfunctional requirements are defined as reusable aspects in the repository and the conventional UML has been extended to support the design of these aspects. FDAF supports the automated translation of extended, aspect-oriented UML designs into existing formal notations, leveraging an extensive body of formal methods work. In this paper, the design and analysis of response time performance aspect is described. An example system, the ATM/banking system has been used to illustrate this process.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1434886,no,undetermined,0
Assessing the performance of erasure codes in the wide-area,"The problem of efficiently retrieving a file that has been broken into blocks and distributed across the wide-area pervades applications that utilize grid, peer-to-peer, and distributed file systems. While the use of erasure codes to improve the fault-tolerance and performance of wide-area file systems has been explored, there has been little work that assesses the performance and quantifies the impact of modifying various parameters. This paper performs such an assessment. We modify our previously defined framework for studying replication in the wide-area to include both Reed-Solomon and low-density parity-check (LDPC) erasure codes. We then use this framework to compare Reed-Solomon and LDPC erasure codes in three wide-area, distributed settings. We conclude that although LDPC codes have an advantage over Reed-Solomon codes in terms of decoding cost, this advantage does not always translate to the best overall performance in wide-area storage situations.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1467792,no,undetermined,0
Bi-layer segmentation of binocular stereo video,"This paper describes two algorithms capable of real-time segmentation of foreground from background layers in stereo video sequences. Automatic separation of layers from colour/contrast or from stereo alone is known to be error-prone. Here, colour, contrast and stereo matching information are fused to infer layers accurately and efficiently. The first algorithm, layered dynamic programming (LDP), solves stereo in an extended 6-state space that represents both foreground/background layers and occluded regions. The stereo-match likelihood is then fused with a contrast-sensitive colour model that is learned on the fly, and stereo disparities are obtained by dynamic programming. The second algorithm, layered graph cut (LGC), does not directly solve stereo. Instead the stereo match likelihood is marginalised over foreground and background hypotheses, and fused with a contrast-sensitive colour model like the one used in LDP. Segmentation is solved efficiently by ternary graph cut. Both algorithms are evaluated with respect to ground truth data and found to have similar p performance, substantially better than stereo or colour/contrast alone. However, their characteristics with respect to computational efficiency are rather different. The algorithms are demonstrated in the application of background substitution and shown to give good quality composite video output.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1467471,no,undetermined,0
Using loop invariants to fight soft errors in data caches,"Ever scaling process technology makes embedded systems more vulnerable to soft errors than in the past. One of the generic methods used to fight soft errors is based on duplicating instructions either in the spatial or temporal domain and then comparing the results to see whether they are different. This full duplication based scheme, though effective, is very expensive in terms of performance, power, and memory space. In this paper, we propose an alternate scheme based on loop invariants and present experimental results which show that our approach catches 62% of the errors caught by full duplication, when averaged over all benchmarks tested. In addition, it reduces the execution cycles and memory demand of the full duplication strategy by 80% and 4%, respectively.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1466586,no,undetermined,0
Digital system for detection and classification of electrical events,"The paper describes an algorithm to detect and classify electrical events related to power quality. The events detection is based on monitoring the statistical characteristics of the energy of the error signal, which is defined as the difference between the monitored waveform and a sinusoidal wave generated with the same magnitude, frequency, and phase as the fundamental sinusoidal component. The novel feature is event recognition based on a neural network that uses the error signal as input. Multi-rate techniques are also employed to improve the system operation for on-line applications. Software tests were performed showing the good performance of the system.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1465861,no,undetermined,0
Algorithmic optimization of H.264/AVC encoder,"Several platform independent optimizations for a baseline profile H.264/AVC encoder are described. The optimizations include adaptive diamond pattern based motion estimation, fast sub-pel motion vector refinement and heuristic intra prediction. In addition, loop unrolling, early out thresholds and adaptive inverse transforms are used. An experimental complexity analysis is presented studying effect of optimizations on the encoding frame rate on the AMD Athlon processor. Trade-offs in rate-distortion performance are also measured. Compared to a public reference encoder, speed-ups of 4-8 have been obtained with 0.6-0.8 dB loss in image quality. In practice, our software only H.264 encoder achieves an encoding rate of 86 QCIF frames/s that is well above real-time limits.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1465374,no,undetermined,0
A locally adaptive subsampling algorithm for software based motion estimation,"Motion estimation is an important technology in video coding systems. Although many algorithms have been proposed to reduce its computational complexity, they are still insufficient for implementation of a software based system. The paper proposes several techniques for low-complexity motion estimation, such as a locally adaptive subsampling. Since they reduce the number of SAD (sum of absolute differences) operations, the computational complexity can be dramatically reduced while maintaining high image quality. Simulation results show that the proposed algorithm has about 20 times the speedup as TSS (three step search). This means that the proposed algorithm can attain enough processing performance for real-time implementation on embedded processors. Furthermore, PSNR of our method is about 0.2 dB superior to that of TSS.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1465231,no,undetermined,0
Implementation aspects of a novel speech packet loss concealment method,"A speech data packet loss concealment algorithm based on pitch period repetition is presented and a novel low complexity method to refine a pitch period estimate is introduced. Objective performance measurements show that this pitch refinement improves the quality of packet loss concealment. Hardware-software codesign techniques have been investigated to implement the algorithm. Using a coprocessor approach, a processing delay of 0.9 ms and a overall speedup of 3.3 was achieved.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1465225,no,undetermined,0
Fast multi-frame motion estimation for H.264 and its applications to complexity-aware streaming,"JVT/H.264 achieves higher compression efficiency than previous video coding standards such as MPEG4 and H.263. However, this comes at the cost of increased complexity due to the use of variable block-size, multiple reference frame motion compensation, and other advanced coding algorithms. In this paper, we present a fast motion estimation algorithm. This algorithm is based on an adaptive search strategy and a flexible multi-frame search scheme. Compared to the H.264 reference software JM 8.5, this algorithm achieves, on average, a 522% reduction of encoding time, with negligible PSNR drop and bit-rate increase. Performance comparison results are described, and the architecture of a complexity-aware streaming server that applies the algorithm to provide differentiated services is discussed.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1464885,no,undetermined,0
"""De-Randomizing"" congestion losses to improve TCP performance over wired-wireless networks","Currently, a TCP sender considers all losses as congestion signals and reacts to them by throttling its sending rate. With Internet becoming more heterogeneous with more and more wireless error-prone links, a TCP connection may unduly throttle its sending rate and experience poor performance over paths experiencing random losses unrelated to congestion. The problem of distinguishing congestion losses from random losses is particularly hard when congestion is light: congestion losses themselves appear to be random. The key idea is to ""de-randomize"" congestion losses. This paper proposes a simple biased queue management scheme that ""de-randomizes"" congestion losses and enables a TCP receiver to diagnose accurately the cause of a loss and inform the TCP sender to react appropriately. Bounds on the accuracy of distinguishing wireless losses and congestion losses are analytically established and validated through simulations. Congestion losses are identified with an accuracy higher than 95% while wireless losses are identified with an accuracy higher than 75%. A closed form is derived for the achievable improvement by TCP endowed with a discriminator with a given accuracy. Simulations confirm this closed form. TCP-Casablanca, a TCP-Newreno endowed with the proposed discriminator at the receiver, yields through simulations an improvement of more than 100% on paths with low levels of congestion and about 1% random wireless packet loss rates. TCP-Ifrane, a sender-based TCP-Casablanca yields encouraging performance improvement.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1458767,no,undetermined,0
Project overlapping and its influence on the product quality,"Time to market, quality and cost are the three most important factors when developing software. In order to achieve and retain a leading position in the market, developers are forced to produce more complex functionalities, much faster and more frequently. In such conditions, it is hard to keep a high quality level and low cost. The article focuses on the reliability aspect of quality as one of the most important quality factors to the customer. Special attention is devoted to the reliability of the software product being developed in project overlapping conditions. The Weibull reliability growth model for predicting the reliability of a product during the development process is adapted and applied to historical data of a sequence of overlapping projects. A few useful tips on reliability modeling for project management and planning in project overlapping conditions are presented",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1458659,no,undetermined,0
A comprehensive model for software rejuvenation,"Recently, the phenomenon of software aging, one in which the state of the software system degrades with time, has been reported. This phenomenon, which may eventually lead to system performance degradation and/or crash/hang failure, is the result of exhaustion of operating system resources, data corruption, and numerical error accumulation. To counteract software aging, a technique called software rejuvenation has been proposed, which essentially involves occasionally terminating an application or a system, cleaning its internal state and/or its environment, and restarting it. Since rejuvenation incurs an overhead, an important research issue is to determine optimal times to initiate this action. In this paper, we first describe how to include faults attributed to software aging in the framework of Gray's software fault classification (deterministic and transient), and study the treatment and recovery strategies for each of the fault classes. We then construct a semi-Markov reward model based on workload and resource usage data collected from the UNIX operating system. We identify different workload states using statistical cluster analysis, estimate transition probabilities, and sojourn time distributions from the data. Corresponding to each resource, a reward function is then defined for the model based on the rate of resource depletion in each state. The model is then solved to obtain estimated times to exhaustion for each resource. The result from the semi-Markov reward model are then fed into a higher-level availability model that accounts for failure followed by reactive recovery, as well as proactive recovery. This comprehensive model is then used to derive optimal rejuvenation schedules that maximize availability or minimize downtime cost.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1453531,no,undetermined,0
Digital gas fields produce real time scheduling based on intelligent autonomous decentralized system,"In the paper, we focus on exploring to construct a novel digital gas fields produce real-time scheduling system, which is based on the theory of intelligent autonomous decentralized system (IADS). The system combines the practical demand and the characteristics of the gas fields, and it aims at dealing with the real-time property, dynamic and complexity during gas fields produce scheduling. Besides embodying on-line intelligent expansion, intelligent fault tolerance and on-line intelligent maintenance of IADS particular properties, the scheme adequately attaches importance to the flexibility. The model & method based on intelligent information pull push (IIPP) and intelligent management (IM) is applied to the system, thus it is helpful to improve the performance of the scheduling system. In according with the current requirement of gas fields produce scheduling, the concrete solvent is put forward. The related system architecture and software structure is presented. An effective method of solving the real-time property was developed for use in scheduling of the gas field produce. We simulated some experiments, and the result demonstrates the validity of the system. At Last, we predict its promising research trend in the future of gas fields practice application.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1452151,no,undetermined,0
Effect of preventive rejuvenation in communication network system with burst arrival,"Long running software systems are known to experience an aging phenomenon called software aging, one in which the accumulation of errors during the execution of software leads to performance degradation and eventually results in failure. To counteract this phenomenon a proactive fault management approach, called software rejuvenation, is particularly useful. It essentially involves gracefully terminating an application or a system and restarting it in a clean internal state. In this paper, we perform the dependability analysis of a client/server software system with rejuvenation under the assumption that the requests arrive according to the Markov modulated Poisson process. Three dependability measures, steady-state availability, loss probability of requests and mean response time on tasks, are derived through the hidden Markovian analysis based on the time-based software rejuvenation scheme. In numerical examples, we investigate the sensitivity of some model parameters to the dependability measures.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1452041,no,undetermined,0
Developing and assuring trustworthy Web services,"Web services are emerging technologies that are changing the way we develop and use computer systems and software. Current Web services testing techniques are unable to assure the desired level of trustworthiness, which presents a barrier to WS applications in mission and business critical environments. This paper presents a framework that assures the trustworthiness of Web services. New assurance techniques are developed within the framework, including specification verification via completeness and consistency checking, specification refinement, distributed Web services development, test case generation, and automated Web services testing. Traditional test case generation methods only generate positive test cases that verify the functionality of software. The Swiss cheese test case generation method proposed in this paper is designed to perform both positive and negative testing that also reveal the vulnerability of Web services. This integrated development process is implemented in a case study. The experimental evaluation demonstrates the effectiveness of this approach. It also reveals that the Swiss cheese negative testing detects even more faults than positive testing and thus significantly reduces the vulnerability of Web services.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1452016,no,undetermined,0
Magnetic field measurements for fast-changing magnetic fields,"Several recent applications for fast ramped magnets have been found that require rapid measurement of the field quality during the ramp. (In one instance, accelerator dipoles will be ramped at 1 T/sec, with measurements needed to the accuracy typically required for accelerators.) We have built and tested a new type of magnetic field measuring system to meet this need. The system consists of 16 stationary pickup windings mounted on a cylinder. The signals induced in the windings in a changing magnetic field are sampled and analyzed to obtain the field harmonics. To minimize costs, printed circuit boards were used for the pickup windings and a combination of amplifiers and ADC's used for the voltage readout system. New software was developed for the analysis. Magnetic field measurements of a model dipole developed for the SIS200 accelerator at GSI are presented. The measurements are needed to ensure that eddy currents induced by the fast ramps do not impact the field quality required for successful accelerator operation.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1439861,no,undetermined,0
Flexible Consistency for Wide Area Peer Replication,"The lack of a flexible consistency management solution hinders P2P implementation of applications involving updates, such as read-write file sharing, directory services, online auctions and wide area collaboration. Managing mutable shared data in a P2P setting requires a consistency solution that can operate efficiently over variable-quality failure-prone networks, support pervasive replication for scaling, and give peers autonomy to tune consistency to their sharing needs and resource constraints. Existing solutions lack one or more of these features. In this paper, we described a new consistency model for P2P sharing of mutable data called composable consistency, and outline its implementation in a wide area middleware file service called Swarm. Composable consistency lets applications compose consistency semantics appropriate for their sharing needs by combining a small set of primitive options. Swarm implements these options efficiently to support scalable, pervasive, failure-resilient, wide-area replication behind a simple yet flexible interface. Two applications was presented to demonstrate the expressive power and effectiveness of composable consistency: a wide area file system that outperforms Coda in providing close-to-open consistency over WANs, and a replicated BerkeleyDB database that reaps order-of-magnitude performance gains by relaxing consistency for queries and updates",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1437084,no,undetermined,0
Parallel processors and an approach to the development of inference engine,"The reliable and fault tolerant computers are key to the success to aerospace, and communication industries where failures of the system can cause a significant economic impact and loss of life. Designing a reliable digital system, and detecting and repairing the faults are challenging tasks in order for the digital system to operate without failures for a given period of time. The paper presents a new and systematic software engineering approach of performing fault diagnosis of digital systems, which have employed multiple processors. The fault diagnosis model is based on the classic PMC model to generate data obtained on the basis of test results performed by the processors. The PMC model poses a tremendous challenge to the user in doing fault analysis on the basis of test results performed by the processors. This paper will perform one fault model for developing software. The effort has been made to preserve the necessary and sufficient.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1434904,no,undetermined,0
"Assessing attitude towards, knowledge of, and ability to apply, software development process","Software development is one of the most economically critical engineering activities. It is unsettling, therefore, that regularly published analyses reveal that the percentage of projects that fail, by coming in far over budget or far past schedule, or by being cancelled with significant financial loss, is considerably greater in software development than in any other branch of engineering. The reason is that successful software development requires expertise in both state of the art (software technology) and state of the practice (software development process). It is widely recognized that failure to follow best practice, rather than technological incompetence, is the cause of most failures. It is critically important, therefore, that (i) computer science departments be able assess the quality of the software development process component of their curricula and that industry be able to assess the efficacy of SPI (software process improvement) efforts. While assessment instruments/tools exist for knowledge of software technology, none exist for attitude toward, knowledge of, or ability to use, software development process. We have developed instruments for measuring attitude and knowledge, and are working on an instrument to measure ability to use. The current version of ATSE, the instrument for measuring attitude toward software engineering, is the result of repeated administrations to both students and software development professionals, post-administration focus groups, rewrites, and statistical reliability analyses. In this paper we discuss the development of ATSE, results, both expected an unexpected, of recent administrations of ATSE to students and professionals, the various uses to which ATSE is currently being put and to which it could be put, and ATSE's continuing development and improvement.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1191386,no,undetermined,0
Implementation and performance of cooperative control of shunt active filters for harmonic damping throughout a power distribution system,"This paper proposes cooperative control of multiple active filters based on voltage detection for harmonic damping throughout a power distribution system. The arrangement of a real distribution system would be changed according to system operation, and/or fault conditions. In addition, shunt capacitors and loads are individually connected to, or disconnected from, the distribution system. Independent control might make multiple active filters produce unbalanced compensating currents. This paper presents hardware and software implementations of cooperative control for two active filters. Experiment results verify the effectiveness of the cooperative control with the help of a communication system.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1189235,no,undetermined,0
Dynamic fault detection approaches,"Several dynamic acceptance test approaches, applicable to any linear, causal, time-invariant control system, are developed. The acceptance tests perform a reasonableness check on controller outputs, based upon critical control theory. The dynamic approach checks the control system online and in run-time and reacts to controller demand changes as the control system operates. Theory is presented to enable one to place dynamic magnitude and rate bounds on controller output signals in the time domain. Practical implementation of the acceptance tests into realistic control systems is discussed leading to verification of a highly successful fault detection technique",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=782904,no,undetermined,0
Analyzing testability on data flow designs,"High testability is a strongly desired feature of software, since it tends to make the validation phase more efficient in exposing faults during testing, and consequently it increases the quality of the end-product. Furthermore, testability is a criterion of crucial importance to software developers, since the sooner it can be estimated, the better the software architecture will be organized to improve subsequent maintenance. This paper is concerned with the testability of data flow software designs, its definition, and the axiomatization of its expected behavior. This behavior is expressed in relation to basic operations that are applicable on designs, and to the dedicated test strategies which are selected. Measurements are proposed which are consistent with the stated axioms. The whole approach is demonstrated using design specifications of embedded software developed in the avionics industry",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885869,no,undetermined,0
Spiral interpolation algorithms for multislice spiral CT. II. Measurement and evaluation of slice sensitivity profiles and noise at a clinical multislice system,"For pt. I see ibid., vol. 19, no. 9, p. 822-34 (2000). The recently introduced multislice data acquisition for computed tomography (CT) is based on multirow detector design, increased rotation speed, and advanced z-interpolation and z-filtering algorithms. The authors evaluated slice sensitivity profiles (SSPs) and noise of a clinical multislice spiral CT (MSCT) scanner with M=4 simultaneously acquired slices and adaptive axial interpolator (AAI) reconstruction software. SSPs were measured with a small gold disk of 50 Î¼m thickness and 2-mm diameter located at the center of rotation (COR) and 100 mm off center. The standard deviation of CT values within a 20-cm water phantom was used as a measure of image noise. With a detector slice collimation of S=1.0 mm, the authors varied spiral pitch p from 0.25 to 2.0 in steps of 0.025. Nominal reconstructed slice thicknesses were 1.25, 1.5, and 2.0 mm. For all possible pitch values, the authors found the full-width at half maximum (FWHM) of the respective sensitivity profile at the COR equivalent to the selected nominal slice thickness. The profiles at 100 mm off center are broadened less than 7% on the average compared with the FWHM at the COR. In addition, variation of the full-width at tenth maximum (FWTM) at the COR was below 10% for pâ‰?.75. Within this range, image noise varied less than 10% with respect to the mean noise level. The slight increase in measured slice-width above p=1.75 for nominal slice-widths of 1.25 and 1.50 mm is accompanied by a decrease of noise according to the inverse square root relationship. The MSCT system that the authors scrutinized provides reconstructed slice-widths and image noise, which can be regarded as constant within a wide range of table speeds. With respect to this, MSCT is superior to single-slice spiral CT. These facts can be made use of when defining and optimizing clinical protocols: the spiral pitch can be selected almost freely, and scan protocols can follow the diagnostic requirements without technical restrictions. In summary, MSCT offers constant image quality while scan times are reduced drastically. Volume scans with three-dimensional (3-D) isotropic resolution are routinely feasible for complete anatomical regions.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=887833,no,undetermined,0
Fuzzy techniques evaluate sausage quality,"The purpose of this study is to reproduce, with software, the crusting evaluation provided by experts of an image of a slice acquired by a camera. The software gives promising results that are very close to the evaluations provided by inspection experts. It appears possible to use this software as a tool to assist operators' evaluation of sausage quality by reducing the evaluation time.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=887454,no,undetermined,0
Single byte error control codes with double bit within a block error correcting capability for semiconductor memory systems,"Computer memory systems when exposed to strong electromagnetic waves or radiation are highly vulnerable to multiple random bit errors. Under this situation, we cannot apply existing SEC-DED or S<sub>b</sub>EC capable codes because they provide insufficient error control performance. This correspondence considers the situation where two random bits in a memory chip are corrupted by strong electromagnetic waves or radioactive particles and proposes two classes of codes that are capable of correcting random double bit errors occurring within a chip. The proposed codes, called Double bit within a block Error Correcting-Single byte Error Detecting ((DEC)<sub>B</sub>-S<sub>b</sub>ED) code and Double bit within a block Error Correcting-Single byte Error Correcting ((DEC)<sub>B</sub>-S<sub>b </sub>EC) code, are suitable for recent computer memory systems",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=887157,no,undetermined,0
Resource sharing estimation by Petri nets in PISH hardware/software co-design system,"The article presents two approaches for computing the number of functional units in a hardware/software codesign context. The proposed hardware/software codesign framework uses the Petri net as a common formalism for performing quantitative and qualitative analysis. The use of the Petri net as an intermediate format allows us to analyze properties of the specification and formally compute performance indices which are used in the partitioning process. The paper is devoted to describing the algorithms for functional unit estimation. The work also proposes a method of extending the Petri net model in order to take into account causal constraints provided by the designers. However, an overview of the general hardware/software codesign method is also presented",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=886483,no,undetermined,0
Software reliability and maintenance concept used for automatic call distributor MEDIO ACD,"The authors present the software reliability and maintenance concept, which is used in the software development, testing, and maintenance process, for automatic call distributor MEDIO ACD. The concept has been successfully applied on systems, which are installed and fully operational in Moscow and Saint Petersburg, Russia. The authors concentrate on two main issues: (i) set of fault-tolerant mechanisms needed for the system exploitation (error logging, checkpoint-restart, overload protection and tandem configuration support); (ii) MEDIO ACD software maintenance concept, in which the quality of the new software update is predicted on the basis of the current update's metrics and quality, and the new update's metrics. This forecast aids software maintenance efficiency, and cost reduction",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885884,no,undetermined,0
On the repeatability of metric models and metrics across software builds,"We have developed various software metrics models over the years: Boolean discriminate functions (BDFs); the Kolmogorov-Smirnov distance; derivative calculations for assessing achievable quality; a stopping rule; point and confidence interval estimates of quality; relative critical value deviation metrics; and nonlinear regression functions. We would like these models and metrics to be repeatable across the n builds of a software system. The advantage of repeatability is that models and metrics only need to be developed and validated once on build, and then applied n-1 times without modification to subsequent builds, with considerable savings in analysis and computational effort. In practical terms, this approach involves using the same model parameters that were validated and applying them unchanged on subsequent builds. The disadvantage is that the quality and metrics data of builds 2, ..., n, which varies across builds, is not utilized. We make a comparison of this approach with one that involves validating models and metrics on each build i and applying them only on build i+1, and then repeating the process. The advantage of this approach is that all available data are used in the models and analysis but at considerable cost in effort. We report on experiments involving large sets of discrepancy reports and metrics data on the Space Shuttle flight software, where we compare the predictive accuracy and effort of the two approaches for BDFs, critical values, derivative quality and inspection calculations, and the stopping rule",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885875,no,undetermined,0
A methodology for architectural-level risk assessment using dynamic metrics,"Risk assessment is an essential process of every software risk management plan. Several risk assessment techniques are based on the subjective judgement of domain experts. Subjective risk assessment techniques are human-intensive and error-prone. Risk assessment should be based on product attributes that we can quantitatively measure using product metrics. This paper presents a methodology for risk assessment at the early stages of the development lifecycle, namely the architecture level. We describe a heuristic risk assessment methodology that is based on dynamic metrics obtained from UML specifications. The methodology uses dynamic complexity and dynamic coupling metrics to define complexity factors for the architecture elements (components and connectors). Severity analysis is performed using FMEA (failure mode and effect analysis), as applied to architecture simulation models. We combine severity and complexity factors to develop heuristic risk factors for the architecture components and connectors. Based on component dependency graphs that were developed earlier for reliability analysis, and using analysis scenarios, we develop a risk assessment model and a risk analysis algorithm that aggregates the risk factors of components and connectors to the architectural level. We show how to analyze the overall risk factor of the architecture as the function of the risk factors of its constituting components and connectors. A case study of a pacemaker is used to illustrate the application of the methodology",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885873,no,undetermined,0
A software falsifier,"A falsifier is a tool for discovering errors by static source-code analysis. Its goal is to discover them while requiring minimal programmer effort. In contrast to lint-like tools or verifiers, which try to maximize the number of errors reported at the expense of allowing â€œfalse errorsâ€? a falsifier's goal is to guarantee no false errors. To further minimize programmer effort, no specification or extra information about the program is required. That, however, does not preclude project-specific information from being built in. The class of errors that are detectable without any specification is important not only because of the low cost of detection, but also because it includes errors of portability, irreproducible behavior, etc., which are very expensive to detect by testing. This paper describes the design and implementation of such a falsifier, and reports on experience with its use for design automation software. The main contribution of this work lies in combining data-flow analysis with symbolic execution to take advantage of their relative advantages",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885870,no,undetermined,0
Contributing to the bottom line: optimizing reliability cost schedule tradeoff and architecture scalability through test technology,"A challenging problem in software testing is finding the optimal point at which costs justify the stop-test decision. We first present an economic model that can be used to evaluate the consequences of various stop-test decisions. We then discuss two approaches for assessing performance, automated load test generation in the context of empirical testing and performance modeling, and illustrate how these techniques can affect the stop-test decision. We then illustrate the application of these two techniques to evaluating the performance of Web servers that performs significant server-side processing through object-oriented (OO) computing. Implications of our work for Web server performance evaluation in general are discussed",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885867,no,undetermined,0
Improved testing using failure cost and intensity profiles,"The cost of field failures is arguably a very important factor in judging the quality of software. It would be nice if field failure costs could be used before the software is released so as to improve its quality. We introduced the expected cost of field failures (Ntafos, 1997) as a measure of test effectiveness and demonstrated that significant benefits are possible even with rather crude cost and failure rate estimates. We discuss the main issues that arise in trying to reduce the cost of field failures through better test effort allocation. We introduce improved models and strategies and present analytical and experimental (simulation) evaluations",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=888068,no,undetermined,0
Assessing the cost-effectiveness of inspections by combining project data and expert opinion,"There is a general agreement among software engineering practitioners that sofware inspections are an important technique to achieve high software quality at a reasonable cost. However, there are many ways to perform such inspections and many factors that affect their cost-effectiveness. It is therefore important to be able to estimate this cost-effectiveness in order to monitor it, improve it, and convince developers and management that the technology and related investments are worthwhile. This work proposes a rigorous but practical way to do so. In particular, a meaningful model to measure cost-effectiveness is selected and a method to determine the cost-effectiveness by combining project data and expert opinion is proposed. To demonstrate the feasibility of the proposed approach, the results of a large-scale industrial case study are presented",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885866,no,undetermined,0
Quantitative software reliability modeling from testing to operation,"We first describe how several existing software reliability growth models based on nonhomogeneous Poisson processes (NHPPs) can be derived based on a unified theory for NHPP models. Under this general framework, we can verify existing NHPP models and derive new NHPP models. The approach covers a number of known models under different conditions. Based on these approaches, we show a method of estimating and computing software reliability growth during the operational phase. We can use this method to describe the transitions from the testing phase to operational phase. That is, we propose a method of predicting the fault detection rate to reflect changes in the user's operational environments. The proposed method offers a quantitative analysis on software failure behavior in field operation and provides useful feedback information to the development process",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885862,no,undetermined,0
FastSplats: optimized splatting on rectilinear grids,"Splatting is widely applied in many areas, including volume, point-based and image-based rendering. Improvements to splatting, such as eliminating popping and color bleeding, occasion-based acceleration, post-rendering classification and shading, have all been recently accomplished. These improvements share a common need for efficient frame-buffer accesses. We present an optimized software splatting package, using a newly designed primitive, called FastSplat, to scan-convert footprints. Our approach does not use texture mapping hardware, but supports the whole pipeline in memory. In such an integrated pipeline, we are then able to study the optimization strategies and address image quality issues. While this research is meant for a study of the inherent trade-off of splatting, our renderer, purely in software, achieves 3- to 5-fold speedups over a top-end texture hardware implementation (for opaque data sets). We further propose a method of efficient occlusion culling using a summed area table of opacity. 3D solid texturing and bump mapping capabilities are demonstrated to show the flexibility of such an integrated rendering pipeline. A detailed numerical error analysis, in addition to the performance and storage issues, is also presented. Our approach requires low storage and uses simple operations. Thus, it is easily implementable in hardware.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885698,no,undetermined,0
IEEE 1232 and P1522 standards,"The 1232 family of standards were developed to provide standard exchange formats and software services for reasoning systems used in system test and diagnosis. The exchange formats and services am based on a model of information required to support test and diagnosis. The standards were developed by the Diagnostic and Maintenance Control (D&MC) subcommittee of IEEE SCC20. The current efforts by the D&MC are a combined standard made up of the 1232 family, and a standard on Testability and Diagnosability Metrics, P1522. The 1232 standards describe a neutral exchange format so one diagnostic reasoner can exchange model information with another diagnostic reasoner. In addition, software interfaces are defined whereby diagnostic tools can be developed to process the diagnostic information in a consistent and reliable way. The objective of the Testability and Diagnosability Metrics standard is to provide notionally correct and mathematically precise definitions of testability measures that may be used to either measure the testability characteristics of a system, or predict the testability of a system. The end purpose is to provide an unambiguous source for definitions of common and uncommon testability and diagnosability terms such that each individual encountering it can know precisely what that term means. This paper describes the 1232 and P1522 standards and details the recent changes in the Information models, restructured higher order services and simplified conformance requirements",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885619,no,undetermined,0
Design of an improved watchdog circuit for microcontroller-based systems,"This paper presents an improved design for a watchdog circuit. Previously, watchdog timers detected refresh inputs that were slower than usual. If a failure causes the microcontroller to produce faster than usual refresh inputs, the watchdog will not detect it. This new design detects failures that produce faster than usual as well as slower than usual refresh inputs. This will greatly improve the reliability of the system protected by this new design.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=884831,no,undetermined,0
Effort-index-based software reliability growth models and performance assessment,"The authors show that the logistic testing effort function is practically acceptable/helpful for modeling the software reliability growth and providing a reasonable description of resource consumption. Therefore, in addition to the exponential shaped models, we integrate the logistic testing effort function into an S-shaped model for further analysis. The model is designated as the Yamada Delayed S-shaped model. A realistic failure data set is used in the experiments to demonstrate the estimation procedures and results. Furthermore, the analysis of the proposed model under an imperfect debugging environment is investigated. In fact, from these experimental results and discussions, it is apparent that the logistic testing-effort function is very suitable for making estimations of resource consumption during the software development/testing phase",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=884764,no,undetermined,0
A phase-based approach to creating highly reliable software,"Software reliability engineering includes: software reliability measurement, which includes estimation and prediction, with the help of software reliability models established in the literature; the attributes and metrics of product design, development process, system architecture, software operational environment, and their implications on reliability; and the application of this knowledge in specifying and guiding system software architecture, development, testing, acquisition, use, and maintenance. My position is that we should attack the problem of software reliability engineering in three phases: modeling and analysis phase; design and implementation phase; and testing and measurement phase. All these phases deal with the management of software faults and failures",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=884732,no,undetermined,0
Inserting software fault measurement techniques into development efforts,"Over the past several years, techniques for estimating software fault content based on measurements of a system's structural evolution during its implementation have been developed. Proper application of the techniques will yield a detailed map of the faults that have been inserted into the system. This information can be used by development organizations to better control the number of residual faults in the operational system. There are several issues that must be resolved if these techniques are to be successfully inserted into a development effort. These issues are identified, as are possibilities for their resolution",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=884731,no,undetermined,0
"Correctly assessing the -ilities"" requires more than marketing hype","Understanding key system qualities can better equip you to correctly assess the technologies you manage. Dot-coms and enterprise systems often use terms like scalability, reliability and availability to describe how well they meet current and future service-level expectations. These ilities characterize an IT solution's architectural and engineering qualities. They collectively provide a vocabulary for discussing an IT solution's performance potential amid ever-changing IT requirements. The paper considers the role that ilities play in the solution architecture. They generally fall into four categories: strategic, systemic, service and user.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=888019,no,undetermined,0
"A statistical, nonparametric methodology for document degradation model validation","Printing, photocopying, and scanning processes degrade the image quality of a document. Statistical models of these degradation processes are crucial for document image understanding research. In this paper, we present a statistical methodology that can be used to validate local degradation models. This method is based on a nonparametric, two-sample permutation test. Another standard statistical device, the power function, is then used to choose between algorithm variables such as distance functions. Since the validation and the power function procedures are independent of the model, they can be used to validate any other degradation model. A method for comparing any two models is also described. It uses p-values associated with the estimated models to select the model that is closer to the real world.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=888707,no,undetermined,0
Comparing fail-silence provided by process duplication versus internal error detection for DHCP server,"This paper uses fault injection to compare the ability of two fault-tolerant software architectures to protect an application from faults. These two architectures are Voltan, which uses process duplication, and Chameleon ARMORs, which use self-checking. The target application is a Dynamic Host Configuration Protocol (DHCP) server, a widely used application for managing IP addresses. NFTAPE, a software-based fault injection environment, is used to inject three classes of faults, namely random memory bit-flip, control-flow and high-level target specific faults, into each software architecture and into baseline Solaris and Linux versions",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=924966,no,undetermined,0
A maintainability model for industrial software systems using design level metrics,"Software maintenance is a time consuming and expensive phase of a software product's life-cycle. The paper investigates the use of software design metrics to statistically estimate the maintainability of large software systems, and to identify error prone modules. A methodology for assessing, evaluating and, selecting software metrics for predicting software maintainability is presented. In addition, a linear prediction model based on a minimal set of design level software metrics is proposed. The model is evaluated by applying it to industrial software systems",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=891476,no,undetermined,0
Determining the expected time to unsafe failure,"The number of applications requiring highly reliable and/or safety-critical computing is increasing. One emerging safety metric is the Mean Time To Unsafe Failure (MTTUF). This paper summarizes a novel technique for determining the MTTUF for a given architecture. The first step in determining the MTTUF for a system is to estimate system Mean Time To Failure (MTTF) and system fault coverage. Once these two parameters are known then the system MTTUF can be calculated. The presented technique allows MTTF and system coverage to be estimated from dependability models that incorporate time varying failure and/or repair rates. Existing techniques for the estimation of MTTUF require constant rate dependability models. For the sake of simplicity, this paper uses Markov models to calculate MTTUF. The presented approach greatly simplifies the calculation of system MTTUF. Finally a comparison is made between reliability expected time metrics (MTTF and MTBF) and safety expected time metrics (MTTUF and MTBUF)",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=895435,no,undetermined,0
Constructions of behaviour observation schemes in software testing,"Software testing is a process in which a software system's dynamic behaviours are observed and analysed so that the system's properties can be inferred from the information revealed by test executions. While the existing theories of software testing might be adequate in describing the testing of sequential systems, they are not capable to describe the testing of concurrent systems that can exhibit different behaviours on the same test case due to non-determinism and concurrency. In our previous work, we proposed a theoretical framework that provides a unified foundation to study all testing methods for both sequential and concurrent systems. The main results of the framework include a formal definition of the notion and a set of the desirable properties of an observation scheme. In this paper, we provide several constructions of observation schemes that have direct implications in current software testing practice. We also study the properties of the constructions",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=895434,no,undetermined,0
Web development: estimating quick-to-market software,"Developers can use this new sizing metric called Web Objects and an adaptation of the Cocomo II model called WebMo to more accurately estimate Web based software development effort and duration. Based on work with over 40 projects, these estimation tools are especially useful for quick-to-market development efforts",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=895169,no,undetermined,0
Empirically guided software effort guesstimation,"Project LEAP (lightweight, empirical, antimeasurement dysfunction, and portable toolkit) is investigating tools and methods to support low-cost, empirically based software developer improvement. LEAP contains tools to simplify the collection of size and effort data. The collected data serves as input to a set of estimation tools. These tools can produce over a dozen analytical estimates of the effort required for a new project given an estimate of its size. To do this, they use various estimation methods such as linear, logarithmic, or exponential regressions. During project planning, the developer can review these estimates and select one of them or substitute a guesstimate based on his or her experience. The authors' study provides evidence that guesstimates, when informed by low-cost analytical methods, might be the most accurate method",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=895168,no,undetermined,0
A domain coverage metric for the validation of behavioral VHDL descriptions,"During functional validation, corner cases and boundary conditions are known to be difficult to verify. We propose a functional fault coverage metric for behavioral VHDL descriptions which evaluates the coverage of faults which cause the behavior to execute an incorrect functional domain. Although domain faults are known to be a source of design errors, they are not targeted explicitly by previous work in functional validation. We propose an efficient method to compute domain coverage in VHDL descriptions and we generate domain coverage results for several benchmark VHDL circuits for comparison to other approaches",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=894218,no,undetermined,0
Monitoring behavior in home using a smart fall sensor and position sensors,"The authors developed a system for remotely monitoring human behavior during daily life at home, to improve the security and the quality of life. The activity was monitored through infrared position sensors and magnetic switches. For the falls detection the authors had to develop a smart sensor. The local communications were performed using RF wireless links to reduce the cabling and to allow mobility of the person. An application software performs data exploitation locally but it also performs remote data transmission through the network. This project aim at expanding the telecare solution to a larger population of elderly people who are presently forced to live in hospices",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=893857,no,undetermined,0
Detecting a network failure,"Measuring the properties of a large, unstructured network can be difficult: one may not have full knowledge of the network topology, and detailed global measurements may be infeasible. A valuable approach to such problems is to take measurements from selected locations within the network and then aggregate them to infer large-scale properties. One sees this notion applied in settings that range from Internet topology discovery tools to remote software agents that estimate the download times of popular Web pages. Some of the most basic questions about this type of approach, however, are largely unresolved at an analytical level. How reliable are the results? How much does the choice of measurement locations affect the aggregate information one infers about the network? We describe algorithms that yield provable guarantees for a particular problem of this type: detecting a network failure. Suppose we want to detect events of the following form: an adversary destroys up to k nodes or edges, after which two subsets of the nodes, each at least an &epsi; fraction of the network, are disconnected from one another. We call such an event an (&epsi;,k) partition. One method for detecting such events would be to place â€œagentsâ€?at a set D of nodes, and record a fault whenever two of them become separated from each other. To be a good detection set, D should become disconnected whenever there is an (&epsi;,k)-partition; in this way, it â€œwitnessesâ€?all such events. We show that every graph has a detection set of size polynomial in k and &epsi;<sup>-1</sup>, and independent of the size of the graph itself. Moreover, random sampling provides an effective way to construct such a set. Our analysis establishes a connection between graph separators and the notion of VC-dimension, using techniques based on matchings and disjoint paths",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=892110,no,undetermined,0
An adaptive PMU based fault detection/location technique for transmission lines. II. PMU implementation and performance evaluation,"Part I of this paper set sets forth theory and algorithms for adaptive fault detection/location technique, which is based on phasor measurement unit (PMU). This paper is Part II of this paper set, A new timing device named â€œGlobal Synchronism Clock Generator, GSCGâ€?including its hardware and software design is described in this paper, Experimental results show that the synchronized error of rising edge between the two GSCGs clock is well within 1 ps when the clock frequency is below 2.499 MHz. The measurement results between Chung-Jeng and Chang-Te 161 kV substations of Taiwan Power company by PMU equipped with GSCG is presented and the accuracy for estimating parameters of line is verified. The new developed DFT based method (termed as smart discrete Fourier transform, SDFT) and line parameter estimation algorithm are combined with PMU configuration to form the adaptive fault detector/locator system. Simulation results have shown that SDFT method can extract exact phasors in the presence of frequency deviation and harmonics, The parameter estimation algorithm can also trace exact parameters very well, The SDFT method and parameter estimation algorithm can achieve accuracies of up to 99.999% and 99.99%, respectively. The EMTP is used to simulate a 345 kV transmission line of Taipower System. Results have shown that the proposed technique yields correct results independent of fault types and is insensitive to the variation of source impedance, fault impedance and line loading. The accuracy of fault location estimation achieved can be up to 99.9% for many simulated cases, The proposed technique will be very suitable for implementation in an integrated digital protection and control system for transmission substations",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=891494,no,undetermined,0
Fault-tolerant Ethernet middleware for IP-based process control networks,"We present an efficient middleware-based fault-tolerant Ethernet (FTE) developed for process control networks. Our approach is unique and practical in the sense that it requires no change to commercial off-the-shelf hardware (switch, hub, Ethernet physical link, and network interface card) and software (commercial Ethernet NIC card driver and standard protocol such as TCP/IP) yet it is transparent to IP-based applications. The FTE performs failure detection and recovery for handling multiple points of network faults and supports communications with non-FTE-capable devices. Our experimentation shows that FTE performs efficiently, achieving less than 1-ms end-to-end swap time and less than 2-sec failover time, regardless of the concurrent application and system loads. In this paper, we describe the FTE architecture, the challenging technical issues addressed, our performance evaluation results, and the lessons learned in design and development of such an open-network-based fault-tolerant network",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=891017,no,undetermined,0
Pro-active QoS scheduling algorithms for real-time multimedia applications in communication system,"This paper studies the performance of the OccuPancy Adjusting (OCP A) dynamic QoS scheduling algorithm and two newly proposed dynamic QoS scheduling algorithm, known as the dynamic QoS scheduling algorithm with delay estimation and the hybrid dynamic QoS scheduling algorithm. The newly designed algorithms are intended to reduce the propagated delay caused by transient packets that will eventually be dropped due to expired delay. The essence of these new algorithms is to apply a pro-active approach in accessing the admission of packets into the buffer, which is based on the estimated delay that is likely to be experienced by the respective packets. The estimation algorithm is driven by a recursive calculation of the estimated delay, which is influenced by the buffer occupancy. The results obtained through the simulation model have indicated that the two newly proposed algorithms are able to improve the average delay, buffer requirements and packet loss ratio for delay sensitive traffic, as compared to the OCP A algorithm",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=888782,no,undetermined,0
VerifyESD: a tool for efficient circuit level ESD simulations of mixed-signal ICs,"For many classes of technologies and circuits, it is beneficial to perform circuit simulations for ESD design, verification, and performance prediction. This is particularly true for mixed-signal ICs, where complex interaction between I/Os and multiple power supplies make manual analysis difficult and error prone. Unfortunately, high node and component counts typically prohibit simulations of an entire circuit. Thus, a manual intervention by the designer is usually required to minimize the circuit size. This paper introduces a new tool which automatically reduces the number of voltage nodes per ESD simulation by including only those devices that are necessary. In addition, a simple method for modeling ESD device failure while maintaining compatibility with existing CAD tools and libraries is discussed.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=890117,no,undetermined,0
Transforming supervised classifiers for feature extraction,"Supervised feature extraction is used in data classification and (unlike unsupervised feature extraction) it uses class labels to evaluate the quality of the extracted features. It can be computationally inefficient to perform exhaustive searches to find optimal subsets of features. This article proposes a supervised linear feature extraction algorithm based on the use of multivariate decision trees. The main motivation in proposing this new approach to feature extraction is to reduce the computation time required to induce new classifiers which are required to evaluate every new subset of features. The new feature extraction algorithm proposed uses an approach that is similar to the wrapper model method used in feature selection. In order to evaluate the performance of the proposed algorithm, several tests with real-world data have been performed. The fundamental importance of this new feature extraction method is found in its ability to significantly reduce the computational time required to extract features from large databases",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=889882,no,undetermined,0
A genetic algorithm-based system for generating test programs for microprocessor IP cores,"The current digital systems design trend is quickly moving toward a design-and-reuse paradigm. In particular, intellectual property cores are becoming widely used. Since the cores are usually provided as encrypted gate-level netlist, they raise several testability problems. The authors propose an automatic approach targeting processor cores that, by resorting to genetic algorithms, computes a test program able to attain high fault coverage figures. Preliminary results are reported to assess the effectiveness of our approach with respect to a random approach",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=889869,no,undetermined,0
Modeling software quality: the Software Measurement Analysis and Reliability Toolkit,"The paper presents the Software Measurement Analysis and Reliability Toolkit (SMART) which is a research tool for software quality modeling using case based reasoning (CBR) and other modeling techniques. Modern software systems must have high reliability. Software quality models are tools for guiding reliability enhancement activities to high risk modules for maximum effectiveness and efficiency. A software quality model predicts a quality factor, such as the number of faults in a module, early in the life cycle in time for effective action. Software product and process metrics can be the basis for such fault predictions. Moreover, classification models can identify fault prone modules. CBR is an attractive modeling method based on automated reasoning processes. However, to our knowledge, few CBR systems for software quality modeling have been developed. SMART addresses this area. There are currently three types of models supported by SMART: classification based on CBR, CBR classification extended with cluster analysis, and module-order models, which predict the rank-order of modules according to a quality factor. An empirical case study of a military command, control, and communications applied SMART at the end of coding. The models built by SMART had a level of accuracy that could be very useful to software developers",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=889846,no,undetermined,0
The nonredundant error correction differential detection in soft-decision decoding,"This paper proposes the application of nonredundant error correction (NEC) in soft-decision decoding. Combined with differential demodulation of Ï€/4DQPSK, we emphasize the analysis of the new algorithm. A practical implementation scheme is presented. The BER performance and correction capability of NEC is investigated by computer simulation in AWGN and Rician fading channels. The simulation results show that the performance improvement of the proposed algorithm is superior to conventional differential demodulation by 1.4 dB at a BER of 10<sup>-4</sup> in the AWGN channel. In the Rician channel there is the same notable performance improvement. The algorithm makes NEC able to be used in soft-decision decoding, which is widely applied in practical satellite communication systems. It overcomes the limitation that NEC could only be used in hard-decision systems. Therefore, the system capacity and communication quality is notably improved. It is worthwhile for satellite communication for which power and spectrum are limited",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=889222,no,undetermined,0
Toward a more reliable theory of software reliability,"The notions of time and the operational profile incorporated into software reliability are incomplete. Reliability should be redefined as a function of application complexity, test effectiveness, and operating environment. We do not yet have a reliability equation that application complexity, test effectiveness, test suite diversity, and a fuller definition of the operational profile. We challenge the software reliability community to consider these ideas in future models. The reward for successfully doing so likely will be the widespread adoption of software reliability prediction by the majority of software publishers.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=889091,no,undetermined,0
JavaSymphony: a system for development of locality-oriented distributed and parallel Java applications,"Most Java-based systems that support portable parallel and distributed computing either require the programmer to deal with intricate low-level details of Java which can be a tedious, time-consuming and error-prone task, or prevent the programmer from controlling locality of data. In this paper we describe JavaSymphony, a programming paradigm for distributed and parallel computing that provides a software infrastructure for wide classes of heterogeneous systems ranging from small-scale cluster computing to large scale wide-area meta-computing. The software infrastructure is written entirely in Java and runs on any standard compliant Java virtual machine. In contrast to most existing systems, JavaSymphony provides the programmer with the flexibility to control data locality and load balancing by explicit mapping of objects to computing nodes. Virtual architectures are specified to impose a virtual hierarchy on a distributed system of physical computing nodes. Objects can be mapped and dynamically migrated to arbitrary components of virtual architectures. A high-level API to hardware/software system parameters is provided to control mapping, migration, and load balancing of objects. Objects can interact through synchronous asynchronous and one-sided method invocation. Selective remote class loading may reduce the overall memory requirement of an application. Moreover; objects can be made persistent by explicitly storing and loading objects to/from external storage. A prototype of the JavaSymphony software infrastructure has been implemented. Preliminary experiments on a heterogeneous cluster of workstations are described that demonstrate reasonable performance values",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=889023,no,undetermined,0
Design and construction of a PC-based partial discharge analyzer using FPGA,"This paper presents the implementation of the PC-based PD (partial discharge) analyzer. This PD analyzer is capable of measuring both PD level and IEC integrated quantities and analyzing PD data stored in a 3-dimensional distribution. FPGA (field programmable gate arrays) is the main IC in digital part for circuits implementation to perform real-time data processing, controlling and interfacing to a PC. In the analysis process, the H<sub>n</sub>(Ï†,q) distribution is derived from the recorded PD data stream. Fractal features are calculated from the surface of the distribution. Finally, automatic classification of defects is performed. Real-time measurement and display on a Windows98 platform are achieved by using the VxD device driver and the visual-style software developed in C++ language. The test result shows that the characteristic of the PD analyzer complies with IEC Publ. 60270",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=888809,no,undetermined,0
Identifying key attributes of projects that affect the field quality of communication software,"The authors identify key attributes of projects in which the number of problem reports after release is remarkable in the communication software. After several interviews with software project managers, we derived candidate attributes of importance to the projects. To find out the most influential attributes of the projects, we conducted statistical analysis using a set of metrics data related to the candidate attributes and the number of problem reports after release. As a result we successfully found that two metrics concerning the origin's quality and the changes in specification are useful to estimate the field quality",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=884711,no,undetermined,0
On the determination of an appropriate time for ending the software testing process,"Software testing is widely used as a means of increasing software reliability. The prohibitive nature of exhaustive testing has given rise to the problem of determining when a system has reached an acceptable reliability slate and can be released. This has probably become the hardest problem facing a project manager. In this paper, a stopping rule that indicates the appropriate time at which to stop testing is presented. The rule automatically adapts to modifications in the assumptions, since it can be applied under any software error-counting model. An investigation of the properties of the rule is described and the results obtained after applying it to a set of real data in conjunction with two statistical models are presented",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=883781,no,undetermined,0
Software quality prediction using mixture models with EM algorithm,"The use of the statistical technique of mixture model analysis as a tool for early prediction of fault-prone program modules is investigated. The expectation-maximum likelihood (EM) algorithm is engaged to build the model. By only employing software size and complexity metrics, this technique can be used to develop a model for predicting software quality even without the prior knowledge of the number of faults in the modules. In addition, Akaike Information Criterion (AIC) is used to select the model number which is assumed to be the class number the program modules should be classified. The technique is successful in classifying software into fault-prone and non fault-prone modules with a relatively low error rate, providing a reliable indicator for software quality prediction",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=883780,no,undetermined,0
Multiphysics modelling for electronics design,"The future of many companies will depend to a large extent on their ability to initiate techniques that bring schedules, performance, tests, support, production, life-cycle-costs, reliability prediction and quality control into the earliest stages of the product creation process. Important questions for an engineer who is responsible for the quality of electronic parts such as printed circuit boards (PCBs) during design, production, assembly and after-sales support are: What is the impact of temperature? What is the impact of this temperature on the stress produced in the components? What is the electromagnetic compatibility (EMC) associated with such a design? At present, thermal, stress and EMC calculations are undertaken using different software tools that each require model build and meshing. This leads to a large investment in time, and hence cost, to undertake each of these simulations. This paper discusses the progression towards a fully integrated software environment, based on a common data model and user interface, having the capability to predict temperature, stress and EMC fields in a coupled manner. Such a modelling environment used early within the design stage of an electronic product will provide engineers with fast solutions to questions regarding thermal, stress and EMC issues. The paper concentrates on recent developments in creating such an integrated modeling environment with preliminary results from the analyses conducted. Further research into the thermal and stress related aspects of the paper is being conducted under a nationally funded project, while their application in reliability prediction will be addressed in a new European project called PROFIT",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=866175,no,undetermined,0
A replicated assessment and comparison of common software cost modeling techniques,"Delivering a software product on time, within budget, and to an agreed level of quality is a critical concern for many software organizations. Underestimating software costs can have detrimental effects on the quality of the delivered software and thus on a company's business reputation and competitiveness. On the other hand, overestimation of software cost can result in missed opportunities to funds in other projects. In response to industry demand, a myriad of estimation techniques has been proposed during the last three decades. In order to assess the suitability of a technique from a diverse selection, its performance and relative merits must be compared. The current study replicates a comprehensive comparison of common estimation techniques within different organizational contexts, using data from the European Space Agency. Our study is motivated by the challenge to assess the feasibility of using multi-organization data to build cost models and the benefits gained from company-specific data collection. Using the European Space Agency data set, we investigated a yet unexplored application domain, including military and space projects. The results showed that traditional techniques, namely, ordinary least-squares regression and analysis of variance outperformed analogy-based estimation and regression trees. Consistent with the results of the replicated study no significant difference was found in accuracy between estimates derived from company-specific data and estimates derived from multi-organizational data.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=870428,no,undetermined,0
Wireless communications based system to monitor performance of rail vehicles,"This paper describes a recently developed remote monitoring system, based on a combination of embedded computing, digital signal processing, wireless communications, GPS, and GIS technologies. The system includes onboard platforms installed on each monitored vehicle and a central station located in an office. Each onboard platform detects various events onboard a moving vehicle, tags them with time and location information, and delivers the data to an office through wireless communications channels. The central station logs the data into a database and displays the location and status of each vehicle, as well as detected events, on a map. Waveform traces from all sensor channels can be sent with each event and can be viewed by the central station operator. The system provides two-way wireless communication between the central station and mobile onboard platforms. Depending on coverage requirements and customer preferences, communication can be provided through satellite, circuit-switch cellular, digital wireless communication links or a combination of these methods. Settings and software changes may be made remotely from the central station, eliminating the need to capture the monitored vehicle. The onboard platform can be configured for installation on any rail vehicle, including locomotives, passenger cars and freight cars. Depending on the application, the onboard platform can monitor either its own sensors or existing onboard sensors. The system has been used for several railroad applications including ride quality measurement, high cant deficiency monitoring, truck hunting detection, and locomotive health monitoring. The paper describes the system, these applications, and discusses some of the results",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=869993,no,undetermined,0
How perspective-based reading can improve requirements inspections,"Because defects constitute an unavoidable aspect of software development, discovering and removing them early is crucial. Overlooked defects (like faults in the software system requirements, design, or code) propagate to subsequent development phases where detecting and correcting them becomes more difficult. At best, developers will eventually catch the defects, but at the expense of schedule delays and additional product-development costs. At worst, the defects will remain, and customers will receive a faulty product. The authors explain their perspective based reading (PBR) technique that provides a set of procedures to help developers solve software requirements inspection problems. PBR reviewers stand in for specific stakeholders in the document to verify the quality of requirements specifications. The authors show how PBR leads to improved defect detection rates for both individual reviewers and review teams working with unfamiliar application domains.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=869376,no,undetermined,0
Optimal and suboptimal reliable scheduling of precedence-constrained tasks in heterogeneous distributed computing,"Introduces algorithms which can produce both optimal and suboptimal task assignments to minimize the probability of failure of an application executing on a heterogeneous distributed computing system. A cost function which defines this probability under a given task assignment is derived. To find optimal and suboptimal task assignments efficiently, a reliable matching and scheduling problem is converted into a state-space search problem in which the cost function derived is used to guide the search. The A* algorithm for finding optimal task assignments and the A*<sub>m</sub> and hill-climbing algorithms for finding suboptimal task assignments are presented. Simulation results are provided to confirm the performance of the proposed algorithms",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=869148,no,undetermined,0
A monitoring sensor management system for grid environments,"Large distributed systems, such as computational grids, require a large amount of monitoring data be collected for a variety of tasks, such as fault detection, performance analysis, performance tuning, performance prediction and scheduling. Ensuring that all necessary monitoring is turned on and that the data is being collected can be a very tedious and error-prone task. We have developed an agent-based system to automate the execution of monitoring sensors and the collection of event data",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=868639,no,undetermined,0
Using idle workstations to implement predictive prefetching,"The benefits of Markov-based predictive prefetching have been largely overshadowed by the overhead required to produce high-quality predictions. While both theoretical and simulation results for prediction algorithms appear promising, substantial limitations exist in practice. This outcome can be partially attributed to the fact that practical implementations ultimately make compromises in order to reduce overhead. These compromises limit the level of algorithm complexity, the variety of access patterns and the granularity of trace data that the implementation supports. This paper describes the design and implementation of GMS-3P (Global Memory System with Parallel Predictive Prefetching), an operating system kernel extension that offloads prediction overhead to idle network nodes. GMS-3P builds on the GMS global memory system, which pages to and from remote workstation memory. In GMS-3P, the target node sends an online trace of an application's page faults to an idle node that is running a Markov-based prediction algorithm. The prediction node then uses GMS to prefetch pages to the target node from the memory of other workstations in the network. Our preliminary results show that predictive prefetching can reduce the remote-memory page fault time by 60% or more and that, by offloading prediction overhead to an idle node, GMS-3P can reduce this improved latency by between 24% and 44%, depending on the Markov model order",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=868638,no,undetermined,0
Bypass: a tool for building split execution systems,"Split execution is a common model for providing a friendly environment on a foreign machine. In this model, a remotely executing process sends some or all of its system calls back to a home environment for execution. Unfortunately, hand-coding split execution systems for experimentation and research is difficult and error-prone. We have built a tool, called Bypass, for quickly producing portable and correct split execution systems for unmodified legacy applications. We demonstrate Bypass by using it to transparently connect a POSIX application to a simple data staging system based on the Globus toolkit",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=868637,no,undetermined,0
A comparison of using icepak and flotherm in electronic cooling,"Simulation software is a good tool to reduce design cycles and prototypes. The accuracy of the results depend on the users' inputs. Software can provide very accurate results when the inputs are reliable. Software can provide very poor results if the model, boundary conditions, or solution parameters are not represented properly. The user, not the software, is responsible for the accuracy of the results. To adequately use software, the user must become familiar with the built in hnctions and their limits of applicability. You must have a complete understanding of thermal theory so you can judge when a calculated result is in error based on an error in your input. Both Flothenn and Icepak are popular thermal analysis computational fluid dynamics (CFD) software packages in electronic cooling. The scope of this comparison is not to be used by the reader as the only basis for selecting a CFD tool. The frst thermal study was conducted using Icepak and Flotherm to calculate heat transfer fiom an aluminum plate due to natural convection and forced convection (turbulent flow, 300 fpm) environment. Three different types of mesh sizes were compared to theory and experimental data. The second study was to simulate the thermal performance of a RF power amplifier in a steady state forced convection environment. Both Icepak and Flotherm can provide almost the same level of performance. The quality of the prediction depends on the users' background and experience.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=866831,no,undetermined,0
Microprocessor-based busbar protection system,"The design, implementation and testing of a microprocessor-based busbar protection system is presented. The proposed system employs a protection technique that uses the positive- and negative-sequence models of the power system. The system is composed of delta-impedance relays which implement a fault-detection algorithm, based on the developed technique. The algorithm, hardware and software of the relays are described. Performance of the proposed system was checked in the laboratory; the testing procedure and test results are presented",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=859358,no,undetermined,0
"Utilizing third harmonic 100% stator ground fault protection, a cogeneration experience","Third harmonic, 100% stator ground fault protection schemes are becoming economically viable for small and mid size generators used in cogeneration applications. Practical considerations must be observed in order that such schemes are successfully applied for different machines. This paper introduces an experience with the applications of 3rd harmonic schemes in cogeneration applications and depicts their advantages and limitations. For a 50 MVA generator, actual measurements of produced third harmonics are portrayed and analyzed. In light of the experience and analysis, applications of certain third harmonic scheme configurations are contemplated",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=882632,no,undetermined,0
Finding faces in wavelet domain for content-based coding of color images,"Human face images form the important database in police departments, banks, security kiosks, and they are also found in abundance in day-to-day life. In these databases the important content, of course, is the face region. We present a highly efficient system that detects the human faces in the wavelet transform for discriminative quantization to achieve high perceptual quality content-based image coding technique. The proposed method gives superior subjective performance over JPEG without sacrificing the performance in the rate-distortion spectrum",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=859309,no,undetermined,0
Simplex minimisation for multiple-reference motion estimation,"This paper investigates the properties of the multiple-reference block motion field. Guided by the results of this investigation, the paper proposes three fast multiple-reference block matching motion estimation algorithms. The proposed algorithms are extensions of the single-reference simplex minimisation search (SMS) algorithm. The algorithms provide different degrees of compromise between prediction quality and computational complexity. Simulation results using a multi-frame memory of 50 frames indicate that the proposed multiple-reference SMS algorithms have a computational complexity comparable to that of single-reference full-search while still maintaining the prediction gain of multiple-reference motion estimation",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=858856,no,undetermined,0
An analysis of a supply chain management agent architecture,"The authors illustrate a methodology for early agent systems architecture analysis and evaluation with the focus on risk identification, evaluation, and mitigation. Architectural decisions on software qualities such as performance, modifiability, and security can be assessed. The illustration is drawn from the supply chain management application domain",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=858495,no,undetermined,0
A study in dynamic neural control of semiconductor fabrication processes,"This paper describes a generic dynamic control system designed for use in semiconductor fabrication process control. The controller is designed for any batch silicon wafer process that is run on equipment having a high number of variables that are under operator control. These controlled variables include both equipment state variables such as power, temperature, etc., and the repair, replacement, or maintenance of equipment parts, which cause parameter drift of the machine over time. The controller consists of three principal components: 1) an automatically updating database, 2) a neural-network prediction model for the prediction of process quality based on both equipment state variables and parts usage, and 3) an optimization algorithm designed to determine the optimal change of controllable inputs that yield a reduced operation cost, in-control solution. The optimizer suggests a set of least cost and least effort alternatives for the equipment engineer or operator. The controller is a PC-driven software solution that resides outside the equipment and does not mandate implementation of recommendations in order to function correctly. The neural model base continues to learn and improve over time. An example of the dynamic process control tool performance is presented retrospectively for a plasma etch system. In this study, the neural networks exhibited overall accuracy to within 20% of the observed values of .986, .938, and .87 for the output quality variables of etch rate, standard deviation, and selectivity, respectively, based on a total sample size of 148 records. The control unit was able to accurately detect the need for parts replacements and wet clean operations in 34 of 40 operations. The controller suggested chamber state variable changes which either improved performance of the output quality variables or adjusted the input variable to a lower cost level without impairment of output quality",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=857946,no,undetermined,0
Evaluation of gradient descent learning algorithms with an adaptive local rate technique for hierarchical feedforward architectures,"Gradient descent learning algorithms (namely backpropagation and weight perturbation) can significantly increase their classification performances by adopting a local and adaptive learning rate management approach. We present the results of the comparison of the classification performance of the two algorithms in a tough application: quality control analysis in the steel industry. The feedforward network is hierarchically organized (i.e. tree of multilayer perceptrons). The comparison has been performed starting from the same operating conditions (i.e. network topology, stopping criterion, etc.): the results show that the probability of correct classification is significantly better for the weight perturbation algorithm",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=857895,no,undetermined,0
Formal modeling and analysis of atomic commitment protocols,"The formal specification and mechanical verification of an atomic commitment protocol (ACP) for distributed real-time and fault-tolerant databases is presented. As an example, the non-blocking ACP of Babaoglu and Toueg (1993) is analyzed. An error in their termination protocol for recovered participants has been detected. We propose a new termination protocol which has been proved correct formally. To stay close to the original formulation of the protocol, timed state machines are used to specify the processes, whereas the communication mechanism between processes is defined using assertions. Formal verification has been performed incrementally: adding recovery from crashes only after having proved the basic protocol. The verification system PVS was used to deal with the complexity of this fault-tolerant protocol",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=857694,no,undetermined,0
Benchmarking anomaly-based detection systems,"Anomaly detection is a key element of intrusion detection and other detection systems in which perturbations of normal behavior suggest the presence of intentionally or unintentionally induced attacks, faults, defects, etc. Because most anomaly detectors are based on probabilistic algorithms that exploit the intrinsic structure (or regularity) embedded in data logs, a fundamental question is whether or not such structure influences detection performance. If detector performance is indeed a function of environmental regularity, it would be critical to match detectors to environmental characteristics. In intrusion-detection settings, however, this is not done, possibly because such characteristics are not easily ascertained. This paper introduces a metric for characterizing structure in data environments, and tests the hypothesis that intrinsic structure influences probabilistic detection. In a series of experiments, an anomaly detection algorithm was applied to a benchmark suite of 165 carefully calibrated, anomaly-injected data sets of varying structure. The results showed performance differences of as much as an order of magnitude, indicating that current approaches to anomaly detection may not be universally dependable",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=857599,no,undetermined,0
Designing high-performance and reliable superscalar architectures-the out of order reliable superscalar (O3RS) approach,"As VLSI geometry continues to shrink and the level of integration increases, it is expected that the probability of faults, particularly transient faults, will increase in future microprocessors. So far, fault tolerance has chiefly been considered for special purpose or safety critical systems, but future technology will likely require integrating fault tolerance techniques into commercial systems. Such systems require low cost solutions that are transparent to the system operation and do not degrade overall performance. This paper introduces a new superscalar architecture, termed as 03RS that aims to incorporate such simple fault tolerance mechanisms as part of the basic architecture",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=857578,no,undetermined,0
Characterizing implicit information during peer review meetings,"Disciplines like software engineering evolve over time by studying some practices and feeding back those results to improve the practice. The empirical approach presented in the paper is used to analyze the nature of the information shared during peer review meetings (PRMs) held in an industrial software engineering project. The results obtained show that although a PRM is categorized as a verification practice, it is also a golden opportunity for project personnel to share information about technical solutions, a decision's rationale or quality guidelines. The contribution of PRMs is not restricted to the rapid detection of anomalies; they also provide the opportunity for project team members to share implicit information. PRM efficiency cannot solely be measured through an anomaly detection rate.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=870436,no,undetermined,0
Lessons learned from teaching reflective software engineering using the Leap toolkit,"After using and teaching the Personal Software Process (PSP) (W.S. Humphrey, 1995) for over four years, the author came to appreciate the insights and benefits that it produced. However, there were some general problems with the PSP. These problems led him to begin work on an alternative software process improvement method called reflective software engineering. Like the PSP, reflective software engineering is based upon a simple idea: people learn best from their own experience. Reflective software engineering supports experience based improvement in developers' professional activities by helping the developer structure their experience, record it, and analyze it. Unlike the PSP, reflective software engineering is designed around the presence of extensive automated support. The support is provided by a Java based toolkit called ""Leap"" <http://csdl.ics.hawaii.edu/Research/LEAP/LEAP.html>. The kinds of structured insights and experiences users can record with Leap include: the size of the work product; the time it takes to develop it; the defects that the user or others find in it; the patterns that they discover during the development of it; checklists that they use during or design as a result of the project; estimates for time or size that they generate during the project; and the goals, questions, and measures that the user uses to motivate the data recording.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=870464,no,undetermined,0
Automatic image event segmentation and quality screening for albuming applications,"In this paper, a system for automatic albuming of consumer photographs is described and its specific core components of event segmentation and screening of low quality images are discussed. A novel event segmentation algorithm was created to automatically cluster pictures into events and sub-events for albuming, based on date/time meta data information as well as color content of the pictures. A new quality-screening is developed based on object quality to detect problematic images due to underexposure, low contrast, and camera defocus or movement. Performance testing of these algorithms was conducted using a database of real consumer photos and showed that these functions provide a useful first-cut album layout for typical rolls of consumer pictures. A first version of the automatic albuming application software was rested through a consumer trial in the United States from August to December 1999",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=871558,no,undetermined,0
Comparative investigation of diagnostic media for induction motors: a case of rotor cage faults,"Results of a comparative experimental investigation of various media for noninvasive diagnosis of rotor faults in induction motors are presented. Stator voltages and currents in an induction motor were measured, recorded, and employed for computation of the partial and total input powers and of the estimated torque. Waveforms of the current, partial powers p<sub>AB</sub> and p<sub>CB</sub>, total power, and estimated torque were subsequently analyzed using the fast Fourier transform. Several rotor cage faults of increasing severity were studied with various load levels. The partial input power p<sub>CB</sub> was observed to exhibit the highest sensitivity to rotor faults. This medium is also the most reliable, as it includes a multiplicity of fault-induced spectral components",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=873218,no,undetermined,0
Integrating reliability and timing analysis of CAN-based systems,"The paper outlines and illustrates a reliability analysis method developed with a focus on CAN based automotive systems. The method considers the effect of faults on schedulability analysis and its impact on the reliability estimation of the system, and attempts to integrate both to aid system developers. We illustrate the method by modeling a simple distributed antilock braking system, showing that even in cases where the worst-case analysis deem the system unschedulable, it may be proven to satisfy its timing requirements with a sufficiently high probability. From a reliability and cost perspective, the paper underlines the tradeoffs between timing guarantees, the level of hardware and software faults, and per-unit cost",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=882547,no,undetermined,0
Predicting and measuring quality of service for mobile multimedia,We show how an understanding of human perception and the simulation of a mobile IP network may be used to tackle the relevant issues of providing acceptable quality of service for mobile multimedia,2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=881578,no,undetermined,0
Computer analysis of LOS microwaves links clear air performance,"Microwave links have to be designed such that propagation effects do not reduce the quality of the transmitted signals. Measurements and the derived propagation parameters are analysed and discussed, for Cluj-Napoca county, in order to improve future planning of the radio links",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=880439,no,undetermined,0
Protection of gate movement in hydroelectric power stations,"Movement of the gates is an everyday task performed on a dam of hydroelectric a power station. This operation is often controlled remotely by measuring the positioning of the gates and a level of the current in the driving motors. This method of control cannot, however, detect anomalies, such as asymmetric movement of gates, faults in drive gearwheel etc. We therefore decided to devise a new improved system for the protection of gate movement which is described in our paper. It is based on measuring the forces applied to the transmission construction. The transducers with resistive strain gauges are mounted on the frame bearings and the strains are subsequently measured. The output signal from the transducer is proportional to a force applied to the frame. The transducers are installed at the points of the largest strain. For the protection against uneven movement of the left and right chains, the strain transmitters are inserted in the bearings of the main gearwheel, to measure the compression. The whole system is controlled by the microprocessor. The details on sensors, the electronic instrumentation needed, and the software of the controlling computer, are also described in the paper. This system has been tested, and regularly used, on the power stations of Drava river in Slovenia.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=879713,no,undetermined,0
"A reusable state-based guidance, navigation and control architecture for planetary missions","JPL has embarked on the Mission Data System (MDS) project to produce a reusable, integrated flight and ground software architecture. This architecture will then be adapted by future JPL planetary projects to form the basis of their flight and ground software. The architecture is based on identifying the states of the system under consideration. States include aspects of the system that must be controlled to accomplish mission objectives, as well as aspects that are uncontrollable but must be known. The architecture identifies methods to measure, estimate, model, and control some of these states. Some states are controlled by goals, and the natural hierarchy of the system is employed by recursively elaborating goals until primitive control actions are reached. Fault tolerance emerges naturally from this architecture. Failures are detected as discrepancies between state and model-based predictions of state. Fault responses are handled either by re-elaboration of goals, or by failures of goals invoking re-elaboration at higher levels. Failure modes an modelled as possible behaviors of the system, with corresponding state estimation processes. Architectural patterns are defined for concepts such as states, goals, and measurements. Aspects of state are captured in a state-analysis database. UML is used to capture mission requirements as Use Cases and Scenarios. Application of the state-based concepts to specific states is also captured in UML, achieving architectural consistency by adapting base classes for all architectural patterns",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=879294,no,undetermined,0
Multi-agent diagnosis and control of an air revitalization system for life support in space,"An architecture of inter-operating agents has been developed to provide control and fault management for advanced life support systems in space. In this multi-agent architecture, cooperating autonomous software agents coordinate with human agents, to provide support in novel fault management situations. This architecture combines the Livingstone model-based mode identification and reconfiguration (MIR) system with elements of the 3T architecture for autonomous flexible command and control. The MIR software agent performs model-based state identification and fault diagnosis. MIR also identifies novel recovery configurations and the set of commands required to accomplish the recovery. The 3T procedural executive and the human operator use the diagnoses and recovery recommendations, and provide command sequencing. Human interface extensions have been developed to support human monitoring and control of both 3T and MIR data and activities. This architecture has been exercised for control and fault management of an oxygen production system for air revitalization in space. The software operates in a dynamic simulation testbed",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=877906,no,undetermined,0
Signal processing and fault detection with application to CH-46 helicopter data,"Central to Qualtech Systems mission is its testability and maintenance software (TEAMS) and derivatives. Many systems comprise components equipped with self-testing capability; but, if the system is complex (and involves feedback and if the self-testing itself may occasionally be faulty) tracing faults to a single or multiple causes is difficult. However, even for systems involving many thousands of components the PC-based TEAMS provides essentially real-time system-state diagnosis. Until recently TEAMS operation was passive: its diagnoses were based on whatever data sensors could provide. Now, however, a signal-processing (SP) â€œfrontendâ€?matched to inference needs is available. Many standard signal processing primitives, such as filtering, spectrum analysis and multi-resolution decomposition are available; the SP toolbox is also equipped with a (supervised) classification capability based on a number of decision-making paradigms. This paper is about the SP toolbox. We show its capabilities, and demonstrate its performance on the CH-46 â€œWestlandâ€?data set",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=877853,no,undetermined,0
A practical classification-rule for software-quality models,"A practical classification rule for a SQ (software quality) model considers the needs of the project to use a model to guide targeting software RE (reliability enhancement) efforts, such as extra reviews early in development. Such a rule is often more useful than alternative rules. This paper discusses several classification rules for SQ models, and recommends a generalized classification rule, where the effectiveness and efficiency of the model for guiding software RE efforts can be explicitly considered. This is the first application of this rule to SQ modeling that we know of. Two case studies illustrate application of the generalized classification rule. A telecommunication-system case-study models membership in the class of fault-prone modules as a function of the number of interfaces to other modules. A military-system case-study models membership in the class of fault-prone modules as a function of a set of process metrics that depict the development history of a module. These case studies are examples where balanced misclassification rates resulted in more useful and practical SQ models than other classification rules",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=877340,no,undetermined,0
Methods based on Petri net for resource sharing estimation,"This work presents two approaches for computing the number of functional units in hardware/software codesign context. The proposed hardware/software codesign framework uses Petri net as common formalism for performing quantitative and qualitative analysis. The use of Petri net as an intermediate format allows to analyze properties of the specification and formally compute performance indices which are used in the partitioning process. This paper is devoted to describe the algorithms for functional unit estimation. This work also proposes a method of extending the Petri net model in order to take into account causal constraints provided by the designers. However, an overview of the general hardware/software codesign method is also presented",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=876011,no,undetermined,0
The use of PSA for fault detection and characterization in electrical apparatus,"The monitoring of the actual condition of high voltage apparatus has become more and more important in the last years. One well established tool to characterize the actual condition of electric equipment is the measurement and evaluation of partial discharge data. Immense effort has been put into sophisticated statistic software-tools, to extract meaningful analyses out of data sets, without taking care of relevant correlations between consecutive discharge pulses. In contrast to these classical methods of partial discharge analysis the application of the Pulse Sequence Analysis allows a far better insight into the local defects. The detailed analysis of sequences of discharges in a voltage- and a current-transformer shows that the sequence of the partial discharge signals may change with time, because either different defects are active at different measuring times or a local defect may change with time as a consequence of the discharge activity. Hence for the evaluation of the state of degradation or the classification of the type of defect the analysis of short `homogeneous' sequences or sequence correlated data is much more meaningful than just the evaluation of a set of independently accumulated discharge data. This is demonstrated by the evaluation of measurements performed on different commercial hv-apparatus",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=875689,no,undetermined,0
New fast binary pyramid motion estimation for MPEG2 and HDTV encoding,"A novel fast binary pyramid motion estimation (FBPME) algorithm is presented in this paper. The proposed FBPME scheme is based on binary multiresolution layers, exclusive-or (XOR) Boolean block matching, and a N-scale tiling search scheme. Each video frame is converted into a pyramid structure of K-1 binary layers with resolution decimation, plus one integer layer at the lowest resolution. At the lowest resolution layer, the N-scale tiling search is performed to select initial motion vector candidates. Motion vector fields are gradually refined with the XOR Boolean block-matching criterion and the N-scale tiling search schemes in higher binary layers. FBPME performs several thousands times faster than the conventional full-search block-matching scheme at the same PSNR performance and visual quality. It also dramatically reduces the bus bandwidth and on-chip memory requirement. Moreover, hardware complexity is low due to its binary nature. Fully functional software MPEG-2 MP@ML encoders and Advanced Television Standard Committee high definition television encoders based on the FBPME algorithm have been implemented. FBPME hardware architecture has been developed and is being incorporated into single-chip MPEG encoders. A wide range of video sequences at various resolutions has been tested. The proposed algorithm is also applicable to other digital video compression standards such as H.261, H.263, and MPEG4",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=875506,no,undetermined,0
Scalable hardware-algorithm for mark-sweep garbage collection,"The memory-intensive nature of object-oriented languages such as C++ and Java has created the need for high-performance dynamic memory management. Object-oriented applications often generate higher memory intensity in the heap region. Thus, a high-performance memory manager is needed to cope with such applications. As today's VLSI technology advances, it becomes increasingly attractive to map software algorithms such as malloc(), free() and garbage collection into hardware. This paper presents a hardware design of a sweeping function (for mark-and-sweep garbage collection) that fully utilizes the advantages of combinational logic. In our scheme, the bit sweep can detect and sweep the garbage in a constant time. Bit-map marking in software can improve the cache performance and reduce number of page faults; however, it often requires several instructions to perform a single mark. In our scheme, only one hardware instruction is required per mark. Moreover, since the complexity of the sweeping phase is often higher than the marking phase, the garbage collection time may be substantially improved. The hardware complexity of the proposed scheme (bit-sweeper) is O(n), where n represents the size of the bit map",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=874643,no,undetermined,0
Software product improvement with inspection. A large-scale experiment on the influence of inspection processes on defect detection in software requirements documents,"In the early stages of software development, inspection of software documents is the most effective quality assurance measure to detect defects and provides timely feedback on quality to developers and managers. The paper reports on a controlled experiment that investigates the effect of defect detection techniques on software product and inspection process quality. The experiment compares defect detection effectiveness and efficiency of a general reading technique that uses checklist based reading, and a systematic reading technique, scenario based reading, for requirements documents. On the individual level, effectiveness was found to be higher for the general reading technique, while the focus of the systematic reading technique led to a higher yield of severe defects compared to the general reading technique. On a group level, which combined inspectors' contributions, the advantage of a reading technique regarding defect detection effectiveness depended on the size of the group, while the systematic reading technique generally exhibited better defect detection efficiency",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=874427,no,undetermined,0
DOORS: towards high-performance fault tolerant CORBA,"An increasing number of applications are being developed using distributed object computing middleware, such as CORBA. Many of these applications require the underlying middleware, operating systems, and networks to provide end-to-end quality of service (QoS) support to enhance their efficiency, predictability, scalability, and fault tolerance. The Object Management Group (OMG), which standardizes CORBA, has addressed many of these application requirements in the Real-time CORBA and Fault-Tolerant CORBA specifications. We provide four contributions to the study of fault-tolerant CORBA middleware for performance-sensitive applications. First, we provide an overview of the Fault Tolerant CORBA specification. Second, we describe a framework called DOORS, which is implemented as a CORBA service to provide end-to-end application-level fault tolerance. Third, we outline how the DOORS' reliability and fault-tolerance model has been incorporated into the standard OMG Fault-tolerant CORBA specification. Finally, we outline the requirements for CORBA ORB core and higher-level services to support the Fault Tolerant CORBA specification efficiently",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=874174,no,undetermined,0
System-level test bench generation in a co-design framework,"Co-design tools represent an effective solution for reducing costs and shortening time-to-market, when System-on-Chip design is considered. In a top-down design flow, designers would greatly benefit from the availability of tools able to automatically generate test benches, which can be used during every design step, from the system-level specification to the gate-level description. This would significantly increase the chance of identifying design bugs early in the design flow, thus reducing the costs and increasing the final product quality. The paper proposes an approach for integrating the ability to generate test benches into an existing co-design tool. Suitable metrics are proposed to guide the generation, and preliminary experimental results are reported, assessing the effectiveness of the proposed technique",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=873775,no,undetermined,0
Toward the automatic assessment of evolvability for reusable class libraries,"Many sources agree that managing the evolution of an OO system constitutes a complex and resource-consuming task. This is particularly true for reusable class libraries, as the user interface must be preserved to allow for version compatibility. Thus, the symptomatic detection of potential instabilities during the design phase of such libraries may serve to avoid later problems. This paper presents a fuzzy logic-based approach for evaluating the interface stability of a reusable class library, by using structural metrics as stability indicators.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=873680,no,undetermined,0
Automated security checking and patching using TestTalk,"In many computer system security incidents, attackers successfully intruded computer systems by exploiting known weaknesses. Those computer systems remained vulnerable even after the vulnerabilities were known because it requires constant attention to stay on top of security updates. It is often both time-consuming and error-prone to manually apply security patches to deployed systems. To solve this problem, we propose to develop a framework for automated security checking and patching. The framework, named Securibot, provides a self-operating mechanism for security checking and patching. Securibot performs security testing using security profiles and security updates. It can also detect compromised systems using attack signatures. Most important, the Securibot framework allows system vendors to publish recently discovered security weaknesses and new patches in a machine-readable form so that the Securibot system running on deployed systems can automatically check out security updates and apply the patches.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=873673,no,undetermined,0
A high-assurance measurement repository system,"High-quality measurement data are very useful for assessing the efficacy of high-assurance system engineering techniques and tools. Given the rapidly evolving suite of modern tools and techniques, it is helpful to have a large repository of up-to-date measurement data that can be used to quantitatively assess the impact of state-of-the-art techniques on the quality of the resulting systems. For many types of defects, including Y2K failures, infinite loops, memory overflow, access violations, arithmetic overflow, divide-by-zero, off-by-one errors, timing errors, deadlocks, etc., it may be possible to combine data from a large number of projects and use these to make statistical inferences. This paper presents a highly secure and reliable measurement repository system for measurement data acquisition, storage and analysis. The system is being used by the QuEST Forum, which is a new industry forum consisting of over 100 leading telecommunications companies. The paper describes the decisions that were made in the design of the measurement repository system, as well as implementation strategies that were used in achieving a high-level of confidence in the security and reliability of the system",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=895471,no,undetermined,0
An exception handling software architecture for developing fault-tolerant software,"Fault-tolerant object-oriented software systems are inherently complex and have to cope with an increasing number of exceptional conditions in order to meet the system's dependability requirements. This work proposes a software architecture which integrates uniformly both concurrent and sequential exception handling. The exception handling architecture is independent of programming language or exception handling mechanism, and its use can minimize the complexity caused by the handling of abnormal behavior. Our architecture provides, during the architectural design stage, the context in which more detailed design decisions related to exception handling are made in later development stages. This work also presents a set of design patterns which describes the static and dynamic aspects of the components of our software architecture. The patterns allow a clear separation of concerns between the system's functionality and the exception handling facilities, applying the computational reflection technique",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=895476,no,undetermined,0
Bayesian framework for reliability assurance of a deployed safety critical system,"The existence of software faults in safety-critical systems is not tolerable. Goals of software reliability assessment are estimating the failure probability of the program, Î¸, and gaining statistical confidence that Î¸ is realistic. While in most cases reliability assessment is performed prior to the deployment of the system, there are circumstances when reliability assessment is needed in the process of (re)evaluation of the fielded (deployed) system. Post deployment reliability assessment provides reassurance that the expected dependability characteristics of the system have been achieved. It may be used as a basis of the recommendation for maintenance and further improvement, or the recommendation to discontinue the use of the system. The paper presents practical problems and challenges encountered in an effort to assess and quantify software reliability of NASA's Day-of-Launch I-Load Update (DOLILU II) system DOLILU II system has been in operational use for several years. A Bayesian framework is chosen for reliability assessment, because it allows incorporation of (in this specific case failure free) program executions observed in the operational environment. Furthermore, we outline the development of a probabilistic framework that allows accounting of rigorous verification and validation activities performed prior to a system's deployment into the reliability assessment",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=895477,no,undetermined,0
A controlled experiment to assess the effectiveness of inspection meetings,"Software inspection is one of the best practices for detecting and removing defects early in the software development process. In a software inspection, review is first performed individually and then by meeting as a team. In the last years, some empirical studies have shown that inspection meetings do not improve the effectiveness of the inspection process with respect to the number of true discovered defects. While group synergy allows inspectors to find some new defects, these meeting gains are offset by meeting losses, that is defects found by individuals but not reported as a team. We present a controlled experiment with more than one hundred undergraduate students who inspected software requirements documents as part of a university course. We compare the performance of nominal and real teams, and also investigate the reasons for meeting losses. Results show that nominal teams outperformed real teams, there were more meeting losses than meeting gains, and that most of the losses were defects found by only one individual in the inspection team",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915514,no,undetermined,0
Aspect-oriented programming takes aim at software complexity,"As global digitalization and the size of applications expand at an exponential rate, software engineering's complexities are also growing. One feature of this complexity is the repetition of functionality throughout an application. An example of the problems this complexity causes occurs when programmers must change an oft-repeated feature for an updated or new version of an application. It is often difficult for programmers to find every instance of such a feature in millions of lines of code. Failing to do so, however, can introduce bugs. To address this issue, software researchers are developing methodologies based on a new programming element: the aspect. An aspect is a piece of code that describes a recurring property of a program. Applications can, of course, have multiple aspects. Aspects provide cross-cutting modularity. In other words, programmers can use aspects to create software modules for issues that cut across various parts of an application. Aspects have the potential to make programmers' work easier, less time-consuming and less error-prone. Proponents say aspects could also lead to less expensive applications, shorter upgrade cycles and software that is flexible and more customizable. A number of companies and universities are working on aspects or aspect-like concepts",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=917532,no,undetermined,0
A vector-based approach to software size measurement and effort estimation,"Software size is a fundamental product measure that can be used for assessment, prediction and improvement purposes. However, existing software size measures, such as function points, do not address the underlying problem complexity of software systems adequately. This can result in disproportional measures of software size for different types of systems. We propose a vector size measure (VSM) that incorporates both functionality and problem complexity in a balanced and orthogonal manner. The VSM is used as the input to a vector prediction model (VPM) which can be used to estimate development effort early in the software life-cycle. We theoretically validate the approach against a formal framework. We also empirically validate the approach with a pilot study. The results indicate that the approach provides a mechanism to measure the size of software systems, classify software systems, and estimate development effort early in the software life-cycle to within Â±20% across a range of application types",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=917523,no,undetermined,0
Investigation of logistic regression as a discriminant of software quality,"Investigates the possibility that logistic regression functions (LRFs), when used in combination with Boolean discriminant functions (BDFs), which we had previously developed, would improve the quality classification ability of BDFs when used alone; this was found to be the case. When the union of a BDF and LRF was used to classify quality, the predictive accuracy of quality and inspection cost was improved over that of using either function alone for the Space Shuttle. Also, the LRFs proved useful for ranking the quality of modules in a build. The significance of these results is that very high-quality classification accuracy (1.25% error) can be obtained while reducing the inspection cost incurred in achieving high quality. This is particularly important for safety-critical systems. Because the methods are general and not particular to the Shuttle, they could be applied to other domains. A key part of the LRF development was a method for identifying the critical value (i.e. threshold) that could discriminate between high and low quality, and at the same time constrain the cost of inspection to a reasonable value",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915540,no,undetermined,0
Better validation in a world-wide development environment,"Increasingly, software projects are handled in a global and distributed project setup. Global software development, however, also challenges traditional techniques of software engineering, such as peer reviews or teamwork. In particular, validation activities during development, such as inspections, need to be adjusted to achieve results which are both efficient and effective. Effective teamwork and the coaching of engineers contribute highly to successful projects. In this article, we evaluate experiences with validation activities in a global setting within Alcatel's switching and routing business. We investigate three hypotheses related, respectively, to the effects of collocated inspections, intensive coaching and feature-oriented development teams on globally distributed projects. As all these activities mean initial investment compared to a standard process with scattered activities, the major validation criterion for the three hypotheses is cost reduction due to earlier defect detection and less defects introduced. The data is taken from a sample of over 60 international projects of various sizes, from which we have collected all types of software product and process metrics in the past four years",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915537,no,undetermined,0
Evaluating software degradation through entropy,"Software systems are affected by degradation as an effect of continuous change. Since late interventions are too much onerous, software degradation should be detected early in the software lifetime. Software degradation is currently detected by using many different complexity metrics, but their use to monitor maintenance activities is costly. These metrics are difficult to interpret, because each emphasizes a particular aspect of degradation and the aspects shown by different metrics are not orthogonal. The purpose of our research is to measure the entropy of a software system to assess its degradation. In this paper, we partially validate the entropy class of metrics by a case study, replicated on successive releases of a set of software systems. The validity is shown through direct measures of software quality, such as the number of detected defects, the maintenance effort and the number of slipped defects",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915530,no,undetermined,0
Understanding and measuring the sources of variation in the prioritization of regression test suites,"Test case prioritization techniques let testers order their test cases so that those with higher priority, according to some criterion, are executed earlier than those with lower priority. In previous work (1999, 2000), we examined a variety of prioritization techniques to determine their ability to improve the rate of fault detection of test suites. Our studies showed that the rate of fault detection of test suites could be significantly improved by using more powerful prioritization techniques. In addition, they indicated that rate of fault detection was closely associated with the target program. We also observed a large quantity of unexplained variance, indicating that other factors must be affecting prioritization effectiveness. These observations motivate the following research questions. (1) Are there factors other than the target program and the prioritization technique that consistently affect the rate of fault detection of test suites? (2) What metrics are most representative of each factor? (3) Can the consideration of additional factors lead to more efficient prioritization techniques? To address these questions, we performed a series of experiments exploring three factors: program structure, test suite composition and change characteristics. This paper reports the results and implications of those experiments",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915525,no,undetermined,0
Influence of team size and defect detection technique on inspection effectiveness,"Inspection team size and the set of defect detection techniques used by the team are major characteristics of the inspection design, which influences inspection effectiveness, benefit and cost. The authors focus on the inspection performance of a nominal, that is non-communicating team, similar to the situation of an inspection team after independent individual preparation. We propose a statistical model based on empirical data to calculate the expected values for the inspection effectiveness and effort of synthetic nominal teams. Further, we introduce an economic model to compute the inspection benefits, net gain, and return on investment. With these models we determine (a) the best mix of reading techniques (RTs) to maximize the average inspection performance for a given team size, (b) the optimal team size and RT mix for a given inspection time budget, and (c) the benefit of an additional inspector for a given team size. Main results of the investigation with data from a controlled experiment are: (a) benefits of an additional inspector for a given RT diminished quickly with growing team size, thus, above a given team size a mix of different RTs is more effective and has a higher net gain than using only one RT; (b) the cost-benefit model limits team size, since the diminishing gain of an additional inspector at some point is more than offset by his additional cost",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915516,no,undetermined,0
Investigating the impact of reading techniques on the accuracy of different defect content estimation techniques,"Software inspections have established an impressive track record for early defect detection and correction. To increase their benefits, recent research efforts have focused on two different areas: systematic reading techniques and defect content estimation techniques. While reading techniques are to provide guidance for inspection participants on how to scrutinize a software artifact in a systematic manner, defect content estimation techniques aim at controlling and evaluating the inspection process by providing an estimate of the total number of defects in an inspected document. Although several empirical studies have been conducted to evaluate the accuracy of defect content estimation techniques, only few consider the reading approach as an influential factor. The authors examine the impact of two specific reading techniques: a scenario based reading technique and checklist based reading, on the accuracy of different defect content estimation techniques. The examination is based on data that were collected in a large experiment with students of the Vienna University of Technology. The results suggest that the choice of the reading technique has little impact on the accuracy of defect content estimation techniques. Although more empirical work is necessary to corroborate this finding, it implies that practitioners can use defect content estimation techniques without any consideration of their current reading technique",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915515,no,undetermined,0
Memory hierarchy optimization of multimedia applications on programmable embedded cores,"Data memory hierarchy optimization and partitioning for a widely used multimedia application kernel known as the hierarchical motion estimation algorithm is undertaken, with the use of global loop and data-reuse transformations for three different embedded processor architecture models. Exhaustive exploration of the obtained results clarifies the effect of the transformations on power, area, and performance and also indicates a relation between the complexity of the application and the power savings obtained by this strategy. Furthermore, the significant contribution of the instruction memory even after the application of performance optimizations to the total power budget becomes evident and a methodology is introduced in order to reduce this component",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915271,no,undetermined,0
A real-time hardware fault detector using an artificial neural network for distance protection,"A real-time fault detector for the distance protection application, based on artificial neural networks, is described. Previous researchers in this field report use of complex filters and artificial neural networks with large structure or long training times. An optimum neural network structure with a short training time is presented. Hardware implementation of the neural network is addressed with a view to improve the performance in terms of speed of operation. By having a smaller network structure the hardware complexity of implementation reduces considerably. Two preprocessors are described for the distance protection application which enhance the training performance of the artificial neural network many fold. The preprocessors also enable real-time functioning of the artificial neural network for the distance protection application. Design of an object oriented software simulator, which was developed to identify the hardware complexity of implementation, and the results of the analysis are discussed. The hardware implementation aspects of the preprocessors and of the neural network are briefly discussed",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=905596,no,undetermined,0
Assessment of true worst case circuit performance under interconnect parameter variations,"The complicated manufacturing processes dictate that process variations are unavoidable in today's VLSI products. Unlike device variations, which can be captured by worst/best case corner points, the effects of interconnect variations are context-dependent, which makes it difficult to capture the true worst-case timing performance. This paper discusses an efficient method to explore the extreme values of performance metrics and the specific parameters that will create these extreme performances. The described approach is based on a iterative search technique which facilitates its proper search direction by calculating an explicit analytical approximation model",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915267,no,undetermined,0
Full chip false timing path identification: applications to the PowerPC<sup>TM</sup> microprocessors,"Static timing anaylsis sets the industry standard in the design methodology of high speed/performance microprocessors to determine whether timing requirements have been met. Unfortunately, not all the paths identified using such analysis can be sensitized. This leads to a pessimistic estimation of the processor speed. Also, no amount of engineering effort spent on optimizing such paths can improve the timing performance of the chip. In the past we demonstrated initial results of how ATPG techniques can be used to identify false paths efficiently. Due to the gap between the physical design on which the static timing analysis of the chip is bused and the test view on which the ATPG techniques are applied to identify false paths, in many cases only sections of some of the paths in the full-chip were analyzed in our initial results. In this paper, we will fully analyze all the timing paths using the ATPG techniques, thus overcoming the gap between the testing and timing analysis techniques. This enables us to do false path identification at the full-chip level of the circuit. Results of applying our technique to the second generation G4 PowerPC<sup>TM</sup> will be presented",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915072,no,undetermined,0
A universal communication model for an automotive system integration platform,"In this paper, we present a virtual integration platform based design methodology for distributed automotive systems. The platform, built within the `Virtual Component Co-Design'' tool (VCC), provides the ability of distributing a given system functionality over an architecture so as to validate different solutions in terms of cost, safety requirements, and real-time constraints. The virtual platform constitutes the foundation for design decisions early in the development phase, therefore enabling decisive and competitive advantages in the development process. This paper focuses on one of the key-enablers of the methodology, the universal communication model (UCM). The UCM is defined at a level of abstraction that allows accurate estimates of the performance including the latencies over the bus network, and good simulation performance. In addition, due to the high level of reusability and parameterization of its components, it can be used as a framework for modeling the different communication protocols common in the automotive domain",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915000,no,undetermined,0
Assessing optimal software architecture maintainability,"Over the last decade, several authors have studied the maintainability of software architectures. In particular, the assessment of maintainability has received attention. However, even when one has a quantitative assessment of the maintainability of a software architecture, one still does not have any indication of the optimality of the software architecture with respect to this quality attribute. Typically, the software architect is supposed to judge the assessment result based on his or her personal experience. In this paper, we propose a technique for analysing the optimal maintainability of a software architecture based on a specified scenario profile. This technique allows software architects to analyse the maintainability of their software architecture with respect to the optimal maintainability. The technique is illustrated and evaluated using industrial cases",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=914981,no,undetermined,0
Prediction models for software fault correction effort,"We have developed a model to explain and predict the effort associated with changes made to software to correct faults while it is undergoing development. Since the effort data available for this study is ordinal in nature, ordinal response models are used to explain the effort in terms of measures of fault locality and the characteristics of the software components being changed. The calibrated ordinal response model is then applied to two projects not used in the calibration to examine predictive validity",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=914975,no,undetermined,0
Experimental application of extended Kalman filtering for sensor validation,"A sensor failure detection and identification scheme for a closed loop nonlinear system is described. Detection and identification tasks are performed by estimating parameters directly related to potential failures. An extended Kalman filter is used to estimate the fault-related parameters, while a decision algorithm based on threshold logic processes the parameter estimates to detect possible failures. For a realistic evaluation of its performance, the detection scheme has been implemented on an inverted pendulum controlled by real-time control software. The failure detection and identification scheme is tested by applying different types of failures on the sensors of the inverted pendulum. Experimental results are presented to validate the effectiveness of the approach",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=911389,no,undetermined,0
An empirical study using task assignment patterns to improve the accuracy of software effort estimation,"In most software development organizations, there is seldom a one-to-one mapping between software developers and development tasks. It is frequently necessary to concurrently assign individuals to multiple tasks and to assign more than one individual to work cooperatively on a single task. A principal goal in making such assignments should be to minimize the effort required to complete each task. But what impact does the manner in which developers are assigned to tasks have on the effort requirements? This paper identifies four task assignment factors: team size, concurrency, intensity, and fragmentation. These four factors are shown to improve the predictive ability of the well-known intermediate COCOMO cost estimation model. A parsimonious effort estimation model is also derived that utilizes a subset of the task assignment factors and unadjusted function points. For the data examined, this parsimonious model is shown to have goodness of fit and quality of estimation superior to that of the COCOMO model, while utilizing fewer cost factors",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=910861,no,undetermined,0
Empirical studies of a prediction model for regression test selection,"Regression testing is an important activity that can account for a large proportion of the cost of software maintenance. One approach to reducing the cost of regression testing is to employ a selective regression testing technique that: chooses a subset of a test suite that was used to test the software before the modifications; then uses this subset to test the modified software. Selective regression testing techniques reduce the cost of regression testing if the cost of selecting the subset from the test suite together with the cost of running the selected subset of test cases is less than the cost of rerunning the entire test suite. Rosenblum and Weyuker (1997) proposed coverage-based predictors for use in predicting the effectiveness of regression test selection strategies. Using the regression testing cost model of Leung and White (1989; 1990), Rosenblum and Weyuker demonstrated the applicability of these predictors by performing a case study involving 31 versions of the KornShell. To further investigate the applicability of the Rosenblum-Weyuker (RW) predictor, additional empirical studies have been performed. The RW predictor was applied to a number of subjects, using two different selective regression testing tools, Deja vu and TestTube. These studies support two conclusions. First, they show that there is some variability in the success with which the predictors work and second, they suggest that these results can be improved by incorporating information about the distribution of modifications. It is shown how the RW prediction model can be improved to provide such an accounting",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=910860,no,undetermined,0
The application of neural networks to fuel processors for fuel-cell vehicles,"Passenger vehicles fueled by hydrocarbons or alcohols and powered by proton exchange membrane (PEM) fuel cells address world air quality and fuel supply concerns while avoiding hydrogen infrastructure and on-board storage problems. Reduction of the carbon monoxide concentration in the on-board fuel processor's hydrogen-rich gas by the preferential oxidizer (PrOx) under dynamic conditions is crucial to avoid poisoning of the PEM fuel cell's anode catalyst and thus malfunction of the fuel-cell vehicle. A dynamic control scheme is proposed for a single-stage tubular cooled PrOx that performs better than, but retains the reliability and ease of use of, conventional industrial controllers. The proposed hybrid control system contains a cerebellar model articulation controller artificial neural network in parallel with a conventional proportional-integral-derivative (PID) controller. A computer simulation of the preferential oxidation reactor was used to assess the abilities of the proposed controller and compare its performance to the performance of conventional controllers. Realistic input patterns were generated for the PrOx by using models of vehicle power demand and upstream fuel-processor components to convert the speed sequences in the Federal Urban Driving Schedule to PrOx inlet temperatures, concentrations, and flow rates. The proposed hybrid controller generalizes well to novel driving sequences after being trained on other driving sequences with similar or slower transients. Although it is similar to the PID in terms of software requirements and design effort, the hybrid controller performs significantly better than the PID in terms of hydrogen conversion setpoint regulation and PrOx outlet carbon monoxide reduction",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=917898,no,undetermined,0
A dynamic group management framework for large-scale distributed event monitoring,"Distributed event monitoring is an important service for fault, performance and security management. Next generation event monitoring services are highly distributed and involve a large number of monitoring agents. In order to support scalable event monitoring, the monitoring agents use IP multicasting for disseminating events and control information. However, due to the dynamic nature of event detection and correlation in distributed monitoring, devising an efficient group management for agents organization and coordination becomes a challenging issue. This paper presents an adaptive group management framework that dynamically re-configures the group structures and membership assignments at run-time according to the event correlation requirements and allows for optimal delivery of multicast messages between the management entities. This framework provides techniques for solving agents' state synchronization, collision-free group allocation and agents bootstrap problems in distributed event monitoring. The presented framework has been implemented within a HiFi system which is a distributed hierarchical monitoring system",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=918053,no,undetermined,0
Semi-active replication of SNMP objects in agent groups applied for fault management,"It is often useful to examine management information base (MIB) objects of a faulty agent in order to determine why it is faulty. This paper presents a new framework for semi-active replication of SNMP management objects in local area networks. The framework is based on groups of agents that communicate with each other using reliable multicast. A group of agents provides fault-tolerant object functionality. An SNMP service is proposed that allows replicated MIB objects of a faulty agent of a given group to be accessed through fault-free agents of that group. The presented framework allows the dynamic definition of agent groups, and management objects to be replicated in each group. A practical fault-tolerant tool for local area network fault management was implemented and is presented. The system employs SNMP agents that interact with a group communication tool. As an example, we show how the examination of TCP-related objects of faulty agents have been used in the fault diagnosis process. The impact of replication on network performance is evaluated",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=918066,no,undetermined,0
Self-aware services: using Bayesian networks for detecting anomalies in Internet-based services,"We propose a general architecture and implementation for the autonomous assessment of the health of arbitrary service elements, as a necessary prerequisite to self-control. We describe a health engine, the central component of our proposed `self-awareness and control' architecture. The health engine combines domain independent statistical analysis and probabilistic reasoning technology (Bayesian networks) with domain dependent measurement collection and evaluation methods. The resultant probabilistic assessment enables open, non-hierarchical communications about service element health. We demonstrate the validity of our approach using HP's corporate email service and detecting email anomalies: mail loops and a virus attack",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=918070,no,undetermined,0
On the use of mobile code technology for monitoring Grid system,"Grid systems are becoming the underlying infrastructure for many high-performance distributed scientific applications. Resources in these systems have heterogeneous features, are connected by potentially unreliable networks, and are often under different administrative management domains. These remarks show how a new generation of techniques, mechanisms and tools need to be considered that can cope with the complexity of such systems and provide the performance suitable for scientific computing applications. In this paper an architecture for monitoring services in Grid environments based on mobile agent technology is presented. One of the most interesting aspects of the proposed approach is the possibility of customizing the parameters to be monitored. After describing the general framework where agents will execute, an application for improving the capabilities of traditional network monitoring paradigms is presented",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=923225,no,undetermined,0
An experiment measuring the effects of personal software process (PSP) training,"The personal software process is a process improvement methodology aimed at individual software engineers. It claims to improve software quality (in particular defect content), effort estimation capability, and process adaptation and improvement capabilities. We have tested some of these claims in an experiment comparing the performance of participants who had just previously received a PSP course to a different group of participants who had received other technical training instead. Each participant of both groups performed the same task. We found the following positive effects: the PSP group estimated their productivity (though not their effort) more accurately, made fewer trivial mistakes, and their programs performed more careful error-checking; further, the performance variability was smaller in the PSP group in various respects. However, the improvements are smaller than the PSP proponents usually assume, possibly due to the low actual usage of PSP techniques in the PSP group. We conjecture that PSP training alone does not automatically realize the PSP's potential benefits (as seen in some industrial PSP success stories) when programmers are left alone with motivating themselves to actually use the PSP techniques",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=922716,no,undetermined,0
An internally replicated quasi-experimental comparison of checklist and perspective based reading of code documents,"The basic premise of software inspections is that they detect and remove defects before they propagate to subsequent development phases where their detection and correction cost escalates. To exploit their full potential, software inspections must call for a close and strict examination of the inspected artifact. For this, reading techniques for defect detection may be helpful since these techniques tell inspection participants what to look for and, more importantly, how to scrutinize a software artifact in a systematic manner. Recent research efforts investigated the benefits of scenario-based reading techniques. A major finding has been that these techniques help inspection teams find more defects than existing state-of-the-practice approaches, such as, ad-hoc or checklist-based reading (CBR). We experimentally compare one scenario-based reading technique, namely, perspective-based reading (PBR), for defect detection in code documents with the more traditional CBR approach. The comparison was performed in a series of three studies, as a quasi experiment and two internal replications, with a total of 60 professional software developers at Bosch Telecom GmbH. Meta-analytic techniques were applied to analyze the data",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=922713,no,undetermined,0
Fault-adaptive control: a CBS application,"Complex control applications require a capability for accommodating faults in the controlled plant. Fault accommodation involves the detection and isolation of faults, and taking an appropriate control action that mitigates the effect of the faults and maintains control. This requires the integration of fault diagnostics with control, in a feedback loop. The paper discusses how a generic framework for building fault-adaptive control systems can be created using a model based approach. Instances of the framework are examples of complex CBSs (computer based systems) that have novel capabilities.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=922423,no,undetermined,0
Adaptive postfiltering of transform coefficients for the reduction of blocking artifacts,"This paper proposes a novel postprocessing technique for reducing blocking artifacts in low-bit-rate transform-coded images. The proposed approach works in the transform domain to alleviate the accuracy loss of transform coefficients, which is introduced by the quantization process. The masking effect in the human visual system (HVS) is considered, and an adaptive weighting mechanism is then integrated into the postfiltering. In low-activity areas, since blocking artifacts appear to be perceptually more detectable, a large window is used to efficiently smooth out the artifacts. In order to preserve image details, a small mask, as well as a large central weight, is employed for processing those high-activity blocks, where blocking artifacts are less noticeable due to the masking ability of local background. The quantization constraint is finally applied to the postfiltered coefficients. Experimental results show that the proposed technique provides satisfactory performance as compared to other postfilters in both objective and subjective image quality",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=920189,no,undetermined,0
Improving validation activities in a global software development,"Global software development challenges traditional techniques of software engineering, such as peer reviews or teamwork. Effective teamwork and coaching of engineers contribute highly towards successful projects. In this case study, we evaluate experiences with validation activities in a global setting within Alcatel's switching and routing business. We investigate three hypotheses related to the effects of (a) collocated inspections, (b) intensive coaching and (c) feature-oriented development teams on globally distributed projects. As all these activities mean an initial investment compared to a standard process with scattered activities, the major validation criteria for the three hypotheses are: (1) cost reduction due to earlier defect detection and (2) less defects introduced. The data is taken from a sample of over 60 international projects of various sizes, from which we have collected all types of product and process metrics over the past four years.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=919129,no,undetermined,0
Theory of software reliability based on components,"We present a foundational theory of software system reliability based on components. The theory describes how component developers can design and test their components to produce measurements that are later used by system designers to calculate composite system reliability, without implementation and test of the system being designed. The theory describes how to make component measurements that are independent of operational profiles, and how to incorporate the overall system-level operational profile into the system reliability calculations. In principle, the theory resolves the central problem of assessing a component, which is: a component developer cannot know how the component will be used and so cannot certify it for an arbitrary use; but if the component buyer must certify each component before using it, component based development loses much of its appeal. This dilemma is resolved if the component developer does the certification and provides the results in such a way that the component buyer can factor in the usage information later without repeating the certification. Our theory addresses the basic technical problems inherent in certifying components to be released for later use in an arbitrary system. Most component research has been directed at functional specification of software components; our theory addresses the other equally important side of the coin: component quality.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=919109,no,undetermined,0
Incorporating varying test costs and fault severities into test case prioritization,"Test case prioritization techniques schedule test cases for regression testing in an order that increases their ability to meet some performance goal. One performance goal, rate of fault detection, measures how quickly faults are detected within the testing process. In previous work (S. Elbaum et al., 2000; G. Rothermel et al., 1999), we provided a metric, APFD, for measuring rate of fault detection, and techniques for prioritizing test cases to improve APFD, and reported the results of experiments using those techniques. This metric and these techniques, however, applied only in cases in which test costs and fault severity are uniform. We present a new metric for assessing the rate of fault detection of prioritized test cases that incorporates varying test case and fault costs. We present the results of a case study illustrating the application of the metric. This study raises several practical questions that might arise in applying test case prioritization; we discuss how practitioners could go about answering these questions.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=919106,no,undetermined,0
Quantifying the costs and benefits of architectural decisions,"The benefits of a software system are assessable only relative to the business goals the system has been developed to serve. In turn, these benefits result from interactions between the system's functionality and its quality attributes (such as performance, reliability and security). Its quality attributes are, in most cases, dictated by its architectural design decisions. Therefore, we argue that the software architecture is the crucial artifact to study in making design tradeoffs and in performing cost-benefit analyses. A substantial part of such an analysis is in determining the level of uncertainty with which we estimate both costs and benefits. We offer an architecture-centric approach to the economic modeling of software design decision making called CBAM (Cost Benefit Analysis Method), in which costs and benefits are traded off with system quality attributes. We present the CBAM, the early results from applying this method in a large-scale case study, and discuss the application of more sophisticated economic models to software decision making.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=919103,no,undetermined,0
Investigating the cost-effectiveness of reinspections in software development,"Software inspection is one of the most effective methods to detect defects. Reinspection repeats the inspection process for software products that are suspected to contain a significant number of undetected defects after an initial inspection. As a reinspection is often believed to be less efficient than an inspection an important question is whether a reinspection justifies its cost. In this paper we propose a cost-benefit model for inspection and reinspection. We discuss the impact of cost and benefit parameters on the net gain of a reinspection with empirical data from an experiment in which 31 student teams inspected and reinspected a requirements document. Main findings of the experiment are: a) For reinspection benefits and net gain were significantly lower than for the initial inspection. Yet, the reinspection yielded a positive net gain for most teams with conservative cost-benefit assumptions. B) Both the estimated benefits and number of major defects are key factors for reinspection net gain, which emphasizes the need for appropriate estimation techniques.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=919090,no,undetermined,0
Design and implementation of a composable reflective middleware framework,"With the evolution of the global information infrastructure, service providers will need to provide effective and adaptive resource management mechanisms that can serve more concurrent clients and deal with applications that exhibit quality-of-service (QoS) requirements. Flexible, scalable and customizable middleware can be used as an enabling technology for next-generation systems that adhere to the QoS requirements of applications that execute in highly dynamic distributed environments. To enable application-aware resource management, we are developing a customizable and composable middleware framework called CompOSE|Q (Composable Open Software Environment with QoS), based on a reflective meta-model. In this paper, we describe the architecture and runtime environment for CompOSE|Q and briefly assess the performance overhead of the additional flexibility. We also illustrate how flexible communication mechanisms can be supported efficiently in the CompOSE|Q framework",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=918995,no,undetermined,0
Enforcing perfect failure detection,Perfect failure detectors can correctly decide whether a computer is crashed. However it is impossible to implement a perfect failure detector in purely asynchronous systems. We show how to enforce perfect failure detection in timed distributed systems with hardware watchdogs. The two main system model assumptions are: each computer can measure time intervals with a known maximum error; and each computer has a watchdog that crashes the computer unless the watchdog is periodically updated. We have implemented a system that satisfies both assumptions using a combination of off-the-shelf software and hardware,2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=918965,no,undetermined,0
Dynamic load sharing with unknown memory demands in clusters,"A compute farm is a pool of clustered workstations to provide high performance computing services for CPU-intensive, memory-intensive, and I/O active jobs in a batch mode. Existing load sharing schemes with memory considerations assume jobs' memory demand sizes are known in advance or predictable based on users' hints. This assumption can greatly simplify the designs and implementations of load sharing schemes, but is not desirable in practice. In order to address this concern, we present three new results and contributions in this study. Conducting Linux kernel instrumentation, we have collected different types of workload execution traces to quantitatively characterize job interactions, and modeled page fault behavior as a function of the overloaded memory sizes and the amount of jobs' I/O activities. Based on experimental results and collected dynamic system information, we have built a simulation model which accurately emulates the memory system operations and job migrations with virtual memory considerations. We have proposed a memory-centric load sharing scheme and its variations to effectively process dynamic memory, allocation demands, aiming at minimizing execution time of each individual job by dynamically migrating and remotely submitting jobs to eliminate or reduce page faults and to reduce the queuing time for CPU services. Conducting trace-driven simulations, we have examined these load sharing policies to show their effectiveness",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=918939,no,undetermined,0
Client-transparent fault-tolerant Web service,"Most of the existing fault tolerance schemes for Web servers detect server failure and route future client requests to backup servers. These techniques typically do not provide transparent handling of requests whose processing was in progress when the failure occurred. Thus, the system may fail to provide the user with confirmation for a requested transaction or clear indication that the transaction was not performed. We describe a client-transparent fault tolerance scheme for Web servers that ensures correct handling of requests in progress at the time of server failure. The scheme is based on a standby backup server and simple proxies. The error handling mechanisms of TCP are used to multicast requests to the primary and backup as well as to reliably deliver replies from a server that may fail while sending the reply. Our scheme does not involve OS kernel changes or use of user-level TCP implementations and requires minimal changes to the Web server software",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=918654,no,undetermined,0
An arrhythmia detector and heart rate estimator for overnight polysomnography studies,"We present an algorithm for automatic on-line analysis of the ECG channel acquired during overnight polysomnography (PSG) studies. The system is independent of ECG morphology, requires no manual initialization, and operates automatically throughout the night. It highlights likely occurrences of arrhythmias and intervals of bad signal quality while outputting a continual estimate of heart rate. Algorithm performance is validated against standard ECG databases and PSG data. Results demonstrate a minimal false negative rate and a low false positive rate for arrhythmia detection, and robustness over a wide range of noise contamination.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=918590,no,undetermined,0
Diagnosis and prognosis of bearings using data mining and numerical visualization techniques,"Traditionally, condition-based monitoring techniques have been used to diagnose failure in rotary machinery by application of low-level signal processing and trend analysis techniques. Such techniques consider small windows of data from large data sets to give preliminary information of developing fault(s) or failure precursor(s). However, these techniques only provide information of a minute portion of a large data set, which limits the accuracy of predicting the remaining useful life of the system. Diagnosis and prognosis (DAP) techniques should be able to identify the origin of the fault(s), estimate the rate of its progression and determine the remaining useful life of the system. This research demonstrates the use of data mining and numerical visualization techniques for diagnosis and prognosis of bearing vibration data. By using these techniques a comprehensive understanding of large vibration data sets can be attained. This approach uses intelligent agents to isolate particular bearing vibration characteristics using statistical analysis and signal processing for data compression. The results of the compressed data can be visualized in 3-D plots and used to track the origination and evolution of failure in the bearing vibration data. The Bearing Test Bed is used for applying measurable static and dynamic stresses on the bearing and collecting vibration signatures from the stressed bearings",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=918553,no,undetermined,0
State estimator condition number analysis,This paper develops formulas for the condition number of the state estimation problem as a function of the different types and number of measurements. We present empirical results using the IEEE RTS-96 and IEEE 118 bus systems that validate the formulas,2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=918298,no,undetermined,0
Design of multi-invariant data structures for robust shared accesses in multiprocessor systems,"Multiprocessor systems are widely used in many application programs to enhance system reliability and performance. However, reliability does not come naturally with multiple processors. We develop a multi-invariant data structure approach to ensure efficient and robust access to shared data structures in multiprocessor systems. Essentially, the data structure is designed to satisfy two invariants, a strong invariant, and a weak invariant. The system operates at its peak performance when the strong invariant is true. The system will operate correctly even when only the weak invariant is true, though perhaps at a lower performance level. The design ensures that the weak invariant will always be true in spite of fail-stop processor failures during the execution. By allowing the system to converge to a state satisfying only the weak invariant, the overhead for incorporating fault tolerance can be reduced. We present the basic idea of multi-invariant data structures. We also develop design rules that systematically convert fault-intolerant data abstractions into corresponding fault-tolerant versions. In this transformation, we augment the data structure and access algorithms to ensure that the system always converges to the weak invariant, even in the presence of fail-stop processor failures. We also design methods for the detection of integrity violations and for restoring the strong invariant. Two data structures, namely binary search tree and double-linked list, are used to illustrate the concept of multi-invariant data structures",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=910857,no,undetermined,0
Framework of end-to-end performance measurement and analysis system for Internet applications,"The Internet provides only the best-effort service for users. There is no guaranteed QoS service for end users. In general, an end-to-end path of the Internet is changed by the bandwidth, delay and packet loss by time. Therefore, the measurement of the end-to-end performance is a difficult task for users who need to establish the guaranteed path for critical applications. This paper proposes a framework for measurement, analysis and application program interface to estimate the end-to-end performance for critical applications. The proposed framework is based on the combination of the historical data for previous measurements and the real-time data for current measurement. The framework is effective for applications which need to know the end-to-end performance. The preliminary measurement in this paper has been done to verify the method",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=905539,no,undetermined,0
Perception-based fast rendering and antialiasing of walkthrough sequences,"We consider accelerated rendering of high quality walkthrough animation sequences along predefined paths. To improve rendering performance, we use a combination of a hybrid ray tracing and image-based rendering (IBR) technique and a novel perception-based antialiasing technique. In our rendering solution, we derive as many pixels as possible using inexpensive IBR techniques without affecting the animation quality. A perception-based spatiotemporal animation quality metric (AQM) is used to automatically guide such a hybrid rendering. The image flow (IF) obtained as a byproduct of the IBR computation is an integral part of the AQM. The final animation quality is enhanced by an efficient spatiotemporal antialiasing which utilizes the IF to perform a motion-compensated filtering. The filter parameters have been tuned using the AQM predictions of animation quality as perceived by the human observer. These parameters adapt locally to the visual pattern velocity",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=895880,no,undetermined,0
Predicting class libraries interface evolution: an investigation into machine learning approaches,"Managing the evolution of an OO system constitutes a complex and resource-consuming task. This is particularly true for reusable class libraries since the user interface must be preserved for version compatibility. Thus, the symptomatic detection of potential instabilities during the design phase of such libraries may help avoid later problems. This paper introduces a fuzzy logic-based approach for evaluating the stability of a reusable class library interface, using structural metrics as stability indicators. To evaluate this new approach, we conducted a preliminary study on a set of commercial C++ class libraries. The obtained results are very promising when compared to those of two classical machine learning approaches, top down induction of decision trees and Bayesian classifiers",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=896734,no,undetermined,0
Australian Snowy Mountains Hydro Scheme earthing system safety assessment,"The task of determining the condition of the earthing in the Upper Tumut generation system, was undertaken as part of the Snowy Mountains Hydro Electric Authority's (SMHEA) safety risk assessment and asset condition monitoring programme. The testing programme to ascertain performance under earth fault and lightning conditions had to overcome considerable physical difficulties as well as the restrictions of 'close' proximity injection loops. The application of software, test instrumentation and testing procedures developed within Australia in collaboration between Energy Australia, Newcastle University, and SMHEA, to obtain real solutions are described in this paper. Also discussed are condition assessment processes that complement the current injection testing programme. This paper also provides a summary of the minimum requirements of an earthing system injection test to satisfactorily assess the condition of complex electrical power system installations",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=898167,no,undetermined,0
Automatic measurements of the radius of curvature of the cornea in Slit Lamp,"We have developed an automatic optical system attached to the Slit Lamp in order to provide automatic measurements of the radius of curvature of the cornea at low cost. The system consists of projecting a light ring as a target at the patient's cornea and the further analysis of the deformation of the target in order to obtain the radius of curvature as well as the axis of the associated astigmatism. The reflected image of the target is displayed in a PC's monitor and a dedicated developed software performs the analysis of the image, that provides the corneal keratometry. Also, the system is able to measure irregular astigmatism and the clinician is able to understand the behavior of the cornea's curvature radius in these regions by comparison of the graphical analysis of regular astigmatism that is generated by the software. There is no system commercially available which is able to measure irregular astigmatism, which is a very important feature for keratoconeous detection. The system is easy to use and it has a friendly software interface for the user. Measurements in volunteer patients have been made and the results that were obtained in the system, for the standard target ring, are in good agreement with commercial automatic and manual systems, presenting a correlation coefficient of 0,99347 and 0,97637, respectively regarding the radius of curvature and 0,96841 and 0,9568, respectively, regarding the axis. The system's precision is 0,005 mm for the curvature radius and 2* for the axis component",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=897853,no,undetermined,0
Identification of the harmonic currents drawn by an institutional building: application of a stochastic approach,"Considering harmonics as a time-dependent stochastic process, this paper proposes prediction tools to identify both the stochastic and the time varying behaviour of harmonics. The randomly operating conditions of several nonlinear loads justify the use of a stochastic approach based on a continuous-time model. This approach is applied to characterise the amplitude of the harmonic currents on the secondary of a distribution transformer feeding an institutional building",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=897746,no,undetermined,0
Effort measurement in student software engineering projects,"Teaching software engineering by means of student involvement in the team development of a product is the most effective way to teach the main issues of software engineering. Some of its difficulties are those of coordinating their work, measuring the time spent by the students (both in individual work and in meetings) and making sure that meeting time will not be excessive. Starting in the academic year 1998/1999, we assessed, improved and documented the development process for the student projects and found that measurement is one of the outstanding issues to be considered. Each week, the students report the time spent on the different project activities. We present and analyze the measurement results for our 16 student teams (each one with around 6 students). It is interesting to note that the time spent in meetings is usually too long, ranging from 46% in the requirements analysis phase to 21% in coding, mainly due to problems of coordination. Results from previous years are analyzed and presented to the following year's students for feedback. In the present year (2000), we have decreased the amount of time spent by the student doing group work, and improved the effectiveness and coordination of the teams",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=897672,no,undetermined,0
Winner take all experts network for sensor validation,"The validation of sensor measurements has become an integral part of the operation and control of modern industrial equipment. The sensor under a harsh environment must be shown to consistently provide the correct measurements. Analysis of the validation hardware or software should trigger an alarm when the sensor signals deviate appreciably from the correct values. Neural network based models can be used to estimate critical sensor values when neighboring sensor measurements are used as inputs. The discrepancy between the measured and predicted sensor values may then be used as an indicator for sensor health. The proposed winner take all experts (WTAE) network is based on a `divide and conquer' strategy. It employs a growing fuzzy clustering algorithm to divide a complicated problem into a series of simpler sub-problems and assigns an expert to each of them locally. After the sensor approximation, the outputs from the estimator and the real sensor value are compared both in the time domain and the frequency domain. Three fault indicators are used to provide analytical redundancy to detect the sensor failure. In the decision stage, the intersection of three fuzzy sets accomplishes a decision level fusion, which indicates the confidence level of the sensor health. Two data sets, the Spectra Quest Machinery Fault Simulator data set and the Westland vibration data set, were used in simulations to demonstrate the performance of the proposed WTAE network. The simulation results show the proposed WTAE is competitive with or even superior to the existing approaches",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=897405,no,undetermined,0
Implementation and evaluation for dependable bus control using CPLD,"Bus systems are used in computers as essential architecture, and dependability of bus systems should be accomplished reasonably for various applications. In this paper, we will present dependable bus operations with actual implementation and evaluation by CPLD. Most of the bus systems control transition of some classified phases with synchronous clock or guard time to avoid incorrect phase transition. However, these phase control methods may degrade system performance or cause incorrect operations. We design an asynchronous sequential circuit for bus phase control without clock or guard time. This circuit prevents incorrect phase transition at the time when large input delay or erroneous input occurs. We estimate probability of incorrect phase transition with single stuck-at fault on input signals. From the result of estimation, we also design checking system verifying outputs of initiator and target devices. Incorrect phase transition with single stuck-at fault occurred between both sequential circuits is inhibited completely by implementation of the system",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=897279,no,undetermined,0
Strategies for optimizing image processing by genetic and evolutionary computation,"We examine the results of major previous attempts to apply genetic and evolutionary computation (GEC) to image processing. In many problems, the accuracy (quality) of solutions obtained by GEC-based methods is better than that obtained by other methods such as neural networks and simulated annealing. However the computation time required is satisfactory in some problems, whereas it is unsatisfactory in other problems. We consider the current problems of GEC-based methods and present the following measures to achieve still better performance: (1) utilizing competent GEC, (2) incorporating other search algorithms such as local hill climbing algorithms, (3) hybridizing with conventional image processing algorithms; (4) modeling the given problem with as smaller parameters as possible, and (5) using parallel processors to evaluate the fitness function",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=897228,no,undetermined,0
Advanced assessment of the power quality events,"This paper introduces a new concept of advanced power quality assessment. The concept is implemented using the software for event detection, classification and characterization. The role of the modeling and simulation in the power quality assessment is also discussed. The use of the field recorded data and simulated data in the equipment performance analysis is outlined. All the mentioned approaches are suggested for power quality assessment use when setting up and verifying power quality contracts",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=896837,no,undetermined,0
Analysis of the impact of reading technique and inspector capability on individual inspection performance,"Inspection of software documents is an effective quality assurance measure to detect defects in the early stages of software development. It can provide timely feedback on product quality to both developers and managers. This paper reports on a controlled experiment that investigated the influence of reading techniques and inspector capability on individual effectiveness to find given sets of defects in a requirements specification document. Experimental results support the hypothesis that reading techniques can direct inspectors' attention towards inspection targets, i.e. on specific document parts or severity levels, which enables inspection planners to divide the inspection work among several inspectors. Further, they suggest a tradeoff between specific and general detection effectiveness regarding document coverage and inspection effort. Inspector capability plays a significant role in inspection performance, while the size of the effect varies with the reading technique employed and the inspected document part",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=896692,no,undetermined,0
A weighted distribution of residual failure data using Weibull model,"In analyzing the distribution of residual failure data using the Weibull distribution from the first detection of a failure to the completion of repairing failures, we have proposed the method of applying a weight to each failure. To put it concretely, we have suggested the method of applying a weight according to the importance of each failure within the analytic area in order to manage and measure efficiently the failure data found in the course of confirmation, verification, debugging, adding, and repairing functions collectively. The procedure is carried out after constituting a model switching system and installing a synthesis software package on the HANbit ACE ATM switching system under the development phase, which is the main node in building the B-ISDN for multimedia services",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=905438,no,undetermined,0
Evaluation of inspectors' defect estimation accuracy for a requirements document after individual inspection,"Project managers need timely feedback on the quality of development products to monitor and control project progress. Inspection is an effective method to identify defects and to measure product quality. Objective and subjective models can be used to estimate the total number of defects in a product based on defect data from inspection. This paper reports on a controlled experiment to evaluate the accuracy of individual subjective estimates of developers, who had just before inspected the document, on the number of defects in a software requirements specification. In the experiment most inspectors underestimated the total number of defects in the document. The number of defects reported and the number of (major) reference defects found were identified as factors that separated groups of inspectors who over- or underestimated on average",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=896688,no,undetermined,0
Bloodshot eyes: workload issues in computer science project courses,"Workload issues in computer science project courses are addressed. We briefly discuss why high workloads occur in project courses and the reasons they are a problem. We then describe some course changes we made to reduce the workload in a software engineering project course, without compromising course quality. The techniques include: adopting an iterative and incremental process, reducing the requirements for writing documents, and gathering accurate data on time spent on various activities. We conclude by assessing the techniques, providing good evidence for a dramatic change in the workload, and an increase in student satisfaction levels. We provide some evidence, and an argument, that learning has not been affected by the changes",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=896682,no,undetermined,0
Problem solving skills,"This paper describes a three-year experiment to determine whether student performance can be improved by making their performance visible in quantitative ways and assisting them to reflect on deviations between desired and actual performance. The experiment focused on improving effort estimation for planning, improving effort allocation over the period of performance, and reducing the recurrence of previously reported defects in subsequent assignments. To accomplish this, a Web-based tool was implemented and used to capture numerous student performance parameters as well as planning rationales, postmortem reflections, and feedback to the students",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=896609,no,undetermined,0
Eliminating annotations by automatic flow analysis of real-time programs,"There is an increasing demand for methods that calculate the worst case execution time (WCET) of real time programs. The calculations are typically based on path information for the program, such as the maximum number of iterations in loops and identification of infeasible paths. Most often, this information is given as manual annotations by the programmer. Our method calculates path information automatically for real time programs, thereby relieving the programmer from tedious and error-prone work. The method, based on abstract interpretation, generates a safe approximation of the path information. A trade-off between quality and calculation cost is made, since finding the exact information is a complex, often intractable problem for nontrivial programs. We describe the method by a simple, worked example. We show that our prototype tool is capable of analyzing a number of program examples from the WCET literature, without using any extra information or consideration of special cases needed in other approaches",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=896435,no,undetermined,0
Practical applications of statistical process control [in software development projects],Applying quantitative methods such as statistical process control (SPC) to software development projects can provide a positive cost-benefit return. The authors used SPC on inspection and test data to assess product quality during testing and to predict post-shipment product quality for a major software release,2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=896249,no,undetermined,0
Implementation and performance evaluation of a real-time e-brokerage system,"Timeliness is an important attribute for e-brokerage both in the detection of opportunities in a narrow time window and also in facilitating differentiation of end-to-end quality of service (QoS). In this paper, we demonstrate how real-time event monitoring techniques can be applied to time-critical e-brokerage. We start with a formal timed event model which provides the semantics for specifying complex timing correlation rules in composite events. The formal event model of the system is based on RTL (Real Time Logic), which is important for disambiguating informal specifications when multiple instances of the same event type may appear in a timing correlation rule involving composite events. This leads to our design of an e-brokerage, online stock monitoring and alerting system which enables users to express their complex preferences and provides an alert service to users in a timely manner. The user's preferences are monitored by a real-time event monitor. We report the design of this system and some performance data characterizing some key timing parameters. Our system is being used by a class of MBA students in a field test",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=896001,no,undetermined,0
Constructing real-time group communication middleware using the Resource Kernel,"Group communication is a widely studied paradigm which is often used in building real-time and fault-tolerant distributed systems. RTCAST is a real-time group communication protocol which has been designed to work with commercial, non-real-time, off-the-shelf hardware and operating systems, such as Solaris, Linux and Windows NT. RTCAST makes probabilistic real-time guarantees based on assumptions about the performance of the underlying system. Unfortunately, the high variability of the access to system resources that these operating systems provide may limit the predictability of the real-time guarantees provided by RTCAST. By taking advantage of a service that provides resource scheduling and reservation in these operating systems, both the hardness and timing granularity of RTCAST's real-time services can be greatly improved. This paper describes an implementation of RTCAST which makes use of the Resource Kernel to provide highly predictable, real-time communication guarantees",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=895990,no,undetermined,0
Computer aided design of fault-tolerant application specific programmable processors,"Application Specific Programmable Processors (ASPP) provide efficient implementation for any of m specified functionalities. Due to their flexibility and convenient performance-cost trade-offs, ASPPs are being developed by DSP, video, multimedia, and embedded lC manufacturers. In this paper, we present two low-cost approaches to graceful degradation-based permanent fault tolerance of ASPPs. ASPP fault tolerance constraints are incorporated during scheduling, allocation, and assignment phases of behavioral synthesis: Graceful degradation is supported by implementing multiple schedules of the ASPP applications, each with a different throughput constraint. In this paper, we do not consider concurrent error detection. The first ASPP fault tolerance technique minimizes the hardware resources while guaranteeing that the ASPP remains operational in the presence of all k-unit faults. On the other hand, the second fault tolerance technique maximizes the ASPP fault tolerance subject to constraints on the hardware resources. These ASPP fault tolerance techniques impose several unique tasks, such as fault-tolerant scheduling, hardware allocation, and application-to-faulty-unit assignment. We address each of them and demonstrate the effectiveness of the overall approach, the synthesis algorithms, and software implementations on a number of industrial-strength designs.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=895942,no,undetermined,0
EMS-vision: recognition of intersections on unmarked road networks,"The ability to recognize intersections enables an autonomous vehicle to navigate on road networks for performing complex missions. The paper gives the geometry model for intersections applied and their interaction with active viewing direction control. Quality measures indicate to performance monitoring processes the reliability of the estimation results. The perception module is integrated in the EMS-Vision system. Results from autonomous turn-off maneuvers, conducted on unmarked campus roads are discussed",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=898359,no,undetermined,0
"Curve evolution, boundary-value stochastic processes, the Mumford-Shah problem, and missing data applications","We present an estimation-theoretic approach to curve evolution for the Mumford-Shah problem. By viewing an active contour as the set of discontinuities in the Mumford-Shah problem, we may use the corresponding functional to determine gradient descent evolution equations to deform the active contour. In each gradient descent step, we solve a corresponding optimal estimation problem, connecting the Mumford-Shah functional and curve evolution with the theory of boundary-value stochastic processes. In employing the Mumford-Shah functional, our active contour model inherits its attractive ability to generate, in a coupled manner, both a smooth reconstruction and a segmentation of the image. Next, by generalizing the data fidelity term of the original Mumford-Shah functional to incorporate a spatially varying penalty, we extend our method to problems in which data quality varies across the image and to images in which sets of pixel measurements are missing. This more general model leads us to a novel PDE-based approach for simultaneous image magnification, segmentation, and smoothing, thereby extending the traditional applications of the Mumford-Shah functional which only considers simultaneous segmentation and smoothing",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=899521,no,undetermined,0
Well-defined intended uses: an explicit requirement for accreditation of modeling and simulation applications,A modeling and simulation (M&S) application is built for a specific purpose and its acceptability assessment is carried out with respect to that purpose. The accreditation decision for an M&S application is also made with respect to that purpose. The purpose is commonly expressed in terms of intended uses. The quality of expressing the intended uses significantly affects the quality of the acceptability assessment as well as the quality of making the accreditation decision. The purpose of this paper is to provide guidance in proper definition of the intended uses. It uses an M&S application for simulating the U.S. National Missile Defense (NMD) system design as an example to illustrate the definition of the intended uses,2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=899883,no,undetermined,0
A new portable electronic device for single exposure half-value layer measurement,A new device to be used for half-value layer (HVL) measurements in quality assurance procedures is presented. This device uses a silicon X-ray sensor coupled to an electronic circuit that is connected to a portable computer (notebook) for data storage and analysis. HVL measurements can be performed for a single X-ray exposure and real-time results are presented on the notebook screen,2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=901336,no,undetermined,0
Probabilistic communication optimizations and parallelization for distributed-memory systems,"In high-performance systems execution time is of crucial importance justifying advanced optimization techniques. Traditionally, optimization is based on static program analysis. The quality of program optimizations, however, can be substantially improved by utilizing runtime information. Probabilistic data-flow frameworks compute the probability with what data-flow facts may hold at some program point based on representative profile runs. Advanced optimizations can use this information in order to produce highly efficient code. In this paper we introduce a novel optimization technique in the context of High Performance Fortran (HPF) that is based on probabilistic data-flow information. We consider statically undefined attributes which play an important role for parallelization and compute for those attributes the probabilities to hold some specific value during runtime. For the most probable attribute values highly-optimized, specialized code is generated. In this way significantly better performance results can be achieved. The implementation of our optimization is done in the context of VFC, a source-to-source parallelizing compiler for HPF/F90",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=905042,no,undetermined,0
Product performance-evaluation using Monte Carlo simulation: a case study,"Product performance evaluation in the room and in the extreme environmental conditions is one of the most important design engineering activities. Is the product designed to operate within the specified tolerances over the specified environment and are the product performance parameters optimized so that the effects of the environmental stresses are minimized? These are the questions that a design engineer has to answer before any new product is released to the market. Therefore, environmental testing and monitoring of the product outputs, intended design performance and other product design parameters need to be performed. Such tests may include significant number of samples, may be time consuming and very costly. Consequently, environmental tests should be designed and planned very carefully and the data analysis methodologies should require the minimum resources for data collection. The proposed methodology is based on the idea of the product performance modeling and monitoring the performance degradation over the range of the environmental stresses. It is very flexible and is applicable in all situations when a product's performance can be modeled with a linear or nonlinear functions, whose parameters may be random, correlated and stress dependent. It provides the tools for performance stress evaluation and reliability estimation. The methodology is presented through the following case study",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=902497,no,undetermined,0
Improvements in reliability-growth modeling,"The Duane model has been used for a number of years for graphically portraying reliability growth. There are, however, a number of inherent limitations which make it unsatisfactory for the regular monitoring of reliability growth progress. These limitations are explored and a new model is presented based on variance stabilizing transformation theory. This model retains the ease of use while avoiding the disadvantages of the Duane model. Computer simulations show that the new model provides a better fit to the data over the range of Duane slopes normally observed during a reliability growth program. Computer simulations also show that the new model provides higher values of instantaneous mean time between failures (MTBF) than the Duane model. Twelve published software reliability datasets are used to illustrate the use and application of the new model. The performance of both the Duane and the new model is compared for these software reliability datasets. The results of the performance comparison are consistent with the computer simulations",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=902483,no,undetermined,0
A methodology for operational profile refinement,"Numerous reliability models and testing practices are based on operational profiles. Typically, a single operational profile is used to represent the usage of a system with the assumption that a homogeneous customer base executes the system. However, if the customer base is heterogeneous, estimates computed on a single operational profile may be inaccurate. A single operational profile does not reflect the diverse customer patterns and it only â€œaveragesâ€?the usage of the system, obscuring the real information about the operations probabilities. Decisions made on these estimates are likely to be biased and of limited usefulness. This paper presents a refinement methodology for the generation of more accurate operational profiles that truly represent the diverse customer usage patterns. Clustering analysis supports the refinement methodology for identifying groups of customers with similar characteristics. Empirical stopping rules and validation procedures complete the refining methodology. A complete example of the methodology is presented on a large application. The example evidences the different perspective and accuracy that can be obtained through this refining methodology",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=902457,no,undetermined,0
System reliability analysis: the advantages of using analytical methods to analyze non-repairable systems,"Most of the system analysis software available on the market today employs the use of simulation methods for estimating the reliability of nonrepairable systems. Even though simulation methods are easy to apply and offer great versatility in modeling and analyzing complex systems, there are some limitations to their effectiveness. For example, if the number of simulations performed is not large enough, these methods can be error prone. In addition, performing a large number of simulations can be extremely time-consuming and simulation offers a small range of calculation results when compared to analytical methods. Analytical methods have been avoided due to their complexity in favor of the simplicity of using simulation. A software tool has been developed that calculates the exact analytical solution for the reliability of a system. Given the reliability equation for the system, further analyses on the system can be performed, such as computing exact values of the reliability, failure rate, at specific points in time, as well as computing the system MTTF (mean time to failure), and reliability importance measures for the components of the system. In addition, optimization and reliability allocation techniques can be utilized to aid engineers in their design improvement efforts. Finally, the time-consuming calculations and the non-repeatability issue of the simulation methodology are eliminated",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=902446,no,undetermined,0
A Short-Circuit Current Study for the Power Supply System of Taiwan Railway,"The western Taiwan railway transportation system consists mainly on a mountain route and ocean route. Taiwan Railway Administration (TRA) has conducted a series of experiments on the ocean route in recent years to identify the possible causes of unknown events that cause the trolley contact wires to melt down frequently. The conducted tests include the short-circuit fault test within the power supply zone of the Ho Long Substation (Zhu Nan to Tong Xiao) that had the highest probability for the melt down events. Those test results, based on the actual measured maximum short-circuit current, provide a valuable reference for TRA when comparing against the said events. The Le Blanc transformer is the main transformer of the Taiwan railway electrification system. The Le Blanc transformer mainly transforms the Taiwan Power Company (TPC) generated three-phase alternating power supply system (69kV, 60Hz) into two single-phase alternating power distribution systems (M phase and T phase) (26kV, 60Hz) needed for the trolley traction. As a unique winding connection transformer, the conventional software for fault analysis will not be able to simulate its internal current and phase difference between each phase current. Therefore, besides extracts of the short-circuit test results, this work presents an EMTP model based on the Taiwan Railway Substation equivalent circuit model with a Le Blanc transformer. The proposed circuit model can simulate the same short-circuit test to verify the actual fault current and accuracy of the equivalent circuit model.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4311565,no,undetermined,0
A Novel Software Implementation Concept for Power Quality Study,"A novel concept for power quality study is proposed. The concept integrates the power system modeling, classifying, and characterizing of power quality events, studying equipment sensitivity to the event disturbance and locating the point of event occurrence into one unified frame. Both Fourier and wavelet analyses are applied for extracting distinct features of various types of events as well as for characterizing the events. A new fuzzy expert system for classifying power quality events based on such features is presented with improved performance over previous neural network-based methods. A novel simulation method is outlined for evaluating the operating characteristics of the equipment during specific events. A software prototype implementing the concept has been developed in MATLAB. The voltage sag event is taken as an example for illustrating the analysis methods and software implementation issues. It is concluded that the proposed approach is feasible and promising for real world applications.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4311180,no,undetermined,0
Simulation and Evaluation of Optimization Problem Solutions in Distributed Energy Management Systems,"Deregulation in electricity markets requires fast and robust optimization tools for a secure and efficient operation of the electric power system. In addition, there is the need for integrating and coordinating operational decisions taken by different utilities acting in the same market. Distributed energy management systems (DEMS) may help to fulfill these requirements. The design of DEMS requires detailed simulation results for the evaluation of its performance. To simulate the operation of DEMS from the optimization standpoint, a general purpose distributed optimization software tool, DistOpt, is used, and its capabilities are extended to handle power system problems. The application to the optimal power flow problem is presented.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4311155,no,undetermined,0
Broadband vector channel sounder for MIMO channel measurement,"3G and 4G mobile communication systems consider multiple antennas at both receiver and transmitter for reaching enhanced channel capacity at well defined quality of service. Perfect design of such systems requires profound knowledge of the radio channel, especially its time-variant characteristics in various radio environments. A precise characterization of the radio channel is given by the vector channel impulse response (VCIR). In multi input multiple output (MIMO) systems the VCIR is measured for each transmitting antenna to a configured RX antenna array. The time variance of the radio channel requires high speed recording of VCIR with respect to the maximum Doppler frequency. The presented vector channel sounder covers all MIMO aspects. Up to 256 MIMO channels can be configured by software. The broadband technique grants high time resolution. Superresolution techniques can be used for joint estimation of time delay, direction of arrival and direction of departure with respect to multipath propagation. The measurement data can be used directly (as stored radio channel) as well as indirectly (as derived channel model) for realistic link- and system-level simulation.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1031883,no,undetermined,0
Neural network detection and identification of actuator faults in a pneumatic process control valve,"This paper establishes a scheme for detection and identification of actuator faults in a pneumatic process control valve using neural networks. First, experimental performance parameters related to the valve step responses, including dead time, rise time, overshoot, and the steady state error are obtained directly from a commercially available software package for a variety of faulty operating conditions. Acquiring training data in this way has eliminated the need for additional instrumentation of the valve. Next, the experimentally determined performance parameters are used to train a multilayer perceptron network to detect and identify incorrect supply pressure, actuator vent blockage and diaphragm leakage faults. The scheme presented here is novel in that it demonstrates that a pattern recognition approach to fault detection and identification, for pneumatic process control valves, using features of the valve step response alone, is possible.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1013191,no,undetermined,0
Fuzzy logic traffic control in broadband communication networks,"Traffic predictions have been demonstrated with the capability to improve network efficiency and QoS in broadband ATM networks. Recent research shows that fuzzy logic prediction outperforms conventional autoregression predictions. The application of fuzzy logic also has a potential to control traffic more effectively. In this paper, we propose the use of the fuzzy logic prediction on connection admission control (CAC) and congestion control on high speed networks. We first modeled traffic characteristics using an on-line fuzzy logic predictor on CAC. Simulation results show that fuzzy logic prediction improves the efficiency of both conventional and measurement-based CAC. In addition, the measurement-based approach incorporating fuzzy logic inference and using fuzzy logic prediction is shown to achieve higher network utilization while maintaining QoS. We then applied the fuzzy logic predictor to congestion control in which the ABR queue is estimated one round-trip in advance. Simulation results show that the fuzzy logic control scheme significantly reduces convergence time and overall buffer requirements as compared with conventional schemes",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1007256,no,undetermined,0
A Gibbs-sampler approach to estimate the number of faults in a system using capture-recapture sampling [software reliability],"A new recapture debugging model is suggested to estimate the number of faults in a system, Î½, and the failure intensity of each fault, Ï†. The Gibbs sampler and the Metropolis algorithm are used in this inference procedure. A numerical illustration suggests a notable improvement on the estimation of Î½ and Ï† compared with that of a removal debugging model",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=922486,no,undetermined,0
Efficient data broadcast scheme on wireless link errors,"As portable wireless computers become popular, mechanisms to transmit data to such users are of significant interest. Data broadcast is effective in dissemination-based applications to transfer the data to a large number of users in the asymmetric environment where the downstream communication capacity is relatively much greater than the upstream communication capacity. Index based organization of data transmitted over wireless channels is very important to reduce power consumption. We consider an efficient (1:m) indexing scheme for data broadcast on unreliable wireless networks. We model the data broadcast mechanism on the error prone wireless networks, using the Markov model. We analyze the average access time to obtain the desired data item and find that the optimal index redundancy (m) is SQRT[Data/{Index*(1-p)<sup>âˆ?/sup>K}], where p is the failure rate of the wireless link, Data is the size of the data in a broadcast cycle, Index is the size of index, and K is the index level. We also measure the performance of data broadcast schemes by parametric analysis",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=920471,no,undetermined,0
Investigations on hydrogen in silicon by means of lifetime measurements,"Various techniques (SIMS, thermal effusion, FTIR) have been suggested for the determination of the diffusion of hydrogen in multicrystalline silicon. However these methods are either laborious or of a minor accuracy. Our work concentrates on the determination of hydrogen passivation depths in mc-Si by determining the minority carrier lifetime as a function of hydrogen passivation time. For the investigations EMC silicon and reference FZ silicon wafers have been used. From experimental data the passivation depth is obtained numerically using the simulation software PC1D and analytically using a simplified equation. For EMC, passivation depths in regions of good and poor quality have been obtained indicating no significant influence on the passivation depth. Further experiments by polishing the wafer prior to lifetime measurements with a small angle have been performed for determination of the SRV",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915828,no,undetermined,0
Effective testing and debugging methods and its supporting system with program deltas,"In the maintenance phase of software development, it is necessary to check that all features still perform correctly after some changes have been applied to existing software. However, it is not easy to debug the software when a defect is found in those features which have not changed during the modification, even using a regression test. Existing approaches employ program deltas to specify defects; they have the limitation of it being hard to enact them, and they don't support any actual debugging activities. Moreover, such a system is hard to introduce into an actual environment. In this paper, we propose DMET (Debugging METhod) to solve such problems. DMET supports debugging activities when a defect is found by regression tests through detection, indication and reflection procedures. We also implement DSUS (Debugging SUpporting System) based on DMET. DSUS executes DMET procedures automatically, and it is easy to configure for an existing environment. Through experimentation with DMET and DSUS, we have confirmed that DMET/DSUS reduce the debugging time of software significantly. As a result, DMET/DSUS help in evolving the software for the software maintenance aspects",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=913249,no,undetermined,0
LACE frameworks and technique-identifying the legacy status of a business information system from the perspectives of its causes and effects,"This paper first presents a definition of the concept `legacy status' with a three-dimensional model. It then discusses LACE frameworks and techniques, which can be used to assess legacy status from the cause and effects perspectives. A method of applying the LACE frameworks is shown and a technique with a mathematical model and metric so that the legacy status of a system can be calculated. This paper describes a novel and practical way to identify legacy status of a system, and has pointed out a new direction for research in this area",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=913235,no,undetermined,0
Mobile agents for personalized information retrieval: when are they a good idea?,Mobile agent technology has been proposed as an alternative to traditional client-server computing for personalized information retrieval by mobile and wireless users from fixed wired servers. We develop a very simplified analytical model that examines the claimed performance benefits of mobile agents over client-server computing for a mobile information retrieval scenario. Our evaluation of this simple model shows that mobile agents are not necessarily better than client-server calls in terms of average response times; they are only beneficial if the space overhead of the mobile agent code is not too large or if the wireless link connecting the mobile user to the fixed servers of the virtual enterprise is error-prone. We quantify the tradeoffs involved for a variety of scenarios and point out issues for further research,2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=904635,no,undetermined,0
Joint evaluation of performance and robustness of a COTS DBMS through fault-injection,"Presents and discusses observed failure modes of a commercial off-the-shelf (COTS) database management system (DBMS) under the presence of transient operational faults induced by SWIFI (software-implemented fault injection). The Transaction Processing Performance Council (TPC) standard TPC-C benchmark and its associated environment is used, together with fault-injection technology, building a framework that discloses both dependability and performance figures. Over 1600 faults were injected in the database server of a client/server computing environment built on the Oracle 8.1.5 database engine and Windows NT running on COTS machines with Intel Pentium processors. A macroscopic view on the impact of faults revealed that: (1) a large majority of the faults caused no observable abnormal impact in the database server (in 96% of hardware faults and 80% of software faults, the database server behaved normally); (2) software faults are more prone to letting the database server hang or to causing abnormal terminations; (3) up to 51% of software faults lead to observable failures in the client processes",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=857547,no,undetermined,0
Loki: a state-driven fault injector for distributed systems,"Distributed applications can fail in subtle ways that depend on the state of multiple parts of a system. This complicates the validation of such systems via fault injection, since it suggests that faults should be injected based on the global state of the system. In Loki, fault injection is performed based on a partial view of the global state of a distributed system, i.e. faults injected in one node of the system can depend on the state of other nodes. Once faults are injected, a post-runtime analysis, using off-line clock synchronization, is used to place events and injections on a single global timeline and to determine whether the intended faults were properly injected. Finally, experiments containing successful fault injections are used to estimate the specified measures. In addition to briefly reviewing the concepts behind Loki and its organization, we detail Loki's user interface. In particular, we describe the graphical user interfaces for specifying state machines and faults, for executing a campaign and for verifying whether the faults were properly injected",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=857544,no,undetermined,0
Fault-tolerant execution of mobile agents,"In this paper, we will address the list of problems that have to be solved in mobile agent systems and we will present a set of fault-tolerance techniques that can increase the robustness of agent-based applications without introducing a high performance overhead. The framework includes a set of schemes for failure detection, checkpointing and restart, software rejuvenation, a resource-aware atomic migration protocol, a reconfigurable itinerary, a protocol that avoids agents to get caught in node failures and a simple scheme to deal with network partitions. At the end, we will present some performance results that show the effectiveness of these fault-tolerance techniques",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=857527,no,undetermined,0
Performance analysis of three multicast resource allocation algorithms,"Multicast applications such as electronic news delivery and live sports Webcasting grow with the Internet. These Web-based services demand high quality of service in the competitive global Internet business. Resources such as bandwidth and buffer space need to be reserved for different sessions to provide such guaranteed services. We analyze the performance of three multicast resource allocation algorithms, namely equal allocation with reclaim on all links (EAR-ALL), EAR-OPT and EAR-LL-a simple algorithm that reclaims only on the last link as opposed to all possible links. Simulation results using packetized voice sources on the MBONE network are reported comparing the performance in terms of bandwidth, call blocking and new receiver joining probability.",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=807213,no,undetermined,0
Predicting deviations in software quality by using relative critical value deviation metrics,"We develop a new metric, relative critical value deviation (RCVD), for classifying and predicting software quality. The RCVD is based on the concept that the extent to which a metric's value deviates from its critical value, normalized by the scale of the metric, indicates the degree to which the item being measured does not conform to a specified norm. For example, the deviation in body temperature above 98.6 Fahrenheit degrees is a surrogate for fever. Similarly, the RCVD is a surrogate for the extent to which the quality of software deviates from acceptable norms (e.g., zero discrepancy reports). Early in development, surrogate metrics are needed to make predictions of quality before quality data are available. The RCVD can be computed for a single metric or multiple metrics. Its application is in assessing newly developed modules by their quality in the absence of quality data. The RCVD is a part of the larger framework of our measurement models that include the use of Boolean discriminant functions for classifying software quality. We demonstrate our concepts using Space Shuttle flight software data",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=809318,no,undetermined,0
An empirical study of experience-based software defect content estimation methods,"Capture-recapture models and curve-fitting models have been proposed to estimate the remaining number of defects after a review. This estimation gives valuable information to monitor and control software reliability. However, the different models provide different estimates making it difficult to know which estimate is the most accurate. One possible solution is to, as in this paper, focus on different opportunities to estimate intervals. The study is based on thirty capture-recapture data sets from software reviews. Twenty of the data sets are used to create different models to perform estimation. The models are then evaluated on the remaining ten data sets. The study shows that the use of historical data in model building is one way to overcome some of the problems experienced with both capture-recapture and curve-fitting models, to estimate the defect content after a review",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=809317,no,undetermined,0
Classification tree models of software quality over multiple releases,"Software quality models are tools for focusing software enhancement efforts. Such efforts are essential for mission-critical embedded software, such as telecommunications systems, because customer-discovered faults have very serious consequences and are very expensive to repair. We present an empirical study that evaluated software quality models over several releases to address the question, â€œHow long will a model yield useful predictions?â€?We also introduce the Classification And Regression Trees (CART) algorithm to software reliability engineering practitioners. We present our method for exploiting CART features to achieve a preferred balance between the two types of misclassification rates. This is desirable because misclassifications of fault-prone modules often have much more severe consequences than misclassifications of those that are not fault-prone. We developed two classification-tree models based on four consecutive releases of a very large legacy telecommunications system. Forty-two software product, process, and execution metrics were candidate predictors. The first software quality model used measurements of the first release as the training data set and measurements of the subsequent three releases as evaluation data sets. The second model used measurements of the second release as the training data set and measurements of the subsequent two releases as evaluation data sets. Both models had accuracy that would be useful to developers",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=809316,no,undetermined,0
A measurement-based model for estimation of resource exhaustion in operational software systems,"Software systems are known to suffer from outages due to transient errors. Recently, the phenomenon of â€œsoftware agingâ€? in which the state of the software system degrades with time, has been reported (S. Garg et al., 1998). The primary causes of this degradation are the exhaustion of operating system resources, data corruption and numerical error accumulation. This may eventually lead to performance degradation of the software or crash/hang failure, or both. Earlier work in this area to detect aging and to estimate its effect on system resources did not take into account the system workload. In this paper, we propose a measurement-based model to estimate the rate of exhaustion of operating system resources both as a function of time and the system workload state. A semi-Markov reward model is constructed based on workload and resource usage data collected from the UNIX operating system. We first identify different workload states using statistical cluster analysis and build a state-space model. Corresponding to each resource, a reward function is then defined for the model based on the rate of resource exhaustion in the different states. The model is then solved to obtain trends and the estimated exhaustion rates and the time-to-exhaustion for the resources. With the help of this measure, proactive fault management techniques such as â€œsoftware rejuvenationâ€?(Y. Huang et al., 1995) may be employed to prevent unexpected outages",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=809313,no,undetermined,0
Confidence interval estimation of NHPP-based software reliability models,"Software reliability growth models, such as the non-homogeneous Poisson process (NHPP) models, are frequently used in software reliability prediction. The estimation of parameters in these models is often done by point estimation. However, some numerical problems arise with this approach, and make the actual computation hard, especially for automated reliability prediction tools. In this paper, confidence interval computation is studied in the Goel-Okumoto (1979) model and the S-shaped model (S. Yamada et al., 1983). The upper and the lower bounds of the parameters can be obtained. For reliability prediction, we implement a simplified Bayesian approach, which delivers improved results. The bounds on the predicted reliability are also computed. Furthermore, the numerical problems encountered in earlier point estimation methods are removed by this approach. Our results can thus be used as an important part of the assessment of software quality",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=809305,no,undetermined,0
Reliable determination of object pose from line features by hypothesis testing,"To develop a reliable computer vision system, the employed algorithm must guarantee good output quality. In this study, to ensure the quality of the pose estimated from line features, two simple test functions based on statistical hypothesis testing are defined. First, an error function based on the relation between the line features and some quality thresholds is defined. By using the first test function defined by a lower bound of the error function, poor input can be detected before estimating the pose. After pose estimation, the second test function can be used to decide if the estimated result is sufficiently accurate. Experimental results show that the first test function can detect input with low qualities or erroneous line correspondences and that the overall proposed method yields reliable estimated results",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=809118,no,undetermined,0
A fuzzy set approach to cost estimation of software projects,The study is concerned with the development of models of software cost estimation using the technology of fuzzy sets. We propose an augmentation of the well-known class of COCOMO cost estimation models by admitting a granular form of the estimates of the variables used there. Granular models of cost estimation are also introduced. The performance of the granular models is illustrated by a series of numerical experiments.,1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=808195,no,undetermined,0
Software product metrics,"Software metrics are measures of software. Their primary use is to help us plan and predict software development. We can better control software quality and development effort if we can quantify the software. This is especially true in the early stages of development. Research has been done in the area of predicting software maintenance effort from software metrics. In general, software metrics can be classified into two categories: software product metrics and software process metrics. Software product metrics are measures of software products such as source code and design documents. Software process metrics are measures of software development process. For example, the size of the software is a measure of the software product itself, thus a product metrics. The effort required to design a software system may be influenced by how the software is designed, thus a process metrics. In this article, only software product metrics are addressed",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=807276,no,undetermined,0
Validation of software effort models,Several static models for software effort were examined on three data sets. Models applied to new data sets need to be evaluated relative to the quality of the data set and this can be benchmarked by fitting a least squares model to the data. This natural model can then be compared to the model that is being evaluated.,1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=807208,no,undetermined,0
Detecting controller malfunctions in electromagnetic environments. II. Design and analysis of the detector,"For part I see ibid. Verifying the integrity of control computers in adverse operating environments is a key issue in the development, validation, certification, and operation of critical control systems. Future commercial aircraft will necessitate flight-critical systems with high reliability requirements for stability augmentation, flutter suppression, and guidance and control. Operational integrity of such systems in adverse environments must be validated. The paper considers the problem of dynamic detection techniques to monitoring the integrity of fault tolerant control computers in critical applications. Specifically, the paper considers the detection of malfunctions in an aircraft flight control computer (FCC) that is subjected to electromagnetic environment (EME) disturbances during laboratory testing. A dynamic monitoring strategy is presented and demonstrated for the FCC from glideslope engaged until flare under clear air turbulence conditions using a detailed simulation of the B737 Autoland. The performance of the monitoring system is analyzed",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=801199,no,undetermined,0
A software model for impact analysis: a validation experiment,"Impact analysis is the process of identifying software work products that may be affected by proposed changes. This requires a software representation model that can formalize the knowledge about the various dependencies between work products. This study was carried out with the aim of objectively assessing whether the effectiveness of an impact analysis approach depends on the software dependency model employed. ANALYST, a tool for impact analysis, was used to implement different impact analysis approaches. The results show that the nature of the components and the traceability relationships employed for impact analysis influence the effectiveness of the approach, but different traceability models affect the various aspects of effectiveness differently. Moreover, this influence is independent of the software development approach, but is sensitive to software quality decay",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=806962,no,undetermined,0
An efficient computation-constrained block-based motion estimation algorithm for low bit rate video coding,"We present an efficient computation constrained block-based motion vector estimation algorithm for low bit rate video coding that yields good tradeoffs between motion estimation distortion and number of computations. A reliable predictor determines the search origin, localizing the search process. An efficient search pattern exploits structural constraints within the motion field. A flexible cost measure used to terminate the search allows simultaneous control of the motion estimation distortion and the computational cost. Experimental results demonstrate the viability of the proposed algorithm in low bit rate video coding applications. The resulting low bit rate video encoder yields essentially the same levels of rate-distortion performance and subjective quality achieved by the UBC H.263+ video coding reference software. However, the proposed motion estimation algorithm provides substantially higher encoding speed as well as graceful computational degradation capabilities",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=806627,no,undetermined,0
Built-in self-test for GHz embedded SRAMs using flexible pattern generator and new repair algorithm,"This paper presents a built-in self-test (BIST) scheme, which consists of a flexible pattern generator and a practical on-macro two-dimensional redundancy analyzer, for GHz embedded SRAMs. In order to meet the system requirements and to detect a wide variety of faults or performance degradation resulting from recent technology advances, the microcode-based pattern generator can generate flexible patterns. A practical new repair algorithm for the Finite State Machine (FSM)-based on-macro redundancy analyzer is also presented. It can be implemented with simple hardware and can show fairly good performance compared with conventional software-based algorithms",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=805644,no,undetermined,0
Adaptive write detection in home-based software DSMs,"Write detection is essential in multiple-writer protocols to identify writes to shared pages so that these writes can be correctly propagated. Software DSMs that implement multiple-writer protocol normally employ the virtual memory page fault to detect writes to shared pages. It write-protects shared pages at the beginning of an interval to detect writes of the interval. This paper proposes a new write detection scheme in a home-based software DSM called JIAJIA. It automatically recognizes single write to a shared page by its home host and assumes the page will continue to be written by the home host in the future until the page is written by remote hosts. During the period the page is assumed to be singly written by its home host, no write detection of this home page is required and page faults caused by home host write detection call are avoided. Evaluation with some well-known DSM benchmarks reveals that the new write detection can reduce page faults dramatically and improve performance significantly",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=805324,no,undetermined,0
Resolving distributed deadlocks in the OR request model,In this work a new distributed deadlock resolution algorithm for the OR model is proposed. The algorithm verifies the correctness criteria: safety-false deadlocks are not resolved; and liveness-deadlocks are resolved in finite time,1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=805104,no,undetermined,0
Fault injection based on a partial view of the global state of a distributed system,"This paper describes the basis for and preliminary implementation of a new fault injector, called Loki, developed specifically for distributed systems. Loki addresses issues related to injecting correlated faults in distributed systems. In Loki, fault injection is performed based on a partial view of the global state of an application. In particular, facilities are provided to pass user-specified state information between nodes to provide a partial view of the global state in order to try to inject complex faults successfully. A post-runtime analysis, using an off-line clock synchronization and a bounding technique, is used to place events and injections on a single global time-line and determine whether the intended faults were properly injected. Finally, observations containing successful fault injections are used to estimate specified dependability measures. In addition to describing the details of our new approach, we present experimental results obtained from a preliminary implementation in order to illustrate Loki's ability to inject complex faults predictably",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=805093,no,undetermined,0
Scalable stability detection using logical hypercube,"This paper proposes to use a logical hypercube structure for detecting message stability in distributed systems. In particular, a stability detection protocol that uses such a superimposed logical structure is presented, and its scalability is compared with other known stability detection protocols. The main benefits of the logical hypercube approach are scalability, fault-tolerance, and refraining from overloading a single node or link in the system. These benefits become evident both by an analytical comparison and by simulations. Another important feature of the logical hypercube approach is that the performance of the protocol is in general not sensitive to the topology of the underlying physical network",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=805089,no,undetermined,0
From system level to defect-oriented test: a case study,"The purpose of this paper is to demonstrate the usefulness of a recently proposed Object-Oriented (OO) based methodology and tools (SysObj and Test-Adder) when applied in the design of testable hardware modules (eventually used as embedded cores in SOCs). A public domain processor (PIG) is the vehicle for such assessment. A boundary scan soft wrapper design is presented and architectural quality is evaluated by adequate metrics. A novel mixed-level, defect-oriented fault simulator, VeriDOS, is used to demonstrate the efficiency of applying a functional lest (stored in the ROM) after the structural test (using built-in scan) to detect a significant number of physical defects not covered by the LSA test set. This illustrates how system-level test can be reused to enhance the quality of a production test.",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=804509,no,undetermined,0
Using simulation for assessing the real impact of test coverage on defect coverage,"The use of test coverage measures (e.g. block coverage) to control the software test process has become an increasingly common practice. This is justified by the assumption that higher test coverage helps achieve higher defect coverage and therefore improves software quality. In practice, data often shows that defect coverage and test coverage grow over time, as additional testing is performed. However, it is unclear whether this phenomenon of concurrent growth can be attributed to a causal dependency or if it is coincidental, simply due to the cumulative nature of both measures. Answering such a question is important as it determines whether a given test coverage measure should be monitored for quality control and used to drive testing. Although this is no general answer to the problem above, we propose a procedure to investigate whether any test coverage criterion has a genuine additional impact on defect coverage when compared to the impact of just running additional test cases. This procedure is applicable in typical testing conditions where the software is tested once, according to a given strategy and where coverage measures are collected as well as defect data. We then test the procedure on published data and compare our results with the original findings. The study outcomes do not support the assumption of a causal dependency between test coverage and defect coverage, a result for which several plausible explanations are provided",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=809319,no,undetermined,0
Failure correlation in software reliability models,"Perhaps the most stringent restriction that is present in most software reliability models is the assumption of independence among successive software failures. Our research was motivated by the fact that although there are practical situations in which this assumption could be easily violated, much of the published literature on software reliability modeling does not seriously address this issue. In this paper we present a software reliability modeling framework based on Markov renewal processes which naturally introduces dependence among successive software runs. The presented approach enables the phenomena of failure clustering to be precisely characterized and its effects on software reliability to be analyzed. Furthermore, it also provides bases for a more flexible and consistent model formulation and solution. The Markov renewal model presented in this paper can be related to the existing software reliability growth models, that is, a number of them can be derived as special cases under the assumption of failure independence. Our future research is focused on developing more specific and detailed models within this framework, as well as statistical inference procedures for performing estimations and predictions based on the experimental data",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=809328,no,undetermined,0
DynaMICs: an automated and independent software-fault detection approach,"Computers are omnipresent in our society, creating a reliance that demands high-assurance systems. Traditional verification and validation approaches may not be sufficient to identify the existence of software faults. Dynamic Monitoring with Integrity Constraints (DynaMICs) augments existing approaches by including: (1) elicitation of constraints from domain experts and developers that capture knowledge about real-world objects, assumptions and limitations, (2) constraints stored and maintained separate from the program, (3) automatic generation of monitoring code and program instrumentation, (4) performance-friendly monitoring, and (5) tracing among specifications, code and documentation. The primary motivation for DynaMICs is to facilitate the detection of faults, in particular those that result from insufficient communication, changes in intended software use and errors introduced through external interfaces. After presenting related work and an overview of DynaMICs, this paper outlines the methodology used to provide an automated and independent software-fault detection system",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=809470,no,undetermined,0
Predicting fault-prone software modules in embedded systems with classification trees,"Embedded-computer systems have become essential elements of the modern world. For example, telecommunications systems are the backbone of society's information infrastructure. Embedded systems must have highly reliable software. The consequences of failures may be severe; down-time may not be tolerable; and repairs in remote locations are often expensive. Moreover, today's fast-moving technology marketplace mandates that embedded systems evolve, resulting in multiple software releases embedded in multiple products. Software quality models can be valuable tools for software engineering of embedded systems, because some software-enhancement techniques are so expensive or time-consuming that it is not practical to apply them to all modules. Targeting such enhancement techniques is an effective way to reduce the likelihood of faults discovered in the field. Research has shown software metrics to be useful predictors of software faults. A software quality model is developed using measurements and fault data from a past release. The calibrated model is then applied to modules currently under development. Such models yield predictions on a module-by-module basis. This paper examines the Classification And Regression Trees (CART) algorithm for predicting which software modules have high risk of faults to be discovered during operations. CART is attractive because it emphasizes pruning to achieve robust models. This paper presents details on the CART algorithm in the context of software engineering of embedded systems. We illustrate this approach with a case study of four consecutive releases of software embedded in a large telecommunications system. The level of accuracy achieved in the case study would be useful to developers of an embedded system. The case study indicated that this model would continue to be useful over several releases as the system evolves",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=809481,no,undetermined,0
Scenario management in Web-based simulation,"Internet communications in general and the World-Wide Web specifically are revolutionizing the computer industry. Today, the Web is full of important documents and clever applets. Java applets and servlets are beginning to appear that provide useful and even mission critical applications. From the perspective of simulation, a future Web will be full of simulation models and large amounts of simulation-generated data. Many of the models will include two or three dimensional animation as well as virtual reality. Others will allow human interaction with simulation models to control or influence their execution no matter where the user is located in the world. Analysis of data from Web-based simulations involves greater degrees of freedom than traditional simulations. The number of simulation models available and the amount of simulation data are likely to be much greater. In order to assure the quality of data, the execution of models under a variety of scenarios should be well managed. Since the user community will also be larger, quality assurance should be delegated to agents responsible for defining scenarios and executing models. A major element of simulation analysis is the analysis of output data, which manages the execution of simulation models, in order to obtain statistical data of acceptable quality. Such data may be used to predict the performance of a single system, or to compare the performance of two or more alternative system designs using a single or multiple performance measures",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=816876,no,undetermined,0
Cost of ensuring safety in distributed database management systems,"Generally, applications employing database management systems (DBMS) require that the integrity of the data stored in the database be preserved during normal operation as well as after crash recovery. Preserving database integrity and availability needs extra safety measures in the form of consistency checks. Increased safety measures inflict adverse effect on performance by reducing throughput and increasing response time. This may not be agreeable for some critical applications and thus, a tradeoff is needed. This study evaluates the cost of extra consistency checks introduced in the data buffer cache in order to preserve the database integrity, in terms of performance loss. In addition, it evaluates the improvement in error coverage and fault tolerance, and occurrence of double failures causing long unavailability, with the help of fault injection. The evaluation is performed on a replicated DBMS, ClustRa. The results show that the checksum overhead in a DBMS inflicted with a very high TPC-B-like workload caused a reduction in throughput up to 5%. The error detection coverage improved from 62% to 92%. Fault injection experiments shows that corruption in database image went down from 13% to 0%. This indicates that the applications that require high safety, but can afford up to 5% performance loss can adopt checksum mechanisms",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=816229,no,undetermined,0
A critique of software defect prediction models,"Many organizations want to predict the number of defects (faults) in software systems, before they are deployed, to gauge the likely delivered quality and maintenance effort. To help in this numerous software metrics and statistical models have been developed, with a correspondingly large literature. We provide a critical review of this literature and the state-of-the-art. Most of the wide range of prediction models use size and complexity metrics to predict defects. Others are based on testing data, the â€œqualityâ€?of the development process, or take a multivariate approach. The authors of the models have often made heroic contributions to a subject otherwise bereft of empirical studies. However, there are a number of serious theoretical and practical problems in many studies. The models are weak because of their inability to cope with the, as yet, unknown relationship between defects and failures. There are fundamental statistical and data quality problems that undermine model validity. More significantly many prediction models tend to model only part of the underlying problem and seriously misspecify it. To illustrate these points the Goldilock's Conjecture, that there is an optimum module size, is used to show the considerable problems inherent in current defect prediction approaches. Careful and considered analysis of past and new results shows that the conjecture lacks support and that some models are misleading. We recommend holistic models for software defect prediction, using Bayesian belief networks, as alternative approaches to the single-issue models used at present. We also argue for research into a theory of â€œsoftware decompositionâ€?in order to test hypotheses about defect introduction and help construct a better science of software engineering",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=815326,no,undetermined,0
Synthesis and decoding of emotionally expressive music performance,"A recently developed application of Director Musices (DM) is presented. The DM is a rule-based software tool for automatic music performance developed at the Speech Music and Hearing Dept. at the Royal Institute of Technology, Stockholm. It is written in Common Lisp and is available both for Windows and Macintosh. It is demonstrated that particular combinations of rules defined in the DM can be used for synthesizing performances that differ in emotional quality. Different performances of two pieces of music were synthesized so as to elicit listeners' associations to six different emotions (fear, anger, happiness, sadness, tenderness, and solemnity). Performance rules and their parameters were selected so as to match previous findings about emotional aspects of music performance. Variations of the performance variables IOI (Inter-Onset Interval), OOI (Offset-Onset Interval) and L (Sound Level) are presented for each rule-setup. In a forced-choice listening test 20 listeners were asked to classify the performances with respect to emotions. The results showed that the listeners, with very few exceptions, recognized the intended emotions correctly. This shows that a proper selection of rules and rule parameters in DM can indeed produce a wide variety of meaningful, emotional performances, even extending the scope of the original rule definition",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=812420,no,undetermined,0
Worst case timing requirement of real-time tasks with time redundancy,"The improvement of system reliability can be achieved through the use of time redundancy techniques including retry, checkpointing and recovery block. The objective of this paper is to analyze the worst case timing requirement of real-time tasks with time redundancy in the presence of multiple transient faults. The timing behavior of system level checkpointing is modeled with additional tasks that save the state of all tasks. The timing information derived in this paper makes time redundancy techniques applicable to real-time systems while keeping the validity of schedulability check developed in the past",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=811290,no,undetermined,0
A novel testing approach for safety-critical software,"In this paper we propose a new black-box testing approach on how to select test cases to overcome the shortcoming that some conventional black-box testing methods lack precise testing adequacy measures to measure the quality of testing and direct the testing process and thoroughly test safety-critical software. The method is based on two key ideas: (1) specification classifying, and (2) controlled objects covering. By using the method, several kinds of computer interlocking safety software have been successfully tested and the testing results show that the method is efficient",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=810759,no,undetermined,0
JMTP: an architecture for exploiting concurrency in embedded Java applications with real-time considerations,"Using Java in embedded systems is plagued by problems of limited runtime performance and unpredictable runtime behavior. The Java Multi-Threaded Processor (JMTP) provides solutions to these problems. The JMTP architecture is a single chip containing an off-the-shelf general purpose processor core coupled with an array of Java Thread Processors (JTPs). Performance can be improved using this architecture by exploiting coarse-grained parallelism in the application. These performance improvements are achieved with relatively small hardware costs. Runtime predictability is improved by implementing a subset of the Java Virtual Machine (JVM) specification in the JTP and trimming away complexity without excessively restricting the Java code a JTP can handle. Moreover the JMTP architecture incorporates hardware to adaptively manage shared JMTP resources in order to satisfy JTP thread timing constraints or provide an early warning for a timing violation. This is an important feature for applications with quality-of-service demands. In addition to the hardware architecture, we describe a software framework that analyzes a Java application for expressed and implicit coarse-grained concurrent threads to execute on JTPs. This framework identifies the optimal mapping of an application to a JMTP with an arbitrary number of JTPs. We have tested this framework on a variety of applications including IDEA encryption with different JTP configurations and confirmed that the algorithm was able to obtain desired results in each case.",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=810710,no,undetermined,0
VERITAS-an application for knowledge verification,"Knowledge management is one of the most important goals of any organization. Therefore, several automatic tools are used for that purpose, e.g. Knowledge Based Systems (KBS), Experts Systems, Data Mining Applications and Computer Aided Decision Systems. The validation and verification (V&V) process is fundamental in order to ensure the quality of used knowledge. The usage of automatic verification tools can be a reliable, inexpensive and reusable way to overcome the constant growth of the Knowledge Bases, the shortening of development times and the costs of Validation, specially field tests. This paper addresses the verification of Knowledge Based Systems, focussing on VERITAS, a verification tool initially developed to verify a KBS used to assist operators of Portuguese Transmission Control Centers in incident analysis and power restoration VERITAS performs knowledge base structural analysis allowing the detection of knowledge anomalies",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=809839,no,undetermined,0
Quantitative modeling of software reviews in an industrial setting,"Technical reviews are a cost effective method commonly used to detect software defects early. To exploit their full potential, it is necessary to collect measurement data to constantly monitor and improve the implemented review procedure. This paper postulates a model of the factors that affect the number of defects detected during a technical review, and tests the model empirically using data from a large software development organization. The data set comes from more than 300 specification, design, and code reviews that were performed at Lucent's Product Realization Center for Optical Networking (PRC-ON) in Nuernberg, Germany. Since development projects within PRC-ON usually spend between 12% and 18% of the total development effort on reviews, it is essential to understand the relationships among the factors that determine review success. One major finding of this study is that the number of detected defects is primarily determined by the preparation effort of reviewers rather than the size of the reviewed artifact. In addition, the size of the reviewed artifact has only limited influence on review effort. Furthermore, we identified consistent ceiling effects in the relationship between size and effort with the number of defects detected. These results suggest that managers at PRC-ON must consider adequate preparation effort in their review planning to ensure high quality artifacts as well as a mature review process",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=809752,no,undetermined,0
A metrics-based decision support tool for software module interfacing technique selection to lower maintenance cost,"The Interfacing Techniques Comparison Graph visually compares applications in terms of attributes that relate to maintenance cost. Applications that have both lower coupling and lower complexity lie closer to the origin of the graph and exhibit lower maintenance cost than those that do not. The study supports the idea that compositional techniques are important for achieving these improved metrics. The graph can be used in three ways. First it serves as a decision support tool for managers to determine whether expected maintenance savings compensate for the additional training, effort and time needed to support compositional development. Second, it functions as a decision support tool for designers and coders as they determine, for each module interface, whether to use coupled techniques or composition. The graph can help identify those situations in which the long term cost gain justifies the extra time needed for compositional design. Third, it can serve as a maintenance cost estimation tool. The study found a close correlation between predicted and actual maintenance effort",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=809738,no,undetermined,0
Can results from software engineering experiments be safely combined?,"Deriving reliable empirical results from a single experiment is an unlikely event. Hence to progress, multiple experiments must be undertaken per hypothesis and the subsequent results effectively combined to produce a single reliable conclusion. Other disciplines use meta-analytic techniques to achieve this result. The treatise of the paper is: can meta-analysis be successfully applied to current software engineering experiments? The question is investigated by examining a series of experiments, which themselves investigate: which defect detection technique is best? Applying meta-analysis techniques to the software engineering data is relatively straightforward, but unfortunately the results are highly unstable, as the meta-analysis shows that the results are highly disparate and don't lead to a single reliable conclusion. The reason for this deficiency is the excessive variation within various components of the experiments. Finally, the paper describes a number of recommendations for controlling and reporting empirical work to advance the discipline towards a position where meta-analysis can be profitably employed",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=809736,no,undetermined,0
Measuring coupling and cohesion: an information-theory approach,"The design of software is often depicted by graphs that show components and their relationships. For example, a structure chart shows the calling relationships among components. Object oriented design is based on various graphs as well. Such graphs are abstractions of the software, devised to depict certain design decisions. Coupling and cohesion are attributes that summarize the degree of interdependence or connectivity among subsystems and within subsystems, respectively. When used in conjunction with measures of other attributes, coupling and cohesion can contribute to an assessment or prediction of software quality. Let a graph be an abstraction of a software system and let a subgraph represent a module (subsystem). The paper proposes information theory based measures of coupling and cohesion of a modular system. These measures have the properties of system level coupling and cohesion defined by L.C. Briand et al. (1996; 1997). Coupling is based on relationships between modules. We also propose a similar measure for intramodule coupling based on an intramodule abstraction of the software, rather than intermodule, but intramodule coupling is calculated in the same way as intermodule coupling. We define cohesion in terms of intramodule coupling, normalized to between zero and one. We illustrate the measures with example graphs. Preliminary analysis showed that the information theory approach has finer discrimination than counting",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=809733,no,undetermined,0
Integrating approximation methods with the generalised proportional sampling strategy,"Previous studies have shown that partition testing strategies can be very effective in detecting faults, but they can also be less effective than random testing under unfavourable circumstances. When test cases are allocated in proportion to the size of subdomains, partition testing strategies are provably better than random testing, in the sense of having a higher or equal probability of detecting at least one failure (the P-measure). Recently, the Generalised Proportional Sampling (GPS) strategy, which is always satisfiable, was proposed to relax the proportionality condition. The paper studies the use of approximation methods to generate test distributions satisfying the GPS strategy, and evaluates this proposal empirically. Our results are very encouraging, showing that on average about 98.72% to almost 100% of the test distributions obtained in this way are better than random testing in terms of the P-measure",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=809655,no,undetermined,0
"Testing, reliability, and interoperability issues in the CORBA programming paradigm","CORBA (Common Object Request Broker Architecture) is widely perceived as an emerging platform for distributed systems development. We discuss CORBA's testing, reliability and interoperability issues among multiple program versions implemented by different languages (Java and C++) based on different vendor platforms (Iona Orbix and Visibroker). We engage 19 independent programming teams to develop a set of CORBA programs from the same requirement specifications, and measure the reliability of these programs. We design the required test cases and develop the operational profile of the programs. After running the test, we classify the detected faults and evaluate the reliability of the programs according to the operational profile. We also discuss how to test the CORBA programs based on their specification and interface design language (IDL). We measure the interoperability of these programs by evaluating the difficulty in exchanging the clients and servers between these programs. The measurement we obtained indicates that without a good discipline on the development of CORBA objects, interoperability would be very poor, and reusability of either client or server programs is very doubtful. We further discuss particular incidences where these programs are not interoperable, and describe future required engineering steps to make these programs interoperable, testable, and reliable",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=809646,no,undetermined,0
Maintainability myth causes performance problems in SMP application,"A challenge in software design is to find solutions that balance and optimize the quality attributes of the application. We present a case study of an application and the results of a design decision made on weak assumptions. The application has been assessed with respect to performance and maintainability. We present and evaluate an alternative design of a critical system component. Based on interviews with the involved designers we establish the design rationale. By analyzing the evaluation data of the two alternatives and the design rationale, we conclude that the design decision was based on a general assumption that an adaptable component design should increase the maintainability of the application. This case study is clearly a counter example to that assumption, and we therefore reject it as a myth. This study shows, however, that the myth is indeed responsible for the major performance problem in the application",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=809644,no,undetermined,0
Towards a broader view on software architecture analysis of flexibility,"Software architecture analysis helps us assess the quality of a software system at an early stage. We describe a case study of software architecture analysis that we have performed to assess the flexibility of a large administrative system. Our analysis was based on scenarios, representing possible changes to the requirements of the system and its environment. Assessing the effect of these scenarios provides insight into the flexibility of the system. One of the problems is to express the effect of a scenario in such a way that it provides insight into the complexity of the necessary changes. Part of our research is directed at developing an instrument for doing just that. This instrument is applied in the analysis presented",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=809608,no,undetermined,0
Building dependable distributed applications using AQUA,"Building dependable distributed systems using ad hoc methods is a challenging task. Without proper support, an application programmer must face the daunting requirement of having to provide fault tolerance at the application level, in addition to dealing with the complexities of the distributed application itself. This approach requires a deep knowledge of fault tolerance on the part of the application designer, and has a high implementation cost. What is needed is a systematic approach to providing dependability to distributed applications. Proteus, part of the AQuA architecture, fills this need and provides facilities to make a standard distributed CORBA application dependable, with minimal changes to an application. Furthermore, it permits applications to specify, either directly or via the Quality Objects (QuO) infrastructure, the level of dependability they expect of a remote object, and will attempt to configure the system to achieve the requested dependability level. Our previous papers have focused on the architecture and implementation of Proteus. This paper describes how to construct dependable applications using the AQuA architecture, by describing the interface that a programmer is presented with and the graphical monitoring facilities that it provides",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=809494,no,undetermined,0
System for automated validation of embedded software in multiple operating configurations,"Embedded controllers in safety critical applications rely on the highest of software quality standards. Testing is performed to ensure that requirements and specifications are met for all the different environments in which the controllers operate. This paper describes the architecture of a system that uses a relational database for tracking tests, requirements, and configurations. The relational sub-schemas are integrated in a data warehouse and allow traceability from requirements to tests. A normalized representation of test cases enables the system to reason about the test topologies and is used for constructing clusters of similar tests. A representative test from each cluster can in turn provide a rapid estimation, of the software's requirement coverage and quality",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=802355,no,undetermined,0
A systems engineering view of TPS rehosting,"The TPS test design process results in UUT detailed test strategy that is independent of its eventual implementation in source code or on a specific ATS. It is test strategy, not source code, that embodies the critical metrics of UUT fault detection and isolation. Source code implementation is the first step in a design chain that finally results in signal-information flow across the test interface. This SI flow is the physical realization of test design. The objective of TPS rehosting is usually preservation of proven legacy test design, but it should also seek to preserve legacy SI flow to avoid the subtle, and often difficult, problems caused by rehosting to the new ATS. These problems can be exacerbated by inability of the original source code to describe what is happening at the UUT test interface, and by the repeated and characteristic need to choose between equivalent performance of, and incidental or substantive enhancements to, the rehosted TPS",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=800420,no,undetermined,0
An efficient algorithm for leader-election in synchronous distributed systems,"Leader election is an important problem in distributed computing. H. Garcia-Molina's (1982) Bully algorithm is a classic solution to leader election in synchronous systems with crash failures. In this paper, we indicate the problems with the Bully algorithm and re-write it to use a failure detector instead of explicit time-outs. We show that this algorithm is more efficient than Garcia-Molina's one in terms of processing time",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=818613,no,undetermined,0
An effective approach for utilities and suppliers to assess the quality of new products before purchase,"Metering technology has advanced from the basic Ferraris technology to microprocessor-controlled meters with solid state sensors and electronic register displays. The functional requirement for the microprocessor-controlled meters and modules has grown dramatically. There may be thousands of lines of software code in an electronic meter to support this additional functionality. Yet the existing type approval methods really only test the basic metering functionality. Should other arrangements be made for type approval of the software component, to reduce the risk for the purchaser of the end equipment? This paper discusses the problems faced by purchasers and manufacturers of utility equipment relying on existing type approval methods, and the philosophy which appears to be emerging from a number of national and European groups to solve some of these problems. The authors propose a cost-effective method that should address these problems for the manufacturers and provide a high degree of quality assurance to the purchasers. The paper focuses on software quality processes but a similar approach could be applied to the hardware aspects of product development. The discussion refers to revenue and payment metering as examples but the ideas proposed could be applied equally well to other products",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=787184,no,undetermined,0
Off-line testing of reluctance machines,Experimental techniques for determining the machine parameters and results demonstrating their performance on an axially-laminated synchronous reluctance motor are presented. A new software implementation of the control algorithm for conducting an instantaneous flux-linkage test is developed. A low frequency AC test allowing better estimation accuracy compared to a standard 50-Hz test is proposed. Effects of stator slotting on the quality of DC torque test estimates are examined,1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=790868,no,undetermined,0
A generic platform for scalable access to multimedia-on-demand systems,"Access to multimedia servers is commonly done according to a client/server model where the end user at the client host retrieves multimedia objects from a multimedia server. In a distributed environment, a number of end users may need to access a number of multimedia servers through one or several communication networks. Such a scenario reveals the requirement for a distributed access platform. In addition, the demand for multimedia information is increasing beyond the capabilities of high performance storage devices. Therefore, load distribution and scalability issues must be addressed while designing and implementing the distributed access platform. This paper introduces a scalable access platform (SAP) for managing user access to multimedia-on-demand systems while optimizing resource utilization. The platform is generic and capable of integrating heterogeneous multimedia servers. SAP operation combines static replication and dynamic load distribution policies. It provides run time redirecting of client requests to multimedia servers according to the workload information dynamically collected in the system. To support multimedia-on-demand systems with differing quality-of-service (QoS) requirements, the platform also takes into account, as part of the access process, user QoS requirements and cost constraints. This paper also presents an application of the generic platform implementing a scalable movie-on-demand system, called SMoD. Performance evaluation based on simulation shows that in many cases SMoD can reduce the blocking probability of user requests, and thus can support more users than classical video-on-demand (VoD) systems. It also shows that the load is better distributed across the video servers of the system",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=790484,no,undetermined,0
Continuity and contingency planning for medical devices,"The Medical Device Agency's (MDA) key aim is to take all reasonable steps to protect the public health and safeguard the interests of patients and users by ensuring that medical devices and equipment meet appropriate standards of safety, quality and performance and that they comply with relevant Directives of the European Union. It is expected that some computers and other electronic equipment will malfunction because of the Year 2000 date change (and other related dates). MDA's involvement is where such equipment is a medical device. Medical device manufacturers are better placed than manufacturers in some other sectors to identify and rectify whatever date-related problems may exist with their products. However they generally do not know exactly which versions of which products are in service in which places. These factors led MDA to identify an appropriate division of responsibility: manufacturers needed to assess each of their products (including any variants of the same model incorporating different software versions or different hardware), to identify any which were affected by year 2000 problems, and to make this information available to users, MDA and other interested parties, with an indication of the appropriate remedial action; users needed to identify what devices they have, by make and model, and if necessary also by serial number and/or software version, and then to contact the manufacturers for information; MDA needed to provide appropriate advice, to encourage manufacturers to provide compliance information, and to investigate any reported non-compliances that might adversely affect the safety of patients or users",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=790357,no,undetermined,0
Prediction error as a quality metric for motion and stereo,"This paper presents a new methodology for evaluating the quality of motion estimation and stereo correspondence algorithms. Motivated by applications such as novel view generation and motion-compensated compression, we suggest that the ability to predict new views or frames is a natural metric for evaluating such algorithms. Our new metric has several advantages over comparing algorithm outputs to true motions or depths. First of all, it does not require the knowledge of ground truth data, which may be difficult or laborious to obtain. Second, it more closely matches the ultimate requirements of the application, which are typically tolerant of errors in uniform color regions, but very sensitive to isolated pixel errors or disocclusion errors. In the paper we develop a number of error metrics based on this paradigm, including forward and inverse prediction errors, residual motion error and local motion-compensated prediction error. We show results on a number of widely used motion and stereo sequences, many of which do not have associated ground truth data",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=790301,no,undetermined,0
A self-validating digital Coriolis mass-flow meter,"A first generation self-validating sensor (SEVA) prototype based on the Coriolis meter consisted of the flowtube and conventional commercial transmitter with a PC. The PC and the modified transmitter together exemplified how a SEVA device would behave. The prototype demonstrated an ability to detect and correct for several fault modes, as well as the generation of SEVA metrics. However, one benefit of carrying out a sensor validation analysis is that it leads to fundamental redesign. The identification of the major limitations and fault modes of an instrument, and their impact on measurement quality, provides strong motivation for improvements in the basic design. The idea of an all-digital transmitter design emerged after this first round of validation activity. The primary intention was to replace all the analogue circuitry (other than essential front-end op-amps) with a small number of digital components, and to carry out almost all functionality within software. This paper describes aspects of the improved performance obtained from the digital transmitter: measurement precision, flowtube control, two-phase flow and batching from empty",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=790265,no,undetermined,0
Comparison of probability of non-visibility in case of faults of some satellites in LEO constellations,"In this paper we analyze the coverage performance of satellite constellation in low earth orbit. First we analyze the various possible configurations of satellite failures in the same orbital plane or in different orbital planes when one satellite or a number of satellites are at fault. To evaluate the performance of each constellation we use one parameter, which is the maximum non-visibility time at one receiver position on Earth by using simulation software. We have also simulated the coverage performance variations depending on the receiver position on earth by varying the latitude of the receive position from 0 to 90 degrees. According to the results of the simulations we compare the maximum non-visibility time depending on the configuration of satellite and identify the worst case combination of satellite failures in satellite constellation. We also evaluate the performances the satellite constellations in 780 km orbit in case of a fault of some satellite in the same orbital plane or in different orbital plane. Finally we evaluate the some orbital manoeuvre strategy, (phase changing in the same orbital plane), to minimize the non-visibility time on Earth in case of failure of number of satellites",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=790207,no,undetermined,0
Software fault tolerance in a clustered architecture: techniques and reliability modeling,"System architectures based on a cluster of computers have gained substantial attention recently. In a clustered system, complex software-intensive applications can be built with commercial hardware, operating systems, and application software to achieve high system availability and data integrity, while performance and cost penalties are greatly reduced by the use of separate error detection hardware and dedicated software fault tolerance routines. Within such a system a watchdog provides mechanisms for error detection and switch-over to a spare or backup processor in the presence of processor failures. The application software is responsible for the extent of the error detection, subsequent recovery actions and data backup. The application can be made as reliable as the user requires, being constrained only by the upper bounds on reliability imposed by the clustered architecture under various implementation schemes. We present reliability modeling and analysis of the clustered system by defining the hardware, operating system, and application software reliability techniques that need to be implemented to achieve different levels of reliability and comparable degrees of data consistency. We describe these reliability levels in terms of fault detection, fault recovery, volatile data consistency, and persistent data consistency, and develop a Markov reliability model to capture these fault detection and recovery activities. We also demonstrate how this cost-effective fault tolerant technique can provide quantitative reliability improvement within applications using clustered architectures",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=790197,no,undetermined,0
Adaptive fuzzy controller for predicting order of concurrent actions in real-time systems,"In controlling a real-time system with a large number of controlling points and when the time spent for setting up the reacting actions is longer than the deadline, we have to predict the reacting actions before receiving complete information about them. These actions are chosen from the concurrent past actions based on the rank of their utilities. Thus in predicting the rank we have to estimate the indefinite qualities defined by the uncompleted information. The paper presents a hardware approach to predict the order of concurrent actions based on a fuzzy and adaptive mechanism which measures the possible utilities given in intervals with additional statistical information. In order to process efficiently the incomplete data we design a fuzzy hardware controller (FHC) which specializes on ranking the intervals according to the observed data in the past. The analysis shows that the fuzzy measurement implemented by a hardware specialized controller with parallel architecture is able to reduce significantly the processing time, compared with implementation by software running on a general computer. Moreover, the FHC can adapt to the types of statistical data which is also safer and can be updated easily within the common memory of the FHC.",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=790160,no,undetermined,0
Experiences from testing of functionality and software of meters for electrical energy,"The de-regulation of the electricity market in many European countries has increased the need for electricity meters with enhanced functionality. Both industrial and household customers are using the new types of meters. The dependability of the new technology has in some cases been discussed. The experiences from the evaluations carried out at SP Swedish National Testing and Research Institute for approximately 15 different electronic meters are that there are ways to test the functionality and the embedded software. The effort depends on the complexity of the system, the size of the code and the documentation available. All faults cannot be claimed to be detected, but that is not the objective. The aim must be to reach a certain level of confidence in the product, not to declare it fault-free. It is possible to design an electronic meter for electrical energy which will be accurate, withstand environmental disturbances, provide adequate safety against shock and fire, functioning according to specification, and dependability",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=787181,no,undetermined,0
A concept for the Naval Oceanographic Office Survey Operations Center,"The Naval Oceanographic Office (NAVOCEANO) Survey Operations Center (SOC) will enable scientists, engineers, and analysts ashore to evaluate the status and performance of shipboard systems and the quality of data collected from ships deployed around the world. Watchstanders will also remotely manage software configuration control, initiate shipboard software upgrades, troubleshoot onboard data collection systems, and monitor survey progress and coverage to assist with on-scene decisions",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=800158,no,undetermined,0
PLS modelling and fault detection on the Tennessee Eastman benchmark,"This paper describes the application of multivariate regression techniques to the Tennessee Eastman benchmark process. Two methods are applied: linear partial least squares, and a nonlinear variant of this procedure using a radial basis function inner relation. These methods are used to create online inferential models of delayed process measurement. The redundancy so obtained is then used to generate a fault detection and isolation scheme for these sensors. The effectiveness of this scheme is demonstrated on a number of test faults",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=786264,no,undetermined,0
An open interface for probabilistic models of text,"Summary form only given. An application program interface (API) for meddling sequential text is described. The API is intended to shield the user from details of the modelling and probability estimation process. This should enable different implementations of models to be replaced transparently in application programs. The motivation for this API is work on the use of textual models for applications in addition to strict data compression. The API is probabilistic, that is, it supplies the probability of the next symbol in the sequence. It is general enough to deal accurately with models that include escapes for probabilities. The concepts abstracted by the API are explained together with details of the API calls. Such predictive models can be used for a number of applications other than compression. Users of the models do not want to be concerned about the details either of the implementation of the models or how they were trained and the sources of the training text. The problem considered is how to permit code for different models and actual trained models themselves to be interchanged easily between users. The fundamental idea is that it should be possible to write application programs independent of the details of particular modelling code, that it should be possible to implement different modelling code independent of the various applications, and that it should be possible to easily exchange different pre-trained models between users. It is hoped that this independence will foster the exchange and use of high-performance modelling code, the construction of sophisticated adaptive systems based on the best available models, and the proliferation and provision of high-quality models of standard text types such as English or other natural languages, and easy comparison of different modelling techniques",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=785679,no,undetermined,0
Learning to assess the quality of genetic programs using cultural algorithms,"We explore solution generalizability, bloat, and effective length as examples of software quality issues and measurements that are useful in the analysis of genetic programming (GP) solution programs. The total program size of GP solution programs can be partitioned into effective program size and several types of excess code for which, following Angeline, we use the term â€œbloatâ€?(P.J. Angeline, 1998). We define several types of bloat: local, global, and representational. We use a cultural algorithm tool called the Metrics Apprentice to explore the relationships between the generalizability of the programmed solution, program size, and the effect of three GP processes purported to reduce bloat",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=785476,no,undetermined,0
Using cultural algorithms to improve performance in semantic networks,"Evolutionary computation has been successfully applied in a variety of problem domains and applications. We describe the use of a specific form of evolutionary computation known as cultural algorithms to improve the efficiency of the subsumption algorithm in semantic networks. Subsumption is the process that determines if one node in the network is a child of another node. As such, it is utilized as part of the node classification algorithm within semantic network based applications, One method of improving subsumption efficiency is to reduce the number of attributes that need to be compared for every node without impacting the results. We suggest that a cultural algorithm approach can be used to identify these defining attributes that are most significant for node retrieval. These results can then be utilized within an existing vehicle assembly process planning application that utilizes a semantic network based knowledge base to improve the performance and reduce complexity of the network",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=785472,no,undetermined,0
Integrated design approach for real-time embedded systems,"System designers face many problems during the design and implementation of real time embedded systems. Not only must such systems produce correct responses (logical correctness), they must also produce these at the correct time (temporal correctness). The paper presents results of research undertaken to develop an integrated design, development and run time environment for embedded systems that addresses the issues of: design correctness; software partitioning; performance prediction (timeliness, utilisation etc.); task management; and reliable, fast and secure run time behaviour. A description of a prototype tool set, based on a networked PC/UNIX system, is given",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=785341,no,undetermined,0
Classification and correlation of monitored power quality events,"Summary form only given. The presentation provides an overview of the existing standards for characterising the quality of electric service to high-tech facilities. The measurement requirements and characteristics of power quality measurements to fully analyse and classify the various PQ events are discussed. The specifications of state-of-the-art PQ monitoring instrumentation are discussed. The requirements of the hardware, the firmware and the software are discussed. These specifications cover the actual data capture, the communication of the instruments to central servers, and the analysis and GUI software requirements. The presentation also includes analysis of events captured at a high-tech facility in the Bay Area of San Francisco, USA. Of special interest is the major event due to a short circuit at one of the substations in the utility's system. The effect of the resulting voltage sag are presented, showing the effect on various systems. This event, and other minor PQ events recorded at the high tech facility are correlated with some of the commonly used PQ standards, and the results are presented",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=784395,no,undetermined,0
Availability modeling of modular software,"Dependability evaluation is a basic component in assessing the quality of repairable systems. A general model (Op) is presented and is specifically designed for software systems; it allows the evaluation of various dependability metrics, in particular, of availability measures. Op is of the structural type, based on Markov process theory. In particular, Op is an attempt to overcome some limitations of the well-known Littlewood reliability model for modular software. This paper gives the: mathematical results necessary to the transient analysis of this general model; and algorithms that can efficiently evaluate it. More specifically, from the parameters describing the: evolution of the execution process when there is no failure; failure processes together with the way they affect the execution; and recovery process, the results are obtained for the: distribution function of the number of failures in a fixed mission; and dependability metrics which are much more informative than the usual ones in a white-box approach. The estimation procedures of the Op parameters are briefly discussed. Some simple examples illustrate the interest in such a structural view and explain how to consider reliability growth of part of the software with the transformation approach developed by Laprie et al. The complete transient analysis of Op allows discussion of the Poisson approximation by Littlewood for his model",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=784274,no,undetermined,0
Fault detection in boilers using canonical variate analysis,"A software based system was developed that monitors industrial boilers on-line for the purpose to detect and identify fault conditions. Dynamic models are used to predict an expected set of process conditions, which form the baseline for evaluation of process conditions. By continuously monitoring the deviations between the baseline and actual observations, insight in the presence of certain faults can be obtained. An online expert system is used to draw conclusions from the observed prediction errors. The dynamic models are based on state space models which are generated by system identification using canonical variate analysis (CVA). The objectives of the system, the approach to the fault detection process and experience with a commercially available system identification tool are discussed",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=783223,no,undetermined,0
Effects of uncompensated vector control on synchronous reluctance motor performance,"The performance of a synchronous reluctance motor digital controller based on a saturated dq model without compensation for slotting and iron loss influences is investigated. The model predictions for the machine torque characteristics at various speeds including standstill, and the effects of using simplified control strategies are verified by experimental results. Issues related to the low cost software implementation of the control algorithm are discussed. Quality speed control of an axially-laminated machine has been achieved",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=790910,no,undetermined,0
Extraction of type style based meta-information from imaged documents,"Extraction of some meta-information from printed documents without an OCR approach is considered. It can be statistically verified that important terms in articles are printed in italic, bold and all capital style. Detection of these type styles helps in automatic extraction of the lines containing titles, authors' names, subtitles, references as well as sentences having important terms occurring in the text. It also helps in improving the OCR performance for reading the italic text. Some experimental results on the performance of the approach on good quality as well as degraded document images are presented",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=791794,no,undetermined,0
Multifont classification using typographical attributes,"This paper introduces a multifont classification scheme to help with the recognition of multifont and multisize characters. It uses typographical attributes such as ascenders, descenders and serifs obtained from a word image. The attributes are used as an input to a neural network classifier to produce the multifont classification results. It can classify 7 commonly used fonts for all point sizes from 7 to 18. The approach developed in this scheme can handle a wide range of image quality even with severely touching characters. The detection of the font can improve character segmentation as well as character recognition because the identification of the font provides information on the structure and typographical design of characters. Therefore, this multifont classification algorithm can be used for maintaining good recognition rates of a machine printed OCR system regardless of fonts and sizes. Experiments have shown that font classification accuracies reach high performance levels of about 95 percent even with severely touching characters. The technique developed for the selected 7 fonts in this paper can be applied to any other fonts",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=791797,no,undetermined,0
Hybrid modeling for testing intelligent software for Lunar-Mars closed life support,"Intelligent software is being developed for closed life support systems with biological components, for human exploration of the moon and Mars. The intelligent software functions include planning/scheduling, reactive discrete control and sequencing, management of continuous control, and fault detection, diagnosis, and management of failures and errors. The CONFIG modeling and simulation tool has been used to model components and systems involved in production and transfer of oxygen and carbon dioxide gas in a plant growth chamber and between that chamber and a habitation chamber with physico-chemical systems for gas processing. CONFIG is a multi-purpose discrete event simulation tool that integrates all four types of models, for use throughout the engineering and operations life cycle. Within CONFIG, continuous algebraic quantitative models are used within an abstract discrete event qualitative modeling framework of component modes and activity phases. Component modes and activity phases are embedded in transition digraphs. Flows and flow reconfigurations are efficiently analyzed during simulations. Modeled systems for the Life Support Testbed have included biological plants and crew members, oxygen concentrators and an oxygen transfer control subsystem, and injectors and flow controllers in a carbon dioxide control subsystem. To test the discrete control software, some elements of the lower level control layer and higher level planning layer of the intelligent software architecture are modeled, using CONFIG activity models. CONFIG simulations show effects of events on a system, including control action or failures, local and remote effects, and behavioral and functional effects, the time course of effects, and how effects may be detected. The CONFIG models are interfaced to the discrete control software layer and used to perform dynamic interactive testing of the software in nominal and off-nominal scenarios",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=792447,no,undetermined,0
Condition monitoring and fault diagnosis of electrical machines-a review,"Research has picked up a fervent pace in the area of fault diagnosis of electrical machines. Like adjustable speed drives, fault prognosis has become almost indispensable. The manufacturers of these drives are now keen to include diagnostic features in the software to decrease machine down time and improve salability. Prodigious improvement in signal processing hardware and software has made this possible. Primarily, these techniques depend upon locating specific harmonic components in the line current, also known as motor current signature analysis (MCSA). These harmonic components are usually different for different types of faults. However with multiple faults or different varieties of drive schemes, MCSA can become an onerous task as different types of faults and time harmonics may end up generating similar signatures. Thus other signals such as speed, torque, noise, vibration etc., are also explored for their frequency contents. Sometimes, altogether different techniques such as thermal measurements, chemical analysis, etc., are also employed to find out the nature and the degree of the fault. Human involvement in the actual fault detection decision making is slowly being replaced by automated tools such as expert systems, neural networks, fuzzy logic based systems to name a few. Keeping in mind the need for future research, this review paper describes different types of faults and the signatures they generate and their diagnostics' schemes",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=799956,no,undetermined,0
Using statistics of the extremes for software reliability analysis,"This paper investigates the use of StEx (statistics of the extremes) [Castillo, Sarabia 1992; Gumbel 1958; Ang, Tang 1984; Castillo 1988; Castillo, Alvarez, Cobo 1993] for analyzing software failure data for highly reliable systems. StEx classifies most distributions into one of three asymptotic families. The asymptotic family to which a set of data belongs is derived graphically on Gumbel-type probability paper. From the resulting empirical Cdf (cumulative distribution function), the asymptotic family to which the data might belong is hypothesized. Once the asymptotic family for a particular data set is determined, the parameters required for an explicit representation of this asymptotic form are derived using known analytic techniques. Using this approach, actual field data from two software case studies [Musa 1979; Kenney 1993] are analyzed using conventional software reliability growth models and using StEx",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=799901,no,undetermined,0
A separable method for incorporating imperfect fault-coverage into combinatorial models,"This paper presents a new method for incorporating imperfect FC (fault coverage) into a combinatorial model. Imperfect FC, the probability that a single malicious fault can thwart automatic recovery mechanisms, is important to accurate reliability assessment of fault-tolerant computer systems. Until recently, it was thought that the consideration of this probability necessitated a Markov model rather than the simpler (and usually faster) combinatorial model. SEA, the new approach, separates the modeling of FC failures into two terms that are multiplied to compute the system reliability. The first term, a simple product, represents the probability that no uncovered fault occurs. The second term comes from a combinatorial model which includes the covered faults that can lead to system failure. This second term can be computed from any common approach (e.g. fault tree, block diagram, digraph) which ignores the FC concept by slightly altering the component-failure probabilities. The result of this work is that reliability engineers can use their favorite software package (which ignores the FC concept) for computing reliability, and then adjust the input and output of that program slightly to produce a result which includes FC. This method applies to any system for which: the FC probabilities are constant and state-independent; the hazard rates are state-independent; and an FC failure leads to immediate system failure",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=799898,no,undetermined,0
"End-system architecture for distributed networked multimedia applications: issues, trends and future directions","The emerging broadband computer networks (wired and wireless) are likely to have numerous multimedia applications, such as videoconferencing, interactive games, and collaborative computing standard features in desktop PCs and portables. In recent years there have been significant developments in the field of multimedia coding algorithms and their VLSI implementations along with the development of high speed networking technology (e.g., ATM, FDDI, fast Ethernet). But the end-system architecture (software and hardware) as a whole is lagging behind these advancements. Optimized hardware and software architectures are needed for end systems that will guarantee predictable performance of real-time multimedia applications according to user provided QoS. In this paper, we identify the major end-system hardware and software requirements for networked multimedia applications. Focusing mainly on hardware and operating system levels, we advocate an integrated end-system architecture rather than some ad-hoc solutions for real-time multimedia computing, in addition to general purpose computing in a distributed environment. A core-based system architecture is proposed. We observe that interfacing with high speed networks, inter-device level data transfer, power efficiency and in addition, system resource utilization are some of the issues that need serious consideration in an end-system architecture",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=799573,no,undetermined,0
Efficient coding of homogeneous textures using stochastic vector quantisation and linear prediction,"Vector quantisation (VQ) has been extensively used as an effective image coding technique. One of the most important steps in the whole process is the design of the codebook. The codebook is generally designed using the LBG algorithm which uses a large training set of empirical data that is statistically representative of the images to be encoded. The LBG algorithm, although quite effective for practical applications, is computationally very expensive and the resulting codebook has to be recalculated each time the type of image to be encoded changes. Stochastic vector quantisation (SVQ) provides an alternative way for the generation of the codebook. In SVQ, a model for the image is computed first, and then the codewords are generated according to this model and not according to some specific training sequence. The SVQ approach presents good coding performance for moderate compression ratios and different type of images. On the other hand, in the context of synthetic and natural hybrid coding (SNHC), there is always need for techniques which may provide very high compression and high quality for homogeneous textures. A new stochastic vector quantisation approach using linear prediction which is able to provide very high compression ratios with graceful degradation for homogeneous textures is presented. Owing to the specific construction of the method, there is no block effect in the synthetised image. Results, implementation details, generation of the bit stream and comparisons with the verification model of MPEG-4 are presented which prove the validity of the approach. The technique has been proposed as a still image coding technique in the SNHC standardisation group of MPEG",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=799045,no,undetermined,0
Using a proportional hazards model to analyze software reliability,"Proportional hazards models (PHMs) are proposed for the analysis of software reliability. PHMs facilitate the merging of two research directions that have to a large extent developed independently-defect modeling based on software static analyses and reliability growth modeling based on dynamic assumptions about the software failure process. Determinants of software reliability include a composite measure of software complexity, software development volatility as measured by non-defect changes, and cumulative testing effort. A PHM is developed using execution time-between-failure data for a collection of subsystems from two software projects. The PHM analysis yields non-parametric estimates of the baseline hazard functions for each of the projects and parametric estimates of the determinants of software reliability. Weibull curves are shown to provide a good fit to the non-parametric estimates of the baseline hazard functions. These curves are used to extrapolate the non-parametric estimates for times between failure to infinity in order to compute the mean time between failures. Failure curves are generated for each of the subsystems as a function of the cumulative project execution times and summed over the subsystems to obtain project failures vs. cumulative project execution times. These estimated project failure curves track the empirical project failure curves quite well. Project failure curves estimated for the case when no non-defect changes are made show that in excess of 50% of failures can be attributed to non-defect changes",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=798487,no,undetermined,0
Application of a new laser scanning pattern wafer inspection tool to leading edge memory and logic applications at Infineon Technologies,"A new patterned wafer laser-based inspection tool has been introduced to the market place, incorporating double darkfield laser scanning technology. Developed from a well-known production-proven platform, the new system is intended to provide the sensitivity required for 0.18 Î¼m design rules, with extendibility to 0.13 Î¼m. The inspection technology combines low angle laser illumination with dual darkfield scattered light collection channels. Enhancements to the illumination and collection optics have allowed for improved defect sensitivity and capture; and enhanced software algorithms have provided greater compensation for process variation, further increasing defect capture. The sensitivity performance and production worthiness of the tool were evaluated at Siemens Microelectronics Centre and the key results are presented. Both memory and logic products were evaluated, including memory products with 0.2 Î¼m design rule, and logic products with 0.2 Î¼m design rule. Layers from the front-end and back-end of the manufacturing process were evaluated. On memory products, sensitivity to defects occurring during capacitor and isolation trench formation was demonstrated, including etch defects deep in the trench structures and sub 0.1 Î¼m discrepancies in the formation of isolation trenches. Results from post-metal etch inspection demonstrated enhanced sensitivity in both array and periphery regions, largely achieved by exploiting the new ability to perform region-based optimisation, allowing full die area inspections. On logic products, surface foreign material less than 0.1 Î¼m in diameter was detected amidst logic structures and, in the same inspection pass, etch residuals affecting the memory cache area were also captured. The machine was installed and operated in a high capacity wafer production environment and adhered to all specified throughput, up-time and reliability matrices throughout the evaluation period",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=798253,no,undetermined,0
Influence of software on fault-tolerant microprocessor control system dependability,"The fault tolerance of the control system in this paper is based on cold standby and software redundancy. The control system is a microprocessor modular system. It consists of standard nonredundant modules and specific additional modules. The fault-tolerant control system performs the validation of the active microprocessor unit and the actualization of the spare unit. In the case of the active unit failure, the control from the failed active unit is switched to the spare unit. The fault detection is based on a watchdog timer, checksums, data duplication, etc. The evaluation of the fault tolerance of this control system is performed by fault injection into the active unit bus lines performed by a fault injection system that records the tested system reaction. The control system is tested for nonredundant and redundant software. The influence of the watchdog period and actualization of standby unit period on the number of detected faults in the active and the standby unit is observed. This experimental method is therefore one of the ways to compare the quality of some fault-tolerant methods. The fault tolerance of the control system could be easily increased by using software redundancy and software methods",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=796866,no,undetermined,0
DSP based OFDM demodulator and equalizer for professional DVB-T receivers,"This work treats the design of base-band processing subsystems for professional DVB-T receivers using 1600 MIPS, fixed point DSPs. We show the results about the implementation of OFDM demodulation, channel estimation and equalization functional blocks, for the 2k/8k modes. The adoption of general purpose DSPs provides a great flexibility in the development of different configurations of the receiver. Furthermore it enables the feasibility of adaptive equalization schemes, where the quality of the channel estimation varies according to both channel characteristics and speed of the channel variations. The 16/32 bit fixed point architecture leads to a very low implementation loss, and a careful optimization of the pipeline architecture of the processor allows the receiver to obtain short processing delays. The flexibility of the software approach along with the obtained performance make the proposed implementation very interesting in professional and high-end receivers for interactive, multimedia applications of DVB-T",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=796275,no,undetermined,0
New fast and efficient two-step search algorithm for block motion estimation,"Block motion estimation using full search is computationally intensive. Previously proposed fast algorithms reduce the computation by limiting the number of searching locations. This is accomplished at the expense of less accuracy of motion estimation and gives rise to an appreciably higher mean squared error (MSE) for motion compensated images. We present a new fast and efficient search algorithm for block motion estimation that produces better quality performance and less computational time compared with a three-step search (TSS) algorithm. The proposed algorithms are based on the ideas of the dithering pattern for pixel decimation, multiple-candidate for pixel-decimation-based full search, and center-based distribution of the motion vector. From the experimental results, the proposed algorithm is superior to the TSS in both quality performance (about 0.2 dB) and computational complexity (about half)",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=795049,no,undetermined,0
An experiment to improve cost estimation and project tracking for software and systems integration projects,"It is becoming increasingly difficult to predict the resources, costs, and time scales for the integration of software and systems built from components supplied by third parties. Many cost models use the concept of product size as the prime driver for cost estimation but our experience has shown that the supplied quality of components and the required quality of the integrated systems are becoming the dominant factors affecting the costs and time scales of many projects today. ICL has undertaken an experiment using an alternative life cycle model, known as the Cellular Manufacturing Process Model (CMPM), which describes how a product is integrated from its constituent components. The experiment has helped to develop a method and sets of metrics to improve cost estimation and project tracking",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=794779,no,undetermined,0
A case for relative differentiated services and the proportional differentiation model,"Internet applications and users have very diverse quality of service expectations, making the same-service-to-all model of the current Internet inadequate and limiting. There is a widespread consensus today that the Internet architecture has to extended with service differentiation mechanisms so that certain users and applications can get better service than others at a higher cost. One approach, referred to as absolute differentiated services, is based on sophisticated admission control and resource reservation mechanisms in order to provide guarantees or statistical assurances for absolute performance measures, such as a minimum service rate or maximum end-to-end delay. Another approach, which is simpler in terms of implementation, deployment, and network manageability, is to offer relative differentiated services between a small number of service classes. These classes are ordered based on their packet forwarding quality, in terms of per-hop metrics for the queuing delays and packet losses, giving the assurance that higher classes are better than lower classes. The applications and users, in this context, can dynamically select the class that best meets their quality and pricing constraints, without a priori guarantees for the actual performance level of each class. The relative differentiation approach can be further refined and quantified using the proportional differentiation model. This model aims to provide the network operator with the â€œtuning knobsâ€?for adjusting the quality spacing between classes, independent of the class loads. When this spacing is feasible in short timescales, it can lead to predictable and controllable class differentiation, which ore two important features for any relative differentiation model. The proportional differentiation model can be approximated in practice with simple forwarding mechanisms (packet scheduling and buffer management) that we describe",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=793688,no,undetermined,0
Real-time video coding,"Many modified three-step search (TSS) algorithms have been studied for the speed up of computation and improved error performance over the original TSS algorithm. In this work, an efficient and fast TSS algorithm is proposed, which is based on the unimodal error search assumption (UESA), error surface properties, the matching error threshold and the partial sum of the matching error. For the search strategy, we propose a new and efficient search method, which shows a good performance in terms of the computational reduction and the prediction error compared with other search algorithms. Also, we add half-stop algorithms to the above algorithm with little degradation of the predicted image quality while obtaining more computational reduction. One of them is based on the assumption that if a small amount of motion compensation error is produced, we can consider the matching block as a matched block and the motion vector as a global one. The other removes the computational redundancy by stopping the useless calculation of the matching error in a matching block. With the added algorithms, we can reduce significantly the computation for the motion vector with a small degradation of the predicted image quality with a proper threshold. Experimentally, it is shown that the proposed algorithm is very efficient in terms of the speed up of the computation and error performance compared with other conventional modified TSS algorithms",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=793427,no,undetermined,0
JSF prognostics and health management,"JSF Prognostics and Health Management (PHM) is a comprehensive system for detecting and isolating failures as well as predicting remaining useful life for critical components. The PHM system is a hierarchical distribution of data collection and information management elements, both on-board and off-board that make maximum use of conventional failure symptom-detecting techniques combined with advanced software modeling to achieve excellent failure detection and isolation with zero false alarms. This same system collects and processes performance information on critical components to enable prediction of remaining useful life for those components. The information processed and managed on-board the aircraft enhances the pilot's knowledge of his remaining capabilities in the event of malfunction and, at the same time, triggers the Autonomic Logistics processes by relaying failure information to the ground. The use of on-board processing resources coupled with the Autonomic Logistics System provides operations and support cost savings over legacy aircraft while providing the Warfighter with superb system management capabilities",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=793190,no,undetermined,0
Quality improvement in switching-system software,"The paper describes the effects on switching system software quality of development and maintenance activities. The effect of the application of object oriented methods and quantitative quality control methods in the development of switching system programs is evaluated by comparing the number of errors after shipping to that of earlier systems. Effective approaches to further improving the quality have been developed by analyzing error data obtained during development and maintenance. The main findings are as follows. (1) Although the error density in switching programs, which include call processing and system management functions, is the same as that in maintenance programs, errors in the switching programs are more likely to have a serious impact on the system when the errors are not discovered. Thus, quality control management that takes the potential impact of errors into account will more effectively improve the total software quality. (2) To increase the error detection rate in each testing phase, the omission of test items must be prevented by managing the correspondence of test items between each testing phase, and by supporting the generation of test items from the system specifications",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=792633,no,undetermined,0
Software quality maintenance model,"We develop a quality control and prediction model for improving the quality of software delivered by development to maintenance. This model identifies modules that require priority attention during development and maintenance. The model also predicts during development the quality that will be delivered to maintenance. We show that it is important to perform a marginal analysis when making a decision about how many metrics to include in a discriminant function. If many metrics are added at once, the contribution of individual metrics is obscured. Also, the marginal analysis provides an effective rule for deciding when to stop adding metrics. We also show that certain metrics are dominant in their effects on classifying quality and that additional metrics are not needed to increase the accuracy of classification. Data from the Space Shuttle flight software are used to illustrate the model process",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=792625,no,undetermined,0
Artificial immune systems in industrial applications,"Artificial immune system (AIS) is a new intelligent problem-solving technique that is being used in some industrial applications. This paper presents an immunity-based algorithm for tool breakage detection. The method is inspired by the negative-selection mechanism of the immune system, which is able to discriminate between the self (body elements) and the nonself (foreign pathogens). However, in our industrial application, the self is defined to be normal cutting operations and the nonself is any deviation beyond allowable variations of the cutting force. The proposed algorithm is illustrated with a simulation study of milling operations. The performance of the algorithm in detecting the occurrence of tool breakage is reported. The results show that the negative-selection algorithm detected tool breakage in all test cases",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=792486,no,undetermined,0
Analysis of fuzzy logic and autoregressive video source predictors using T-tests,"This paper presents performance comparison of a fuzzy logic predictor and an autoregressive predictor. Multi-step ahead predictions were made on the traffic intensity of digital video sources coded with an MPEG coder (hybrid motion compensation/differential pulse code modulation/discrete cosine transform method). Although current coding standards recommend constant bit rate (CBR) output by means of a smoothing buffer, the method inherently produces variable bit rate (VBR) output, and VBR transmission is necessary for high quality delivery. Prediction results described in this paper have been obtained using a fuzzy logic prediction scheme, and the conventional linear autoregressive (AR) algorithm. Prediction errors are analysed with low order statistical parameters and hypothesis tests. The proposed fuzzy prediction methods offer higher accuracy and can be applied to the development of usage parameter control (UPC), connection admission control (CAC) and congestion control algorithms in ATM networks",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=818101,no,undetermined,0
Finds in testing experiments for model evaluation,"To evaluate the fault location and the failure prediction models, simulation-based and codebased experiments were conducted to collect the required failure data. The PIE model was applied to simulate failures in the simulation-based experiment. Based on syntax and semantic level fault injections, a hybrid fault injection model is presented. To analyze the injected faults, the difficulty to inject (DTI) and difficulty to detect (DTD) are introduced and are measured from the programs used in the code-based experiment. Three interesting results were obtained from the experiments: 1) Failures simulated by the PIE model without consideration of the program and testing features are unreliably predicted; 2) There is no obvious correlation between the DTI and DTD parameters; 3) The DTD for syntax level faults changes in a different pattern to that for semantic level faults when the DTI increases. The results show that the parameters have a strong effect on the failures simulated, and the measurement of DTD is not strict.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6076037,no,undetermined,0
Software-implemented fault detection for high-performance space applications,"We describe and test a software approach to overcoming radiation-induced errors in spaceborne applications running on commercial off-the-shelf components. The approach uses checksum methods to validate results returned by a numerical subroutine operating subject to unpredictable errors in data. We can treat subroutines that return results satisfying a necessary condition having a linear form; the checksum tests compliance with this condition. We discuss the theory and practice of setting numerical tolerances to separate errors caused by a fault from those inherent infinite-precision numerical calculations. We test both the general effectiveness of the linear fault tolerant schemes we propose, and the correct behavior of our parallel implementation of them",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=857522,no,undetermined,0
"Adaptive and automated detection of service anomalies in transaction-oriented WANs: network analysis, algorithms, implementation, and deployment","Algorithms and software for proactive and adaptive detection of network/service anomalies (i.e., performance degradations) have been developed, implemented, deployed, and field-tested for transaction-oriented wide area networks (WANs). A real-time anomaly detection system called TRISTAN (transaction instantaneous anomaly notification) has been implemented, and is deployed in the commercially important AT&T transaction access services (TAS) network. TAS is a high volume, multiple service classes, hybrid telecom and data WAN that services transaction traffic in the U.S. and neighboring countries. TRISTAN adaptively and preactively detects network/service performance anomalies in multiple-service-class-based and transaction-oriented networks, where performances of service classes are mutually dependent and correlated, where environmental factors (e.g., nonmanaged or nonmonitored equipment within customer premises) can strongly impact network and service performances. Specifically, TRISTAN implements algorithms that: 1) sample and convert raw transaction records to service-class based performance data in which potential network anomalies are highlighted; 2) automatically construct adaptive and service-class-based performance thresholds from historical transaction records for detecting network and service anomalies; and 3) perform real-time network/service anomaly detection. TRISTAN is demonstrated to be capable of proactively detecting network/service anomalies, which easily elude detection by the traditional alarm-based network monitoring systems.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=842990,no,undetermined,0
A short circuit current study for the power supply system of Taiwan railway,"The western Taiwan railway consists mainly of a mountain route and ocean route. Taiwan Railway Administration (TRA) has conducted a series of experiments on the ocean route to identify the possible causes of unknown events which frequently cause trolley contact wires meltdown. The tests conducted include the short circuit fault test within the power supply zone of the Ho Long Substation (Zhu Nan to Tong Xiao) that had the highest probability for the meltdown events. Those test results based on the actual measured maximum short circuit current provide a valuable reference for TRA when comparing against the said events. Le Blanc transformer is the main transformer of the Taiwan railway electrification system. The Le Blanc transformer mainly transforms the Taiwan Power Company (TPC) generated three phase alternating power supply system (69 kV, 60 Hz) into two single phase alternating power distribution systems (M phase and T phase) (26 kV, 60 Hz) needed for the trolley traction. As a unique winding connection transformer, the conventional software for fault analysis will not be able to simulate its internal current and phase difference between each phase currents. Therefore, besides extracts of the short circuit test results, this work presents an EMTP model based on Taiwan Railway Substation equivalent circuit model with Le Blanc transformer. The proposed circuit model can simulate the same short circuit test to verify the actual fault current and accuracy of the equivalent circuit model. Moreover, the maximum short circuit current is further evaluated with reference to the proposed equivalent circuit",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=850083,no,undetermined,0
Diagnosis of a continuous dynamic system from distributed measurements,A diagnosis application has been built for a three-tank fluid system. The tank system is equiped with a distributed measurement and control system based on smart transducer nodes with embedded computing and networking capabilities that use the IEEE 1451.1 object model to provide high level sensing and actuation abstractions. The diagnosis system operates on-line on a workstation that-appears on the network as another transducer node. The diagnosis methodology has several aspects that allow distribution of the monitoring and diagnosis functionality on a network of embedded processors. The current application represents the initial phase in building a truly distributed monitoring and diagnosis application,2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=848797,no,undetermined,0
A novel automated measurement and control system based on VXIbus for I&D logic analysis,"This paper describes an instrument system which can be easily applied in instrument and data (I&D) simulation and faults detection, especially tracing some minute errors within hardware or software. This novel automated measurement and testing system mainly includes: the control and DSP module, data emulation and generation module and logic analysis module. The set of simulation and testing system is based on latest VXIbus standard and designed as message based and register based C-size modules covering several sophisticated techniques of direct digital synthesis in the secondary interface. After the frame description of test and measurement architectures, this paper details the hardware techniques and software strategies as well as simulation results",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=848698,no,undetermined,0
A skeleton-based approach for the design and implementation of distributed virtual environments,"It has long been argued that developing distributed software is a difficult and error-prone activity. Based on previous work on design patterns and skeletons, this paper proposes a template-based approach for the high-level design and implementation of distributed virtual environments (DVEs). It describes a methodology and its associated tool, which includes a user interface, a performance analyser and an automatic code generation facility. It also discusses some preliminary results on a surgical training system",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=847846,no,undetermined,0
A quality control method for nuclear instrumentation and control systems based on software safety prediction,"In the case of safety-related applications like nuclear instrumentation and control (NI&C), safety-oriented quality control is required. The objective of this paper is to present a software safety classification method as a safety-oriented quality control tool. Based on this method, we predict the risk (and thus safety) of software items that are at the core of NI&C systems. Then we classify the software items according to the degree of the risk. The method can be used earlier than at the detailed design phase. Furthermore, the method can also be used in all the development phases without major changes. The proposed method seeks to utilize the measures that can be obtained from the safety analysis and requirements analysis. Using the measures proved to be desirable in a few aspects. The authors have introduced fuzzy approximate reasoning to the classification method because experts' knowledge covers the vague frontiers between good quality and bad quality with linguistic uncertainty and fuzziness. Fuzzy Colored Petri Net (FCPN) is introduced in order to offer a formal framework for the classification method and facilitate the knowledge representation, modification, or verification. Through the proposed quality control method, high-quality NI&C systems can be developed effectively and used safely",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=846274,no,undetermined,0
Flexible processing framework for online event data and software triggering,"The BABAR experiment is the particle detector at the new PEP-II â€œB factoryâ€?facility at the Stanford Linear Accelerator Center. The experiment's goal is the detailed study of the fundamental phenomenon of charge-parity symmetry breaking in B meson decays. Within the detector's online system, the Online Event Processing subsystem (OEP) provides for near real-time processing of data delivered from the data acquisition system's event builder. It supports four principal tasks: the final â€œLevel 3â€?software trigger, rapid-feedback fast data quality monitoring, event displays, and the final steps in online calibrations. In order to accommodate the expected 2000 Hz event rate, the OEP subsystem runs on computing farm, receiving events on each node into a shared memory buffer system and coordinating processes performing analysis on this data. Analysis tasks are assigned administrative characteristics providing for protection of data integrity and ensuring that indispensable tasks such as triggering are always performed, while others are performed as resources are available. Data access in OEP uses BABAR's standard offline reconstruction framework. Thus, notably, the Level 3 trigger software could be developed in advance in the full offline simulation environment and yet be suitable for near-real-time online running with minimal changes",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=846180,no,undetermined,0
Modeling and analysis of software aging and rejuvenation,"Software systems are known to suffer from outages due to transient errors. Recently, the phenomenon of â€œsoftware agingâ€? one in which the state of the software system degrades with time, has been reported. To counteract this phenomenon, a proactive approach of fault management, called â€œsoftware rejuvenationâ€? has been proposed. This essentially involves gracefully terminating an application or a system and restarting it in a clean internal state. We discuss stochastic models to evaluate the effectiveness of proactive fault management in operational software systems and determine optimal times to perform rejuvenation, for different scenarios. The latter part of the paper deals with measurement-based methodologies to detect software aging and estimate its effect on various system resources. Models are constructed using workload and resource usage data collected from the UNIX operating system over a period of time. The measurement-based models are intended to help development of strategies for software rejuvenation triggered by actual measurements",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=844925,no,undetermined,0
A comparison of mobile agent and client-server paradigms for information retrieval tasks in virtual enterprises,"In next-generation enterprises it will become increasingly important to retrieve information efficiently and rapidly from widely dispersed sites in a virtual enterprise, and the number of users who wish to do using wireless and portable devices will increase significantly. We consider the use of mobile agent technology rather than traditional client-server computing for information retrieval by mobile and wireless users in a virtual enterprise. We argue that to be successful mobile agent platforms must coexist with, and be presented to the applications programmer side-by-side with, traditional client-server middleware like CORBA and DOOM, and we sketch a middleware architecture for doing so. We then develop an analytical model that examines the claimed performance benefits of mobile agents over client-server computing for a mobile information retrieval scenario. Our evaluation of the model shows that mobile agents are not always better than client-server calls in terms of average response times; they are only beneficial if the space overhead of the mobile agent code is not too large or if the wireless link connecting the mobile user to the fixed servers of the virtual enterprise is error-prone",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=843295,no,undetermined,0
An analytical model for loop tiling and its solution,"The authors address the problem of estimating the performance of loop tiling, an important program transformation for improved memory hierarchy utilization. We introduce an analytical model for estimating the memory cost of a loop nest as a rational polynomial in tile size variables. We also present a constant-time algorithm for finding an optimal solution to the model (i.e., for selecting optimal tile sizes) for the case of doubly nested loops. This solution can be applied to tiling of three loops by performing an iterative search on the value of the first tile size variable, and using the constant-time algorithm at each point in the search to obtain optimal tile size values for the remaining two loops. Our solution is efficient enough to be used in production-quality optimizing compilers, and has been implemented in the IBM XL Fortran product compilers. This solution can also be used by processor designers to efficiently predict the performance of a set of tiled loops for a range of memory hierarchy parameters",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=842294,no,undetermined,0
Sensitivity analysis of modular dynamic fault trees,"Dynamic fault tree analysis, as currently supported by the Galileo software package, provides an effective means for assessing the reliability of embedded computer-based systems. Dynamic fault trees extend traditional fault trees by defining special gates to capture sequential and functional dependency characteristics. A modular approach to the solution of dynamic fault trees effectively applies Binary Decision Diagram (BOD) and Markov model solution techniques to different parts of the dynamic fault tree model. Reliability analysis of a computer-based system tells only part of the story, however. Follow-up questions such as â€œWhere are the weak links in the system?â€? â€œHow do the results change if my input parameters change?â€?and â€œWhat is the most cost effective way to improve reliability?â€?require a sensitivity analysis of the reliability analysis. Sensitivity analysis (often called Importance Analysis) is not a new concept, but the calculation of sensitivity measures within the modular solution methodology for dynamic and static fault trees raises some interesting issues. In this paper we address several of these issues, and present a modular technique for evaluating sensitivity, a single traversal solution to sensitivity analysis for BOD, a simplified methodology for estimating sensitivity for Markov models, and a discussion of the use of sensitivity measures in system design. The sensitivity measures for both the Binary Decision Diagram and Markov approach presented in this paper is implemented in Galileo, a software package for reliability analysis of complex computer-based systems",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=839462,no,undetermined,0
Real-time image on QoS Web,"Digital images have become a dominant information source on the Web. Emerging Web-enabled applications, such as global collaboration in environmental studies, point and click manufacturing, and electronic commerce, to name a few, push for timely processing and transmission of images. This real-time imaging is much more than variations on image processing without regard to time. Its performance requirement on networking is beyond the capability of the current Internet, which provides a best-effort service model compromised with end-system traffic management. For the Web to support real-time images, we need to understand real-time image features, design a new Web service model, and convert a user's application requirement into a system service agreement. This paper presents a Web architecture that provides predictable and different service levels for specific quality of service (QoS) requirements, called QoS Web. We examine the Web image features in depth, which leads to a generic formula describing a user's preferences for image quality and timing constraints. It then maps onto the QoS requirements as resource parameters expected from the Web. The influence of the QoS settings on the perceived image performance is analyzed in theory and tested with experimental simulation. We present a procedure for users to specify their requirements to the QoS Web for real-time image transfer. In addition, an inverse mapping mechanism is included for re-negotiation when there is a shortage of network resources or the user's requirements change",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=842284,no,undetermined,0
A novel fuzzy logic system based on N-version programming,"For the consideration of different application systems, modeling the fuzzy logic rule, and deciding the shape of membership functions are very critical issues due to they play key roles in the design of fuzzy logic control system. This paper proposes a novel design methodology of fuzzy logic control system using the neural network and fault-tolerant approaches. The connectionist architecture with the learning capability of neural network and N-version programming development of a fault-tolerant technique are implemented in the proposed fuzzy logic control system. In other words, this research involves the modeling of parameterized membership functions and the partition of fuzzy linguistic variables using neural networks trained by the unsupervised learning algorithms. Based on the self-organizing algorithm, the membership function and partition of fuzzy class are not only derived automatically, but also the preconditions of fuzzy IF-THEN rules are organized. We also provide two examples, pattern recognition and tendency prediction, to demonstrate that the proposed system has a higher computational performance and its parallel architecture supports noise-tolerant capability. This generalized scheme is very satisfactory for pattern recognition and tendency prediction problems",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=842150,no,undetermined,0
Application of genetic algorithms to pattern recognition of defects in GIS,"A computerized pattern recognition system based on the analysis of phase resolved partial discharge (PRPD) measurements, and utilizing genetic algorithms, is presented. The recognition system was trained to distinguish between basic types of defects appearing in gas-insulated system (GIS), such as voids in spacers, moving metallic particles, protrusions on electrodes, and floating electrodes. The classification of defects is based on 60 measurement parameters extracted from PRPD patterns. Classification of defects appearing in GIS installations is performed using the Bayes classifier combined with genetic algorithms and is compared to the performance of the other classifiers, including minimal-distance, percent score and polynomial classifiers. Tests with a reference database of more than 600 individual measurements collected during laboratory experiments gave satisfactory results of the classification process",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=841804,no,undetermined,0
Efficient free-energy calculations by the simulation of nonequilibrium processes,"Discusses a particular application of computational physics: the calculation of thermodynamic properties by computer simulation methods. More specifically, we focus on one of the most difficult tasks in this context: the estimation of free energies in problems such as the study of phase transformations in solids and liquids, the influence of lattice defects on the mechanical properties of technological materials, the kinetics of chemical reactions, and protein-folding mechanisms in biological processes. Because the free energy plays a fundamental role, the development of efficient and accurate techniques for its calculation has attracted considerable attention during the past 15 years and is still an active field of research. We discuss some general aspects of equilibrium and nonequilibrium approaches to free-energy measurement and present the â€œreversible scalingâ€?technique which we have recently developed",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=841802,no,undetermined,0
Diagnosing rediscovered software problems using symptoms,"This paper presents an approach to automatically diagnosing rediscovered software failures using symptoms, in environments in which many users run the same procedural software system. The approach is based on the observation that the great majority of field software failures are rediscoveries of previously reported problems and that failures caused by the same defect often share common symptoms. Based on actual data, the paper develops a small software failure fingerprint, which consists of the procedure call trace, problem detection location, and the identification of the executing software. The paper demonstrates that over 60 percent of rediscoveries can be automatically diagnosed based on fingerprints; less than 10 percent of defects are misdiagnosed. The paper also discusses a pilot that implements the approach. Using the approach not only saves service resources by eliminating repeated data collection for and diagnosis of reoccurring problems, but it can also improve service response time for rediscoveries",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=841113,no,undetermined,0
SAABNet: Managing qualitative knowledge in software architecture assessment,"Quantitative techniques have traditionally been used to assess software architectures. We have found that early in development process there is often insufficient quantitative information to perform such assessments. So far the only way to make qualitative assessments about an architecture, is to use qualitative assessment techniques such as peer reviews. The problem with this type of assessment is that they depend on the techniques knowledge of the expert designers who use them. In this paper we introduce a technique, SAABNet (Software Architecture Assessment Belief Network), that provides support to make qualitative assessments of software architectures",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=839860,no,undetermined,0
End-to-end predictability in real-time push-pull communications,"Push-pull communications is a real-time middleware service that has been implemented on top of a resource kernel operating system. It is a many-to-many communication model that can support multi-participant real-time applications. It covers both â€œpushâ€?(publisher/subscriber model) and â€œpullâ€?(data transfer initiated by a receiver) communications. Unlike the publisher/subscriber model, different publishers and subscribers can operate at different data rates and also can choose another (intermediate) node to act as their proxy and deliver data at their desired frequency. We specifically address end-to-end predictability of the push-pull model. The scheduling mechanisms in the OS, the middleware architecture and the underlying network QoS support can impact the timeliness of data. We obtain our end-to-end timeliness and bandwidth guarantees by using a resource kernel offering CPU reservations and the use of a guaranteed bandwidth network (DARWIN) between push-pull end-points. We formally analyze the problem of choosing an optimal proxy location within a network. We discuss our implementation of this system and carry out a detailed performance evaluation on an integrated RT-Mach-Darwin testbed at Carnegie Mellon. Our results open up interesting research directions for the scheduling of computation and communication resources for the applications using the push-pull service. The push-pull framework can easily be incorporated in an RT-CORBA Event Service model",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=839541,no,undetermined,0
Scheduling solutions for supporting dependable real-time applications,"This paper deals with tolerance to timing faults in time-constrained systems. TAFT (Time Aware Fault-Tolerant) is a recently devised approach which applies tolerance to timing violations. According to TAFT, a task is structured in a pair, to guarantee that deadlines are met (although possibly offering a degraded service) without requiring the knowledge of task attributes difficult to estimate in practice. Wide margin of actions is left by the TAFT approach in scheduling the task pairs, leading to disparate performances; up to now, poor attention has been devoted to analyse this aspect. The goal of this work is to investigate on the most appropriate scheduling policies to adopt in a system structured in the TAFT fashion, in accordance with system conditions and application requirements. To this end, all experimental evaluation will be conducted based on a variety of scheduling policies, to derive useful indications for the system designer about the most rewarding policies to apply",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=839519,no,undetermined,0
The future growth trend of neutral currents in three-phase computer power systems caused by voltage sags,"This paper presents a summary of the power quality related concerns associated with the applications of the future growth trend of neutral currents in three-phase computer power systems. These concerns include power system harmonics and neutral current caused by voltage sags and short interruption. The neutral current characteristics under that condition are described. Methods for identifying these problems, analysis, determining their impact on utility and customer are also described. The EMTP simulation, field measurement and experimental results are used to verify the disturbance problems. The applications of regression model to predict neutral current growth trend during power line disturbances from measured data is proposed",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=850176,no,undetermined,0
Grading and predicting networked application behaviour,"The user-perceived quality of an application operating over a communication network has a considerable influence on the usefulness of that application. Intuitively, it may be assumed that this quality will be related to the current network performance, but in practice the relationship is often complex and difficult to determine. A scheme has been developed whereby the performance of network applications can be assessed and empirically graded for various controlled network loading conditions. Given the current network loading conditions on an operational network and information generated by the grading process, it is possible to predict the performance of the network application before the application is actually run. This has the potential for reducing the amount of traffic forced on a network as a consequence of aborted connections.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=850841,no,undetermined,0
A MAC protocol with priority splitting algorithm for wireless ATM networks,"This paper deals proposes a medium access control (MAC) protocol for ensuring the quality of service (QoS) of integrated multimedia services on wireless links. The wireless ATM MAC protocol which incorporates dynamical polling, idle uplink (UL) data channel conversion, piggybacking and an interruptable priority splitting algorithm (named THBPSA) for resolving random access collisions is proposed and named TPICC. The effect of the priority splitting algorithm on the performance of the TPICC is simulated and compared with a counterpart of the TPICC which uses an unprioritised binary splitting algorithm (UBSA) in the RA slots. The effect of an invalid polling detecting (IPD) mechanism on the UL bandwidth efficiency is also simulated. The simulation results show that the THBPSA algorithm ensures a smaller medium access delay for realtime traffic classes than for non-realtime traffic classes. Comparing THBPSA with a priority scheduling scheme which is used in the base station (BS) and features packet time stamps, THBPSA provides realtime traffic classes with a much less UL packet delay than non-realtime traffic classes. The UL bandwidth used by the dynamic polling of realtime traffic classes is tolerable",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=851271,no,undetermined,0
An automated profiling subsystem for QoS-aware services,"The advent of QoS-sensitive Internet applications such as multimedia and the proliferation of priced performance-critical applications such as online trading raise a need for building server systems with guaranteed performance. The new applications must run on different heterogeneous platforms, provide soft performance guarantees commensurate with platform capacity and adapt efficiently to upgrades in platform resources over the system's lifetime. Profiling the application for the purposes of providing QoS guarantees on each new platform becomes a significant undertaking. Automated profiling mechanisms must be built to enable efficient computing of QoS guarantees tailored to platform capacity and facilitate wide deployment of soft performance-guaranteed systems on heterogeneous platforms. The article investigates the design of the automated profiling subsystem: an essential component of future â€œgeneral-purposeâ€?QoS-sensitive systems. The subsystem estimates application resource requirements and adapts the software transparently to the resource capacity of the underlying platform. A novel aspect of the proposed profiling subsystem is its use of estimation theory for profiling. Resource requirements are estimated by correlating applied workload with online resource utilization measurements. We focus explicitly on profiling server software. The convergence and accuracy of our online profiling techniques are evaluated in the context of an Apache Web server serving both static Web pages and dynamic content. Results show the viability of using estimation theory for automated online profiling and for achieving QoS guarantees",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=852465,no,undetermined,0
Assessment of the applicability of COTS microprocessors in high-confidence computing systems: a case study,Commercial-off-the-shelf (COTS) components are increasingly used in building high-confidence systems to assure their dependability in an affordable way. The effectiveness of such a COTS-based design critically depends on the design of the COTS components. Their dependability attributes need a thorough understanding and rigorous assessment yet has received relatively little attention. The research presented in this paper investigates the error detection and recovery features of contemporary high-performance COTS microprocessors. A method of assessment is proposed and two state-of-the-art microprocessors are studied and compared,2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=857519,no,undetermined,0
Rerouting for handover in mobile networks with connection-oriented backbones: an experimental testbed,"The rerouting of connections for handover in a broadband mobile cellular network is investigated. We address networks with a connection-oriented backbone, which supports quality-of-service (QoS). Moreover it is assumed an IP-style multicast on top of the connection-oriented network. We advocate to utilize the IP-style multicast in order to reroute connections for handover. Three rerouting schemes are proposed, which are based on the IP-style multicast. One of the schemes realizes a predictive approach, where a call is pre-established to potential new base stations by setting up a multicast tree with neighboring base stations as leaf nodes. This approach reduces the handover latency caused by rerouting to an absolute minimum and improves communication with strict time constraints and frequent handover. For experimental investigations of the rerouting schemes a testbed is set up, which is based on an open, nonproprietary, experimental networking environment. Its signaling software offers a direct many-to-many communication, rather than requiring one-to-many overlays and the switching hardware supports scalable and efficient multicast by cell-recycling. The testbed is used to prove the feasibility of the approach and for performance evaluation of the rerouting scheme",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=856700,no,undetermined,0
Micro-checkpointing: checkpointing for multithreaded applications,"In this paper we introduce an efficient technique for checkpointing multithreaded applications. Our approach makes use of processes constructed around the ARMOR (Adaptive Reconfigurable Mobile Objects of Reliability) paradigm implemented in our Chameleon testbed. ARMOR processes are composed of disjoint elements (objects) with controlled manipulation of element state. These characteristics of ARMORS allow the process state to be collected during runtime in an efficient manner and saved to disk when necessary. We call this approach micro-checkpointing. We demonstrate micro-checkpointing in the Chameleon testbed, an environment for developing reliable distributed applications. Our results show that the overhead ranges from between 39% to 141% with an aggressive checkpointing policy, depending upon the degree to which the process conforms to our ARMOR paradigm",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=856604,no,undetermined,0
A low-cost power quality meter for utility and consumer assessments,"Power quality has become a major concern, in particular, to the domestic consumers because the reliability, efficiency and liability of their devices very much rely on the quality of the electric supply. However, up to now, there is no cost-effective instrument for power quality measurement. It is partially due to the lacking of a world-wide accepted indicator, power performance index (PPI) in the authors' case, and partially due to the conventionally high cost of power quality measurement equipment. In this paper, a good performance index, PPI, is proposed for the consideration by the power industry. Individual components arriving at the PPI are defined and the method of calculation is explained in detail. Such PPI will also be useful to assess the supply quality of independent power providers, especially for deregulation purposes. All algorithms have been implemented on a cost-effective power quality meter (PQM) while both hardware and software designs are described in this paper",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=855645,no,undetermined,0
Using simulation for assessing the real impact of test-coverage on defect-coverage,"The use of test-coverage measures (e.g., block-coverage) to control the software test process has become an increasingly common practice. This is justified by the assumption that higher test-coverage helps achieve higher defect-coverage and therefore improves software quality. In practice, data often show that defect-coverage and test-coverage grow over time, as additional testing is performed. However, it is unclear whether this phenomenon of concurrent growth can be attributed to a causal dependency, or if it is coincidental, simply due to the cumulative nature of both measures. Answering such a question is important as it determines whether a given test-coverage measure should be monitored for quality control and used to drive testing. Although it is no general answer to this problem, a procedure is proposed to investigate whether any test-coverage criterion has a genuine additional impact on defect-coverage when compared to the impact of just running additional test cases. This procedure applies in typical testing conditions where the software is tested once, according to a given strategy, coverage measures are collected as well as defect data. This procedure is tested on published data, and the results are compared with the original findings. The study outcomes do not support the assumption of a causal dependency between test-coverage and defect-coverage, a result for which several plausible explanations are provided",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=855537,no,undetermined,0
Failure correlation in software reliability models,"Perhaps the most stringent restriction in most software reliability models is the assumption of statistical independence among successive software failures. The authors research was motivated by the fact that although there are practical situations in which this assumption could be easily violated, much of the published literature on software reliability modeling does not seriously address this issue. The research work in this paper is devoted to developing the software reliability modeling framework that can consider the phenomena of failure correlation and to study its effects on the software reliability measures. The important property of the developed Markov renewal modeling approach is its flexibility. It allows construction of the software reliability model in both discrete time and continuous time, and (depending on the goals) to base the analysis either on Markov chain theory or on renewal process theory. Thus, their modeling approach is an important step toward more consistent and realistic modeling of software reliability. It can be related to existing software reliability growth models. Many input-domain and time-domain models can be derived as special cases under the assumption of failure s-independence. This paper aims at showing that the classical software reliability theory can be extended to consider a sequence of possibly s-dependent software runs, viz, failure correlation. It does not deal with inference nor with predictions, per se. For the model to be fully specified and applied to estimations and predictions in real software development projects, we need to address many research issues, e.g., the detailed assumptions about the nature of the overall reliability growth, way modeling-parameters change as a result of the fault-removal attempts",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=855535,no,undetermined,0
Second generation DSP software for picture rate conversion,"Great progress has been made in motion estimation (ME). This has led to a high-quality second-generation scan-rate conversion (SRC) algorithm which runs on a commercially available programmable platform. The new system has a higher quality of motion estimation/compensation and de-interlacing than the first generation. The paper presents the new algorithm, and highlights the advances in performance.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=854598,no,undetermined,0
Scheduling algorithms for dynamic message streams with distance constraints in TDMA protocol,"In many real-time communication applications, predictable and guaranteed timeliness is one of the critical components of the quality of service (QoS) requirements. In this paper, we propose a new real-time message model with both rate requirements and distance constraints. Two algorithms are presented to schedule dynamic real-time message streams in a TDMA (time division multiple access) frame based on different scheduling policies by making greedy choices or optimization choices. The performance of the two algorithms is evaluated and compared in terms of time complexity, acceptance ratio and scheduling jitter via simulation",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=854012,no,undetermined,0
Worst-case execution times analysis of MPEG-2 decoding,"Presents the first worst-case execution times (WCET) analysis of MPEG decoding. Solutions for two scenarios-video-on-demand (VoD) and live-are presented, serving as examples for a variety of real-world applications. A significant reduction of over-estimations (down to 17%, including overheads) during WCET analysis of the live scenario can be achieved by using our new two-phase decoder with built-in WCET analysis, which can be universally applied. It is even possible to predict the exact execution times in the VoD scenario. This work is motivated by the fact that media streaming service providers are under great pressure to fulfil the quality of service promised to their customers, preferably in an efficient way",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=853994,no,undetermined,0
QoS mapping along the protocol stack: discussion and preliminary results,"Quality of service (QoS) mechanisms are needed in integrated networks so that the performance requirements of heterogeneous applications are met. We outline a framework for predicting end-to-end QoS at the application layer based on mapping of QoS guarantees across layers in the protocol stack and concatenation of guarantees across multiple dissimilar sub-networks. Mapping is needed to translate QoS guarantees provided in the lower layers into their effects on upper-layer performance indicators. We illustrate the process with some preliminary results for QoS mapping due to segmentation of packets; we conduct a worst-case analysis, generating a closed-form mapping of delay and losses between two layers. Future extensions of this work will take into account flow aggregation processes and the combination of quantitative and qualitative guarantees",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=853592,no,undetermined,0
An efficient wire sweep analysis solution for practical applications,"Wire sweep is one of the IC packaging defects that CAE tools have been applied for years by many researchers. Meanwhile, various methodologies have been introduced to get better prediction and matching to the experimental measurements. These studies mostly emphasized on the accuracy of the wire sweep calculation. The discussions about analysis efficiency, especially for the high pin-count package (100, 208 leads or more), are seldom touched. This study introduces a practical wire sweep analysis solution not only to meet the need of accuracy but also enhance the efficiency for actual applications. Coupling the design philosophy of these two needs, an in-house wire sweep analysis software (InPack) was developed. This software combines global flow analysis (C-MOLD) and structure analysis (ANSYS) to become a solution for general wire sweep evaluation. The wire geometry modeling procedure is neglected to improve the analytical efficiency. In addition, the multi-layer data requisition criteria is captured to simulate the actual flow conditions around a wire inside the mold cavity. With the Select (Input)-Execute-and-See user interface, the cycle time of wire sweep problem analysis is significantly reduced in practical application",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=853416,no,undetermined,0
Implementing enhanced network maintenance for transaction access services: tools and applications,"We describe the design and implementation of automated methodologies and tools that facilitate the implementation of an enhanced maintenance process in order to provide improved quality of service offered to the customers for the transaction access services (TAS) network. The emphasis of our work is placed on the identification, filtering and correlation of event occurrences that indicate problems that do not necessarily generate alarmed conditions on the conventional network management systems, and on the proactive service/fault management and detection based on dynamically defined violations of the base-lined performance profiles. Such an approach enhances considerably the network management and maintenance process, by identifying possible situations/incidents that may affect the network performance, and by correlating those incidents to the potentially affected elements, services and customer applications",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=853095,no,undetermined,0
2000 IEEE International Conference on Communications. ICC 2000. Global Convergence Through Communications. Conference Record,"The following topics were dealt with: communication theory; equalization and sequence estimation; fading channels; QoS management in wireless networks; signal processing and coding for storage; satellite communication system performance; communications software; network operations and management; multimedia wireless communication; video communications; radio communication channels and systems; space-time coding and processing; synchronization; channel resource allocation; digital signal processing for receivers; multiple access techniques; SATCOM signal design; packet scheduling and discarding; computer communications; turbo codes and iterative decoding; signal detection and estimation; mobility management; advanced signal processing for communications; traffic engineering and modeling; multimedia communications infrastructure; enterprise networking; multiuser detection and interference suppression; personal communications; multiple antennas; CDMA and optical FDM systems; digital subscriber line technology; wireless ATM networks; OFDM and multicarrier systems; network architecture; coding and coded modulation; interference cancellation techniques; WDM networks; resource control in ATM-based networks; routing; communications switching; power control/management; communication quality and reliability; teletraffic and MPLS networks; adaptive channel estimation and equalization; ad hoc networks; switch architecture, design and performance",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=853051,no,undetermined,0
Digital fault location for parallel double-circuit multi-terminal transmission lines,"Two new methods are proposed for fault point location in parallel double-circuit multi-terminal transmission lines by using voltages and currents information from CCVTs and CTs at all terminal. These algorithms take advantage of the fact that the sum of currents flowing into a fault section equals the sum of the currents at all terminals. Algorithm 1 employs an impedance calculation and algorithm 2 employs the current diversion ratio method. Computer simulations are carried out and applications of the proposed methods are discussed. Both algorithms can be applied to all types of fault such as phase-to-ground and phase-to-phase faults. As one equation can be used for all types of fault, classification of fault types and selection of faulted phase are not required. Phase components of the line impedance are used directly, so compensation of unbalanced line impedance is not required",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=852980,no,undetermined,0
Validating the ISO/IEC 15504 measure of software requirements analysis process capability,"ISO/IEC 15504 is an emerging international standard on software process assessment. It defines a number of software engineering processes and a scale for measuring their capability. One of the defined processes is software requirements analysis (SRA). A basic premise of the measurement scale is that higher process capability is associated with better project performance (i.e., predictive validity). The paper describes an empirical study that evaluates the predictive validity of SRA process capability. Assessments using ISO/IEC 15504 were conducted on 56 projects world-wide over a period of two years. Performance measures on each project were also collected using questionnaires, such as the ability to meet budget commitments and staff productivity. The results provide strong evidence of predictive validity for the SRA process capability measure used in ISO/IEC 15504, but only for organizations with more than 50 IT staff. Specifically, a strong relationship was found between the implementation of requirements analysis practices as defined in ISO/IEC 15504 and the productivity of software projects. For smaller organizations, evidence of predictive validity was rather weak. This can be interpreted in a number of different ways: that the measure of capability is not suitable for small organizations or that the SRA process capability has less effect on project performance for small organizations",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=852742,no,undetermined,0
A comprehensive evaluation of capture-recapture models for estimating software defect content,"An important requirement to control the inspection of software artifacts is to be able to decide, based on more objective information, whether the inspection can stop or whether it should continue to achieve a suitable level of artifact quality. A prediction of the number of remaining defects in an inspected artifact can be used for decision making. Several studies in software engineering have considered capture-recapture models to make a prediction. However, few studies compare the actual number of remaining defects to the one predicted by a capture-recapture model on real software engineering artifacts. The authors focus on traditional inspections and estimate, based on actual inspections data, the degree of accuracy of relevant state-of-the-art capture-recapture models for which statistical estimators exist. In order to assess their robustness, we look at the impact of the number of inspectors and the number of actual defects on the estimators' accuracy based on actual inspection data. Our results show that models are strongly affected by the number of inspectors, and therefore one must consider this factor before using capture-recapture models. When the number of inspectors is too small, no model is sufficiently accurate and underestimation may be substantial. In addition, some models perform better than others in a large number of conditions and plausible reasons are discussed. Based on our analyses, we recommend using a model taking into account that defects have different probabilities of being detected and the corresponding Jackknife Estimator. Furthermore, we calibrate the prediction models based on their relative error, as previously computed on other inspections. We identified theoretical limitations to this approach which were then confirmed by the data",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=852741,no,undetermined,0
Validation of an approach for improving existing measurement frameworks,"Software organizations are in need of methods to understand, structure, and improve the data their are collecting. We have developed an approach for use when a large number of diverse metrics are already being collected by a software organization (M.G. Mendonca et al., 1998; M.G. Mendonca, 1997). The approach combines two methods. One looks at an organization's measurement framework in a top-down goal-oriented fashion and the other looks at it in a bottom-up data-driven fashion. The top-down method is based on a measurement paradigm called Goal-Question-Metric (GQM). The bottom-up method is based on a data mining technique called Attribute Focusing (AF). A case study was executed to validate this approach and to assess its usefulness in an industrial environment. The top-down and bottom-up methods were applied in the customer satisfaction measurement framework at the IBM Toronto Laboratory. The top-down method was applied to improve the customer satisfaction (CUSTSAT) measurement from the point of view of three data user groups. It identified several new metrics for the interviewed groups, and also contributed to better understanding of the data user needs. The bottom-up method was used to gain new insights into the existing CUSTSAT data. Unexpected associations between key variables prompted new business insights, and revealed problems with the process used to collect and analyze the CUSTSAT data. The paper uses the case study and its results to qualitatively compare our approach against current ad hoc practices used to improve existing measurement frameworks.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=852739,no,undetermined,0
Reaching efficient fault-tolerance for cooperative applications,"Cooperative applications are widely used, e.g. as parallel calculations or distributed information processing systems. Whereby such applications meet the users demand and offer a performance improvement, the susceptibility to faults of any used computer node is raised. Often a single fault may cause a complete application failure. On the other hand, the redundancy in distributed systems can be utilized for fast fault detection and recovery. So, we followed an approach that is based an duplication of each application process to detect crashes and faulty functions of single computer nodes. We concentrate on two aspects of efficient fault-tolerance-fast fault detection and recovery without delaying the application progress significantly. The contribution of this work is first a new fault detecting protocol for duplicated processes. Secondly, we enhance a roll forward recovery scheme so that it is applicable to a set of cooperative processes in conformity to the protocol",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=839463,no,undetermined,0
Realistic worst-case modeling by performance level principal component analysis,A new algorithm to determine the number and value of realistic worst-case models for the performance of module library components is presented in this paper. The proposed algorithm employs principal components analysis (PCA) at the performance level to identify the main independent sources of variance for the performance of a set of library modules. Response surfaces methodology (RSM) and propagation of variance (POV) based algorithms are used to efficiently compute the performance level covariance matrix and nonlinear maximum likelihood optimization to trace back worst case models at the SPICE level. The effectiveness of the proposed methodology has been demonstrated by determining a realistic set of worst case models for a 0.25 Î¼m CMOS standard cell library,2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=838920,no,undetermined,0
Content based very low bit rate video coding using wavelet transform,"In this paper we present a new hybrid compression scheme for videoconferencing and videotelephony applications at very low bit rates (i.e., 32 kbits/s). In these applications, human face is the most important region within a frame and should be coded with high fidelity. To preserve perceptually important information at low bit rates, such as face regions, skin-tone is used to detect and adaptively quantize these regions. Novel features of this coder are the use of overlapping block motion compensation in combination with discrete wavelet transform, followed by zerotree entropy coding with new scanning procedure of wavelet blocks such that the rest of the H.263 framework can be used. At the same total bit-rate, coarser quantization of the background enables the face region to be quantized finely and coded with higher quality. The simulation results demonstrates comparable objective and superior subjective performance when compared with H.263 video coding standard, while providing the advanced feature like scalability functionalities.",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=822856,no,undetermined,0
Autonomous agents for online diagnosis of a safety-critical system based on probabilistic causal reasoning,"The goal of decentralization in failure detection, identification, and recovery of high-assurance systems is to focus diagnosis on safety-critical components. The goal of probabilistic causal reasoning in diagnosis is to improve performance of fault isolation, However, this reasoning method is dependent on prior reliability knowledge. Our approach aims at focusing the overall diagnostic cycle in two independent ways: first, autonomous agents diagnose high-consequence appliances of a modular manufacturing system and second prior reliability data needed is derived from a quality assurance program. Therefore we present a hybrid technique in combining quality assurance data, failure mode and effects analysis and probabilistic causal reasoning. We develop a dynamic Bayesian network which, given evidence from sensor observations, is able to learn and reason over time. We successfully apply autonomous diagnosis agents in concert with reliability assessment to improve online diagnosis of a pneumatic-mechanical device",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=838436,no,undetermined,0
A resource allocation mechanism to provide guaranteed service to mobile multimedia applications,"We introduce an extended RSVP protocol called Mobile RSVP which support multimedia and real-time traffic in a Mobile IP based environment. Our proposed protocol is appropriate for those mobile nodes that can not a-prior determine their mobility behaviour. However, we have also analysed the effect of a deterministic mobility specification. We also describe the conceptual design of the reservation mechanism of Mobile RSVP and discuss the hand-over procedure. Efforts have been made to analyse the performance of Mobile RSVP and to assess its overhead and control traffic.",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=874001,no,undetermined,0
Performance evaluation of power line filter. A case study,"One of the basic controls of electromagnetic interference (EMI) is a power line filter. The performance of a filter depends on various factors, such as, voltage and current ratings, environmental conditions, mechanical considerations and the like. Filter performance is required to be evaluated in full load conditions rather than no load conditions, since the former can predict the actual loss levels, which the latter cannot. Another factor, which dominates filter performance, is the type of load to which the filter will be exposed in real practice. In this paper an attempt has been made to show that different types of load may result in different loss levels for the filter over the desired frequency spectrum. Comparison of loss levels obtained during the test and also during actual use has also been dealt with.",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=871629,no,undetermined,0
Solving problems with advanced technology,"Ten Years of Air Force experience has demonstrated that advanced NDE makes a quantum improvement in these areas. In NDE, as in many other industrial process control applications the trend is clearly to computers, information technology (IT), and matching of materials with testing methods. This leaves only the strategic decisions to the human. The inspection and information management technologies are particularly effective for rapid fault detection and fault assessment when hidden structural corrosion, hidden structural cracks, composite manufacturing anomalies or disbonding occurs. Fault detection and standardized repair information is readily available in such documentation as the structural manual for standardized repair and the NDI Manual. The use of advanced technology provides many benefits such as: improved aircraft safety, reduction in total operating costs, improved aircraft design and manufacturing processes, improved maintenance practices, and significant reduction of aircraft accident rates. This paper describes the use of X-ray radiography, laser ultrasonics, and neutron radiography as the methods of choice to inspect helicopter rotor blades or any composite part",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=863718,no,undetermined,0
"Performance driven architecture development: issues, methods, tools and paradigms","This paper addresses the process of deriving, implementing, and analyzing complex systems architecture. The top-level concerns that take precedence when performance constraints must be met invert the traditional order and concerns within the design process. This paper presents a non-traditional view of systems architecture development which is motivated by the requirement to produce an architecture capable of predictably meeting its time constraints in addition to all the traditional correctness, quality, and maintainability concerns",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=863696,no,undetermined,0
Flexible processing framework for online event data and software triggering,"The BABAR experiment is the particle detector at the new PEP-II â€œB factoryâ€?facility at the Stanford Linear Accelerator Center. The experiment's goal is the detailed study of the fundamental phenomenon of charge-parity symmetry breaking in B meson decays. Within the detector's online system, the Online Event Processing subsystem (OEP) provides for near-real-time processing of data delivered from the data acquisition system's event builder. It supports four principal tasks: the final â€œLevel 3â€?software trigger, rapid-feedback fast data quality monitoring, event displays, and the final steps in online calibrations. In order to accommodate the expected 2000 Hz event rate, the OEP subsystem runs on computing farm, receiving events on each node into a shared memory buffer system and coordinating processes performing analysis on this data. Analysis tasks are assigned administrative characteristics providing for protection of data integrity and ensuring that indispensable tasks such as triggering are always performed, while others are performed as resources are available. Data access in OEP uses BABAR's standard offline reconstruction framework. Thus, notably, the Level 3 trigger software could be developed in advance in the full offline simulation environment and yet be suitable for near-real-time online running with minimal changes",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=842649,no,undetermined,0
High-level Integrated Design Environment for dependability (HIDE),"For most systems, especially dependable real-time systems for critical applications, an effective design process requires an early validation of the concepts and architectural choices, without wasting time and resources prior of checking whether the system fulfils its objectives or needs some re-design. Although a thorough system specification surely increases the level of confidence that can be put on a system, it is insufficient to guarantee that the system will adequately perform its tasks during its entire life-cycle. The early evaluation of system characteristics like dependability, timeliness and correctness is thus necessary to assess the conformance of the system under development to its targets. This paper presents some activities currently being performed towards an integrated environment for the design and the validation of dependable systems",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=842338,no,undetermined,0
Applying a scalable CORBA event service to large-scale distributed interactive simulations,"Next-generation distributed interactive simulations (DISs) will have stringent quality of service (QoS) requirements for throughput, latency and scalability, as well as requirements for a flexible communication infrastructure to reduce software lifecycle costs. The CORBA event service provides a flexible model for asynchronous communication among distributed and collocated objects. However, the standard CORBA event service specification lacks important features and QoS optimizations required by DIS systems. This paper makes five contributions to the design, implementation and performance measurement of DIS systems. First, it describes how the CORBA event service can be implemented to support key QoS features. Second, it illustrates how to extend the CORBA event service so that it is better suited for DISs. Third, it describes how to develop efficient event dispatching and scheduling mechanisms that can sustain high throughput. Fourth, it describes how to use multicast protocols to reduce network traffic transparently and to improve system scalability. Finally, it illustrates how an event service framework can be strategized to support configurations that facilitate high throughput, predictable bounded latency, or some combination of each",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=842335,no,undetermined,0
Evaluation of telemedical services,"With the rapidly increasing development of telemedicine technology, the evaluation of telemedical services is becoming more and more important. However, professional views of the aims and methods of evaluation are different from the perspective of computer science and engineering compared to that of medicine and health policy. We propose that a continuous evaluation strategy should be chosen which guides the development and implementation of telemedicine technologies and applications. The evaluation strategy is divided into four phases, in which the focus of evaluation is shifted from technical performance of the system in the early phases to medical outcome criteria and economical aspects in the later phases. We review the study design methodology established for clinical trials assessing therapeutic effectiveness and diagnostic accuracy, and discuss how it can be adapted to evaluation studies in telemedicine. As an example, we describe our approach to evaluation in a teleconsultation network in ophthalmology",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=842309,no,undetermined,0
DFT and on-line test of high-performance data converters: a practical case,This paper discusses a Design-for-Testability (DFT) technique applicable to pipelined Analog-to-Digital Converters (ADC). The objective of this DFT is to improve both the on- and off-line testability of these important mixed-signal ICs,1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=833602,no,undetermined,0
Advancing customer-perceived quality in the EDA industry,"This paper describes the initial efforts of two quality groups, one representing EDA software suppliers, and the other representing their major customers, to understand and systematically improve EDA industry quality, as perceived by the customers. The groups continue to work toward this goal, and have reached agreement on several issues, including a defect classification system by severity, eight quality metrics, and a plan for reporting and publishing industry average quality trends using these metrics",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=838908,no,undetermined,0
Seamless Interconnection for Universal Services. Global Telecommunications Conference. GLOBECOM'99. (Cat. No.99CH37042),"The following topics were dealt with: satellite systems and wireless access; ATM switches; multimedia systems; quality of service and performance analysis; CDMA; communication hardware and implementation; traffic models and control for multimedia; mobile ad-hoc networks; future satellite communication systems; wireless components design and implementation; emerging agent software technology; channel modeling and analysis; equalization and adaptive filters; interference suppression; receiver processing and decoding; network operation and distributed management; medium access control; ATM based information infrastructure; packet radio resource management; coding and modulation issues in wireless systems; turbo codes for storage; TCP issues; communications service software; wireless receivers and diversity; signal design for communication systems; signal processing for storage; multiuser detection performance; optical networks; high speed networks; switch architectures; flow control; network protocols design and analysis; scheduling; ATM networks; optical switching and WDM networks; resource allocation; routing and network reliability; fast IP routing; admission control; resource management for wireless networks; buffer management mechanisms; application level services; differentiated services; multimedia traffic; Internet traffic; Web-based management; enterprise network design, implementation, operation and management; antenna and array processing; channel estimation; communication signal processors; interference cancellation; signal detection; multiple antenna systems; iterative decoding; multiuser communications; source and channel coding; modulation and demodulation; fading channels; radio link scheduling for multimedia; IMT-2000 systems; network access and handoff",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=831596,no,undetermined,0
UPC parameter estimation using virtual buffer measurement with application to AAL2 traffic,"This paper describes an efficient, measurement-based technique for estimating all minimal sets of usage parameter control (UPC) parameters for variable bit rate (VBR) ATM traffic streams. This virtual buffer measurement technique is applicable to simulation or live traffic measurement environments and is considerably more efficient than direct evaluation using the generic cell rate algorithm (GCRA). We derive analytically the relationships between virtual buffer measurements and minimal sets of UPC parameters as determined by GCRA, then illustrate the technique by estimating and verifying minimal UPC parameters for ATM Adaptation Layer, Type 2 (AAL2) traffic streams representing voice traffic",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=829999,no,undetermined,0
Dynamic channel allocation by using allocation function in PCS,"A dynamic channel allocation method applicable to the microcell system such as the personal communications service (PCS) is presented. The factors determining the quality of the channels are analyzed. The allocation function that consists of these factors are defined. The channel allocation for a call is processed by using the function value. While a channel is being allocated to a call, an adequate quality channel is used instead of the best quality channel. The probability of call blocking is decreased even in the cells having a high traffic load.",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=824934,no,undetermined,0
The effect of traffic prediction on ABR buffer requirement,"It has been demonstrated that traffic prediction improves the network efficiency and QoS performance in ATM networks. This was achieved by avoidance and control of congestion. In this paper, the effects of traffic prediction schemes on the available bit rate buffer are studied. It can be seen that under the same network environment and traffic intensity, traffic prediction reduces the buffer size for the ABR service. In particular, the fuzzy logic prediction introduced by Qiu (see Proc. IEEE GLOBECOM'97, p.967-71, 1997, and Proc. IEEE International Conference on Communications (ICC'98), p.1769-73, 1998 performs better than conventional autoregressive prediction.",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=824490,no,undetermined,0
Performance enhancement of ERICA+ using fuzzy logic prediction,"The long propagation delay and the existence of highly bursty VBR traffic pose a significant challenge for any end-to-end feedback control scheme on an ATM network. A fuzzy logic based scheme aims at improving the QoS of the ABR service under these conditions is proposed. This paper applies the proposed fuzzy logic control technique of Qiu(see Proc. IEEE GLOBE-COM'97, Arizona, USA, p.967-71, 1997 and Proc. IEEE ICC'98, Atlanta, Georgia, USA, vol. 3, p.1769-73, 1998) to the standard proposed switch algorithm-Explicit Rate Indication for Congestion Avoidance (ERICA+) so as to study its performance. A fuzzy logic predictor is used to estimate the ABR output switch queue length one round-trip delay in advance. This information, together with the total queue growth rate and current ABR queue length are provided to a fuzzy inference system which generates a rate factor. This factor is used as an additional component to increase/decrease the explicit rate (ER) value of a backward resource management (BRM) cell. Simulation results show that the fuzzy logic control scheme can out-perform ERICA+ in terms of queue delay, variation and cell loss ratio under different propagation delays with the existence of CBR and VBR traffic.",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=824486,no,undetermined,0
"Measuring and evaluating maintenance process using reliability, risk, and test metrics","In analyzing the stability of a software maintenance process, it is important that it is not treated in isolation from the reliability and risk of deploying the software that result from applying the process. Furthermore, we need to consider the efficiency of the test effort that is a part of the process and a determinate of reliability and risk of deployment. The relationship between product quality and process capability and maturity has been recognized as a major issue in software engineering based on the premise that improvements in the process will lead to higher-quality products. To this end, we have been investigating an important facet of process capability-stability-as defined and evaluated by trend, change and shape metrics, across releases and within a release. Our integration of product and process measurement serves the dual purpose of using metrics to assess and predict reliability and risk and to evaluate process stability. We use the NASA Space Shuttle flight software to illustrate our approach",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=824387,no,undetermined,0
Scalable latency tolerant architecture (SCALT) and its evaluation,"The deviation of the memory latency is hard to be predicted for in software, especially on the SMP or NUMA systems. As a hardware correspondent method, the multi-thread processor has been devised. However, it is difficult to improve the processor performance with a single program. We have proposed SCALT that uses a buffer in a software context. For the deviation of a latency problem, we have proposed a instruction to check the data arrival existence in a buffer. This paper describes the SCALT, which uses a buffer check instruction, and its performance evaluation results, obtained analyzing the SMP system through event-driven simulation",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=824068,no,undetermined,0
Effect of a sparse architecture on generalization behaviour of connectionist networks: a comparative study,"Generalization, the ability of achieving equal performance with respect to the training patterns for the design of the system as well as the unknown test patterns outside the training set, is considered to be the most desirable aspect of a cognitive learning system. For connectionist classifiers, generalization depends on several factors like the network architecture and size, learning algorithm, complexity of the problem and the quality and quantity of the training samples. Various studies on the generalization behaviour of a neural classifier suggest that the architecture and the size of the network should match the size and complexity of the training sample set of the particular problem. The popular ideas for improving generalization is network pruning by removing redundant nodes or growing by adding nodes to reach the optimum size for matching. In this work a feedforward multilayer connectionist model proposed earlier by Chakraborty et al. (1997) with a sparse fractal connection structure between the neurons of adjacent layers has been studied for its suitability compared to other architectures for achieving good generalization. A comparative study with another feedforward multilayer model with network pruning algorithms for pattern classification problem revealed that it is easier to achieve good generalization with the proposed sparse fractal architecture",1999,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=823241,no,undetermined,0
An application of fuzzy clustering to software quality prediction,"The ever increasing demand for high software reliability requires more robust modeling techniques for software quality prediction. The paper presents a modeling technique that integrates fuzzy subtractive clustering with module-order modeling for software quality prediction. First fuzzy subtractive clustering is used to predict the number of faults, then module-order modeling is used to predict whether modules are fault-prone or not. Note that multiple linear regression is a special case of fuzzy subtractive clustering. We conducted a case study of a large legacy telecommunication system to predict whether each module will be considered fault-prone. The case study found that using fuzzy subtractive clustering and module-order modeling, one can classify modules which will likely have faults discovered by customers with useful accuracy prior to release",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=888052,yes,undetermined,0
3-D Imaging using Novel Techniques in Ultra-Wideband Radar,"The RadioStar programme at the Norwegian University of Science and Technology is focused on understanding the limiting factors in subsurface imaging using electromagnetic waves. A prototype radar system using state-of-the-art technology is developed as a platform for research in ultra-wideband 3-D radar imaging methods. A particular focus is placed in the problem of microwave rendering, or how to make use of microwaves to produce images that go beyond the quality of traditional speckled radar images. The programme aims at developing new and improved hardware and software components that will ultimately lead to more capable and easy to use radar imagers.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4139749,no,undetermined,0
A Software Development Process for Small Projects,"The authors' development process integrates portions of an iterative, incremental process model with a quality assurance process that assesses each process phase's quality and a measurement process that collects data to guide process improvement. The process's goal is to produce the high quality and timely results required for today's market without imposing a large overhead on a small project.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6156718,no,undetermined,0
A model for the sizing of software updates,"The quality and reliability of software updates (SUs) are critical to a system vendor and its customers. As a result, it is important that SUs shipped to customers be successfully integrated into the field generic. A large amount of code must be shipped in an SU because customers want as many fixes and features as possible without compromising the reliability of their systems. However, as the size of an SU increases, so does its probability of field failure, thus making larger SUs riskier. The fundamental question is: How large should an SU be to keep the risk under control? This paper studies the tradeoff between the desire to ship large SUs and the failure risk carried with them. We formulate the problem as a nonlinear programming (NLP) problem, investigate it under various conditions, and derive sizing strategies for the SU. In particular, we derive a formula for the maximal SU size. We make a connection between software reliability and linear programming which, to the best of our knowledge, appears here for the first time. We also introduce some basic ideas related to the customer operational environment and explain the importance of the environment to software performance using an interesting analogy.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6772126,no,undetermined,0
Achieving the quality of verification for behavioral models with minimum effort,"When designing a system in the behavioral level, one of the most important steps to be taken is verifying its functionality before it is released to the logic/PD design phase. One may consider behavioral models as oracles in industries to test against when the final chip is produced. In this work, we use branch coverage as a measure for the quality of verifying/testing behavioral models. Minimum effort for achieving a given quality level can be realized by using the proposed stopping rule. The stopping rule guides the process to switch to a different testing strategy using different types of patterns, i.e. random vs. functional, or using different set of parameters to generate patterns/test cases, when the current strategy is expected not to increase the coverage. We demonstrate the use of the stopping rule on two complex behavioral level VHDL models that were tested for branch coverage with 4 different testing phases. We compare savings of the number of applied testing patterns and quality of testing both with and without using the stopping rule, and show that switching phases at certain points guided by the stopping rule would yield to the same or even better coverage with less number of testing patterns",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=838877,no,undetermined,0
Quality of electronic design: from architectural level to test coverage,"The purpose of this paper is to present a design methodology that complements existing methodologies by addressing the upper and the lower extremes of the design flow. The aim of the methodology is to increase design and product quality. At system level, emphasis is given to architecture generation, reconfiguration and quality assessment. Quality metrics and criteria, focused on design and test issues, are used for the purpose. At physical level, a Defect-Oriented Test (DOT) approach and test reuse are the basis of the methodology to estimate test effectiveness, or defects coverage. Tools which implement the methodology are presented. Results are shown for a public domain PIC processor, used as a SOC embedded core",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=838874,no,undetermined,0
A network measurement architecture for adaptive applications,"The quality of network connectivity between a pair of Internet hosts can vary greatly. Adaptive applications can cope with these differences in connectivity by choosing alternate representations of objects or streams or by downloading the objects from alternate locations. In order to effectively adapt, applications must discover the condition of the network before communicating with distant hosts. Unfortunately, the ability to predict or report the quality of connectivity is missing in today's suite of Internet services. To address this limitation, we have developed SPAND (shared passive network performance discovery), a system that facilitates the development of adaptive network applications. In each domain, applications make passive application specific measurements of the network and store them in a local centralized repository of network performance information. Other applications may retrieve this information from the repository and use the shared experiences of all hosts in a domain to predict future performance. In this way, applications can make informed decisions about adaptation choices as they communicate with distant hosts. In this paper, we describe and evaluate the SPAND architecture and implementation. We show how the architecture makes it easy to integrate new applications into our system and how the architecture has been used with specifics types of data transport. Finally, we describe LookingGlass, a WWW mirror site selection tool that uses SPAND. LookingGlass meets the conflicting goals of collecting passive network performance measurements and maintaining good client response times. In addition, LookingGlass's server selection algorithms based on application level measurements perform much better than techniques that rely on geographic location or route metrics",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=832198,no,undetermined,0
Proactive maintenance tools for transaction oriented wide area networks,"The motivation of the work presented in this paper comes from a real network management center in charge of supervising a very large hybrid telecommunications/data transaction-oriented network. We present a set of tools that we have developed and implemented in the AT&T Transaction Access Services (TAS) network, in order to automate and facilitate the process of diagnosing network faults and identifying the potentially affected elements, resources and customers. Specifically in this paper we describe the development implementation and use of the following systems: (a) the TAS Information and Tracking System (TIMATS) that provides a common framework for the storage and retrieval of provisioning, capacity management and maintenance data; (b) the Transactions Event Viewer (TEVIEW) system that generates, filters, and presents diagnostic events that indicate system occurrences or conditions that may cause a degradation of the service; and (c) the Transaction Instantaneous Anomaly Notification (TRISTAN) system which implements an adaptive network anomaly detection software that detects network and service anomalies of TAS as dynamically defined violations of the base-lined performance characteristics and profiles",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=830433,no,undetermined,0
A software supervision approach for distributed system performance modelling,"Large software systems, such as telecom applications, are often built on reused components. Such systems are often developed using components from previous similar projects, or simply using a reconfiguration of the same set of components from a previous similar project. Performance prediction of such software system architectural design provides a quantified measurement for better design quality. The design is specified in a communicating extended finite state machine model. The model is extended with stochastic information and simulated for performance prediction. The stochastic extension requires performance data for each component and load information of the system environment. This paper addresses the problem of abstracting stochastic performance model of a component to be reused in a software architectural design. We use a software supervision approach to monitor the performance of a deployed component and collect its execution trace, including individual time stamps of the externally observable signals. We then derive a stochastic performance model of the component from the trace. The model can be used later in performance prediction when the component is reused. We applied this method to a control program of a small telephone exchange. We were able to reuse a component and its performance data in a new exchange design",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=830355,no,undetermined,0
On the dynamic quality of service in wireless computing environments,"This paper considers adaptive and reliable quality of service in a dynamic wireless computing environment. We first develop a quality of service architecture that spans the networking and end-to-end systems software layers. Then, we design resource management algorithms for adaptively sharing the networking resources among applications. Finally, we develop a seamless networking infrastructure integrating the underlying networks",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=830322,no,undetermined,0
The effectiveness of software development technical reviews: a behaviorally motivated program of research,"Software engineers use a number of different types of software development technical review (SDTR) for the purpose of detecting defects in software products. This paper applies the behavioral theory of group performance to explain the outcomes of software reviews. A program of empirical research is developed, including propositions to both explain review performance and identify ways of improving review performance based on the specific strengths of individuals and groups. Its contributions are to clarify our understanding of what drives defect detection performance in SDTRs and to set an agenda for future research. In identifying individuals' task expertise as the primary driver of review performance, the research program suggests specific points of leverage for substantially improving review performance. It points to the importance of understanding software reading expertise and implies the need for a reconsideration of existing approaches to managing reviews",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=825763,no,undetermined,0
Survivability through customization and adaptability: the Cactus approach,"Survivability, the ability of a system to tolerate intentional attacks or accidental failures or errors, is becoming increasingly important with the extended use of computer systems in society. While techniques such as cryptographic methods, intrusion detection, and traditional fault tolerance are currently being used to improve the survivability of such systems, new approaches are needed to help reach the levels that will be required in the near future. This paper proposes the use of fine-grain customization and dynamic adaptation as key enabling technologies in a new approach designed to achieve this goal. Customization not only supports software diversity, but also allows customized tradeoffs to be made between different QoS attributes including performance, security, reliability and survivability. Dynamic adaptation allows survivable services to change their behavior at runtime as a reaction to anticipated or detected intrusions or failures. The Cactus system provides support for both fine-grain customization and dynamic adaptation, thereby offering a potential solution for building survivable software in networked systems",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=825033,no,undetermined,0
Evaluation of fault tolerance latency from real-time application's perspectives,"Information on Fault Tolerance Latency (FTL), which is defined as the total time required by all sequential steps taken to recover from an error, is important to the design and evaluation of fault-tolerant computers used in safety-critical real-time control systems with deadline information. In this paper, we evaluate FTL in terms of several random and deterministic variables accounting for fault behaviors and/or the capability and performance of error-handling mechanisms, while considering various fault tolerance mechanisms based on the trade-off between temporal and spatial redundancy, and use the evaluated FTL to check if an error-handling policy can meet the Control System Deadline (CSD) for a given real-time application",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=822564,no,undetermined,0
Applying adaptation spaces to support quality of service and survivability,"Adaptation is a key technique in constructing survivable information systems. Allowing a system to continue running, albeit with reduced functionality or performance, in the face of reduced resources, attacks, or broken components is often preferable to either complete shutdown or continued normal operation in compromised mode. However, unpredictable adaptation can sometimes be worse than the problem it seeks to cope with. In this paper we introduce adaptation spaces, which precisely and predictably specify the adaptation of a software component. We then present two survivable systems that have been specified and implemented using adaptation spaces. The first example uses user preferences regarding quality in an audio application to guide the adaptation when available bandwidth decreases. The second trades off performance overhead with intrusion resistance for â€œstack-smashingâ€?attacks. We formally define an adaptation space and show briefly how it enables certain kinds of reasoning about adaptive applications. We conclude with related work and future plans",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=821526,no,undetermined,0
Confidence limits on the inherent availability of equipment,"The inherent availability, is an important performance index for a repairable system, and is usually estimated from the times-between-failures and the times-to-restore data. The formula for calculating a point estimate of the inherent availability from collected data is well known. But the quality of the calculated inherent availability is suspect because of small data sample sizes. The solution is to use the confidence limits on the inherent availability at a given confidence level, in addition to the point estimator. However, there is no easy way to compute the confidence limits on the calculated availability. Actually, no adequate approach to compute the confidence interval for the inherent availability, based on sample data, is available. In this paper, the uncertainties of small random samples are taken into account. The estimated mean times between failures, mean times to restore and the estimated inherent availability are treated as random variables. When the distributions of both times-between-failures and times-to-restore are exponential, the exact confidence limits on the inherent availability are derived. Based on reasonable assumptions, a nonparametric method of determining the approximate confidence limits on the inherent availability from data are proposed, without assuming any times-between-failures and times-to-restore distributions. Numerical examples are provided to demonstrate the validity of the proposed solution, which are compared with the results obtained from Monte Carlo simulations. It turns out that the proposed method yields satisfactory accuracy for engineering applications",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=816301,no,undetermined,0
Modeling and analysis for multiple stress-type accelerated life data,"This paper describes a model for multiple stress-type accelerated life data. In addition, the use of an algorithm, which was specifically developed for this model is illustrated. The model is based on the widely known log-linear model (1990) and is formulated for the Weibull and lognormal distributions for a variety of censoring schemes using likelihood theory. An algorithm has been developed for the solution of this model, and implemented in a recently released software package, ALTA Pro<sup>TM</sup>, specific to accelerated life data analysis. The algorithm has been specifically designed to be very flexible and has the capability of simultaneously solving for up to eight different stress-types. The advantage of this formulation is that it combines in one model most of the known life-stress relationships for one or two types of stresses (such as the temperature-nonthermal model), as well as the multivariable proportional hazards model. This yields a single general likelihood function (for a given distribution) whose solution is independent of both the chosen life-stress relationship and the number of stress-types. In addition, this model allows for simultaneous analysis of continuous, categorical and indicator variables. The solution to this model provides the engineers with an opportunity to expand their selection of types of stresses and test conditions when testing products",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=816297,no,undetermined,0
A practical software-reliability measurement framework based on failure data,"Software-reliability measurement based on failure data can provide estimations of software reliability at its present state and predictions of software reliability in the future as testing goes on. This paper analyzes the problems existed in the measurement process and proposes a practical software-reliability measurement framework based on state of the art to give relatively better software reliability predictions. Study on the validity of the modification methods i.e. the recalibration technique and the combination method also has been done. The results show that the modification methods could give better prediction results but not always, so predictive quality analysis still needs to be done",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=816296,no,undetermined,0
Enhancing the predictive performance of the Goel-Okumoto software reliability growth model,"In this paper, enhancement of the performance of the Goel-Okumoto Reliability Growth model is investigated using various smoothing techniques. The method of parameter estimation for the model is the maximum likelihood method. The evaluation of the performance of the model is judged by the relative error of the predicted number of failures over future time intervals relative to the number of failures eventually observed during the interval. The use of data analysis procedures utilizing the Laplace trend test are investigated. These methods test for reliability growth throughout the data and establish ""windows"" that censor early failure data and provide better model fits. The research showed conclusively that the data analysis procedures resulted in improvement in the models' predictive performance for 41 different sets of software failure data collected from software development labs in the United States and Europe",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=816292,no,undetermined,0
A Petri-net approach for early-stage system-level software reliability estimation,"This paper addresses software reliability modeling issues at the early stage of a software development for large-scale software products. The hierarchical view of a software product provides the modeling framework at the system level. This paper shows how to take the hierarchical description and perform the system-level software reliability estimation using Petri net mechanisms. The Petri net modeling techniques are proposed for handling the dependency among software modules. Furthermore, this paper addresses issues of early-stage software reliability modeling when failure data is not available",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=816291,no,undetermined,0
Use of fault tree analysis for evaluation of system-reliability improvements in design phase,"Traditional failure mode and effects analysis is applied as a bottom-up analytical technique to identify component failure modes and their causes and effects on the system performance, estimate their likelihood, severity and criticality or priority for mitigation. Failure modes and their causes, other than those associated with hardware, primarily electronic, remained poorly addressed or not addressed at all. Likelihood of occurrence was determined on the basis of component failure rates or by applying engineering judgement in their estimation. Resultant prioritization is consequently difficult so that only the apparent safety-related or highly critical issues were addressed. When thoroughly done, traditional FMEA or FMECA were too involved to be used as a effective tool for reliability improvement of the product design. Fault tree analysis applied to the product as a top down in view of its functionality, failure definition, architecture and stress and operational profiles provides a methodical way of following products functional flow down to the low level assemblies, components, failure modes and respective causes and their combination. Flexibility of modeling of various functional conditions and interaction such as enabling events, events with specific priority of occurrence, etc., using FTA, provides for accurate representation of their functionality interdependence. In addition to being capable of accounting for mixed reliability attributes (failure rates mixed with failure probabilities), fault trees are easy to construct and change for quick tradeoffs as roll up of unreliability values is automatic for instant evaluation of the final quantitative reliability results. Failure mode analysis using fault tree technique that is described in this paper allows for real, in-depth engineering evaluation of each individual cause of a failure mode regarding software and hardware components, their functions, stresses, operability and interactions",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=816275,no,undetermined,0
Reducing the complexity of wireless Q3 adapter development,"The AUTOPLEXÂ® Q3 adapter was designed to create a telecommunication management network (TMN) agent for the AUTOPLEX System 1000. The agent, which carries out directives and emits notification, provides network management capability for an operations support system (OSS) â€?also known as a manager â€?via standard TMN communication protocol and services. The Q3 adapter provides a full range of TMN system management functions, which consist of fault management, configuration management, performance management, and security management. The development of the Q3 adapter was considered very costly, based on the feature estimate by the development organization. This paper describes an overlay development approach and a flexible software architecture that have been incorporated into the Q3 adapter design to make it more cost effective. These factors reduce the complexity of its development and make it easier to maintain different software releases and to modify different TMN information models. The design also creates an independent software platform that enables many other wireless applications to easily build their interfaces to the AUTOPLEX System 1000.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6772131,no,undetermined,0
A probabilistic priority scheduling discipline for high speed networks,"In high speed networks, the strict priority (SP) scheduling discipline is perhaps the most common and simplest method to schedule packets from different classes of applications, each with diverse performance requirements. With this discipline, however, packets at higher priority levels can starve packets at lower priority levels. To resolve this starvation problem, we propose to assign a parameter to each priority queue in the SP discipline. The assigned parameter determines the probability with which its corresponding queue is served when the queue is polled by the server. We thus form a new packet scheduling discipline, referred to as the probabilistic priority (PP) discipline. By properly setting the assigned parameters, service differentiation as well as fairness among traffic classes can be achieved in PP. In addition, the PP discipline can be easily reduced to the ordinary SP discipline or to the reverse SP discipline",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=923593,no,undetermined,0
Trading off execution time for reliability in scheduling precedence-constrained tasks in heterogeneous computing,"This paper investigates the problem of matching and scheduling of an application, which is composed of tasks with precedence constraints, to minimize both execution time and probability of failure of the application in a heterogeneous computing system. In general, however, it is impossible to satisfy both objectives at the same time because of conflicting requirements. The best one can do is to trade off execution time for reliability or vice versa, according to users' needs. Furthermore, there is a need for an algorithm which can assign tasks of an application to satisfy both of the objectives to some degree. Motivated from these facts, two different algorithms, which are capable of trading off execution time for reliability, are developed. To enable the proposed algorithms to account for the reliability of resources in the system, an expression which gives the reliability of the application under a given task assignment is derived. The simulation results are provided to validate the performance of the proposed algorithms",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=925005,no,undetermined,0
Estimation of bus performance for a tuplespace in an embedded architecture [factory automation application],"This paper describes a design methodology for the estimation of bus performance of a tuplespace for factory automation. The need of a tuplespace is motivated by the characteristics of typical embedded architectures for factory automation. We describe the features of a bus for embedded applications and the problem of estimating its performance, and present a rapid prototyping design methodology developed for a qualitative and quantitative estimation. The methodology is based on a mix of different modeling languages such as Java, C++, SystemC and Network Simulator2 (NS2). Its application allows the estimation of the expected performance of the bus under design in relation to the developed tuplespace.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1186693,no,undetermined,0
Combining and adapting software quality predictive models by genetic algorithms,"The goal of quality models is to predict a quality factor starting from a set of direct measures. Selecting an appropriate quality model for a particular software is a difficult, non-trivial decision. In this paper, we propose an approach to combine and/or adapt existing models (experts) in such way that the combined/adapted model works well on the particular system. Test results indicate that the models perform significantly better than individual experts in the pool.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1115031,no,undetermined,0
Using a pulsed supply voltage for delay faults testing of digital circuits in a digital oscillation environment,"High-performance digital circuits with aggressive timing constraints are usually very susceptible to delay faults. Much research done on delay fault detection needs a rather complicated test setup together with precise test clock requirements. In this paper, we propose a test technique based on the digital oscillation test method. The technique, which was simulated in software, consists of sensitizing a critical path in the digital circuit under test and incorporating the path into an oscillation ring. The supply voltage to the oscillation ring is then varied to detect delay and stuck-at faults in the path.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1146804,no,undetermined,0
On the evaluation of JavaSymphony for cluster applications,"In the past few years, increasing interest has been shown in using Java as a language for performance-oriented distributed and parallel computing. Most Java-based systems that support portable parallel and distributed computing either require the programmer to deal with intricate low level details of Java which can be a tedious, time-consuming and error-prone task, or prevent the programmer from controlling locality of data. In contrast to most existing systems, JavaSymphony - a class library written entirely in Java - allows to control parallelism, load balancing and locality at a high level. Objects can be explicitly distributed and migrated based on virtual architectures which impose a virtual hierarchy on a distributed/parallel system of physical computing nodes. The concept of blocking/nonblocking remote method invocation is used to exchange data among distributed objects and to process work by remote objects. We evaluate the JavaSymphony programming API for a variety of distributed/parallel algorithms which comprises backtracking, N-body, encryption/decryption algorithms and asynchronous nested optimization algorithms. Performance results are presented for both homogeneous and heterogeneous cluster architectures. Moreover, we compare JavaSymphony with an alternative well-known semi-automatic system.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1137772,no,undetermined,0
Performance management in component-oriented systems using a Model Driven Architectureâ„?approach,"Developers often lack the time or knowledge to profoundly understand the performance issues in largescale component-oriented enterprise applications. This situation is further complicated by the fact that such applications are often built using a mix of in-house and commercial-off-the-shelf (COTS) components. This paper presents a methodology for understanding and predicting the performance of component-oriented distributed systems both during development and after they have been built. The methodology is based on three conceptually separate parts: monitoring, modelling and performance prediction. Performance predictions are based on UML models created dynamically by monitoring-and-analysing a live or under-development system. The system is monitored using non-intrusive methods and run-time data is collected. In addition, static data is obtained by analysing the deployment configuration of the target application. UML models enhanced with performance indicators are created based on both static and dynamic data, showing performance hot spots. To facilitate the understanding of the system, the generated models are traversable both horizontally at the same abstraction level between transactions, and vertically between different layers of abstraction using the concepts defined by the Model Driven Architecture. The system performance is predicted and performance-related issues are identified in different scenarios by generating workloads and simulating the performance models. Work is under way to implement a framework for the presented methodology with the current focus on the Enterprise Java Beans technology.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1137712,no,undetermined,0
A study on a garbage collector for embedded applications,"In general, embedded systems, such as cellular phones and PDAs are provided with small amounts of memory and a low power processor that is slower than desktop ones. Despite these limited resources, present technology allows designers to integrate, in a single chip, an entire system. In this scenario, software development for embedded systems is an error-prone operation. In order to develop better code in less time, Java technology has gained a lot of interest from developers of embedded systems in the last few years, mainly because of its portability, code reuse, and object-oriented paradigm. On the other hand, Java requires an automatic memory management system in Java processors. This paper presents a garbage collection technique based on a software approach for an embedded Java processor. This technique is targeted for applications that are used in portable embedded systems. This paper discusses the most suited algorithm for such applications, showing also some performance overhead results.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1137648,no,undetermined,0
The design and implementation of the intelÂ® real-time performance analyzer,"Modern PCs support growing numbers of concurrently active independently authored real-time software applications and device drivers. The non realtime nature of PC OSes (Linux, Microsoft Windows, etc.) means that robust real-time software must cope with hold-offs without degradation in user perceivable application quality of service. The open nature of the PC platform necessitates measuring OS interrupt and thread latencies under concurrent load in order to determine with how much hold-off the application must cope. The IntelÂ® Real-Time Performance Analyzer is a toolkit for PCs running Microsoft Windows. The toolkit statistically characterizes thread and interrupt latencies plus Windows Deferred Procedure Call (DPC) and kernel Work Item latencies. The toolkit also has facilities for analyzing the causes of long latencies. These latencies can then be incorporated as additional blocking times in a real-time schedulability analysis. An isochronous workload tool is included to model thread and DPC based computation and detect missed deadlines.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1137387,no,undetermined,0
Asymptotics of quickest change detection procedures under a Bayesian criterion,"The optimal detection procedure for detecting changes in independent and identically distributed sequences (i.i.d.) in a Bayesian setting was derived by Shiryaev in the nineteen sixties. However, the analysis of the performance of this procedure in terms of the average detection delay and false alarm probability has been an open problem. In this paper, we investigate the performance of Shiryaev's procedure in an asymptotic setting where the false alarm probability goes to zero. The asymptotic study is performed not only in. the i.d.d. case where the Shiryaev's procedure is optimal but also in a general, non-i.i.d. case. In the latter case, we show that Shiryaev's procedure is asymptotically optimum under mild conditions. We also show that the two popular non-Bayesian detection procedures, namely the Page and Shiryaev-Roberts-Pollak procedures, are not optimal (even asymptotically) under the Bayesian criterion. The results of this study are shown to be especially important in studying the asymptotics of decentralized quickest change detection procedures.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1115427,no,undetermined,0
Distance dependent thresholding search for fast motion estimation in real world video coding application,"This paper presents a distance dependent thresholding search (DTS) block motion estimation algorithm that employs the novel concept of distance dependent thresholds. The key feature of this algorithm is its flexibility, trading-off quality and complexity with threshold variation. Whereas the performance of the existing algorithms is fixed in terms of prediction quality as well as complexity, DTS can be used as full search (FS), where high quality video entertainments require motion estimation with small prediction error, as well as fast motion estimation such as three-step search (TSS), new-three-step search (NTSS) etc. while real-time video applications, such as the speed-oriented video conferencing require fast motion estimation with sacrificing quality. Experimental results show that this DTS algorithm also achieves better peak signal-to-noise ratio (PSNR), as well as lower search times in comparison to both the TSS and NTSS algorithms.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1115326,no,undetermined,0
Predicting software stability using case-based reasoning,"Predicting stability in object-oriented (OO) software, i.e., the ease with which a software item can evolve while preserving its design, is a key feature for software maintenance. We present a novel approach which relies on the case-based reasoning (CBR) paradigm. Thus, to predict the chances of an OO software item breaking downward compatibility, our method uses knowledge of past evolution extracted from different software versions. A comparison of our similarity-based approach to a classical inductive method such as decision trees, is presented which includes various tests on large datasets from existing software.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1115033,no,undetermined,0
Model-based tests of truisms,"Software engineering (SE) truisms capture broadly-applicable principles of software construction. The trouble with truisms is that such general principles may not apply in specific cases. This paper tests the specificity of two SE truisms: (a) increasing software process level is a desirable goal; and (b) it is best to remove errors during the early parts of a software lifecycle. Our tests are based on two well-established SE models: (1) Boehm et.al.'s COCOMO II cost estimation model; and (2) Raffo's discrete event software process model of a software project life cycle. After extensive simulations of these models, the TAR2 treatment learner was applied to find the model parameters that most improved the potential performance of the real-world systems being modelled. The case studies presented here showed that these truisms are clearly sub-optimal for certain projects since other factors proved to be far more critical. Hence, we advise against truism-based process improvement. This paper offers a general alternative framework for model-based assessment of methods to improve software quality: modelling + validation + simulation + sensitivity. That is, after recording what is known in a model, that model should be validated, explored using simulations, then summarized to find the key factors that most improve model behavior.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1115012,no,undetermined,0
Self-commissioning training algorithms for neural networks with applications to electric machine fault diagnostics,"The main limitations of neural network (NN) methods for fault diagnostics applications are training data and data memory requirements, and computational complexity. Generally, a NN is trained offline with all the data obtained prior to commissioning, which is not possible in a practical situation. In this paper, three novel and self-commissioning training algorithms are proposed for online training of a feedforward NN to effectively address the aforesaid shortcomings. Experimental results are provided for an induction machine stator winding turn-fault detection scheme, to illustrate the feasibility of the proposed online training algorithms for implementation in a commercial product.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1159001,no,undetermined,0
No Java without caffeine: A tool for dynamic analysis of Java programs,"To understand the behavior of a program, a maintainer reads some code, asks a question about this code, conjectures an answer, and searches the code and the documentation for confirmation of her conjecture. However, the confirmation of the conjecture can be error-prone and time-consuming because the maintainer has only static information at her disposal. She would benefit from dynamic information. In this paper, we present Caffeine, an assistant that helps the maintainer in checking her conjecture about the behavior of a Java program. Our assistant is a dynamic analysis tool that uses the Java platform debug architecture to generate a trace, i.e., an execution history, and a Prolog engine to perform queries over the trace. We present a usage scenario based on the n-queens problem, and two real-life examples based on the Singleton design pattern and on the composition relationship.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1115000,no,undetermined,0
Concurrent error detection schemes for fault-based side-channel cryptanalysis of symmetric block ciphers,"Fault-based side-channel cryptanalysis is very effective against symmetric and asymmetric encryption algorithms. Although straightforward hardware and time redundancy-based concurrent error detection (CED) architectures can be used to thwart such attacks, they entail significant overheads (either area or performance). The authors investigate systematic approaches to low-cost low-latency CED techniques for symmetric encryption algorithms based on inverse relationships that exist between encryption and decryption at algorithm level, round level, and operation level and develop CED architectures that explore tradeoffs among area overhead, performance penalty, and fault detection latency. The proposed techniques have been validated on FPGA implementations of Advanced Encryption Standard (AES) finalist 128-bit symmetric encryption algorithms.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1097871,no,undetermined,0
Integrating reliability and timing analysis of CAN-based systems,"This paper presents and illustrates a reliability analysis method developed with a focus on controller-area-network-based automotive systems. The method considers the effect of faults on schedulability analysis and its impact on the reliability estimation of the system, and attempts to integrate both to aid system developers. The authors illustrate the method by modeling a simple distributed antilock braking system, and showing that even in cases where the worst case analysis deems the system unschedulable, it may be proven to satisfy its timing requirements with a sufficiently high probability. From a reliability and cost perspective, this paper underlines the tradeoffs between timing guarantees, the level of hardware and software faults, and per-unit cost.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1097745,no,undetermined,0
Applying checkers to improve the correctness and integrity of software [Air Force systems],"Testing of mission-critical systems to a high degree of reliability has been a long time problem for the Air Force. As a result system failures may occur in the field due to faults that result from unusual environmental conditions or unexpected sequences of events that were never encountered in the laboratory. To improve the validation and test process and deliver more reliable systems, under the Air Force self-checking embedded information system software (SCEISS) program, we have performed several demonstrations of self-checking systems that continuously monitor themselves to report suspicious events and software faults. Based on theoretical University results, the SCEISS program has demonstrated improvements in the reliability and quality of software-intensive systems through employing self-checking techniques. This paper presents metrics that show the value of checkers in finding errors earlier with less cost and effort on several Raytheon applications and describes a checker library that is under development to automate the inclusion of checkers in a software integration environment.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1067983,no,undetermined,0
Validation of mission critical software design and implementation using model checking [spacecraft],"Over the years, the complexity of space missions has dramatically increased with more of the critical aspects of a spacecraft's design being implemented in software. With the added functionality and performance required by the software to meet system requirements, the robustness of the software must be upheld. Traditional software validation methods of simulation and testing are being stretched to adequately cover the needs of software development in this growing environment. It is becoming increasingly difficult to establish traditional software validation practices that confidently confirm the robustness of the design in balance with cost and schedule needs of the project. As a result, model checking is emerging as a powerful validation technique for mission critical software. Model checking conducts an exhaustive exploration of all possible behaviors of a software system design and as such can be used to detect defects in designs that are typically difficult to discover with conventional testing approaches.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1067982,no,undetermined,0
Dynamic simulation inspection test for protection equipment in China,"Testing the protection equipment on dynamic simulation test systems is very valuable not only for finding software bugs and design deficiencies of protection but also for performance and characteristics understanding. To strictly test different kinds of protection relay, the test systems must been designed carefully by selecting the connection and parameters of the system. The test items are designed specially for every kind of protection. This paper introduces some special technical requirements in China, and important test items for different protection schemes. Several ideas and algorithms of some protections are also compared in this paper, such as power swing blocking methods in line protection, current transformer saturation countermeasures in transformer and busbar protection, inrush currents, blocking methods in transformer protection, current transformer single-phase open detection methods, etc. Several problems appeared in the test and the reason of these problems are described and analyzed in this paper.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1053549,no,undetermined,0
Requirements in the medical domain: Experiences and prescriptions,"Research shows that information flow in health care systems is inefficient and prone to error. Data is lost, and physicians must repeat tests and examinations because the results are unavailable at the right place and time. Cases of erroneous medication - resulting from misinterpreted, misunderstood, or missing information - are well known and have caused serious health problems and even death. We strongly believe that through effective use of information technology, we can improve both the quality and efficiency of the health sector's work. Introducing a new system might shift power from old to young, from doctor to nurse, or from medical staff to administration. Few people appreciate loss of power, but even fewer will admit that the loss of power is why they resist the new system. Thus, we must work hard to bring this into the open and help people realize that a new system doesn't have to threaten their positions. Again, knowledge and understanding of a hospital's organizational structure, both official and hidden, is necessary if the system's introduction is to be successful.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1049394,no,undetermined,0
Investigating the influence of software inspection process parameters on inspection meeting performance,"The question of whether inspection meetings justify their cost has been discussed in several studies. However, it is still open as to how modern defect detection techniques and team size influence meeting performance, particularly with respect to different classes of defect severity. The influence of software inspection process parameters (defect detection technique, team size, meeting effort) on defect detection effectiveness is investigated, i.e. the number of defects found for 31 teams which inspected a requirements document, to shed light on the performance of inspection meetings. The sets of defects reported by each team after the individual preparation phase (nominal-team performance) and after the team meeting (real-team performance) are compared. The main findings are that nominal teams perform significantly more effectively than real teams for all defect classes. This implies that meeting losses are on average higher than meeting gains. Meeting effort was positively correlated with meeting gains, indicating that synergy effects can only be realised if enough time is available. With regard to meeting losses, existing reports are confirmed that for a given defect, the probability of being lost in a meeting decreases with an increase in the number of inspectors who detected this defect during individual preparation.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1049199,no,undetermined,0
Process activities in a project based course in software engineering,"""Studio in Software Engineering"" is a curriculum component for the undergraduate-level software engineering program at Ecole Polytechnique de Montreal. The main teaching objective is to develop in students a professional attitude towards producing high quality software. The course is based on a project approach in a collaborative learning environment. The software development process used is based on the Unified Process for EDUcation, which is customized from the Rational Unified Process. An insight into the dynamics of three teams involved in the development of the same project allows us to present and interpret data concerning the effort spent by students during particular process activities. The contribution of this paper is to illustrate an approach involving qualitative analysis of the effort spent by the students on each software process activity. Such an approach may allow the development of a model that would lead to effort prediction within a software process in order to designate the actions for improving academic projects.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1158710,no,undetermined,0
Review of condition assessment of power transformers in service,"As transformers age, their internal condition degrades, which increases the risk of failure. To prevent these failures and to maintain transformers in good operating condition is a very important issue for utilities. Traditionally, routine preventative maintenance programs combined with regular testing were used. The change to condition-based maintenance has resulted in the reduction, or even elimination, of routine time-based maintenance. Instead of doing maintenance at a regular interval, maintenance is only carried out if the condition of the equipment requires it. Hence, there is an increasing need for better nonintrusive diagnostic and monitoring tools to assess the internal condition of the transformers. If there is a problem, the transformer can then be repaired or replaced before it fails. An extensive review is given of diagnostic and monitoring tests, and equipment available that assess the condition of power transformers and provide an early warning of potential failure.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1161455,no,undetermined,0
Performance analysis of image compression using wavelets,"The aim of this paper is to examine a set of wavelet functions (wavelets) for implementation in a still image compression system and to highlight the benefit of this transform relating to today's methods. The paper discusses important features of wavelet transform in compression of still images, including the extent to which the quality of image is degraded by the process of wavelet compression and decompression. Image quality is measured objectively, using peak signal-to-noise ratio or picture quality scale, and subjectively, using perceived image quality. The effects of different wavelet functions, image contents and compression ratios are assessed. A comparison with a discrete-cosine-transform-based compression system is given. Our results provide a good reference for application developers to choose a good wavelet compression system for their application",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=925596,no,undetermined,0
Discrete availability models to rejuvenate a telecommunication billing application,"Software rejuvenation is a proactive fault management technique that has been extensively studied in the recent literature. We focus on an example for a telecommunication billing application considered in Huang et al. (1995) and develop the discrete-time stochastic models to estimate the optimal software rejuvenation schedule. More precisely, two software availability models with rejuvenation are formulated via the discrete semi-Markov processes, and the optimal software rejuvenation schedules which maximize the steady-state availabilities are derived analytically. Further, we develop statistically nonparametric algorithms to estimate the optimal software rejuvenation schedules, provided that the complete sample data of failure times are given. Then, a new statistical device, called discrete total time on test statistics, is introduced. Finally, we examine asymptotic properties for the statistical estimation algorithms proposed in this paper through a simulation experiment.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173117,no,undetermined,0
Numerical simulation of car crash analysis based on distributed computational environment,"Automobile CAE software is mainly used to assess the performance quality of vehicles. As the automobile is a product of technology intensive complexity, its design analysis involves a broad range of CAE simulation techniques. An integrated CAE solution of automobiles can include comfort analysis (vibration and noise analysis), safety analysis (car body collision analysis), process-cycle analysis, structural analysis, fatigue analysis, fluid dynamics analysis, test analysis, material data information system and system integration. We put an emphasis on simulation of a whole automobile collision process, which will bring a breakthrough to the techniques of CAE simulation based on high performance computing. In addition, we carry out simulation for a finite-element car model in a distributed computation environment and accomplish coding-and-programming of DAYN3D. We also provide computational examples and a user handbook. Our research collects almost ten numerical automobile models such as Honda, Ford, etc. Moreover, we also deal with different computational scales for the same auto model and some numerical models of air bags are included. Based on the numerical auto model, referring to different physical parameters and work conditions of the auto model, we can control the physical parameters for the numerical bump simulation and analyze the work condition. The result of our attempt conduces to the development of new auto models.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173597,no,undetermined,0
A research on multi-level networked RAID based on cluster architecture,"Storage networks is a popular solution to constraint servers in storage field. As described by Gibson's metrics, the performance of multi-level networked RAID (redundant arrays of inexpensive disks) based on cluster is almost the same to that of improved 2D-parity. Compared with other schemes, it is lower cost and easier to realize.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173578,no,undetermined,0
Automatic synthesis of dynamic fault trees from UML system models,"The reliability of a computer-based system may be as important as its performance and its correctness of computation. It is worthwhile to estimate system reliability at the conceptual design stage, since reliability can influence the subsequent design decisions and may often be pivotal for making trade-offs or in establishing system cost. In this paper we describe a framework for modeling computer-based systems, based on the Unified Modeling Language (UML), that facilitates automated dependability analysis during design. An algorithm to automatically synthesize dynamic fault trees (DFTs) from the UML system model is developed. We succeed both in embedding information needed for reliability analysis within the system model and in generating the DFT Thereafter, we evaluate our approach using examples of real systems. We analytically compute system unreliability from the algorithmically developed DFT and we compare our results with the analytical solution of manually developed DFTs. Our solutions produce the same results as manually generated DFTs.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173261,no,undetermined,0
Toward semi-automatic construction of training-corpus for text classification,"Text classification is becoming more and more important with the rapid growth of on-line information available. It was observed that the quality of the training corpus impacts the performance of the trained classifier. This paper proposes an approach to build high-quality training corpuses for better classification performance by first exploring the properties of training corpuses, and then giving an algorithm for constructing training corpuses semi-automatically. Preliminary experimental results validate our approach: classifiers based on the training corpuses constructed by our approach can achieve good performance while the training corpus' size is significantly compressed. Our approach can be used for building an efficient and lightweight classification system.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173245,no,undetermined,0
Dependability analysis of a client/server software system with rejuvenation,"Long running software systems are known to experience an aging phenomenon called software aging, one in which the accumulation of errors during the execution of software leads to performance degradation and eventually results in failure. To counteract this phenomenon an active fault management approach, called software rejuvenation, is particularly useful. It essentially involves gracefully terminating an application or a system and restarting it in a clean internal state. We deal with dependability analysis of a client/server software system with rejuvenation. Three dependability measures in the server process, steady-state availability, loss probability of requests and mean response time on tasks, are derived from the well-known hidden Markovian analysis under the time-based software rejuvenation scheme. In numerical examples, we investigate the sensitivity of some model parameters to the dependability measures.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173241,no,undetermined,0
On estimating testing effort needed to assure field quality in software development,"In practical software development, software quality is generally evaluated by the number of residual defects. To keep the number of residual defects within a permissible value, too much effort is often assigned to software testing. We try to develop a statistical model to determine the amount of testing effort which is needed to assure the field quality. The model explicitly includes design, review, and test (including debug) activities. Firstly, we construct a linear multiple regression model that can clarify the relationship among the number of residual defects and the efforts assigned to design, review, and test activities. We then confirm the applicability of the model by statistical analysis using actual project data. Next, we obtain an equation based on the model to determine the test effort. As parameters in the equation, the permissible number of residual defects, the design effort, and the review effort are included. Then, the equation determines the test effort that is needed to assure the permissible residual defects. Finally, we conduct an experimental evaluation using actual project data and show the usefulness of the equation.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173234,no,undetermined,0
"Automatic failure detection, logging, and recovery for high-availability Java servers","Many systems and techniques exist for detecting application failures. However, previously known generic failure detection solutions are only of limited use for Java applications because they do not take into consideration the specifics of the Java language and the Java execution environment. In this article, we present the application-independent Java Application Supervisor (JAS). JAS can automatically detect, log, and resolve a variety of execution problems and failures in Java applications. In most cases, JAS requires neither modifications nor access to the source code of the supervised application. A set of simple user-specified policies guides the failure detection, logging, and recovery process in JAS. A JAS configuration manager automatically generates default policies from the bytecode of an application. The user can modify these default policies as needed. Our experimental studies show that JAS typically incurs little execution time and memory overhead for the target application. We describe an experiment with a Web proxy that exhibits reliability and performance problems under heavy load and demonstrate an increase in the rate of successful requests to the server by almost 33% and a decrease in the average request processing time by approximately 22% when using JAS.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173217,no,undetermined,0
Object-oriented system decomposition quality,"Object-oriented design is becoming very popular in today's software development. An object-oriented information system is decomposed into subjects; each subject is decomposed into classes of objects. Good object-oriented system design should exhibit high cohesion inside subjects and low coupling among subjects. Yet, few quantitative studies of the actual use of cohesion and coupling have been conducted at the system level. These two concepts are defined qualitatively, and only at the class level, not at the system level. In this paper, metrics are introduced for cohesion and coupling and used to define a quality metric at the system level. The feasibility of the approach is demonstrated by an example using a real information system.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173127,no,undetermined,0
Cost-sensitive boosting in software quality modeling,"Early prediction of the quality of software modules prior to software testing and operations can yield great benefits to the software development teams, especially those of high-assurance and mission-critical systems. Such an estimation allows effective use of the testing resources to improve the modules of the software system that need it most and achieve high reliability. To achieve high reliability, by the means of predictive methods, several tools are available. Software classification models provide a prediction of the class of a module, i.e., fault-prone or not fault-prone. Recent advances in the data mining field allow to improve individual classifiers (models) by using the combined decision from multiple classifiers. This paper presents a couple of algorithms using the concept of combined classification. The algorithms provided useful models for software quality modeling. A comprehensive comparative evaluation of the boosting and cost-boosting algorithms is presented. We demonstrate how the use of boosting algorithms (original and cost-sensitive) meets many of the specific requirements for software quality modeling. C4.5 decision trees and decision stumps were used to evaluate these algorithms with two large-scale case studies of industrial software systems.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173102,no,undetermined,0
Is prior knowledge of a programming language important for software quality?,"Software engineering is human intensive. Thus, it is important to understand and evaluate the value of different types of experiences, and their relation to the quality of the developed software. Many job advertisements focus on requiring knowledge of specific programming languages. This may seem sensible at first sight, but maybe it is sufficient to have general knowledge in programming and then it is enough to learn a specific language within the new job. A key question is whether prior knowledge actually does improve software quality. This paper presents an empirical study where the programming experience of students is assessed using a survey at the beginning of a course on the Personal Software Process (PSP), and the outcome of the course is evaluated, for example, using the number of defects and development time. Statistical tests are used to analyse the relationship between programming experience and the performance of the students in terms of software quality. The results are mostly unexpected, for example, we are unable to show any significant relation between experience in the programming language used and the number of defects detected.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166922,no,undetermined,0
A table reduction approach for software structure testing,"One major aspect of software structural testing emphasizes the coverage criterion. Not only procedure-oriented programs, but also member functions of object-oriented programs can be dealt with by structure testing. A program with higher degree of testing coverage indicates that the program might achieve better reliability. There is a conventional metric for testing coverage measurement. However, the metric is affected inadvertently by including undesirable entities. The measurement of coverage rate increases rapidly when the initial groups of test data are executed, which results in the overestimation of the software reliability. The goal of this research is to improve the conventional testing coverage metric, then to use it to precisely estimate the ratio of elements tested. In this paper, a set of rules is developed to identify dominant tested elements. Measuring the coverage rate of the dominant elements can rationalize the measurement results.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1167798,no,undetermined,0
Combining software quality predictive models: an evolutionary approach,"During the last ten years, a large number of quality models have been proposed in the literature. In general, the goal of these models is to predict a quality factor starting from a set of direct measures. The lack of data behind these models makes it hard to generalize, cross-validate, and reuse existing models. As a consequence, for a company, selecting an appropriate quality model is a difficult, non-trivial decision. In this paper, we propose a general approach and a particular solution to this problem. The main idea is to combine and adapt existing models (experts) in such a way that the combined model works well on the particular system or in the particular type of organization. In our particular solution, the experts are assumed to be decision tree or rule-based classifiers and the combination is done by a genetic algorithm. The result is a white-box model: for each software component, not only does the model give a prediction of the software quality factor, it also provides the expert that was used to obtain the prediction. Test results indicate that the proposed model performs significantly better than individual experts in the pool.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1167795,no,undetermined,0
Modeling the cost-benefits tradeoffs for regression testing techniques,"Regression testing is an expensive activity that can account for a large proportion of the software maintenance budget. Because engineers add tests into test suites as software evolves, over time, increased test suite size makes revalidation of the software more expensive. Regression test selection, test suite reduction, and test case prioritization techniques can help with this, by reducing the number of regression tests that must be run and by helping testers meet testing objectives more quickly. These techniques, however can be expensive to employ and may not reduce overall regression testing costs. Thus, practitioners and researchers could benefit from cost models that would help them assess the cost-benefits of techniques. Cost models have been proposed for this purpose, but some of these models omit important factors, and others cannot truly evaluate cost-effectiveness. In this paper, we present new cost-benefits models for regression test selection, test suite reduction, and test case prioritization, that capture previously omitted factors, and support cost-benefits analyses where they were not supported before. We present the results of an empirical study assessing these models.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1167767,no,undetermined,0
Empirical validation of class diagram metrics,"As a key early artefact in the development of OO software, the quality of class diagrams is crucial for all later design work and could be a major determinant for the quality of the software product that is finally delivered. Quantitative measurement instruments are useful to assess class diagram quality in an objective way, thus avoiding bias in the quality evaluation process. This paper presents a set of metrics - based on UML relationships $which measure UML class diagram structural complexity following the idea that it is related to the maintainability of such diagrams. Also summarized are two controlled experiments carried out in order to gather empirical evidence in this sense. As a result of all the experimental work, we can conclude that most of the metrics we proposed (NAssoc, NAgg, NaggH, MaxHAgg, NGen, NgenH and MaxDIT) are good indicators of class diagram maintainability. We cannot, however, draw such firm conclusions regarding the NDep metric.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166940,no,undetermined,0
Elimination of crucial faults by a new selective testing method,"Recent software systems contain a lot of functions to provide various services. According to this tendency, software testing becomes more difficult than before and cost of testing increases so much, since many test items are required. In this paper we propose and discuss such a new selective software testing method that is constructed from previous testing method by simplifying testing specification. We have presented, in the previous work, a selective testing method to perform highly efficient software testing. The selective testing method has introduced an idea of functional priority testing and generated test items according to their functional priorities. Important functions with high priorities are tested in detail, and functions with low priorities are tested less intensively. As a result, additional cost for generating testing instructions becomes relatively high. In this paper in order to reduce its cost, we change the way of giving information, with respect to priorities. The new method gives the priority only rather than generating testing instructions to each test item, which makes the testing method quite simple and results in cost reduction. Except for this change, the new method is essentially the same as the previous method. We applied this new method to actual development of software tool and evaluated its effectiveness. From the result of the application experiment, we confirmed that many crucial faults can be detected by using the proposed method.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166937,no,undetermined,0
An approach for estimation of software aging in a Web server,"A number of recent studies have reported the phenomenon of ""software aging"", characterized by progressive performance degradation or a sudden hang/crash of a software system due to exhaustion of operating system resources, fragmentation and accumulation of errors. To counteract this phenomenon, a proactive technique called ""software rejuvenation"" has been proposed. This essentially involves stopping the running software, cleaning its internal state and then restarting it. Software rejuvenation, being preventive in nature, begs the question as to when to schedule it. Periodic rejuvenation, while straightforward to implement, may not yield the best results. A better approach is based on actual measurement of system resource usage and activity that detects and estimates resource exhaustion times. Estimating the resource exhaustion times makes it possible for software rejuvenation to be initiated or better planned so that the system availability is maximized in the face of time-varying workload and system behavior. We propose a methodology based on time series analysis to detect and estimate resource exhaustion times due to software aging in a Web server while subjecting it to an artificial workload. We first collect and log data on several system resource usage and activity parameters on a Web server. Time-series ARMA models are then constructed from the data to detect aging and estimate resource exhaustion times. The results are then compared with previous measurement-based models and found to be more efficient and computationally less intensive. These models can be used to develop proactive management techniques like software rejuvenation which are triggered by actual measurements.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166929,no,undetermined,0
Estimating mixed software reliability models based on the EM algorithm,"This paper considers the mixture models to unify software reliability models (SRMs). The modeling framework is based on the randomization of a software fault detection rate. Our models, called the mixed SRMs in this paper, can include some existing SRMs such as Littlewood NHPP model and the hyperexponential SRM. We develop an efficient parameter estimation algorithm for the mixed SRMs based on the EM (Expectation-Maximization) principle, and investigate both the effectiveness of the EM algorithm and the validity of the mixed SRMs through the numerical examples with real software failure data.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166927,no,undetermined,0
An approach to experimental evaluation of software understandability,"Software understandability is an important characteristic of software quality because it can influence cost or reliability of software evolution in reuse or maintenance. However, it is difficult to evaluate software understandability in practice because understanding is an internal process of humans. This paper proposes ""software overhaul"" as a method for externalizing the process of understanding and presents a probability model to use process data of overhauling to estimate software understandability. An example describes an overhaul tool and its application.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166925,no,undetermined,0
Engineering real-time behavior,"This article presents a process that evaluates an application for real-time correctness throughout development and maintenance. It allows temporal correctness to be designed-in during development, rather than the more typical effort to test-in timing performance at the end of development. It avoids the costly problems that can arise when timing faults are found late in testing or, worse still, after deployment.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1048980,no,undetermined,0
Application of linguistic techniques for Use Case analysis,"The Use Case formalism is an effective way of capturing both business process and functional system requirements in a very simple and easy-to-learn way. Use Cases may be modeled in a graphical way (e.g. using the UML notation), mainly serving as a table of content for Use Cases. System behavior can more effectively be specified by structured natural language (NL) sentences. The use of NL as a way to specify the behavior of a system is however a critical point, due to the inherent ambiguity originating from different interpretations of natural language descriptions. We discuss the use of methods, based on a linguistic approach, to analyze functional requirements expressed by means of textual (NL) Use Cases. The aim is to collect quality metrics and detect defects related to such inherent ambiguity. In a series of preliminary experiments, we applied a number of tools for quality evaluation of NL text (and, in particular, of NL requirements documents) to an industrial Use Cases document. The result of the analysis is a set of metrics that aim to measure the quality of the NL textual description of Use Cases. We also discuss the application of selected linguistic analysis techniques that are provided by some of the tools to semantic analysis of NL expressed Use Case.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1048518,no,undetermined,0
Reducing No Fault Found using statistical processing and an expert system,"This paper describes a method for capturing avionics test failure results from Automated Test Equipment (ATE) and statistically processing this data to provide decision support for software engineers in reducing No Fault Found (NFF) cases at various testing levels. NFFs have plagued the avionics test and repair environment for years at enormous cost to readiness and logistics support. The costs in terms of depot repair and user exchange dollars that are wasted annually for unresolved cases are graphically illustrated. A diagnostic data model is presented, which automatically captures, archives and statistically processes test parameters and failure results which are then used to determine if an NFF at the next testing level resulted from a test anomaly. The model includes statistical process methods, which produce historical trend patterns for each part and serial numbered unit tested. An Expert System is used to detect statistical pattern changes and stores that information in a knowledge base. A Decision Support System (DSS) provides advisories for engineers and technicians by combining the statistical test pattern with unit performance changes in the knowledge base. Examples of specific F-16 NFF reduction results are provided.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1047966,no,undetermined,0
Strategy to improve the indoor coverage for mobile station,"This paper presents an evaluation of whether the indoor signal strength for commercial buildings fulfills the cell planning requirement. In order to provide high quality cellular service, it is necessary to place an array of distributed antennas connected using feeder cable within the building. With the feeder cable approach, splitters and computer software, we develop using Visual Basic 6.0. The effective radiated power (ERP) at the distributed antenna can be calculated and it can also be predicted how far the signal can go with the calculated ERP. This design process can be used to obtain an estimated indoor system requirement. All of the requirements can be achieved by applying the method in this paper.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1033145,no,undetermined,0
Experience of applying statistical control techniques to the function test phase of a large telecommunications system,"The software test process of a large telecommunications system is presented. The feasibility of introducing statistical process control techniques to one crucial software test phase, namely the function test is explored. This phase has been identified as a strategic one for meeting the commitments to customers with respect to quality objectives. Analysis of past released products revealed that a high percentage of the failures experienced in operation corresponded to software faults that could have been discovered during the function test phase. A brief description of the statistical estimators investigated (Classical vs. Bayesian) is provided, along with a few examples of their use over real sets of data. Far from being aimed at identifying new statistical models, the focus of this work is rather about putting measurement in practice: easy and effective steps to improve the status of control over the test process in a smooth, bottom-up approach are suggested",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1040129,no,undetermined,0
Using SPIN model checking for flight software verification,"Flight software is the central nervous system of modern spacecraft. Verifying spacecraft flight software to assure that it operates correctly and safely is presently an intensive and costly process. A multitude of scenarios and tests must be devised, executed and reviewed to provide reasonable confidence that the software will perform as intended and not endanger the spacecraft. Undetected software defects on spacecraft and launch vehicles have caused embarrassing and costly failures in recent years. Model checking is a technique for software verification that can detect concurrency defects that are otherwise difficult to discover. Within appropriate constraints, a model checker can perform an exhaustive state-space search on a software design or implementation and alert the implementing organization to potential design deficiencies. Unfortunately, model checking of large software systems requires an often-too-substantial effort in developing and maintaining the software functional models. A recent development in this area, however, promises to enable software-implementing organizations to take advantage of the usefulness of model checking without hand-built functional models. This development is the appearance of ""model extractors"". A model extractor permits the automated and repeated testing of code as built rather than of separate design models. This allows model checking to be used without the overhead and perils involved in maintaining separate models. We have attempted to apply model checking to legacy flight software from NASA's Deep Space One (DS1) mission. This software was implemented in C and contained some known defects at launch that are detectable with a model checker. We describe the model checking process, the tools used, and the methods and conditions necessary to successfully perform model checking on the DS1 flight software.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1036832,no,undetermined,0
Upgrading engine test cells for improved troubleshooting and diagnostics,"Upgrading military engine test cells with advanced diagnostic and troubleshooting capabilities will play a critical role in increasing aircraft availability and test cell effectiveness while simultaneously reducing engine operating and maintenance costs. Sophisticated performance and mechanical anomaly detection and fault classification algorithms utilizing thermodynamic, statistical, and empirical engine models are now being implemented as part of a United States Air Force Advanced Test Cell Upgrade Initiative. Under this program, a comprehensive set of realtime and post-test diagnostic software modules, including sensor validation algorithms, performance fault classification techniques and vibration feature analysis are being developed. An automated troubleshooting guide is also being implemented to streamline the troubleshooting process for both inexperienced and experienced technicians. This artificial intelligence based tool enhances the conventional troubleshooting tree architecture by incorporating probability of occurrence statistics to optimize the troubleshooting path. This paper describes the development and implementation of the F404 engine test cell upgrade at the Jacksonville Naval Air Station.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1036142,no,undetermined,0
Watermark detection: benchmarking perspectives,"Benchmarking of watermarking algorithms is a complicated task that requires examination of a set of mutually dependent performance factors (algorithm complexity, decoding/detection performance, and perceptual quality). This paper will focus on detection/decoding performance evaluation and try to summarize its basic principles. A methodology for deriving the corresponding performance metrics will also be provided.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1035654,no,undetermined,0
A new audio skew detection and correction algorithm,"The lack of synchronisation between a sender clock and a receiver audio clock in an audio application results in an undesirable effect known as ""audio skew"". This paper proposes and implements a new approach to detecting and correcting audio skew, focusing on the accuracy of measurements and on the algorithm's effect on the audio experience of the listener. The algorithms presented are shown to remove audio skew successfully, thus reducing delay and loss and hence improving audio quality.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1035569,no,undetermined,0
"Strategies for fault-tolerant, space-based computing: Lessons learned from the ARGOS testbed","The Advanced Space Computing and Autonomy Testbed on the ARGOS satellite provides the first direct, on orbit comparison of a modem radiation hardened 32 bit processor with a similar COTS processor. This investigation was motivated by the need for higher capability computers for space flight use than could be met with available radiation hardened components. The use of COTS devices for space applications has been suggested to accelerate the development cycle and produce cost effective systems. Software-implemented corrections of radiation-induced SEUs (SIHFT) can provide low-cost solutions for enhancing the reliability of these systems. We have flown two 32-bit single board computers (SBCs) onboard the ARGOS spacecraft. One is full COTS, while the other is RAD-hard. The COTS board has an order of magnitude higher computational throughput than the RAD-hard board, offsetting the performance overhead of the SIHFT techniques used on the COTS board while consuming less power.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1035377,no,undetermined,0
"QoS tradeoffs for guidance, navigation, and control","Future space missions will require onboard autonomy to reduce data, plan activities, and react appropriately to complex dynamic events. Software to support such behaviors is computationally-intensive but must execute with sufficient speed to accomplish mission goals. The limited processing resources onboard spacecraft must be split between the new software and required guidance, navigation, control, and communication tasks. To-date, control-related processes have been scheduled with fixed execution period, then autonomy processes are fit into remaining slack time slots. We propose the use of quality-of-service (QoS) negotiation to explicitly trade off the performance of all processing tasks, including those related to spacecraft control. We characterize controller performance based on exhaustive search and a Lyapunov optimization technique and present results that analytically predict worst-case performance degradation characteristics. The results are illustrated by application to a second-order linear system with a linear state feedback control law.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1035311,no,undetermined,0
Neighborhood selection for I<sub>DDQ</sub> outlier screening at wafer sort,"To screen defective dies, I<sub>DDQ</sub> tests require a reliable estimate of each die's defect-free measurement. The nearest-neighbor residual (NNR) method provides a straightforward, data-driven estimate of test measurements for improved identification of die outliers",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1033795,no,undetermined,0
Application of ANN to power system fault analysis,"This paper presents the computer architecture development using Artificial Neural Network (ANN) as an approach for predicting fault in a large interconnected transmission system. Transmission line faults can be classified using the bus voltage and line fault current. Monitoring the performance of these two factors are very useful for power system protection devices. The ANN is designed to be incorporated with a matrix based software tool MATLAB Version 6.0, which deals with fault diagnosis in power system. In MATLAB software modules, the balanced and unbalanced fault can be simulated. The data generated from this software are to be used as training and testing sets in the Neural Ware Simulator.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1033109,no,undetermined,0
An application of Bayesian reasoning to improve functional test diagnostic effectiveness,"This paper describes a software package that embodies a Bayesian reasoning engine and modeling schema to significantly improve the ability to discern the defective component causing a failed functional test. This software approach brings to functional test similar diagnostic capabilities that have become familiar to test engineers working with X-ray, automatic optical inspection (AOI) and in-circuit test (ICT) test technologies. This software package, known as Fault Detective, provides significantly improved diagnostic accuracy as compared to human efforts, and works with exactly the same data set as is currently available for diagnostic purposes. The model is based on the interaction of the functional test suite with the product functional block diagram. This approach also means that the software package is highly independent of the technology behind the system being diagnosed.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1047952,no,undetermined,0
A LabVIEW based data acquisition system for vibration monitoring and analysis,"LabVIEW (Laboratory Virtual Instrument Engineering Workbench) is gaining popularity as a graphical programming language, especially for data acquisition and measurement. This is due to the vast array of data acquisition cards and measurement systems which can be supported by LabVIEW as well as the relative ease with which advanced software can be programmed. One area of application of LabVIEW is the monitoring and analysis of vibration signals. The analysis and monitoring of the signal are of concern for fault detection and predictive maintenance. This paper describes LabVIEW based data acquisition and analysis developed specifically for vibration monitoring and used with vibration fault simulation systems (VFSS). On-line displays of time and frequency domains of the vibration signal provide a user-friendly data acquisition interface.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1033055,no,undetermined,0
"Mathematical modeling, performance analysis and simulation of current Ethernet computer networks","This work describes an object oriented software, to be portable among different types of computer platforms, able to perform the simulation of different components of Ethernet local and long distance area networks by using the TCP/IP protocol. It is also able to detect problems in projects and operation of communication networks through the separate or joint analysis of these elements. The functions of the system are as follows: (a) analysis of elements from different layers and protocols of the simulated network (reference to OSI model); (b) analysis and efficiency measurement (quality of transmission, transfer rate, error rate) of the information transmitted on the network; (c) network performance evaluation and link capability analysis; (d) analysis of error detection and further correction capability as well as analysis of network failure tolerance. The software works using mathematical models that represent elements of different layers (reference to OSI model) of the network to be simulated as well as the performance of the abovementioned joint elements.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1032612,no,undetermined,0
Air damping of mechanical microbeam resonators with optical excitation and optical detection,"This paper is dedicated to both Finite Element Method (FEM) simulations and optical measurements of the mechanical quality factor dependence on air pressure of microbeam resonators. Firstly, a summary of previous theoretical works is given, for two different air cavity cases. Different damping mechanisms are discussed. Secondly, FEM simulation results using MEMCAD software are obtained for resonators with small air gaps. Finally, prototypes of silicon resonators are fabricated using SOI technology, and characterized under a precisely controlled vacuum chamber. In order to investigate both air cavity cases, optical excitation/detection is used. Good agreements are obtained with theory and simulations.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1031443,no,undetermined,0
Predictable instruction caching for media processors,"The determinism of instruction cache performance can be considered a major problem in multimedia devices which hope to maximise their quality of service. If instructions are evicted from the cache by competing blocks of code, the running application will take significantly longer to execute than if the instructions were present. Since it is difficult to predict when this interference will occur the performance of the algorithm at a given point in time is unclear We propose the use of an automatically configured partitioned cache to protect regions of the application code from each other and hence minimise interference. As well as being specialised to the purpose of providing predictable performance, this cache can be specialised to the application being run, rather than for the average case, using simple compiler algorithms.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1030713,no,undetermined,0
Error rate estimation for a flight application using the CEU fault injection approach,This paper aims at validating the efficiency of a fault injection approach to predict error rate on applications devoted to operate in radiation environment. Soft error injection experiments and radiation ground testing were performed on software modules using a digital board built on a digital signal processor which is included in a satellite instrument. The analysis of experimental results put in evidence the potentialities offered by the used methodology to predict the error rate of complex applications.,2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1030218,no,undetermined,0
Bit flip injection in processor-based architectures: a case study,"This paper presents the principles of two different approaches for the study of the effect of transient bit flips on the behavior of processor-based digital architectures: one of them based on the on-line ""injection"" and execution of pieces of code (called CEU codes) using a suitable hardware architecture, while the other is performed using a behavioral level processor description; being based on the so-called ""saboteurs"" method. Results obtained for benchmark programs executed by a widely used commercial 8-bit microprocessor, allow to validate both approaches which provide inputs for an original error rate prediction methodology. The comparison of predictions to measured error rates issued from radiation ground testing validates the proposed error rate prediction approach.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1030194,no,undetermined,0
Analysis of SEU effects in a pipelined processor,"Modern processors embed features such as pipelined execution units and cache memories that can hardly be controlled by programmers through the processor instruction set. As a result, software-based fault injection approaches are no longer suitable for assessing the effects of SEUs in modern processors, since they are not able to evaluate the effects of SEUs affecting pipelines and caches. In this paper we report an analysis of a commercial processor core where the effects of SEUs located in the processor pipeline and cache memories are studied. Moreover the obtained results are compared with those software-based approaches provide. Experimental results show that software-based approaches may lead to errors during the failure rate estimation of up to 400%.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1030193,no,undetermined,0
A system level approach in designing dual-duplex fault tolerant embedded systems,"This paper presents an approach for designing embedded systems able to tolerate hardware faults, defined as an evolution of our previous work proposing an hardware/software co-design framework for realizing reliable embedded systems. The framework is extended to support the designer in achieving embedded systems with fault tolerant properties minimizing overheads and limiting power consumption. A reference system architecture is proposed; the specific hardware/software implementation and reliability methodologies (to achieve the fault tolerance properties) are the result of an enhanced hw/sw partitioning process driven by the designer' constraints and by the reliability constraints, set at the beginning of the design process. By introducing also the reliability constraints during specification, the final system can benefit from the introduced redundancy also for performance gains, while limiting area, time, performance and power consumption overheads.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1030180,no,undetermined,0
Control-theoretic bandwidth-on-demand protocol for satellite networks,"In this paper, a control-theoretic DAMA (Demand Assignment Multiple Access) protocol for ATM-based geostationary (GEO) satellite networks is introduced The proposed scheme, developed for the European Union, project ""GOECAST"" (Information Society and Technology (IST), programme), deals with the lower priority traffic categories, which have to adapt their transmission rates to the traffic conditions. The novelty of this paper consists of the use of control theory concepts to model the satellite system and to generate the bandwidth requests. The proposed scheme is stable, avoids wasting the assigned capacity and is capable of dealing with congestions. Software simulations have been performed with the OPNET tool, in order to test the effectiveness of the proposed protocol.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1040241,no,undetermined,0
Quality-based tuning of cell downlink load target and link power maxima in WCDMA,"The objective of the paper is to validate the feasibility of auto-tuning WCDMA link power maxima and adjust cell downlink load level targets based on quality of service. The downlink cell load level is measured using total wideband transmission power. The quality indicators used are call-blocking probability, packet queuing probability and downlink link power outage. The objective is to improve performance and operability of the network with control software aiming for a specific quality of service. The downlink link maxima in each cell are regularly adjusted with a control method in order to improve performance under different load level targets. The approach is validated using a dynamic WCDMA system simulator. The conducted simulations support the assumption that the downlink performance can be managed and improved by the proposed cell-based automated optimization.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1040620,no,undetermined,0
First experiments relating behavior selection architectures to environmental complexity,"Assessing the performance of behavior selection architectures for autonomous robots is a complex task that depends on many factors. This paper reports a study comparing four motivated behavior-based architectures in different worlds with varying degrees and types of complexity, and analyzes performance results (in terms of viability, life span, and global life quality) relating architectural features to environmental complexity.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1041732,no,undetermined,0
Static analysis of SEU effects on software applications,"Control flow errors have been widely addressed in literature as a possible threat to the dependability of computer systems, and many clever techniques have been proposed to detect and tolerate them. Nevertheless, it has never been discussed if the overheads introduced by many of these techniques are justified by a reasonable probability of incurring control flow errors. This paper presents a static executable code analysis methodology able to compute, depending on the target microprocessor platform, the upper-bound probability that a given application incurs in a control flow error.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1041800,no,undetermined,0
Binarization of low quality text using a Markov random field model,"Binarization techniques have been developed in the document analysis community for over 30 years and many algorithms have been used successfully. On the other hand, document analysis tasks are more and more frequently being applied to multimedia documents such as video sequences. Due to low resolution and lossy compression, the binarization of text included in the frames is a non-trivial task. Existing techniques work without a model of the spatial relationships in the image, which makes them less powerful. We introduce a new technique based on a Markov random field model of the document. The model parameters (clique potentials) are learned from training data and the binary image is estimated in a Bayesian framework. The performance is evaluated using commercial OCR software.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1047819,no,undetermined,0
Study of integration method for on-line monitoring system of electrical equipment insulation and AM/FM/GIS,"After analyzing integration. of SCADA with AM/FM/GIS in distribution management system (DMS), we point out that integrated on-line SCADA/AM/FM/GIS just monitors macroscopic run specificity of the electric power system, but does not take into account insulation run state and fault diagnosis situation of electrical equipment, so it has certain limitations. Meanwhile, the classification and function of an on-line monitoring system for electrical equipment are discussed in this paper, and a thought is proposed that integrates AM/FM/GIS with on-line monitoring system of electrical equipment. The new integrated system not only comprehensively reflects the run state of electric power system and equipment but also can monitor on-line insulation state of electrical equipment, so it has quite good economic and practical value. A few of methods to integrate AM/FM/GIS with equipment insulation on-line monitor system are proposed by studying the direction of modern software exploitation technique and practical application. The practical application instance shows that it has excellent effectiveness.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1047568,no,undetermined,0
Teletraffic simulation of cellular networks: modeling the handoff arrivals and the handoff delay,The paper presents an analysis of teletraffic variables in cellular networks. The variables studied are the time between two consecutive handoff arrivals and the handoff delay. These teletraffic variables are characterized by means of an advanced software simulator that models several scenarios assuming fixed channel allocation. Information about the quality of service is also provided. A large set of scenarios has been simulated and the characterization results derived from its study have been presented and analyzed.,2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1046536,no,undetermined,0
SPiCE in action - experiences in tailoring and extension,"Today the standard ISO/IEC TR 15504: software process assessment commonly known as SPiCE has been in use for more than 5 years, with hundreds of software process assessments performed in organizations around the world. The success of the ISO 15504 approach is demonstrated by its application in and extension to all sectors featuring software development, in particular, space, automotive, finance, healthcare, and electronics. As the current Technical Report makes the transition into an international standard, many initiatives are underway to expand the application of process assessment to areas even outside of software development. This paper reports on experiences in the use of ISO 15504 both in tailoring the standard for particular industrial sectors and in expanding the process assessment approach into new domains. In particular, three projects are discussed: SPiCE for SPACE, a ISO/IEC TR 15504 conformant method of software process assessment developed for the European space industry; SPiCE-9000 for SPACE, an assessment method for space quality management systems, based on ISO 9001:2000; and NOVE-IT, a project of the Swiss federal government to establish and assess processes covering IT procurement, development, operation, and service provision.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1046215,no,undetermined,0
Software quality knowledge discovery: a rough set approach,"This paper presents a practical knowledge discovery approach to software quality and resource allocation that incorporated recent advances in rough set theory, parameterized approximation spaces and rough neural computing. In addition, this research utilizes the results of recent studies of software quality measurement and prediction. A software quality measure quantifies the extent, to which some specific attribute is present in a system. Such measurements are considered in the context of rough sets. This research provides a framework for making resource allocation decisions based on evaluation of various measurements of the complexity of software. Knowledge about software quality is gained when preprocessing during which, software measurements are analyzed using discretization techniques, genetic algorithms in deriving reducts, and in the derivation of training and testing sets, especially in the context of the rough sets exploration system (RSES) developed by the logic group at the Institute of Mathematics at Warsaw University. Experiments show that both RSES and rough neural network models are effective in classifying software modules.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1045165,no,undetermined,0
Maintenance in joint software development,"The need to combine several efforts in software development has become critical because of the software development requirements and geographically dispersed qualified human resources, background skills, working methods, and software tools among autonomous software enterprises - joint software development. We know that the organization and the development of software depend largely on human initiatives. All human initiatives are subject to change and perpetual evolution. A variety of studies have contributed to highlight the problems arising from the change and the perpetual evolution of the software development. These studies revealed that the majority of the effort spent on the software process is spent on maintenance. To reduce the efforts of software maintenance in joint software development, it is therefore necessary to detect the features of software maintenance processes susceptible to be automatized. The software maintenance problems in joint software development are caused by the changes and perpetual evolutions at the organizational level of an enterprise or at the software development level. The complex nature of changes and perpetual evolutions at the organizational and the development levels includes the following factors: the maintenance contracts among enterprises, the abilities to develop, experiences and background in software development, methodologies of software development, tools available and localization of the organizations. This paper looks into the new software maintenance problems in joint software development by geographically dispersed virtual enterprises",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1045152,no,undetermined,0
A structured approach to handling on-line interface upgrades,"The integration of complex systems out of existing systems is an active area of research and development. There are many practical situations in which the interfaces of the component systems, for example belonging to separate organisations, are changed dynamically and without notification. In this paper we propose an approach to handling such upgrades in a structured and disciplined fashion. All interface changes are viewed as abnormal events and general fault tolerance mechanisms (exception handling, in particular) are applied to dealing with them. The paper outlines general ways of detecting such interface upgrades and recovering after them. An Internet Travel Agency is used as a case study",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1045137,no,undetermined,0
Private information retrieval in the presence of malicious failures,"In the application domain of online information services such as online census information, health records and real-time stock quotes, there are at least two fundamental challenges: the protection of users' privacy and the assurance of service availability. We present a fault-tolerant scheme for private information retrieval (FT-PIR) that protects users' privacy and ensure service provision in the presence of malicious server failures. An error detection algorithm is introduced into this scheme to detect the corrupted results from servers. The analytical and experimental results show that the FT-PIR scheme can tolerate malicious server failures effectively and prevent any information of users front being leaked to attackers. This new scheme does not rely on any unproven cryptographic premise and the availability of tamperproof hardware. An implementation of the FT-PIR scheme on a distributed database system suggests just a modest level of performance overhead.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1045104,no,undetermined,0
Managing software quality with defects,This paper describes two common approaches to measuring and modeling software quality across the project life cycle so that it can be made visible to management. It discusses examples of their application in real industry settings. Both of the examples presented come from CMM Level 4 organizations.,2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1045048,no,undetermined,0
Edge color distribution transform: an efficient tool for object detection in images,"Object detection in images is a fundamental task in many image analysis applications. Existing methods for low-level object detection always perform the color-similarity analyses in the 2D image space. However, the crowded edges of different objects make the detection complex and error-prone. The paper proposes to detect objects in a new edge color distribution space (ECDS) rather than in the image space. In the 3D ECDS, the edges of different objects are segregated and the spatial relation of a same object is kept as well, which make the object detection easier and less error-prone. Since uniform-color objects and textured objects have different distribution characteristics in ECDS, the paper gives a 3D edge-tracking algorithm for the former and a cuboid-growing algorithm for the latter. The detection results are correct and noise-free, so they are suitable for the high-level object detection. The experimental results on a synthetic image and a real-life image are included.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1044814,no,undetermined,0
Rejection strategies and confidence measures for a k-NN classifier in an OCR task,"In handwritten character recognition, the rejection of extraneous patterns, like image noise, strokes or corrections, can improve significantly the practical usefulness of a system. In this paper a combination of two confidence measures defined for a k-nearest neighbors (NN) classifier is proposed. Experiments are presented comparing the performance of the same system with and without the new rejection rules.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1044806,no,undetermined,0
A fuzzy logic framework to improve the performance and interpretation of rule-based quality prediction models for OO software,"Current object-oriented (OO) software systems must satisfy new requirements that include quality aspects. These, contrary to functional requirements, are difficult to determine during the test phase of a project. Predictive and estimation models offer an interesting solution to this problem. This paper describes an original approach to build rule-based predictive models that are based on fuzzy logic and that enhance the performance of classical decision trees. The approach also attempts to bridge the cognitive gap that may exist between the antecedent and the consequent of a rule by turning the latter into a chain of sub rules that account for domain knowledge. The whole framework is evaluated on a set of OO applications.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1044543,no,undetermined,0
Implementation and performance of cooperative control of shunt active filters for harmonic damping throughout a power distribution system,"This paper proposes the cooperative control of multiple active filters based on voltage detection for harmonic damping throughout a power distribution system. The arrangement of a real distribution system would be changed according to system operation, and/or fault conditions. In addition, shunt capacitors and loads are individually connected to, or disconnected from, the distribution system. Independent control might make multiple active filters produce unbalanced compensating currents. This paper presents hardware and software implementations of cooperative control for two active filters. Experimental results verify the effectiveness of the cooperative control with the help of a communication system.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1044066,no,undetermined,0
Equal resistance control-a control methodology for line-connected converters that offers balanced resistive load under unbalanced input voltage conditions,"The result of several control schemes, applied to three-phase line-connected PWM converters under unbalanced input voltage conditions, is to further reduce the utility supply voltages due to the drawn currents. The proposed method, named equal resistance control (ERC), considers the health of the phases during unbalanced conditions in order to minimize the effects of the drawn power on the network voltages. The control philosophy is to make the current drawn from a particular phase proportional to the amplitude of remaining voltage on that phase and in-synchronism with its phase angle. The power transfer to the load is therefore optimal since no reactive power is drawn from the network. The ERC philosophy uses a feedforward per-phase control structure, where the voltage drop across the input impedance is estimated. An application example is included where this scheme is applied to a power quality device designed to mitigate voltage disturbances. Theoretical analysis, computer simulations and experimental measurements are included.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1043747,no,undetermined,0
Measuring Web application quality with WebQEM,"This article discusses using WebQEM, a quantitative evaluation strategy to assess Web site and application quality. Defining and measuring quality indicators can help stakeholders understand and improve Web products. An e-commerce case study illustrates the methodology's utility in systematically assessing attributes that influence product quality",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1041945,no,undetermined,0
Application of high-quality built-in test to industrial designs,"This paper presents an approach for high-quality built-in test using a neighborhood pattern generator (NPG). The proposed NPG is practically acceptable because (a) its structure is independent of circuit under test, (b) it requires low area overhead and no performance degradation, and (c) it can encode deterministic test cubes, not only for stuck-at faults but also transition faults, with high probability. Experimental results for large industrial circuits illustrate the efficiency of the proposed approach.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1041856,no,undetermined,0
Testing finite state machines based on a structural coverage metric,"Verification is a critical phase in the development of any hardware and software system. Finite state machines have been widely used to model hardware and software systems. Therefore, testing finite state machines (FSMs) is an important issue. Coverage analysis of a test suite for a system's implementation determines the adequacy and the confidence level of the verification phase. In this paper, we derive a fault coverage metric for a test suite for an FSM specification. We also extend this metric for fault coverage estimation of interconnected FSMs, and we propose symbolic input based fault coverage for large FSMs. Finally, we also study incremental construction of a test suite associated with a coverage for a given FSM specification.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1041830,no,undetermined,0
Immune mechanism based computer security design,"Referring to the mechanism of biological immune system, a novel model of computer security system is proposed, which is a dynamic, multi-layered and co-operational system. Through dynamically supervising abnormal behaviors with multi agent immune systems, a two-level defense system is set up for improving the whole performance: one is based on a host and mainly used for. detecting viruses; and the other is based on a network for supervising potential attacks. On the other hand, a pseudo-random technology is adopted for designing the sub-system of data transmission, in order to increase the ability of protecting information against intended interference and monitoring. Simulations on information transmission show that this system has good robustness, error tolerance and self-adaptiveness, although more practice is needed.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1175366,no,undetermined,0
Integrated analysis and design method for distributed real-time systems based on computational grid,"Advances in networking infrastructure have led to the development of a new type of ""computational grid"" infrastructure that provides predictable, consistent and uniform access to geographically distributed resources such as computers, data repositories, scientific instruments, and advanced display devices. Such Grid environments are being used to construct sophisticated, performance-sensitive applications in such areas as dynamic, distributed real-time applications. We propose an integrated approach for analyzing and designing distributed real-time systems based on the computational grid. The proposed approach is based on a new methodology and integrates the models that the methodology proposed for analyzing and designing real-time systems.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1175576,no,undetermined,0
An integrated approach for automation of distribution system,"Normally the distribution automation functions are studied individually. In this paper the step-by-step approach to automate the distribution system is discussed. It cant be applied to existing distribution systems as well as for new design of distribution systems. This integrated approach begins with radial distribution systems, preparation of data base, load flow, location of sectionalizing, tie switches, capacitor switching system reconfiguration, fault identification, supply restoration and metering at substation. Current and voltage signals are sampled and a digital signal processing algorithm is used for phasor estimation. The status of DA system equipment can also be viewed and monitored by user-friendly software.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1177786,no,undetermined,0
Providing packet-loss guarantees in DiffServ architectures,"Differentiated Services (DiffServ) is a proposed architecture for the Internet in which various applications are supported using a simple classification scheme. Packets entering the DiffServ domain are marked depending on the packets' class. Premium service and assured service are the first two types of services proposed within the DiffServ architecture other than the best effort service. Premium service provides a strict guarantee on users' peak rates, hence delivering the highest quality of service (QoS). However, it expects to charge at high prices and also has low bandwidth utilization. The assured service provides high priority packets with preferential treatment over low priority packets but without any quantitative QoS guarantees. In this paper, we propose a new service, which is called loss guaranteed (LG) service for DiffServ architectures. This service can provide a quantitative QoS guarantee in terms of loss rate. A measurement-based admission control scheme and a signaling protocol are designed to implement the LG service. An extensive simulation model has been developed to study the performance and viability of the LG service model. We have tested a variety of traffic conditions and measurement parameter settings in our simulation. The results show that the LG can achieve a high level of utilization while still reliably keeping the traffic within the maximum loss rate requirement. Indeed, we show that the DiffServ architecture can provide packet-loss guarantees without the need for explicit resource reservation",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=995146,no,undetermined,0
VC rating and quality metrics: why bother? [SoC],"System-on-a-chip (SoC) is the paramount challenge of the electronic industry for the next millennium. The semiconductor industry has delivered what we were expecting and what was predicted: silicon availability for over 10 million gates. The VSIA (Virtual Socket Initiative Alliance) has defined industry standards and data formats for SoC. The reuse methodology manual, first 'how-to-do' book to create reusable IPs (intellectual properties) for SoC designs has been published. EDA tool providers understand the issues and are proposing new tools and solutions on a quarterly basis. The last stage needs to be run: consolidate the experience and know-how of VSIA and IP OpenMORE rating system into an industry adopted VC (virtual component) quality metrics, and then pursue to tackle the next challenges: formal system specifications and VC transfer infrastructure. The objective of this paper is to set the stage for the final step towards a VC quality metrics effort that the industry needs to adopt, and define the next achievable goals.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=996745,no,undetermined,0
Condition based maintenance of PE/XLPE-insulated medium voltage cable networks-verification of the IRC-analysis to determine the cable age,For a preventive maintenance and a more economical operation of polymer insulated cable networks a destruction free estimation of the status of laid PE/XLPE-cables is a basic need. This contribution presents a condition based maintenance concept with the IRC-Analysis applied on a part of the 20 kV-cable network of a major German utility. With this concept the distributor was able to reduce the amount of insulation failures-especially for cables manufactured before 1984. The general tendency of increasing faults was broken and a higher quality of energy supply was achieved.,2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=995879,no,undetermined,0
On the predictability of program behavior using different input data sets,"Smaller input data sets such as the test and the train input sets are commonly used in simulation to estimate the impact of architecture/micro-architecture features on the performance of SPEC benchmarks. They are also used for profile feedback compiler optimizations. In this paper, we examine the reliability of reduced input sets for performance simulation and profile feedback optimizations. We study the high level metrics such as IPC and procedure level profiles as well as lower level measurements such as execution paths exercised by various input sets on the SPEC2000int benchmark. Our study indicates that the test input sets are not suitable to be used for simulation because they do not have an execution profile similar to the reference input runs. The train data set is better than the test data sets at maintaining similar profiles to the reference input set. However, the observed execution paths leading to cache misses are very different between using the smaller input sets and the reference input sets. For current profile based optimizations, the differences in quality of profiles may not have a significant impact on performance, as tested on the Itanium processor with an Intel compiler. However, we believe the impact of profile quality will be greater for more aggressive profile guided optimizations, such as cache prefetching",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=995842,no,undetermined,0
Architecture-centric software evolution by software metrics and design patterns,"It is shown how software metrics and architectural patterns can be used for the management of software evolution. In the presented architecture-centric software evolution method the quality of a software system is assured in the software design phase by computing various kinds of design metrics from the system architecture, by automatically exploring instances of design patterns and anti-patterns from the architecture, and by reporting potential quality problems to the designers. The same analysis is applied in the implementation phase to the software code, thus ensuring that it matches the quality and structure of the reference architecture. Finally, the quality of the ultimate system is predicted by studying the development history of previous projects with a similar composition of characteristic software metrics and patterns. The architecture-centric software evolution method is supported by two integrated software tools, the metrics and pattern-mining tool Maisa and the reverse-engineering tool Columbus",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=995795,no,undetermined,0
SUE inspection: an effective method for systematic usability evaluation of hypermedia,"SUE inspection is a novel usability evaluation method for hypermedia, which falls into the category of inspection methods. Its primary goal is to favor a systematic usability evaluation (SUE), for supplying hypermedia usability inspectors with a structured flow of activities, allowing them to obtain more reliable, comparable, and cost-effective evaluation results. This is obtained especially due to the use of evaluation patterns, called abstract tasks (AT), which describe in details the activities that evaluators must carry out during inspection. AT helps share and transfer evaluation know-how among different evaluators, thus making it easier to learn the SUE inspection method by newcomers. A further notable advantage provided by the SUE inspection over other existent approaches is that it focuses on navigation and information structures, making evident some problems that other ""surface-oriented"" approaches might not reveal. This paper describes the SUE inspection for hypermedia. It also reports on a validation experiment, involving 28 evaluators, that has shown the major effectiveness and efficiency of the SUE inspection for hypermedia, with respect to traditional heuristic evaluation techniques",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=995532,no,undetermined,0
Energizing software engineering education through real-world projects as experimental studies,"Our experience shows that a typical industrial project can enhance software engineering research and bring theories to life. The University of Kentucky (UK) is in the initial phase of developing a software engineering curriculum. The first course, a graduate-level survey of software engineering, strongly emphasized quality engineering. assisted by the UK clinic, the students undertook a project to develop a phenylalanine milligram tracker. It helps phenylketonuria (PKU) sufferers to monitor their diet as well as assists PKU researchers to collect data. The project was also used as an informal experimental study. The applied project approach to teaching software engineering appears to be successful thus far. The approach taught many important software and quality engineering principles to inexperienced graduate students in an accurately simulated industrial development environment. It resulted in the development of a framework for describing and evaluating such a real-world project, including evaluation of the notion of a user advocate. It also resulted in interesting experimental trends, though based on a very small sample. Specifically, estimation skills seem to improve over time and function point estimation may be more accurate than LOC estimation",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=995211,no,undetermined,0
Adapting extreme programming for a core software engineering course,"Over a decade ago, the manufacturing industry determined it needed to be more agile to thrive and prosper in a changing, nonlinear, uncertain and unpredictable business environment The software engineering community has come to the same realization. A group of software methodologists has created a set of software development processes, termed agile methodologies that have been specifically designed to respond to the demands of the turbulent software industry. Each of the processes in the set of agile processes comprises a set of practices. As educators, we must assess the emerging agile practices, integrate them into our courses (carefully), and share our experiences and results from doing so. The paper discusses the use of extreme programming, a popular agile methodology, in a senior software engineering course at North Carolina State University. It then provides recommendations for integrating agile principles into a core software engineering course",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=995210,no,undetermined,0
Diagnosing quality of service faults in distributed applications,"QoS management refers to the allocation and scheduling of computing resources. Static QoS management techniques provide a guarantee that resources will be available when needed. These techniques allocate resources based on worst-case needs. This is especially important for applications with hard QoS requirements. However, this approach can waste resources. In contrast, a dynamic approach allocates and deallocates resources during the lifetime of an application. In the dynamic approach the application is started with an initial resource allocation. If the application does not meet its QoS requirements, a resource manager attempts to allocate more resources to the application until the application's QoS requirement is met. While this approach offers the opportunity to better manage resources and meet application QoS requirements, it also introduces a new set of problems. In particular, a key problem is detecting why a QoS requirement is not being satisfied and determining the cause and, consequently, which resource needs to be adjusted. This paper investigates a policy-based approach for addressing these problems. An architecture is presented and a prototype described. This is followed by a case study in which the prototype is used to diagnose QoS problems for a web application based on Apache",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=995173,no,undetermined,0
Error detection by duplicated instructions in super-scalar processors,"This paper proposes a pure software technique ""error detection by duplicated instructions"" (EDDI), for detecting errors during usual system operation. Compared to other error-detection techniques that use hardware redundancy, EDDI does not require any hardware modifications to add error detection capability to the original system. EDDI duplicates instructions during compilation and uses different registers and variables for the new instructions. Especially for the fault in the code segment of memory, formulas are derived to estimate the error-detection coverage of EDDI using probabilistic methods. These formulas use statistics of the program, which are collected during compilation. EDDI was applied to eight benchmark programs and the error-detection coverage was estimated. Then, the estimates were verified by simulation, in which a fault injector forced a bit-flip in the code segment of executable machine codes. The simulation results validated the estimated fault coverage and show that approximately 1.5% of injected faults produced incorrect results in eight benchmark programs with EDDI, while on average, 20% of injected faults produced undetected incorrect results in the programs without EDDI. Based on the theoretical estimates and actual fault-injection experiments, EDDI can provide over 98% fault-coverage without any extra hardware for error detection. This pure software technique is especially useful when designers cannot change the hardware, but they need dependability in the computer system. To reduce the performance overhead, EDDI schedules the instructions that are added for detecting errors such that ""instruction-level parallelism"" (ILP) is maximized. Performance overhead can be reduced by increasing ILP within a single super-scalar processor. The execution time overhead in a 4-way super-scalar processor is less than the execution time overhead in the processors that can issue two instructions in one cycle",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=994913,no,undetermined,0
Projecting advanced enterprise network and service management to active networks,"Active networks is a promising technology that allows us to control the behavior of network nodes by programming them to perform advanced operations and computations. Active networks are changing considerably the scenery of computer networks and, consequently, affect the way network management is conducted. Current management techniques can be enhanced and their efficiency can be improved, while novel techniques can be deployed. This article discusses the impact of active networks on current network management practice by examining network management through the functional areas of fault, configuration, accounting, performance and security management. For each one of these functional areas, the limitations of the current applications and tools are presented, as well as how these limitations can be overcome by exploiting active networks. To illustrate the presented framework, several applications are examined. The contribution of this work is to analyze, classify, and assess the various models proposed in this area, and to outline new research directions",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=980542,no,undetermined,0
Approximating a data stream for querying and estimation: algorithms and performance evaluation,"Obtaining fast and good-quality approximations to data distributions is a problem of central interest to database management. A variety of popular database applications, including approximate querying, similarity searching and data mining in most application domains, rely on such good-quality approximations. Histogram-based approximation is a very popular method in database theory and practice to succinctly represent a data distribution in a space-efficient manner. In this paper, we place the problem of histogram construction into perspective and we generalize it by raising the requirement of a finite data set and/or known data set size. We consider the case of an infinite data set in which data arrive continuously, forming an infinite data stream. In this context, we present single-pass algorithms that are capable of constructing histograms of provable good quality. We present algorithms for the fixed-window variant of the basic histogram construction problem, supporting incremental maintenance of the histograms. The proposed algorithms trade accuracy for speed and allow for a graceful tradeoff between the two, based on application requirements. In the case of approximate queries on infinite data streams, we present a detailed experimental evaluation comparing our algorithms with other applicable techniques using real data sets, demonstrating the superiority of our proposal",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=994775,no,undetermined,0
Detecting changes in XML documents,"We present a diff algorithm for XML data. This work is motivated by the support for change control in the context of the Xyleme project that is investigating dynamic warehouses capable of storing massive volumes of XML data. Because of the context, our algorithm has to be very efficient in terms of speed and memory space even at the cost of some loss of quality. Also, it considers, besides insertions, deletions and updates (standard in diffs), a move operation on subtrees that is essential in the context of XML. Intuitively, our diff algorithm uses signatures to match (large) subtrees that were left unchanged between the old and new versions. Such exact matchings are then possibly propagated to ancestors and descendants to obtain more matchings. It also uses XML specific information such as ID attributes. We provide a performance analysis of the algorithm. We show that it runs in average in linear time vs. quadratic time for previous algorithms. We present experiments on synthetic data that confirm the analysis. Since this problem is NP-hard, the linear time is obtained by trading some quality. We present experiments (again on synthetic data) that show that the output of our algorithm is reasonably close to the optimal in terms of quality. Finally we present experiments on a small sample of XML pages found on the Web",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=994696,no,undetermined,0
Estimation of clock offset from one-way delay measurement on asymmetric paths,"As the Internet is shifting towards a reliable QoS-aware network, accurately synchronized clocks distributed on the Internet are becoming more significant. The network time protocol (NTP) is broadly deployed on the Internet for clock synchronization among distributed hosts, but is weak in asymmetric paths, i.e., it cannot accurately estimate the clock offset between two hosts when the forward and backward paths between them have different one-way delays. In this paper, we focus on estimating the offset and skew of a clock from one-way delay measurement between two hosts, and propose an idea for improvement of such estimations, which reduces estimation errors when the forward and backward paths have different bandwidths, a major factor in asymmetric delays",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=994563,no,undetermined,0
On the performance of a survivability architecture for networked computing systems,"This research focusses on the performance and timing behavior of a two level survivability architecture. The lower level of the architecture involves attack analysis based on kernel attack signatures and survivability handlers. Higher level survivability mechanisms are implemented using migratory autonomous agents. The potential for fast response to, and recovery from, malicious attacks is the main motivation to implement attack detection and survivability mechanisms at the kernel level. A timing analysis is presented that suggests the real-time feasibility of the two level approach. The limits to real-time response are identified from the host and network point of view. The experimental data derived is important for risk management and analysis in the presence of malicious network and computer attacks.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=994194,no,undetermined,0
UMTS EASYCOPE: a tool for UMTS network and algorithm evaluation,"For the UMTS radio access network, the problems of dimensioning/planning and algorithm evaluation/optimization is a challenging task, which requires methods and tools that are essentially different from the ones applied for 2nd generation systems. In order to generate reliable results for the highly flexible and dynamic UMTS, sophisticated simulation tools have to be applied. A UMTS model and its implementation is described. It allows an efficient evaluation of UMTS in terms of system performance figures such as capacity, coverage and QoS",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=991794,no,undetermined,0
Body of knowledge for software quality measurement,"Measuring quality is the key to developing high-quality software. The author describes two approaches that help to identify the body of knowledge software engineers need to achieve this goal. The first approach derives knowledge requirements from a set of issues identified during two standards efforts: the IEEE Std. 1061-1998 for a Software Quality Metrics Methodology and the American National Standard Recommended Practice for Software Reliability (ANSI/AIAA R-013-1992). The second approach ties these knowledge requirements to phases in the software development life cycle. Together, these approaches define a body of knowledge that shows software engineers why and when to measure quality. Focusing on the entire software development life cycle, rather than just the coding phase, gives software engineers the comprehensive knowledge they need to enhance software quality and supports early detection and resolution of quality problems. The integration of product and process measurements lets engineers assess the interactions between them throughout the life cycle. Software engineers can apply this body of knowledge as a guideline for incorporating quality measurement in their projects. Professional licensing and training programs will also find it useful",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=982919,no,undetermined,0
Vision-model-based impairment metric to evaluate blocking artifacts in digital video,"In this paper investigations are conducted to simplify and refine a vision-model-based video quality metric without compromising its prediction accuracy. Unlike other vision-model-based quality metrics, the proposed metric is parameterized using subjective quality assessment data recently provided by the Video Quality Experts Group. The quality metric is able to generate a perceptual distortion map for each and every video frame. A perceptual blocking distortion metric (PBDM) is introduced which utilizes this simplified quality metric. The PBDM is formulated based on the observation that blocking artifacts are noticeable only in certain regions of a picture. A method to segment blocking dominant regions is devised, and perceptual distortions in these regions are summed up to form an objective measure of blocking artifacts. Subjective and objective tests are conducted and the performance of the PBDM is assessed by a number of measures such as the Spearman rank-order correlation, the Pearson correlation, and the average absolute error The results show a strong correlation between the objective blocking ratings and the mean opinion scores on blocking artifacts",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=982412,no,undetermined,0
Validation of guidance control software requirements specification for reliability and fault-tolerance,"A case study was performed to validate the integrity of a software requirements specification (SRS) for guidance control software (GCS) in terms of reliability and fault-tolerance. A partial verification of the GCS specification resulted. Two modeling formalisms were used to evaluate the SRS and to determine strategies for avoiding design defects and system failures. Z was applied first to detect and remove ambiguity from a part of the natural language based (NL-based) GCS SRS. Next, statecharts and activity-charts were constructed to visualize the Z description and make it executable. Using this formalism, the system behavior was assessed under normal and abnormal conditions. Faults were seeded into the model (i.e., an executable specification) to probe how the system would perform. The result of our analysis revealed that it is beneficial to construct a complete and consistent specification using this method (Z-to-statecharts). We discuss the significance of this approach, compare our work with similar studies, and propose approaches for improving fault tolerance. Our findings indicate that one can better understand the implications of the system requirements using Z-statecharts approach to facilitate their specification and analysis. Consequently, this approach can help to avoid the problems that result when incorrectly specified artifacts (i.e., in this case requirements) force corrective rework",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=981660,no,undetermined,0
Improving the efficiency and quality of simulation-based behavioral model verification using dynamic Bayesian criteria,"In order to improve the effectiveness of simulation-based behavioral verification, it is important to determine when to stop the current test strategy and to switch to an expectantly more rewarding test strategy. The location of a stopping point is dependent on the statistical model one chooses to describe the coverage behavior during verification. In this paper, we present dynamic Bayesian (DB) and confidence-based dynamic Bayesian (CDB) stopping rules for behavioral VHDL model verification. The statistical assumptions of the proposed stopping rules are based on experimental evaluation of probability distribution functions and correlation functions. Fourteen behavioral VHDL models were experimented with to determine the high efficiency of the proposed stopping rules over the existing ones. Results show that the DB and the CDB stopping rules outperform all the existing stopping rules with an average improvement of at least 69% in coverage per testing patterns used.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=996761,no,undetermined,0
Integrated inductors modeling and tools for automatic selection and layout generation,In this work we propose new equivalent circuit models for integrated inductors based on the conventional lumped element model. Automatic tools to assist the designers in selecting and automatically laying-out integrated inductors are also reported. Model development is based on measurements taken from more than 100 integrated spiral inductors designed and fabricated in a standard silicon process. We demonstrate the capacity of the proposed models to accurately predict the integrated inductor behavior in a wider frequency range than the conventional model. Our equations are coded in a set of tools that requests the desired inductance value at a determined frequency and gives back the geometry of the better inductors available in a particular technology.,2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=996779,no,undetermined,0
A performance model of a PC based IP software router,"We can define a software router as a general-purpose computer that executes a computer program capable of forwarding IP datagrams among network interface cards attached to its I/O bus. This paper presents a parametrical model of a PC based IP software router. Validation results clearly show that the model accurately estimates the performance of the modeled system at different levels of detail. On the other hand, the paper presents experimental results that provide insights about the detailed functioning of such a system and demonstrate the model is valid not only for the characterized systems but for a reasonably range of CPU, memory and I/O bus operation speeds.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=997046,no,undetermined,0
FPGA realization of wavelet transform for detection of electric power system disturbances,"Realization of wavelet transform on field-programmable gate array (FPGA) device for the detection of power system disturbances is proposed in this paper. This approach provides an integral signal-processing paradigm, where its embedded wavelet basis serves as a window function to monitor the signal variations very efficiently. By using this technique, the time information and frequency information can be unified as a visualization scheme, facilitating the supervision of electric power signals. To improve its computation performance, the proposed method starts with the software simulation of wavelet transform in order to formulate the mathematical model. This is followed by the circuit synthesis and timing analysis for the validation of the designated circuit. Then, the designated portfolio can be programmed into the FPGA chip through the download cable. And the completed prototype will be tested through software-generated signals and utility-sampled signals, in which test scenarios covering several kinds of electric power quality disturbances are examined thoroughly. From the test results, they support the practicality and advantages of the proposed method for the applications",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=997905,no,undetermined,0
Scientific management meets the personal software process,"There are several ways that the personal software process (PSP) can bolster the principles of scientific management to provide needed artifacts. PSP data can be gathered and then collated to make early time and budget estimates more accurate. Also, PSP data indicates that the oft-repeated belief that projects should spend more time on planning and design is correct. In both cases, calculating time across cyclic development is important. A century apart, two pioneers meet to improve the often-black art of management.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1184159,no,undetermined,0
Analysis and FPGA implementation of image restoration under resource constraints,"Programmable logic is emerging as an attractive solution for many digital signal processing applications. In this work, we have investigated issues arising due to the resource constraints of FPGA-based systems. Using an iterative image restoration algorithm as an example we have shown how to manipulate the original algorithm to suit it to an FPGA implementation. Consequences of such manipulations have been estimated, such as loss of quality in the output image. We also present performance results from an actual implementation on a Xilinx FPGA. Our experiments demonstrate that, for different criteria, such as result quality or speed, the best implementation is different as well.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1183952,no,undetermined,0
Parametric fault tree for the dependability analysis of redundant systems and its high-level Petri net semantics,"In order to cope efficiently with the dependability analysis of redundant systems with replicated units, a new, more compact fault-tree formalism, called Parametric Fault Tree (PFT), is defined. In a PFT formalism, replicated units are folded and indexed so that only one representative of the similar replicas is included in the model. From the PFT, a list of parametric cut sets can be derived, where only the relevant patterns leading to the system failure are evidenced regardless of the actual identity of the component in the cut set. The paper provides an algorithm to convert a PFT into a class of High-Level Petri Nets, called SWN. The purpose of this conversion is twofold: to exploit the modeling power and flexibility of the SWN formalism, allowing the analyst to include statistical dependencies that could not have been accommodated into the corresponding PFT and to exploit the capability of the SWN formalism to generate a lumped Markov chain, thus alleviating the state explosion problem. The search for the minimal cut sets (qualitative analysis) can be often performed by a structural T-invariant analysis on the generated SWN. The advantages that can be obtained from the translation of a PFT into a SWN are investigated considering a fault-tolerant multiprocessor system example.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1183940,no,undetermined,0
A unified scheme of some Nonhomogenous Poisson process models for software reliability estimation,"In this paper, we describe how several existing software reliability growth models based on Nonhomogeneous Poisson processes (NHPPs) can be comprehensively derived by applying the concept of weighted arithmetic, weighted geometric, or weighted harmonic mean. Furthermore, based on these three weighted means, we thus propose a more general NHPP model from the quasi arithmetic viewpoint. In addition to the above three means, we formulate a more general transformation that includes a parametric family of power transformations. Under this general framework, we verify the existing NHPP models and derive several new NHPP models. We show that these approaches cover a number of well-known models under different conditions.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1183936,no,undetermined,0
Reliability Centered Maintenance Maturity Level Roadmap,"Numerous maintenance organizations are implementing various forms of reliability centered maintenance (RCM). Whether it is a classic or a streamlined RCM program, the challenge is to do it fast, but with predictable performance, quality, cost, and schedule. Hence, organizations need guidance to ensure their RCM programs are consistently implemented across the company and to improve their ability to manage key RCM process areas such as analysis, training, and metrics. The RCM Maturity Level Roadmap provides the structure for an organization to assess its RCM maturity and key process area capability. In addition, the Roadmap helps establish priorities for improvement and guide the implementation of these improvements.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181930,no,undetermined,0
"Comparing reliability predictions to field data for plastic parts in a military, airborne environment","This paper examines two popular prediction methods and compares the results to field data collected on plastic encapsulated microcircuits (PEMs) operating in a military, airborne environment. The comparison study focused on three digital circuit card assemblies (CCAs) designed primarily with plastic, surface mount parts. Predictions were completed using MIL-HDBK-217 models and PRISWÂ®, the latest software tool developed by the Reliability Analysis Center (RAC). The MIL-HDBK-217 predictions which correlated best to the field data were based on quality levels (Ï€<sub>Q</sub>) of 2 and 3, rather than the typical Ï€<sub>Q</sub> values of 5 or higher, traditionally assigned per the handbook's screening classifications for commercial, plastic parts. The initial findings from the PRISMÂ® tool revealed the predictions were optimistic in comparison to the observed field performance, meaning the predictions yielded higher mean time to failure (MTTF) values than demonstrated. Further evaluation of the PRISMÂ® models showed how modifying default values could improve the prediction accuracy. The impact of the system level multiplier was also determined to be a major contributor to the difference between PRISMÂ® predictions and field data. Finally, experience data proved valuable in refining the prediction results. The findings from this study provide justification to modify specific modeling factors to improve the predictions for PEMs, and also serve as a baseline to evaluate future alternative prediction methods.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181927,no,undetermined,0
Robust reliability design of diagnostic systems,"Diagnostic systems are software-based built-in-test systems which detect, isolate and indicate the failures of the prime systems. The use of diagnostic systems reduces the losses due to the failures of the prime systems and facilitates subsequent repairs. Thus diagnostic systems have found extensive applications in industry. The algorithms performing operations for diagnosis are important parts of the diagnostic systems. If the algorithms are not adequately designed, the systems will be sensitive to noise sources, and commit type I error (a) and type II error (Î²). This paper is to improve the robustness and reliability of the diagnostic systems through robust design of the algorithms by using reliability as an experimental response. To conduct the design, we define the reliability and robustness of the systems, and propose their metrics. The influences of Î± and Î² errors on reliability are evaluated and discussed. The effects of noise factors on robustness are assessed. The classical P-diagram is modified; a generic P-diagram containing both prime and diagnostic systems is created. Based on the proposed dynamic reliability metric, we describe the steps for robust reliability design and develop a method for experimental data analysis. The robustness and reliability of the diagnostic systems are maximized by choosing optimal levels of algorithm parameters. An automobile example is presented to illustrate how the proposed design method is used. The example shows that the method is efficient in defining, measuring and building robustness and reliability.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181899,no,undetermined,0
Nonblocking k-fold multicast networks,"Multicast communication involves transmitting information from a single source to multiple destinations and is a requirement in high-performance networks. Current trends in networking applications indicate an increasing demand in future networks for multicast capability. Many multicast applications require not only multicast capability, but also predictable communication performance such as guaranteed multicast latency and bandwidth. In this paper, we present a design for a nonblocking k-fold multicast network, in which any destination node can be involved in up to k simultaneous multicast connections in a nonblocking manner. We also develop an efficient routing algorithm for the network. As can be seen, a k-fold multicast network has significantly lower network cost than that of k copies of ordinary 1-fold multicast networks and is a cost effective choice for supporting arbitrary multicast communication.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1178877,no,undetermined,0
Transparent recovery from intermittent faults in time-triggered distributed systems,"The time-triggered model, with tasks scheduled in static (off line) fashion, provides a high degree of timing predictability in safety-critical distributed systems. Such systems must also tolerate transient and intermittent failures which occur far more frequently than permanent ones. Software-based recovery methods using temporal redundancy, such as task reexecution and primary/backup, while incurring performance overhead, are cost-effective methods of handling these failures. We present a constructive approach to integrating runtime recovery policies in a time-triggered distributed system. Furthermore, the method provides transparent failure recovery in that a processor recovering from task failures does not disrupt the operation of other processors. Given a general task graph with precedence and timing constraints and a specific fault model, the proposed method constructs the corresponding fault-tolerant (FT) schedule with sufficient slack to accommodate recovery. We introduce the cluster-based failure recovery concept which determines the best placement of slack within the FT schedule so as to minimize the resulting time overhead. Contingency schedules, also generated offline, revise this FT schedule to mask task failures on individual processors while preserving precedence and timing constraints. We present simulation results which show that, for small-scale embedded systems having task graphs of moderate complexity, the proposed approach generates FT schedules which incur about 30-40 percent performance overhead when compared to corresponding non-fault-tolerant ones.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1176980,no,undetermined,0
Perfect failure detection in timed asynchronous systems,"Perfect failure detectors can correctly decide whether a computer is crashed. However, it is impossible to implement a perfect failure detector in purely asynchronous systems. We show how to enforce perfect failure detection in timed asynchronous systems with hardware watchdogs. The two main system model assumptions are: 1) each computer can measure time intervals with a known maximum error and 2) each computer has a watchdog that crashes the computer unless the watchdog is periodically updated. We have implemented a system that satisfies both assumptions using a combination of off-the-shelf software and hardware. To implement a perfect failure detector for process crash failures, we show that, in some systems, a hardware watchdog is actually not necessary.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1176979,no,undetermined,0
Assessing the quality of a cross-national e-government Web site: a study of the forum on strategic management knowledge exchange,"As organizations have begun increasingly to communicate and interact with consumers via the Web, so the appropriate design of offerings has become a central issue. Attracting and retaining consumers requires acute understanding of the requirements of users and appropriate tailoring of solutions. Recently, the development of Web offerings has moved beyond the commercial domain to government, both national and international. In this paper, we examine the results of a quality survey of a cross-national e-government Web site provided by the OECD. The site is examined before and after a major redesign process. The instrument, WebQual, draws on previous work in three areas: Web site usability, information quality, and service interaction quality to provide a rounded framework for assessing e-commerce and e-government offerings. The metrics and findings demonstrate not only the strengths and weaknesses of the sites before and after design, but the very different impressions of users in different member countries. These findings have implications for cross-national e-government Web site offerings.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1174306,no,undetermined,0
Multiparadigm scheduling for distributed real-time embedded computing,"Increasingly complex requirements, coupled with tighter economic and organizational constraints, are making it hard to build complex distributed real-time embedded (DRE) systems entirely from scratch. Therefore, the proportion of DRE systems made up of commercial-off-the-shelf (COTS) hardware and software is increasing significantly. There are relatively few systematic empirical studies, however, that illustrate how suitable COTS-based hardware and software have become for mission-critical DRE systems. This paper provides the following contributions to the study of real-time quality-of-service (QoS) assurance and performance in COTS-based DRE systems: it presents evidence that flexible configuration of COTS middleware mechanisms, and the operating system (OS) settings they use, allows DRE systems to meet critical QoS requirements over a wider range of load and jitter conditions than statically configured systems; it shows that in addition to making critical QoS assurances, noncritical QoS performance can be improved through flexible support for alternative scheduling strategies; and it presents an empirical study of three canonical scheduling strategies; specifically the conditions that predict success of a strategy for a production-quality DRE avionics mission computing system. Our results show that applying a flexible scheduling framework to COTS hardware, OSs, and middleware improves real-time QoS assurance and performance for mission-critical DRE systems.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173210,no,undetermined,0
Automatic QoS control,"User sessions, usually consisting of sequences of consecutive requests from customers, comprise most of an e-commerce site's workload. These requests execute e-business functions such as browse, search, register, login, add to shopping cart, and pay. Once we properly understand and characterize a workload, we must assess its effect on the site's quality of service (QoS), which is defined in terms of response time, throughput, the probability that requests will be rejected, and availability. We can assess an e-commerce site's QoS in many different ways. One approach is by measuring the site's performance, which we can determine from a production site using a real workload or from a test site using a synthetic workload (as in load testing). Another approach consists of using performance models. I look at the approach my colleagues at George Mason and I took that uses performance models in the design and implementation of automatic QoS controller for e-commerce sites.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1167347,no,undetermined,0
AQuA: an adaptive architecture that provides dependable distributed objects,"Building dependable distributed systems from commercial off-the-shelf components is of growing practical importance. For both cost and production reasons, there is interest in approaches and architectures that facilitate building such systems. The AQuA architecture is one such approach; its goal is to provide adaptive fault tolerance to CORBA applications by replicating objects. The AQuA architecture allows application programmers to request desired levels of dependability during applications' runtimes. It provides fault tolerance mechanisms to ensure that a CORBA client can always obtain reliable services, even if the CORBA server object that provides the desired services suffers from crash failures and value faults. AQuA includes a replicated dependability manager that provides dependability management by configuring the system in response to applications' requests and changes in system resources due to faults. It uses Maestro/Ensemble to provide group communication services. It contains a gateway to intercept standard CORBA IIOP messages to allow any standard CORBA application to use AQuA. It provides different types of replication schemes to forward messages reliably to the remote replicated objects. All of the replication schemes ensure strong, data consistency among replicas. This paper describes the AQuA architecture and presents, in detail, the active replication pass-first scheme. In addition, the interface to the dependability manager and the design of the dependability manager replication are also described. Finally, we describe performance measurements that were conducted for the active replication pass-first scheme, and we present results from our study of fault detection, recovery, and blocking times.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1159752,no,undetermined,0
Application of task-specific metrics in JPEG2000 ROI compression,"In this paper we consider the problem of quantifying target distortion in lossy compression using the JPEG2000 Region Of Interest (ROI) option. This approach brings about new problems that need to be addressed, such as automated selection of target regions and ROI preservation parameters. By combining variations of traditional metrics and newly developed segmentation-based image quality metrics, we illustrate several ways to quantify target distortion using ROI preserving compression for various methods including JPEG2000",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=999911,no,undetermined,0
Recording and analyzing eye movements during ocular fixation in schizophrenic subjects,"Previous studies have been shown that schizophrenic patient compared to healthy subject present abnormality in eye fixation tasks. But in these studies the evaluations of the eye movement are not objective. They are based on visual inspection of the records. The quality of fixation is assessed in term of absence of saccades. By using a predefined scale the records are rating from the best to worst. In this paper, we propose a new method to quantify eye fixation. in this method, our analyze examine the metric proprieties of each component of eye fixation movement (saccades, square wave jerks, and drift). A computer system is developed to record, stimulate and analyze the eye movement. A variety of software tools are developed to assist a clinician in the analysis of the data",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=999543,no,undetermined,0
A novel software implementation concept for power quality study,"A novel concept for power quality study is proposed. The concept integrates the power system modeling, classifying and characterizing of power quality events, studying equipment sensitivity to the event disturbance, and locating point of event occurrence into one unified frame. Both Fourier and wavelet analyzes are applied far extracting distinct features of various types of events as well as for characterizing the events. A new fuzzy expert system for classifying power quality events based on such features is presented with improved performance over previous neural network based methods. A novel simulation method is outlined for evaluating the operating characteristics of the equipment during specific events. A software prototype implementing the concept has been developed in MATLAB. The voltage sag event is taken as an example for illustrating the analysis methods and software implementation issues. It is concluded that the proposed approach is feasible and promising for real world applications",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=997935,no,undetermined,0
Utility of popular software defect models,Numerical models can be used to track and predict the number of defects in developmental and operational software. This paper introduces techniques to critically assess the effectiveness of software defect reduction efforts as the data is being gathered. This can be achieved by observing the fundamental shape of the cumulative defect discovery curves and by judging how quickly the various defect models converge to common predictions of long term software performance,2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=981659,no,undetermined,0
Test generation and testability alternatives exploration of critical algorithms for embedded applications,"Presents an analysis of the behavioral descriptions of embedded systems to generate behavioral test patterns that are used to perform the exploration of design alternatives based on testability. In this way, during the hardware/software partitioning of the embedded system, testability aspects can be considered. This paper presents an innovative error model for algorithmic (behavioral) descriptions, which allows for the generation of behavioral test patterns. They are converted into gate-level test sequences by using more-or-less accurate procedures based on scheduling information or both scheduling and allocation information. The paper shows, experimentally, that such converted gate-level test sequences provide a very high stuck-at fault coverage when applied to different gate-level implementations of the given behavioral specification. For this reason, our behavioral test patterns can be used to explore testability alternatives, by simply performing fault simulation at the gate level with the same set of patterns, without regenerating them for each circuit. Furthermore, whenever gate-level ATPGs are applied on the synthesized gate-level circuits, they obtain lower fault coverage with respect to our behavioral test patterns, in particular when considering circuits with hard-to-detect faults",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=980008,no,undetermined,0
Basic theory of adaptive data transmission,"This paper presents the basic theory of adaptive data transmission. First we describe a bit stream which has to be transmitted on line or in the air. In the transmission of the bit stream we use digital modulation methods symbol by symbol. We describe one symbol with a waveform, that contains several bits. The adaptive modulation method is used. It means that we can adapt the generated symbol waveform to the analogue channel used. The modulation is made by a software algorithm which converts each symbol to a specific amplitude-phase constellation point of the known carrier and uses a proper symbol time. Depending on the channel bandwidth B (wired or radio) we have one or several transmission carriers in use for the optimal Shannon capacity. Depending on the channel characteristics or signal to noise ratio we can select the amplitude phase constellation optimally. We describe then the detection and the demodulation of the symbol waveform and use there the discrete Fourier transform (DFT). First the received waveform is sampled and the number of samples is selected by the sampling frequency. We evaluate the proper sample number used in the DFT calculations, which give the estimates of the transmitted symbols of each carrier (amplitude, phase and frequency). Finally we summarize our results using the Shannon capacity formula B*<sub>2</sub>log(S/N+1) and bit error rate (BER) calculations in the evaluations. We discuss the adaptive modulation method properties and functionality found in the simulations and field tests. We find out, that the high performance of the adaptive modulation method is generated by the waveforms designed in the way that they occupy fully the channel in use. The full capacity of any channel is used by selection of proper multiple carriers (bandwidth). The full QoS (S/N) of the channel is applied by the selection of the best amplitude-phase constellation and symbol time selection. These features of the modulation method makes it adaptive, which we could design only using software modem technology. The result is the full practical Shannon's capacity for any analog transmission case. The channel may be a radio, satellite, and wired telecommunication channel which has a limited bandwidth for transmission. Also any coded digital voice transmission c- an be used for data transmission with the adaptive modem because it produces and decodes data as voice signal designed within the wanted bandwidth. We believe that the adaptive modem is a cornerstone for modern data transmission and thus it may be used in many applications.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1179622,no,undetermined,0
Statistical analysis of time series data on the number of faults detected by software testing,"According to a progress of the software process improvement, the time series data on the number of faults detected by the software testing are collected extensively. In this paper, we perform statistical analyses of relationships between the time series data and the field quality of software products. At first, we apply the rank correlation coefficient Ï„ to the time series data collected from actual software testing in a certain company, and classify these data into four types of trends: strict increasing, almost increasing, almost decreasing, and strict decreasing. We then investigate, for each type of trend, the field quality of software products developed by the corresponding software projects. As a result of statistical analyses, we showed that software projects having trend of almost or strict decreasing in the number of faults detected by the software testing could produce the software products with high quality.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181723,no,undetermined,0
Application of neural networks and filtered back projection to wafer defect cluster identification,"During an electrical testing stage, each die on a wafer must be tested to determine whether it functions as it was originally designed. In the case of a clustered defect on the wafer, such as scratches, stains, or localized failed patterns, the tester may not detect all of the defective dies in the flawed area. To avoid the defective dies proceeding to final assembly, an existing tool is currently used by a testing factory to detect the defect cluster and mark all the defective dies in the flawed region or close to the flawed region; otherwise, the testing factory must assign five to ten workers to check the wafers and hand mark the defective dies. This paper proposes two new wafer-scale defect cluster identifiers to detect the defect clusters, and compares them with the existing tool used in the industry. The experimental results verify that one of the proposed algorithms is very effective in defect identification and achieves better performance than the existing tool.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1188820,no,undetermined,0
Delivering error detection capabilities into a field programmable device: the HORUS processor case study,"Designing a complete SoC or reuse SoC components to create a complete system is a common task nowadays. The flexibility offered by current design flows offers the designer an unprecedented capability to incorporate more and more demanded features like error detection and correction mechanisms to increase the system dependability. This is especially true for programmable devices, were rapid design and implementation methodologies are coupled with testing environments that are easily generated and used. This paper describes the design of the HORUS processor, a RISC processor augmented with a concurrent error mechanism, the architectural modifications needed on the original design to minimize the resulting performance penalty.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1188724,no,undetermined,0
Definition of fault loads based on operator faults for DMBS recovery benchmarking,"The characterization of database management system (DBMS) recovery mechanisms and the comparison of recovery features of different DBMS require a practical approach to benchmark the effectiveness of recovery in the presence of faults. Existing performance benchmarks for transactional and database areas include two major components: a workload and a set of performance measures. The definition of a benchmark to characterize DBMS recovery needs a new component the faultload. A major cause of failures in large DBMS is operator faults, which make them an excellent starting point for the definition of a generic faultload. This paper proposes the steps for the definition of generic faultloads based on operator faults for DBMS recovery benchmarking. A classification for operator faults in DBMS is proposed and a comparative analysis among three commercially DBMS is presented. The paper ends with a practical example of the use of operator faults to benchmark different configurations of the recovery mechanisms of the Oracle 8i DBMS.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1185646,no,undetermined,0
Reliability evaluation of multi-state systems subject to imperfect coverage using OBDD,"This paper presents an efficient approach based on OBDD for the reliability analysis of a multi-state system subject to imperfect fault-coverage with combinatorial performance requirements. Since there exist dependencies between combinatorial performance requirements, we apply the multi-state dependency operation (MDO) of OBDD to deal with these dependencies in a multi-state system. In addition, this OBDD-based approach is combined with the conditional probability methods to find solutions for the multi-state imperfect coverage models. Using conditional probabilities, we can also apply this method for modular structures. The main advantage of this algorithm is that it will take computational time that is equivalent to the same problem without assuming imperfect coverage (i.e. with perfect coverage). This algorithm is very important for complex systems such as fault-tolerant computer systems, since it can obtain the complete results quickly and accurately even when there exist a number of dependencies such as shared loads (reconfiguration), degradation and common-cause failures.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1185638,no,undetermined,0
Performance guarantee for cluster-based Internet services,"As Web-based transactions become an essential element of everyday corporate and commerce activity, it becomes increasingly important for the performance of Web application services to be predictable and adequate even in the presence of wildly fluctuating input loads. In this work we propose a general implementation framework to provide quality of service (QoS) guarantee for cluster-based Web application services, such as e-commerce or directory services, that is largely independent of the Web application and the hardware/software platform used in the cluster. This paper describes the design, implementation, and evaluation of a Web request distribution system called Gage, which is able to guarantee a service subscriber a pre-defined number of generic Web requests serviced per second regardless of the total input loads at run time. Gage is one of the first, if not the first system that can support QoS guarantees which involves multiple system resources, i.e., CPU, disk, and network. The fully operational Gage prototype shows that the proposed architecture can indeed provide a guaranteed level of service for specific classes of Web accesses according to their QoS requirements in the presence of excessive input loads. In addition, empirical measurement on the Gage prototype demonstrates that the additional performance overhead associated with Gage's QoS guarantee support for Web service is merely 3.06%.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1183420,no,undetermined,0
Reliable file transfer in Grid environments,Grid-based computing environments are becoming increasingly popular for scientific computing. One of the key issues for scientific computing is the efficient transfer of large amounts of data across the Grid. In this paper we present a reliable file transfer (RFT) service that significantly improves the efficiency of large-scale file transfer. RFT can detect a variety of failures and restart the file transfer from the point of failure. It also has capabilities for improving transfer performance through TCP tuning.,2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181855,no,undetermined,0
Robust playout mechanism for Internet audio applications,"In Internet audio applications, delay and delay jitter affect mostly the applications' quality of service. Since packet delays are different and changing over time, the receiver needs to buffer some amount of packets before playout. Therefore, the amount of buffered packets and the timing of playout are very important for the performance of applications. We adopt an autoregressive (AR) model for estimation of packet delay and deploy a robust identification algorithm for adjustment of the parameters of the AR process. In our preliminary experiments, this robust algorithm leads to better performance when the noise is correlated and/or non-stationary, and also it is robust to model uncertainties.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181801,no,undetermined,0
An analytic software testability model,"Software testability, which has been discussed in the past decade, has been defined as provisions that can be taken into consideration at the early step of software development. This paper gives software testability, previously defined by Voas, a new model and measurement without performing testing with respect to a particular input distribution.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181724,no,undetermined,0
Maximum distance testing,"Random testing has been used for years in both software and hardware testing. It is well known that in random testing each test requires to be selected randomly regardless of the tests previously generated. However, random testing could be inefficient for its random selection of test patterns. This paper, based on random testing, introduces the concept of Maximum Distance Testing (MDT) for VLSI circuits in which the total distance among all test patterns is chosen maximal so that the set of faults detected by one test pattern is as different as possible from that of faults detected by the tests previously applied. The procedure for constructing a Maximum Distance Testing Sequence (MDTS) is described in detail. Experimental results on Benchmark as well as other circuits are also given to evaluate the performances of our new approach.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181678,no,undetermined,0
Low-cost error containment and recovery for onboard guarded software upgrading and beyond,"Message-driven confidence-driven (MDCD) error containment and recovery, a low-cost approach to mitigating the effect of software design faults in distributed embedded systems, is developed for onboard guarded software upgrading for deep-space missions. In this paper, we first describe and verify the MDCD algorithms in which we introduce the notion of ""confidence-driven"" to complement the ""communication-induced"" approach employed by a number of existing checkpointing protocols to achieve error containment and recovery efficiency. We then conduct a model-based analysis to show that the algorithms ensure low performance overhead. Finally, we discuss the advantages of the MDCD approach and its potential utility as a general-purpose, low-cost software fault tolerance technique for distributed embedded computing",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=980004,no,undetermined,0
Fault detection effectiveness of spathic test data,"This paper presents an approach for generating test data for unit-level, and possibly integration-level, testing based on sampling over intervals of the input probability distribution, i.e., one that has been divided or layered according to criteria. Our approach is termed ""spathic"" as it selects random values felt to be most likely or least likely to occur from a segmented input probability distribution. Also, it allows the layers to be further segmented if additional test data is required later in the test cycle. The spathic approach finds a middle ground between the more difficult to achieve adequacy criteria and random test data generation, and requires less effort on the part of the tester. It can be viewed as guided random testing, with the tester specifying some information about expected input. The spathic test data generation approach can be used to augment ""intelligent"" manual unit-level testing. An initial case study suggests that spathic test sets defect more faults than random test data sets, and achieve higher levels of statement and branch coverage.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181511,no,undetermined,0
A framework for performability modeling of messaging services in distributed systems,"Messaging services are a useful component in distributed systems that require scalable dissemination of messages (events) from suppliers to consumers. These services decouple suppliers and consumers, and take care of client registration and message propagation, thus relieving the burden on the supplier Recently performance models for the configurable delivery and discard policies found in messaging services have been developed, that can be used to predict response time distributions and discard probabilities under failure-free conditions. However, these messaging service models do not include the effect of failures. In a distributed system, supplier, consumer and messaging services can fail independently leading to different consequences. In this paper we consider the expected loss rate associated with messaging services as a performability measure and derive approximate closed-form expressions for three different quality of service settings. These measures provide a quantitative framework that allows different messaging service configurations to be compared and design trade-off decisions to be made.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181495,no,undetermined,0
A case for exploiting self-similarity of network traffic in TCP congestion control,"Analytical and empirical studies have shown that self-similar traffic can have a detrimental impact on network performance including amplified queuing delay and packet loss ratio. On the flip side, the ubiquity of scale-invariant burstiness observed across diverse networking contexts can be exploited to design better resource control algorithms. We explore the issue of exploiting the self-similar characteristics of network traffic in TCP congestion control. We show that the correlation structure present in long-range dependent traffic can be detected on-line and used to predict future traffic. We then devise an novel scheme, called TCP with traffic prediction (TCP-TP), that exploits the prediction result to infer, in the context of AIMD (additive increase, multiplicative decrease) steady-state dynamics, the optimal operational point for a TCP connection. Through analytical reasoning, we show that the impact of prediction errors on fairness is minimal. We also conduct ns-2 simulation and FreeBSD 4.1-based implementation studies to validate the design and to demonstrate the performance improvement in terms of packet loss ratio and throughput attained by connections.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181384,no,undetermined,0
Software quality classification modeling using the SPRINT decision tree algorithm,"Predicting the quality of system modules prior to software testing and operations can benefit the software development team. Such a timely reliability estimation can be used to direct cost-effective quality improvement efforts to the high-risk modules. Tree-based software quality classification models based on software metrics are used to predict whether a software module is fault-prone or not fault-prone. They are white box quality estimation models with good accuracy, and are simple and easy to interpret. This paper presents an in-depth study of calibrating classification trees for software quality estimation using the SPRINT decision tree algorithm. Many classification algorithms have memory limitations including the requirement that data sets be memory resident. SPRINT removes all of these limitations and provides a fast and scalable analysis. It is an extension of a commonly used decision tree algorithm, CART, and provides a unique tree-pruning technique based on the minimum description length (MDL) principle. Combining the MDL pruning technique and the modified classification algorithm, SPRINT yields classification trees with useful prediction accuracy. The case study used comprises of software metrics and fault data collected over four releases from a very large telecommunications system. It is observed that classification trees built by SPRINT are more balanced and demonstrate better stability in comparison to those built by CART.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1180826,no,undetermined,0
Algorithm to optimize the operational spectrum effectiveness (OSE) of wireless communication systems,"The advent of technically agile"" communication devices, such as software-defined radio (SDR), presents the military services with both a regulatory challenge and a technological opportunity for spectrum sharing. For OSAM, we have proposed a method to transform the technical parameters of a system into a set of nonlinear utility functions, which measure operational spectrum effectiveness (OSE). This paper describes our investigations into appropriate algorithms for optimizing the derived OSE metrics. By adapting equilibrium concepts from microeconomic theory, we demonstrate that, with the spectrum management authority acting as ""market arbiter"", OSE optimization for individual entities can lead to efficient group-level spectrum utilization solutions. We further contend that the solutions thus obtained will be of at least equal quality to those derived by traditional methods, with the added advantage of near real-time adaptability.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1180478,no,undetermined,0
Optimistic Byzantine agreement,"The paper considers the Byzantine agreement problem in a fully asynchronous network, where some participants may be actively malicious. This is an important building block for fault-tolerant applications in a hostile environment, and a non-trivial problem: An early result by Fischer et al. (1985) shows that there is no deterministic solution in a fully asynchronous network subject to even a single crash failure. The paper introduces an optimistic protocol that combines the two best known techniques to solve agreement, randomization and timing. The timing information is used only to increase performance; safety and liveness of the protocol are guaranteed independently of timing. Under certain ""normal"" conditions, the protocol decides quickly and deterministically without using public-key cryptography, approximately as fast as a timed protocol subject to crash failures does. Otherwise, a randomized fallback protocol ensures safety and liveness. For this, we present an optimized version of the randomized Byzantine agreement protocol of Cachin et al. (2000), which is computationally less expensive and not only tolerates malicious parties, but also some loss of messages; it might therefore be of independent interest.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1180196,no,undetermined,0
A fault-tolerant approach to secure information retrieval,"Several private information retrieval (PIR) schemes were proposed to protect users' privacy when sensitive information stored in database servers is retrieved. However, existing PIR schemes assume that any attack to the servers does not change the information stored and any computational results. We present a novel fault-tolerant PIR scheme (called FT-PIR) that protects users' privacy and at the same time ensures service availability in the presence of malicious server faults. Our scheme neither relies on any unproven cryptographic assumptions nor the availability of tamper-proof hardware. A probabilistic verification function is introduced into the scheme to detect corrupted results. Unlike previous PIR research that attempted mainly to demonstrate the theoretical feasibility of PIR, we have actually implemented both a PIR scheme and our FT-PIR scheme in a distributed database environment. The experimental and analytical results show that only modest performance overhead is introduced by FT-PIR while comparing with PIR in the fault-free cases. The FT-PIR scheme tolerates a variety of server faults effectively. In certain fail-stop fault scenarios, FT-PIR performs even better than PIR. It was observed that 35.82% less processing time was actually needed for FT-PIR to tolerate one server fault.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1180169,no,undetermined,0
Portable real-time protocols to make real-time communications affordable,"The cost and capabilities of networking technology is rapidly improving. As a result, the world is rapidly becoming networked for virtually every application. As networking becomes more important, there is a growing corresponding demand for networks to support real-time applications. Current planning for future military engagement networks serves as an example. Taking advantage of the new communications technology involves both technical and economic issues. Economic issues will slow the pace of network deployment especially for real-time systems. This paper offers a solution to make real-time communications affordable. For the purpose of this presentation we describe real-time applications as applications that have demanding performance requirements. This includes applications where timeline requirements must be predictable (deterministic) particularly those that ""require"" rapid response to an input, those that are safety critical, those that require a high degree of fault tolerance, those that ensure quality of service, and those with custom security considerations. The deterministic requirements of real-time software are generally categorized as applications that must meet deadlines most of the time (soft real-time) or 100 percent of the time within human and hardware limitations (hard real-time). The bottom line is that real-time is not defined as highest speed but rather on predictability within defined timing limits.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1179646,no,undetermined,0
gMeasure: a group-based network performance measurement service for peer-to-peer applications,"To enhance the efficiency of resource discovery and distribution, it is necessary to effectively estimate the network performance, in terms of metrics such as round-trip time and hop-count, between any peer-pair in a peer-to-peer application. Measurement in large scale network for peer-to-peer applications and services faces many challenges, such as scalability and quality of service (QoS) support. To address these issues, we present an architecture of a group-based performance measurement service (gMeasure), which disseminates and estimates network performance information for peer-to-peer applications. The key concept in gMeasure is Measurement Groups (MeGroups), in which hosts are self-organized to form a scalable hierarchical structure. A set of heuristic algorithms are proposed to optimize the group organization to reduce system cost and improve system measurement accuracy.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1189086,no,undetermined,0
Forward resource reservation for QoS provisioning in OBS systems,"This paper addresses the issue of providing QoS services for optical burst switching (OBS) systems. We propose a linear predictive filter (LPF)-based forward resource reservation method to reduce the burst delay at edge routers. An aggressive reservation method is proposed to increase the successful forward reservation probability and to improve the delay reduction performance. We also discuss a QoS strategy that achieves burst delay differentiation for different classes of traffic by extending the FRR scheme. We analyze the latency reduction improvement gained by our FRR scheme, and evaluate the bandwidth cost of the FRR-based QoS strategy. Our scheme yields significant delay reduction for time-critical traffic, while maintaining the bandwidth overhead within limits.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1189135,no,undetermined,0
Dealing with increasing data volumes and decreasing resources,"The US Naval Oceanographic Office (NAVOCEANO) has recently updated its survey vessels and launches to include the latest generation of high-resolution multibeam and digital side-scan sonar systems, along with state-of-the-art ancillary sensors. This has resulted in NAVOCEANO possessing a tremendous ocean observing and mapping capability. However, these systems produce massive amounts of data that must be validated prior to inclusion in various bathymetry, hydrography, and imagery products. It is estimated that the amount of data to be processed will increase by an overwhelming 2000 times above present data quantities. NAVOCEANO is meeting this challenge on a number of fronts that include a series of hardware and software improvements. The key to meeting the challenge of the massive data volumes was to change the approach that required every data point to be viewed and validated. This was achieved with the replacement of the traditional line-by-line editing approach with an automated cleaning module, and an area-based editor (ABE) integrated with existing commercial off-the-shelf processing and visualization packages. NAVOCEANO has entered into two cooperative research and development agreements (CRADAs) - one with the Science Applications International Corporation (SAIC), Newport, RI, USA, and the other with Interactive Visualization Systems (IVS), Fredericton, N.B., Canada, to integrate the ABE with SAIC's SABER product and IVS's Fledermaus 3D visualization product. This paper presents an overview of the new approach and data results and metrics of the effort required to process data, including editing, quality control, and product generation for multibeam data utilizing targets from digital imagery data and automated techniques.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1192139,no,undetermined,0
Intelligent multi-agent approach to fault location and diagnosis on railway 10kv automatic blocking and continuous power lines,"This paper discusses the intelligent multi-agent technology, and proposes an intelligent multi-agent based accurate fault location detection and fault diagnosis system applied in 10kv automatic blocking and continuous power transmission lines along the railway. Agents are software processes capable of searching for information in the networks, interacting with pieces of equipment and performing tasks on behalf of their owner(device). Moreover, they are autonomous and cooperative. Intelligent agents also have the capability to learn as the power supply network topology or environment changed. The system architecture is proposed, the features of each agents are described. Analysis brings forth the merits of this fault location and diagnosis system.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1194694,no,undetermined,0
Pitch estimation based on Circular AMDF,"Pitch period is a key parameter in speech compression, synthesis and recognition. The AMDF is often used to determine this parameter. Due to the falling trend of the AMDF curve, however, it is easy to make the estimated pitch doubled or halved To correct these errors, the algoritlun has to increase its complexity. In this paper, we propose a new function, Circular AMDF (CAMDF), and describe a new pitch detection algoritlnn based on it. The CAMDF conquers the defect of AMDF, and brings us some useful features. which not only simplifies the procedure of estimation, but also efficiently reduces the estimation errors and improves the estimation precision. Lots of experiment shows that the performance of CAMDF outperforms the others AMDF-based functions, such as HRAMDF and LVAMDF.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5743724,no,undetermined,0
An Adaptive Distance Relay and Its Performance Comparison with a Fixed Data Window Distance Relay,"This paper describes the design, implementation, and testing of an adaptive distance relay. The relay uses a fault detector to determine the inception of a fault and then, uses data windows of appropriate length for estimating phasors and seen impedances. Hardware and software of the proposed adaptive relay are described in the paper. The relay was tested using a model power system and a real-time playback simulator. Performance of the relay was compared with a fixed data window distance relay. Some results are reported in the paper. The results indicate that the adaptive distance relay provides faster tripping in comparison to the fixed data window distance relay.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4312219,no,undetermined,0
FPGA Realization of Wavelet Transform for Detection of Electric Power System Disturbances,"Realization of wavelet transform on field-programmable gate array (FPGA) devices for the detection of power system disturbances is proposed in this paper. This approach provides an integral signal-processing paradigm, where its embedded wavelet basis serves as a window function to monitor the signal variations very efficiently. By using this technique, the time information and frequency information can be unified as a visualization scheme, facilitating the supervision of electric power signals. To improve its computation performance, the proposed method starts with the software simulation of wavelet transform in order to formulate the mathematical model. This is followed by the circuit synthesis and timing analysis for the validation of the designated circuit. Then, the designated portfolio can be programmed into the FPGA chip through the download cable. The completed prototype will be tested through software-generated signals and utility-sampled signals, in which test scenarios covering several kinds of electric power quality disturbances are examined thoroughly. From the test results, they support the practicality and advantages of the proposed method for the applications.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4312020,no,undetermined,0
The importance of life cycle modeling to defect detection and prevention,"In many low mature organizations dynamic testing is often the only defect detection method applied. Thus, defects are detected rather late in the development process. High rework and testing effort, typically under time pressure, lead to unpredictable delivery dates and uncertain product quality. This paper presents several methods for early defect detection and prevention that have been in existence for quite some time, although not all of them are common practice. However, to use these methods operationally and scale them to a particular project or environment, they have to be positioned appropriately in the life cycle, especially in complex projects. Modeling the development life cycle, that is the construction of a project-specific life cycle, is an indispensable first step to recognize possible defect injection points throughout the development project and to optimize the application of the available methods for defect detection and prevention. This paper discusses the importance of life cycle modeling for defect detection and prevention and presents a set of concrete, proven methods that can be used to optimize defect detection and prevention. In particular, software inspections, static code analysis, defect measurement and defect causal analysis are discussed. These methods allow early, low cost detection of defects, preventing them from propagating to later development stages and preventing the occurrence of similar defects in future projects.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1267624,no,undetermined,0
New directions in measurement for software quality control,"Assessing and controlling software quality is still an immature discipline. One of the reasons for this is that many of the concepts and terms that are used in discussing and describing quality are overloaded with a history from manufacturing quality. We argue in this paper that a quite distinct approach is needed to software quality control as compared with manufacturing quality control. In particular, the emphasis in software quality control is in design to fulfill business needs, rather than replication to agreed standards. We will describe how quality goals can be derived from business needs. Following that, we will introduce an approach to quality control that uses rich causal models, which can take into account human as well as technological influences. A significant concern of developing such models is the limited sample sizes that are available for eliciting model parameters. In the final section of the paper we will show how expert judgment can be reliably used to elicit parameters in the absence of statistical data. In total this provides an agenda for developing a framework for quality control in software engineering that is freed from the shackles of an inappropriate legacy.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1267623,no,undetermined,0
Development and evaluation of physical properties for digital intra-oral radiographic system,"As a part of the development of a dental digital radiographic (DDR) system using a CMOS sensor, we developed hardware and software based on a graphical user interface (GUI) to acquire and display intra-oral images. The aims of this study were to develop the DDR system and evaluate its physical properties. Electric signals generated from the CMOS sensor were transformed to digital images through a control computer equipped with a USB board. The distance between the X-ray tube and the CMOS sensor was varied between 10-40 cm for optimal image quality. To evaluate the image quality according to dose variance, phantom images (60 kVp, 7 mA) were obtained at 0.03, 0.05, 0.08, 0.10, and 0.12 s of exposure time and signal-to-noise ratio (SNR) was calculated form the phantom image data. The modulation transfer function (MTF) was obtained as the Fourier transform of the line spread function (LSF), a derivative of the edge spread function (ESF) of sharp edge images acquired at exposure conditions of 60 kVp and 0.56 mA. The most compatible contrast and distinct focal point length was recorded at 20 cm. The resolution of the DDR system was approximately 6.2 line pair per mm. The developed DDR system could be used for clinical diagnosis with improvement of acquisition time and resolution. Measurement of other physical factors such as detected quantum efficiency (DQE) would be necessary to evaluate the physical properties of the DDR system.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1239571,no,undetermined,0
Subjective evaluation of synthetic intonation,"The paper describes a method for evaluating the quality of synthetic intonation using subjective techniques. This perceptual method of assessing intonation, not only evaluates the quality of synthetic intonation, but also allows us to compare different models of intonation to know which one is the most natural from a perceptual point of view. This procedure has been used to assess the quality of an implementation of Fujisaki's intonation model (Fujisaki, H. and Hirose, K., 1984) for the Basque language (Navas, E. et al., 2000). The evaluation involved 30 participants and results show that the intonation model developed has introduced a considerable improvement and that the overall quality achieved is good.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1224364,no,undetermined,0
Identification of voltage sag characteristics from the measured responses,The paper proposes new approach for extraction of voltage sag characteristics from the recorded waveforms. A dedicated software was developed in Matlab for the extraction of voltage sag characteristics from recorded waveforms. The method and capability are illustrated on voltage waveforms obtained from IEEE P1159.2 Working Group on Power Quality Event Characterisation.,2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1221492,no,undetermined,0
The harmonic impact of self-generating in power factor correction equipment of industrial loads: real cases studies,"This paper shows the impact of the self-generating installation in industrial loads, the problems occurred in field and the proposed solutions based from the harmonic point of view. To illustrate these points, the paper describes two facilities that installed self-generating and all the measurements and studies performed to analyze the electrical problems detected. Some studies results are shown and the implemented solutions are also described.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1221416,no,undetermined,0
Voice over asynchronous transfer modc(ATM),"<div style=""font-variant: small-caps; font-size: .9em;"">First Page of the Article</div><img class=""img-abs-container"" style=""width: 95%; border: 1px solid #808080;"" src=""/xploreAssets/images/absImages/01214600.png"" border=""0"">",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1214600,no,undetermined,0
Using functional view of software to increase performance in defect testing,"Summary form only given, as follows. Software at its primitive level may be viewed as function or mapping according to some specification from set of inputs and their related outputs. In this view the system is considered as a `black box?? whose behavior can only be determined by analyzing its inputs and outputs. Within this domain higher performance in defect testing can be achieved by using techniques that can be executed within resource limitations, thus understanding the software requirements adequately enough to expose majority of errors in system function, performance and behavior. The paper uncovers some innovative input and test reduction approaches considering the functional view of software, leading to efficient and increased fault detection. Functional view of software, black box testing, test reduction, domain to range ratio, input output analysis, testability, equivalence partitioning.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1214599,no,undetermined,0
Things can reunite,"<div style=""font-variant: small-caps; font-size: .9em;"">First Page of the Article</div><img class=""img-abs-container"" style=""width: 95%; border: 1px solid #808080;"" src=""/xploreAssets/images/absImages/01214598.png"" border=""0"">",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1214598,no,undetermined,0
A SOM-based method for feature selection,"This paper presents a method, called feature competitive algorithm (FCA), for feature selection, which is based on an unsupervised neural network, the self-organising map (SOM). The FCA is capable of selecting the most important features describing target concepts from a given whole set of features via the unsupervised learning. The FCA is simple to implement and fast in feature selection as the learning can be done automatically and no need for training data. A quantitative measure, called average distance distortion ratio, is figured out to assess the quality of the selected feature set. An asymptotic optimal feature set can then be determined on the basis of the assessment. This addresses an open research issue in feature selection. This method has been applied to a real case, a software document collection consisting of a set of UNIX command manual pages. The results obtained from a retrieval experiment based on this collection demonstrated some very promising potential.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1202830,no,undetermined,0
Software reliability corroboration,"We suggest that subjective reliability estimation from the development lifecycle, based on observed behavior or the reflection of one's belief in the system quality, be included in certification. In statistical terms, we hypothesize that a system failure occurs with the estimated probability. Presumed reliability needs to be corroborated by statistical testing during the reliability certification phase. As evidence relevant to the hypothesis increases, we change the degree of belief in the hypothesis. Depending on the corroboration evidence, the system is either certified or rejected. The advantage of the proposed theory is an economically acceptable number of required system certification tests, even for high assurance systems so far considered impossible to certify.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1199453,no,undetermined,0
A rigorous approach to reviewing formal specifications,"A new approach to rigorously reviewing formal specifications to ensure their internal consistency and validity is forwarded. This approach includes four steps: (1) deriving properties as review targets based on the syntax and semantics of the specification, (2) building a review task tree to present all the necessary review tasks for each property, (3) carrying out reviews based on the review task tree, and (4) analyzing the review results to determine whether faults are detected or not. We apply this technique to the SOFL specification language, which is an integrated formalism of VDM, Petri nets, and data flow diagrams to discuss how each step is performed.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1199452,no,undetermined,0
Proactive detection of software aging mechanisms in performance critical computers,"Software aging is a phenomenon, usually caused by resource contention, that can cause mission critical and business critical computer systems to hang, panic, or suffer performance degradation. If the incipience or onset of software aging mechanisms can be reliably detected in advance of performance degradation, corrective actions can be taken to prevent system hangs, or dynamic failover events can be triggered in fault tolerant systems. In the 1990 's the U.S. Dept. of Energy and NASA funded development of an advanced statistical pattern recognition method called the multivariate state estimation technique (MSET) for proactive online detection of dynamic sensor and signal anomalies in nuclear power plants and Space Shuttle Main Engine telemetry data. The present investigation was undertaken to investigate the feasibility and practicability of applying MSET for realtime proactive detection of software aging mechanisms in complex, multiCPU servers. The procedure uses MSET for model based parameter estimation in conjunction with statistical fault detection and Bayesian fault decision processing. A realtime software telemetry harness was designed to continuously sample over 50 performance metrics related to computer system load, throughput, queue lengths, and transaction latencies. A series of fault injection experiments was conducted using a ""memory leak"" injector tool with controllable parasitic resource consumption rates. MSET was able to reliably detect the onset of resource contention problems with high sensitivity and excellent false-alarm avoidance. Spin-off applications of this NASA-funded innovation for business critical eCommerce servers are described.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1199445,no,undetermined,0
Air quality data remediation by means of ANN,"We present an application of neural networks to air quality time series remediation. The focus has been set on photochemical pollutants, and particularly on ozone, considering statistical correlations between precursors and secondary pollutants. After a preliminary study of the phenomenon, we tried to adapt a predictive MLP (multi layer perceptron) network to fulfill data gaps. The selected input was, along with ozone series, ozone precursors (NO<sub>x</sub>) and meteorological variables (solar radiation, wind velocity and temperature). We then proceeded in selecting the most representative periods for the ozone cycle. We ran all tests for a 80-hours validation set (the most representative gap width in our data base) and an accuracy analysis with respect to gap width as been performed too. In order to maximize the process automation, a software tool has been implemented in the Matlabâ„?environment. The ANN validation showed generally good results but a considerable instability in data prediction has been found out. The re-introduction of predicted data as input of following simulations generates an uncontrolled error propagation scarcely highlighted by the error autocorrelation analysis usually performed.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1198163,no,undetermined,0
ATPG for timing-induced functional errors on trigger events in hardware-software systems,We consider timing-induced functional errors in inter process communication. We present an Automatic Test Pattern Generation (ATPG) algorithm for the co-validation of hardware-software systems. Events on trigger signals (signals contained in the sensitivity list of a process) implement the basic synchronization mechanism in most hardware-software description languages. Timing faults on trigger signals can have a serious impact on system behavior. We target timing faults on trigger signals by enhancing a timing fault model proposed in previous work. The ATPG algorithm which we present targets the new timing fault model and provides significant performance benefits over manual test generation which is typically used for co-validation.,2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1029635,no,undetermined,0
A compositional approach to monitoring distributed systems,"This paper proposes a specification-based monitoring approach for automatic run-time detection of software errors and failures of distributed systems. The specification is assumed to be expressed in communicating finite state machines based formalism. The monitor observes the external I/O and partial state information of the target distributed system and uses them to interpret the specification. The approach is compositional as it achieves global monitoring by combining the component-level monitoring. The core of the paper describes the architecture and operations of the monitor The monitor includes several independent mechanisms, each tailored to detecting specific kinds of errors or failures. Their operations are described in detail using illustrative examples. Techniques for dealing with nondeterminism and concurrency issues in monitoring a distributed system are also discussed with respect to the considered model and specification. A case study describing the application of the prototype monitor to an embedded system is presented.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1029022,no,undetermined,0
Recovery and performance balance of a COTS DBMS in the presence of operator faults,"A major cause of failures in large database management systems (DBMS) is operator faults. Although most of the complex DBMS have comprehensive recovery mechanisms, the effectiveness of these mechanisms is difficult to characterize. On the other hand, the tuning of a large database is very complex and database administrators tend to concentrate on performance tuning and disregard the recovery mechanisms. Above all, database administrators seldom have feedback on how good a given configuration is concerning recovery. This paper proposes an experimental approach to characterize both the performance and the recoverability in DBMS. Our approach is presented through a concrete example of benchmarking the performance and recovery of an Oracle DBMS running the standard TPC-C benchmark, extended to include two new elements: a fault load based on operator faults and measures related to recoverability. A classification of operator faults in DBMS is proposed. The paper ends with the discussion of the results and the proposal of guidelines to help database administrators in finding the balance between performance and recovery tuning.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1029007,no,undetermined,0
A short circuit current study for the power supply system of Taiwan railway,"The western Taiwan railway transportation system consists mainly of a mountain route and ocean route. The Taiwan Railway Administration (TRA) has conducted a series of experiments on the ocean route in recent years to identify the possible causes of unknown events which cause the trolley contact wires to melt down frequently. The conducted tests include the short circuit fault test within the power supply zone of the Ho Long substation (Zhu Nan to Tong Xiao) that had the highest probability for the melt down events. Those test results based on the actual measured maximum short circuit current provide a valuable reference for TRA when comparing against the said events. The Le Blanc transformer is the main transformer of the Taiwan railway electrification system. The Le Blanc transformer mainly transforms the Taiwan Power Company (TPC) generated three phase alternating power supply system (69 kV, 60 Hz) into a two single-phase alternating power distribution system (M phase and T phase) (26 kV, 60 Hz) needed for the trolley traction. As a unique winding connection transformer, the conventional software for fault analysis will not be able to simulate its internal current and phase difference between each phase currents. Therefore, besides extracts of the short circuit test results, this work presents a EMTP model based on a Taiwan Railway substation equivalent circuit model with the Le Blanc transformer. The proposed circuit model can simulate the same short circuit test to verify the actual fault current and accuracy of the equivalent circuit model. Moreover, the maximum short circuit current is further evaluated with reference to the proposed equivalent circuit. Preliminary inspection of trolley contact wire reveals the possible causes of melt down events based on the simulation results",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=956727,no,undetermined,0
Prioritizing test cases for regression testing,"Test case prioritization techniques schedule test cases for execution in an order that attempts to increase their effectiveness at meeting some performance goal. Various goals are possible; one involves rate of fault detection, a measure of how quickly faults are detected within the testing process. An improved rate of fault detection during testing can provide faster feedback on the system under test and let software engineers begin correcting faults earlier than might otherwise be possible. One application of prioritization techniques involves regression testing, the retesting of software following modifications; in this context, prioritization techniques can take advantage of information gathered about the previous execution of test cases to obtain test case orderings. We describe several techniques for using test execution information to prioritize test cases for regression testing, including: 1) techniques that order test cases based on their total coverage of code components; 2) techniques that order test cases based on their coverage of code components not previously covered; and 3) techniques that order test cases based on their estimated ability to reveal faults in the code components that they cover. We report the results of several experiments in which we applied these techniques to various test suites for various programs and measured the rates of fault detection achieved by the prioritized test suites, comparing those rates to the rates achieved by untreated, randomly ordered, and optimally ordered suites",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=962562,no,undetermined,0
Software cost estimation with incomplete data,"The construction of software cost estimation models remains an active topic of research. The basic premise of cost modeling is that a historical database of software project cost data can be used to develop a quantitative model to predict the cost of future projects. One of the difficulties faced by workers in this area is that many of these historical databases contain substantial amounts of missing data. Thus far, the common practice has been to ignore observations with missing data. In principle, such a practice can lead to gross biases and may be detrimental to the accuracy of cost estimation models. We describe an extensive simulation where we evaluate different techniques for dealing with missing data in the context of software cost modeling. Three techniques are evaluated: listwise deletion, mean imputation, and eight different types of hot-deck imputation. Our results indicate that all the missing data techniques perform well with small biases and high precision. This suggests that the simplest technique, listwise deletion, is a reasonable choice. However, this will not necessarily provide the best performance. Consistent best performance (minimal bias and highest precision) can be obtained by using hot-deck imputation with Euclidean distance and a z-score standardization",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=962560,no,undetermined,0
The effect of timeout prediction and selection on wide area collective operations,"Failure identification is a fundamental operation concerning exceptional conditions that network programs must be able to perform. In this paper, we explore the use of timeouts to perform failure identification at the application level. We evaluate the use of static timeouts and of dynamic timeouts based on forecasts using the Network Weather Service. For this evaluation, we perform experiments on a wide-area collection of 31 machines distributed in eight institutions. Though the conclusions are limited to the collection of machines used, we observe that a single static timeout is not reasonable, even for a collection of similar machines over time. Dynamic timeouts perform roughly as well as the best static timeouts and, more importantly, they provide a single methodology for timeout determination that should be effective for wide-area applications",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=962548,no,undetermined,0
Scenario-based functional regression testing,"Regression testing has been a popular quality-assurance technique. Most regression testing techniques are based on code or software design. This paper proposes a scenario-based functional regression testing, which is based on end-to-end (E2E) integration test scenarios. The test scenarios are first represented in a template model that embodies both test dependency and traceability. By using test dependency information, one can obtain a test slicing algorithm to detect the scenarios that are affected and thus they are candidates for regression testing. By using traceability information, one can find affected components and their associated test scenarios and test cases for regression testing. With the same dependency and traceability information one can use the ripple effect analysis to identify all affected, including directly or indirectly, scenarios and thus the set of test cases can be selected for regression testing. This paper also provides several alternative test-case selection approaches and a hybrid approach to meet various requirements. A web-based tool has been developed to support these regression testing tasks",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=960659,no,undetermined,0
"The Exu approach to safe, transparent and lightweight interoperability","Exu is a new approach to automated support for safe, transparent and lightweight interoperability in multilanguage software systems. The approach is safe because it enforces appropriate type compatibility across language boundaries. It is transparent since it shields software developers from the details inherent in low-level language-based interoperability mechanisms. It is lightweight for developers because it eliminates tedious and error-prone coding (e.g., JNI) and lightweight at run-time since it does not unnecessarily incur the performance overhead of distributed, IDL-based approaches. The Exu approach exploits and extends the object-oriented concept of meta-object, encapsulating interoperability implementation in meta-classes so that developers can produce interoperating code by simply using meta-inheritance. An example application of Exu to the development of Java/C++ (i.e., multilanguage) programs illustrates the safety and transparency advantages of the approach. Comparing the performance of the Java/C++ programs produced by Exu to the same set of programs developed using IDL-based approaches provides preliminary evidence of the performance advantages of Exu",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=960644,no,undetermined,0
Information theoretic metrics for software architectures,"Because it codifies best practices, and because it supports various forms of software reuse, the discipline of software architecture is emerging as an important branch of software engineering research and practice. Because architectural-level decisions are prone to have a profound impact on finished software products, it is important to apprehend their quality attributes and to quantify them (as much as possible). In this paper, we discuss an information-theoretic approach to the definition and validation of architectural metrics, and illustrate our approach on a sample example",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=960611,no,undetermined,0
N-dimensional zonal algorithms. The future of block based motion estimation?,"The popularity of zonal based algorithms for block based motion estimation has been increasing due to their superior performance in both terms of reduced complexity and superior quality versus other preexisting algorithms. In our previous work we mainly focused on generalizing the different parameters used in these algorithms and finding the possibly most efficient implementation. In this paper a further generalization of these algorithms is presented, where instead we mainly consider the way zones can be designed and what should be the ultimate goal of such an algorithm. As a result we present a framework of algorithms which can have applications not only in video coding, but also in other video signal processing areas, such as computer vision, video analysis, salient stills etc. We do so by initially considering the dimensionality of video data and how it can be most efficiently analyzed and exploited in the context of zonal algorithms. A formulization of these algorithms is then presented, according to which different implementations for different applications can be selected. Simulation results, for the simple 3-D case using the predictive diamond search (PDS) algorithm, a low complexity zonal algorithm, demonstrate the efficacy of the proposed techniques while still having low complexity. Higher order implementations using more dimensions and more efficient zonal algorithms can also be considered",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=958162,no,undetermined,0
Benchmark the software based MPEG-4 video codec,"Software-based implementations of H.263 and MPEG-2 video standards are well documented, recently reporting faster than or close to real-time performance. Since the complexity of MPEG-4 is higher than its predecessor standards, real time video encoding and decoding can exhaust computational resource without achieving real-time speed. In this paper, we report a software-based real-time MPEG-4 video codec (encoder and decoder) on a single-processor PC, with no frame-skip, or profile simplifying tricks, or quality loss compromise. The proposed codec is an embodiment of a number of novel algorithms. Specifically, we have designed a fast binary shape coding algorithm, a fast motion estimation algorithm, and a technique for detection of all-zero quantized blocks. To enhance the computation speed, we harness Intel's SIMD (Single Instruction Stream, Multiple Data Stream) instructions to implement these algorithms. On the 800 MHz Intel Pentium III, our decoder can play real-time CIF video with less than 20% system resource consumption; and our encoder realizes up to 70 frames per second for CIF resolution video, with the similar picture quality as the reference software",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=957736,no,undetermined,0
On interference cancellation and iterative techniques,"Recent research activities in the area of mobile radio communications have moved to third generation (3G) cellular systems to achieve higher quality with variable transmission rate of multimedia information. In this paper, an overview is presented of various interference cancellation and iterative detection techniques that are believed to be suitable for 3G wireless communications systems. Key concepts are space-time processing and space-division multiple access (or SDMA) techniques. SDMA techniques are possible with software antennas. Furthermore, to reduce receiver implementation complexity, iterative detection techniques are considered. A particularly attractive method uses tentative hard decisions, made on the received positions with the highest reliability, according to some criterion, and can potentially yield an important reduction in the computational requirements of an iterative receiver, with minimum penalty in error performance. A study of the tradeoffs between complexity and performance loss of iterative multiuser detection techniques is a good research topic",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=955129,no,undetermined,0
Systematic defects in software cost-estimation models harm management,"As software development becomes an increasingly important enterprise, managerial requirements for cost estimation increase, yet developmers continue a rather a long history of failing to cost software systems development adequately. Here, it is contented that poor results are due, in part, to some traditionally recognized problems and, in part, to a defect in the models themselves. Identifying the defect of software cost models is the purpose of this paper",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=952325,no,undetermined,0
Rational interpolation examples in performance analysis,"The rational interpolation approach has been applied to performance analysis of computer systems previously. In this paper, we demonstrate the effectiveness of the rational interpolation technique in the analysis of randomized algorithms and the fault probability calculation for some real-time systems",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=954515,no,undetermined,0
Continual on-line training of neural networks with applications to electric machine fault diagnostics,"An online training algorithm is proposed for neural network (NN) based electric machine fault detection schemes. The algorithm obviates the need for large data memory and long training time, a limitation of most AI-based diagnostic methods for commercial applications, and in addition, does not require training prior to commissioning. Experimental results are provided for an induction machine stator winding turn-fault detection scheme that uses a feedforward NN to compensate for machine and instrumentation nonidealities, to illustrate the feasibility of the new training algorithm for real-time implementation",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=954450,no,undetermined,0
Role of 3-D graphics in NDT data processing,"Visualisation in 3-D can significantly improve the interpretation and understanding of imaged NDT data sets, The advantages of using 3-D graphics to assist in the interpretation of ultrasonic data are discussed in the context of an advanced software environment for the reconstruction, visualisation and analysis of 3-D images within a component CAD model. The software combines the analysis features of established 2-D packages with facilities for real-time data rotation, interactive orthogonal/oblique slicing and `true' image reconstruction, where scanning-surface shape and reflection from component boundaries are accounted for through interaction with the full 3-D model of the component. A number of novel facilities exploit the graphics capability of the system. These include the overlay of 3-D images with individual control of image transparency; a floating tooltip window for interrogation of data point coordinates and amplitude; image annotation tools, including 3-D distance measurement; and automated defect sizing based on `6 dB drop' and `maximum amplitude' methods. A graphical user interface has also been designed for a well established flaw response model, which allows the user to easily specify the flaw size, shape, orientation and location; probe parameters and scan pattern on the component. The output is presented as a simulated ultrasound image",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=954108,no,undetermined,0
Coordinating the simultaneous upgrade of multiple CORBA application objects,"The Eternal system provides CORBA applications with robust fault tolerance and the ability to be upgraded without a service interruption. The Eternal Evolution Manager handles both interface-preserving upgrades and interface-changing upgrades. The syntax and semantics of the application objects' IDL interfaces are unchanged by an interface-preserving upgrade. Interface-changing upgrades allow modification to one or more application object's IDL interfaces. Because modification to an object's IDL interface can affect clients of that object, an interface-changing upgrade requires coordinating the upgrade of multiple application objects at the same time",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=954096,no,undetermined,0
Probabilistic model for segmentation based word recognition with lexicon,"We describe the construction of a model for off-line word recognizers based on over-segmentation of the input image and recognition of segment combinations as characters in a given lexicon word. One such recognizer, the Word Model Recognizer (WMR), is used extensively. Based on the proposed model it was possible to improve the performance of WMR",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=953776,no,undetermined,0
Avoiding faulty privileges in fast stabilizing rings,"Most conventional studies on self-stabilization have been indifferent to the safety under convergence. This paper investigates how mutual exclusion property can be achieved in self-stabilizing rings even for illegitimate configurations. We present a new method which uses a state with a large state space to detect faults. If some faults are detected, every process is reset and not given a privilege. Even if the reset values are different between processes, our protocol mimics the behavior of Dijkstra's K-state protocol (1974). Then we have a fast and safe mutual exclusion protocol. A simulation study also shows its performance",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=953655,no,undetermined,0
Mobile database procedures in MDBAS,"MDBAS is a prototype of a multidatabase management system based on mobile agents. The system integrates a set of autonomous databases distributed over a network, enables users to create a global database scheme, and manages transparent distributed execution of user requests and procedures including distributed transactions. The paper highlights the issues related to mobile database procedures, especially the MDBAS execution strategy. In order to adequately assess MDBAS's qualities and bottlenecks, we have carried out complex performance evaluation with real databases distributed in a real Internet. The evaluation included a comparison to a commercial database with distributed database capabilities. The most interesting results are presented and commented",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=953118,no,undetermined,0
Using reading techniques to focus inspection performance,"Software inspection is a quality assurance method to detect defects early during the software development process. For inspection planning there are defect detection techniques, so-called reading techniques, which let the inspection planner focus the effectiveness of individual inspectors on specific sets of defects. For realistic planning it is important to use empirically evaluated defect detection techniques. We report on the replication of a large-scale experiment in an academic environment. The experiment evaluated the effectiveness of defect detection for inspectors who use a checklist or focused scenarios on individual and team level. A main finding of the experiments is that the teams were effective to find defects: In both experiments the inspection teams found on average more than 70% of the defects in the product. The checklist consistently was overall somewhat more effective on individual level, while the scenarios traded overall defect detection effectiveness for much better effectiveness regarding their target focus, in our case specific parts of the documents. Another main result of the study is that scenario-based reading techniques can be used in inspection planning to focus individual performance without significant loss of effectiveness on team level",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=952461,no,undetermined,0
"On comparisons of random, partition, and proportional partition testing","Early studies of random versus partition testing used the probability of detecting at least one failure as a measure of test effectiveness and indicated that partition testing is not significantly more effective than random testing. More recent studies have focused on proportional partition testing because a proportional allocation of the test cases (according to the probabilities of the subdomains) can guarantee that partition testing will perform at least as well as random testing. We show that this goal for partition testing is not a worthwhile one. Guaranteeing that partition testing has at least as high a probability of detecting a failure comes at the expense of decreasing its relative advantage over random testing. We then discuss other problems with previous studies and show that failure to include important factors (cost, relative effectiveness) can lead to misleading results",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=962563,no,undetermined,0
The development of security system and visual service support software for on-line diagnostics,"Hitachi's CD-SEM achieves the highest tool availability in the industry. However, efforts to further our performance are continuously underway. The proposed on-line diagnostics system can allow senior technical staff to monitor and investigate tool status by connecting the equipment supplier and the device manufacturer sites through the Internet. The advanced security system ensures confidentiality by firewalls, digital certification, and advanced encryption algorithms to protect device manufacturer data from unauthorized access. Service support software, called DDS (defective part diagnosis support system), will analyze the status of mechanical, evacuation, and optical systems. Its advanced overlay function on a timing chart identifies failed components in the tool and allows on-site or remote personnel to predict potential failures prior to their occurrence. Examples of application shows that the proposed system is expected to reduce repair time, improve availability and lower cost of ownership",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=962911,no,undetermined,0
Effect of code coverage on software reliability measurement,"Existing software reliability-growth models often over-estimate the reliability of a given program. Empirical studies suggest that the over-estimations exist because the models do not account for the nature of the testing. Every testing technique has a limit to its ability to reveal faults in a given system. Thus, as testing continues in its region of saturation, no more faults are discovered and inaccurate reliability-growth phenomena are predicted from the models. This paper presents a technique intended to solve this problem, using both time and code coverage measures for the prediction of software failures in operation. Coverage information collected during testing is used only to consider the effective portion of the test data. Execution time between test cases, which neither increases code coverage nor causes a failure, is reduced by a parameterized factor. Experiments were conducted to evaluate this technique, on a program created in a simulated environment with simulated faults, and on two industrial systems that contained tenths of ordinary faults. Two well-known reliability models, Goel-Okumoto and Musa-Okumoto, were applied to both the raw data and to the data adjusted using this technique. Results show that over-estimation of reliability is properly corrected in the cases studied. This new approach has potential, not only to achieve more accurate applications of software reliability models, but to reveal effective ways of conducting software testing",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=963124,no,undetermined,0
A stochastic model of fault introduction and removal during software development,"Two broad categories of human error occur during software development: (1) development errors made during requirements analysis, design, and coding activities; (2) debugging errors made during attempts to remove faults identified during software inspections and dynamic testing. This paper describes a stochastic model that relates the software failure intensity function to development and debugging error occurrence throughout all software life-cycle phases. Software failure intensity is related to development and debugging errors because data on development and debugging errors are available early in the software life-cycle and can be used to create early predictions of software reliability. Software reliability then becomes a variable which can be controlled up front, viz, as early as possible in the software development life-cycle. The model parameters were derived based on data reported in the open literature. A procedure to account for the impact of influencing factors (e.g., experience, schedule pressure) on the parameters of this stochastic model is suggested. This procedure is based on the success likelihood methodology (SLIM). The stochastic model is then used to study the introduction and removal of faults and to calculate the consequent failure intensity value of a small-software developed using a waterfall software development",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=963126,no,undetermined,0
A neural network based fault detection and identification scheme for pneumatic process control valves,"This paper outlines a method for detection and identification of actuator faults in a pneumatic process control valve using a neural network. First, the valve signature and dynamic error band tests, used by specialists to determine valve performance parameters, are carried out for a number of faulty operating conditions. A commercially available software package is used to carry out the diagnostic tests, thus eliminating the need for additional instrumentation of the valve. Next, the experimentally determined valve performance parameters are used to train a multilayer feedforward network to successfully detect and identify incorrect supply pressure, actuator vent blockage, and diaphragm leakage faults",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=969794,no,undetermined,0
Why is it so hard to predict software system trustworthiness from software component trustworthiness?,"When software is built from components, nonfunctional properties such as security, reliability, fault-tolerance, performance, availability, safety, etc. are not necessarily composed. The problem stems from our inability to know a priori, for example, that the security of a system composed of two components can be determined from knowledge about the security of each. This is because the security of the composite is based on more than just the security of the individual components. There are numerous reasons for this. The article considers only the factors of component performance and calendar time. It is concluded that no properties are easy to compose and some are much harder than others",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=969773,no,undetermined,0
Assessing inter-modular error propagation in distributed software,"With the functionality of most embedded systems based on software (SW), interactions amongst SW modules arise, resulting in error propagation across them. During SW development, it would be helpful to have a framework that clearly demonstrates the error propagation and containment capabilities of the different SW components. In this paper, we assess the impact of inter-modular error propagation. Adopting a white-box SW approach, we make the following contributions: (a) we study and characterize the error propagation process and derive a set of metrics that quantitatively represents the inter-modular SW interactions, (b) we use a real embedded target system used in an aircraft arrestment system to perform fault-injection experiments to obtain experimental values for the metrics proposed, (c) we show how the set of metrics can be used to obtain the required analytical framework for error propagation analysis. We find that the derived analytical framework establishes a very close correlation between the analytical and experimental values obtained. The intent is to use this framework to be able to systematically develop SW such that inter-modular error propagation is reduced by design",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=969769,no,undetermined,0
Detecting heap smashing attacks through fault containment wrappers,"Buffer overflow attacks are a major cause of security breaches in modern operating systems. Not only are overflows of buffers on the stack a security threat, overflows of buffers kept on the heap can be too. A malicious user might be able to hijack the control flow of a root-privileged program if the user can initiate an overflow of a buffer on the heap when this overflow overwrites a function pointer stored on the heap. The paper presents a fault-containment wrapper which provides effective and efficient protection against heap buffer overflows caused by <e2>C</e2> library functions. The wrapper intercepts every function call to the <e2>C</e2> library that can write to the heap and performs careful boundary checks before it calls the original function. This method is transparent to existing programs and does not require source code modification or recompilation. Experimental results on Linux machines indicate that the performance overhead is small",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=969756,no,undetermined,0
The SASHA architecture for network-clustered web servers,"We present the Scalable, Application-Space, Highly-Available (SASHA) architecture for network-clustered web servers that demonstrates high performance and fault tolerance using application-space software and Commercial-Off-The-Shelf (COTS) hardware and operating systems. Our SASHA architecture consists of an application-space dispatcher, which performs OSI layer 4 switching using layer 2 or layer 3 address translation; application-space agents that execute on server nodes to provide the capability for any server node to operate as the dispatcher, a distributed state-reconstruction algorithm; and a token-based communications protocol that supports self-configuring, detecting and adapting to the addition or removal of servers. The SASHA architecture of clustering offers a flexible and cost-effective alternative to kernel-space or hardware-based network-clustered servers with performance comparable to kernel-space implementations",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=966817,no,undetermined,0
Fault-based side-channel cryptanalysis tolerant Rijndael symmetric block cipher architecture,"Fault-based side channel cryptanalysis is very effective against symmetric and asymmetric encryption algorithms. Although straightforward hardware and time redundancy based Concurrent Error Detection (CED) architectures can be used to thwart such attacks, they entail significant overhead (either area or performance). In this paper we investigate systematic approaches to low-cost, low-latency CED for Rijndael symmetric encryption algorithm. These approaches exploit the inverse relationship that exists between Rijndael encryption and decryption at various levels and develop CED architectures that explore the trade-off between area overhead, performance penalty and error detection latency. The proposed techniques have been validated on FPGA implementations",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=966796,no,undetermined,0
A validation fault model for timing-induced functional errors,"The violation of timing constraints on signals within a complex system can create timing-induced functional errors which alter the value of output signals. These errors are not detected by traditional functional validation approaches because functional validation does not consider signal timing. Timing-induced functional errors are also not detected by traditional timing analysis approaches because the errors may affect output data values without affecting output signal timing. A timing fault model, the Mis-Timed Event (MTE) fault model, is proposed to model timing-induced functional errors. The MTE fault model formulates timing errors in terms of their effects on the lifespans of the signal values associated with the fault. We use several examples to evaluate the MTE fault model. MTE fault coverage results shows that it efficiently captures an important class of errors which are not targeted by other metrics",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=966703,no,undetermined,0
Measuring voice readiness of local area networks,"It is well known that company intranets are growing into ubiquitous communications media for everything. As a consequence, network traffic is notoriously dynamic, and unpredictable. In most scenarios, the data network requires tuning to achieve acceptable quality for voice integration. This paper introduces a performance measurement method based on widely used IP protocol elements, which allows measurement of network performance criteria to predict the voice transmission feasibility of a given local area network. The measurement does neither depend on special VoIP (Voice over IP) equipment, nor does it need network monitoring hardware. Rather it uses special payload samples to detect unloaded network conditions to receive reference values. These samples are followed by a typical VoIP application payload to obtain real-world measurement conditions. We successfully validate our method within a local area network and present all captured values that describe important aspects of voice quality",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=966227,no,undetermined,0
Deadline based channel scheduling,"The use of deadline based channel scheduling in support of real time delivery of application data units (ADU's) is investigated. Of interest is priority scheduling where a packet with a smaller ratio of delivery deadline over number of hops to destination is given a higher priority. It has been shown that a variant of this scheduling algorithm, based on head-of-the-line priority, is efficient and effective in supporting real time delivery of ADU's. In this variant, packets with a ratio smaller than or equal to a given threshold are sent to the higher priority queue. We first present a technique to select this threshold dynamically. The effectiveness of our technique is evaluated by simulation. We then study the performance of deadline based channel scheduling for large networks, with multiple autonomous systems. For this case, accurate information on number of hops to destination may not be available. A technique to estimate this distance metric is presented. The effectiveness of our algorithm with this estimated distance metric is evaluated. In addition, we study the performance of a multi-service scenario where only a fraction of the routers deploy deadline based channel scheduling",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=966200,no,undetermined,0
Fast IP packet classification with configurable processor,"The next generation IP routers/switches need to provide quality of service (QoS) guarantees and differentiated services. These capabilities require a packet to be classified according to multiple fields in order to determine which flow an incoming packet belongs to. We present a way of achieving fast IP packet classification with a configurable processor as a more flexible and future proof alternative to using a hard-wired ASIC. Configurable processors can be tuned by the system designer with new instructions and hardware. To accelerate table lookups and bitmap manipulation, we develop several customized instructions that are specially optimized to yield large performance improvements. It is shown that one packet needs 16 cycles in the case of cache hit and 30 cycles in the worst case of cache miss for multiple field classification (compared to hundreds of cycles for a standard RISC processor). Thus, it is demonstrated that by using two 200 MHz processors with a proper configuration, OC48 wire-speed packet classification can be achieved while matching multiple fields with sophisticated rules of ranges and/or prefixes",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=966183,no,undetermined,0
Accelerating learning from experience: avoiding defects faster,"All programmers learn from experience. A few are rather fast at it and learn to avoid repeating mistakes after once or twice. Others are slower and repeat mistakes hundreds of times. Most programmers' behavior falls somewhere in between: They reliably learn from their mistakes, but the process is slow and tedious. The probability of making a structurally similar mistake again decreases slightly during each of some dozen repetitions. Because of this a programmer often takes years to learn a certain rule-positive or negative-about his or her behavior. As a result, programmers might turn to the personal software process (PSP) to help decrease mistakes. We show how to accelerate this process of learning from mistakes for an individual programmer, no matter whether learning is currently fast, slow, or very slow, through defect logging and defect data analysis (DLDA) techniques",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=965803,no,undetermined,0
Extreme programming from a CMM perspective,"Extreme programming has been advocated recently as an appropriate programming method for the high-speed, volatile world of Internet and Web software development. The author reviews XP from the perspective of the capability maturity model for software, gives overviews of both approaches, and critiques XP from a SW-CMM perspective. He concludes that lightweight methodologies such as XP advocate many good engineering practices and that both perspectives have something to offer the other",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=965798,no,undetermined,0
Automated video chain optimization,"Video processing algorithms found in complex video appliances such as television sets and set top boxes exhibit an interdependency that makes it is difficult to predict the picture quality of an end product before it is actually built. This quality is likely to improve when algorithm interaction is explicitly considered. Moreover, video algorithms tend to have many programmable parameters, which are traditionally tuned in manual fashion. Tuning these parameters automatically rather than manually is likely to speed up product development. We present a methodology that addresses these issues by means of a genetic algorithm that, driven by a novel objective image quality metric, finds high-quality configurations of the video processing chain of complex video products.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=964153,no,undetermined,0
Automated video chain optimization,"Video processing algorithms found in complex video appliances such as television sets and set top boxes exhibit an interdependency that makes it is difficult to predict the picture quality of an end product before it is actually built. This quality is likely to improve when algorithm interaction is explicitly considered. Moreover, video algorithms tend to have many programmable parameters, which are traditionally tuned in manual fashion. Tuning these parameters automatically rather than manually is likely to speed up product development. We present a methodology that addresses these issues by means of a genetic algorithm that, driven by a novel objective image quality metric, finds high-quality configurations of the video processing chain of complex video products",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=964152,no,undetermined,0
PSP-EAT-enhancing a personal software process course,"The main objective of teaching the personal software process (PSP) is to develop in students a professional attitude towards producing software. PSP improves performance in size and effort estimation accuracy, software reusability, product quality while maintaining or increasing overall productivity. This work presents the experience of the first author in teaching PSP at graduate level for three years, and a tool, PSP-EAT, the authors have built to reduce both student and instructor clerical work in learning and teaching PSP. The tool helps also in increasing the students' data collection accuracy and their receptability to the PSP principles",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=963883,no,undetermined,0
Research of steep-front wave impulse voltage test effectiveness in finding internal fault of composite insulators,"In order to verify the effectiveness of steep-front impulse voltage testing in finding the internal faults of composite insulators, some insulators with faults are modeled which include conductive channel, semi-conductive airy channel and partial little air bubbles that occur separately at different places. A steep-front wave impulse voltage test (steepness of wave front is 1000-4000 kV/Î¼s) is respectively made for these faulty and normal insulators. At the same-time the internal electric field intensity and its distribution in the insulator is calculated by making use of infinite element analysis software in order to test whether breakdown has occurred. The result is consistent with the experimental one. The final result shows that steep-front wave impulse voltage testing plays a very effective part in finding severe faults of the insulator, however, tiny faults are not easy found using this method. The result of this research gives a reference to revise the steep-front wave impulse voltage test standard",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=963500,no,undetermined,0
Future architecture for flight control systems,"The development of fault tolerant embedded control systems, such as flight control systems, FCS, is currently highly specialized and time consuming. We introduce a conceptual architecture for the next decade control system where all control and logic is distributed to a number of computer nodes locally linked to actuators and connected via a communication network. In this way we substantially decrease the lifecycle cost of such embedded systems and acquire scalable fault tolerance. Fault tolerance is based on redundancy and in our concept permanent faults are covered by hardware replication and transient faults, fault detection and processing by software techniques. With intelligent nodes and the use of inherent redundancy a robust and simple fault tolerant system is introduced with a minimum of both hardware and bandwidth requirements. The study is based on an FCS for JAS 39 Gripen, a multirole combat aircraft that is statically unstable at subsonic speed",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=963308,no,undetermined,0
Documentation as a software process capability indicator,"Summary form only given. In a small software organization, a close and intense relationship with its customers is often a substitute for documentation along the software processes. Nevertheless, according to the quality standards, the inadequacy of the required documentation will retain the assessed capability of software processes on the lowest level. This article describes the interconnections between software process documentation and the maturity of the organization. The data is collected from the SPICE assessment results of small and medium sized software organizations in Finland. The aim of the article is to visualise the necessity of documentation throughout the software engineering processes in order to achieve a higher capability level. In addition we point out that processes with insufficient documentation decrease the chance to improve the quality of the processes, as it is impossible to track and analyse them",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=952347,no,undetermined,0
Management indicators model to evaluate performance of IT organizations,"There is no arguing nowadays about the importance of IT for the growth and competitive edge of organizations. But if technology is to be a true asset for a company, it must be aligned with the business strategic goals by means of a formalized system of strategic planning, maturity of development process, technology management and corporative quality vision. The accrued benefits can be manifold: the development of training and learning environments for an effective improvement of procedures and product quality, efficient use of assets and resources, opportunity for innovation and technologic advancement, an approach to problem solving in areas critical to the organization among others. Many companies make use of these practices, but find it hard to evaluate how effective they are and what is the final quality of the achieved results at diverse customer levels both in project vision and the continuity of service. One cause of these drawbacks is failure to apply measurement models which provide objective pointers to assess how effective the IT strategies used have actually been considering the strategic business goals. To incorporate models of measures is no easy task because it entails working on several aspects: technical, processes, products and the peculiar culture of each organization. This paper presents a model of indicators to evaluate IT performance using three well known methods: balanced scorecard, GQM and PSM",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=952021,no,undetermined,0
"Automated analysis of voltage sags, their causes and impacts","This paper focuses on voltage sags. It introduces an automated approach to the analysis of voltage sags, their causes and impacts. The sag analysis is performed using software tools developed for this purpose. First, the software performs detection, classification and characterization of voltage sags. Next, if the voltage sag is caused by a fault, the software finds the fault location. Finally the software allows replaying of the sag waveforms for the purpose of evaluating of the impacts on equipment operation.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=970220,no,undetermined,0
An integrated diagnostics virtual test bench for life cycle support,"Qualtech Systems, Inc. (QSI) has developed an architecture that utilizes the existing TEAMS (Testability Engineering and Maintenance Systems) integrated tool set as the foundation to a computing environment for modeling and rigorous design analysis. This architecture is called a Virtual Test Bench (VTB) for Integrated Diagnostics. The VTB approach addresses design for testability, safety, and risk reduction because it provides an engineering environment to develop/provide: 1. Accurate, comprehensive, and graphical model based failure mode, effects and diagnostic analysis to understand failure modes, their propagation, effects, and ability of diagnostics to address these failure modes. 2. Optimization of diagnostic methods and test sequencing supporting the development of an effective mix of diagnostic methods. 3. Seamless integration from analysis, to run-time implementation, to maintenance process and life cycle support. undetected fault lists, ambiguity group lists, and optimized diagnostic trees. 4. A collaborative, widely distributed engineering environment to ""ring-out"" the design before it is built and flown. The VTB architecture offers an innovative solution in a COTS package for system/component modeling, design for safety, failure mode/effect analysis, testability engineering, and rigorous integration/testing of the IVHM (Integrated Vehicle Health Management) function with the rest of the vehicle. The VTB approach described in this paper will use the TEAMS software tool to generate detailed, accurate ""failure"" models of the design, assess the propagation of the failure mode effects, and determine the impact on safety, mission and support costs. It will generate FMECA, mission reliability assessments, incorporate the diagnostic and prognostic test designs, and perform testability analysis. Diagnostic functions of the VTB include fault detection and isolation metrics undetected fault lists, ambiguity group lists, and optimized diagnostic trees",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=931400,no,undetermined,0
Optimization of H.263 video encoding using a single processor computer: performance tradeoffs and benchmarking,"We present the optimization and performance evaluation of a software-based H.263 video encoder. The objective is to maximize the encoding rate without losing the picture quality on an ordinary single processor computer such as a PC or a workstation. This requires optimization at all design and implementation phases, including algorithmic enhancements, efficient implementations of all encoding modules, and taking advantage of certain architectural features of the machine. We design efficient algorithms for DCT and fast motion estimation, and exploit various techniques to speed up the processing, including a number of compiler optimizations and removal of redundant operations. For exploiting the architectural features of the machine, we make use of low-level machine primitives such as Sun UltraSPARC's visual instruction set and Intel's multimedia extension, which accelerate the computation in a single instruction stream multiple data stream fashion. Extensive benchmarking is carried out on three platforms: a 167-MHz Sun UltraSPARC-1 workstation, a 233-MHz Pentium II PC, and a 600-MHz Pentium III PC. We examine the effect of each type of optimization for every coding mode of H.263, highlighting the tradeoffs between quality and complexity. The results also allow us to make an interesting comparison between the workstation and the PCs. The encoder yields 45.68 frames per second (frames/s) on the Pentium III PC, 18.13 frames/s on the Pentium II PC, and 12.17 frames/s on the workstation for QCIF resolution video with high perceptual quality at reasonable bit rates, which are sufficient for most of the general switched telephone networks based video telephony applications. The paper concludes by suggesting optimum coding options",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=937424,no,undetermined,0
A predictive measurement-based fuzzy logic connection admission control,"This paper presents a novel measurement-based connection admission control (CAC) which uses fuzzy set and fuzzy logic theory. Unlike conventional CAC, the proposed CAC does not use complicated analytical models or a priori traffic descriptors. Instead, traffic parameters are predicted by an on-line fuzzy logic predictor (Qiu et al. 1999). QoS requirements are targeted indirectly by an adaptive weight factor. This weight factor is generated by a fuzzy logic inference system which is based on arrival traffic, queue occupancy and link load. Admission decisions are then based on real-time measurement of aggregate traffic statistics with the fuzzy logic adaptive weight factor as well as the predicted traffic parameters. Both homogeneous and heterogeneous traffic were used in the simulation. Fuzzy logic prediction improves the efficiency of both conventional and measurement-based CAC. In addition, the measurement-based approach incorporating fuzzy logic inference and using fuzzy logic prediction is shown to achieve higher network utilization while maintaining QoS",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=937372,no,undetermined,0
A fast IP classification algorithm applying to multiple fields,"With the network applications development, routers must support those functions such as firewalls, provision of QoS and traffic billing etc. All these functions need classification of IP packets, according to which it is determined how different packets are processed subsequently. A novel IP classification algorithm is proposed based on the grid of tries algorithm. The new algorithm not only eliminates original limitations in the case of multiple fields but also shows better performance in regard to both time and space. It has better overall performance than many other algorithms",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=937021,no,undetermined,0
Application QoS management for distributed computing systems,"As a large number of distributed multimedia systems are deployed on computer networks, quality of service (QoS) for users becomes more important. This paper defines it as application QoS, and proposes the application QoS management system (QMS). It controls the application QoS according to the system environment by using simple measurement-based control methods. QMS consists of three types of modules. These are a notificator for module detecting QoS deterioration, a manager module for deciding the control method according to the application management policies, and a controller module for executing the control. The QMS manages the application QoS by communicating between these modules distributed on the network. Moreover, this paper especially focuses on the function setting QoS management policies to the QMS and proposes the setting method. By a simulation experiment, we confirmed that the system made it possible to negotiate the QoS among many applications and it was able to manage the whole applications according to the policies",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=936872,no,undetermined,0
Application of vibration sensing in monitoring and control of machine health,"In this paper, an application for monitoring and control of machine health using vibration sensing is developed. This vibration analyzer is able to continuously monitor and compare the actual vibration pattern against a vibration signature, based on a fuzzy fusion technique. More importantly, this intelligent knowledge-based real-time analyzer is able to detect excessive vibration conditions much sooner than a resulting fault could be detected by an operator. Subsequently, appropriate actions can be taken, say to provide a warning or automatic corrective action. This approach may be implemented independently of the control system and as such can be applied to existing equipment without modification of the normal mode of operation. Simulation and experimental results are provided to illustrate the advantages of the approach taken in this application",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=936484,no,undetermined,0
ICC 2001. IEEE International Conference on Communications. Conference Record (Cat. No.01CH37240),"The following topics were dealt with: multiuser detection; turbo codes; iterative decoding; network elements functionality; QoS in IP networks; transmission systems; wireless LAN; ad hoc networks; modulation; coding; wireless networks performance; synchronization; channel estimation; equalization; wireless multimedia; high speed networking; CDMA; multiple antennas and diversity; high speed network architecture; QoS in switching, routing and integration; optical networks; QoS for multimedia applications, services and networks; signal processing for communications; multimedia technologies; traffic and communication architectures; advanced signal processing for multimedia; fading channels; space-time codes; personal communications; medium access; next generation applications and services; mobile data networks; radio resource management; CDMA receiver algorithms; multicarrier and spread spectrum communications; advanced network surveillance and traffic management techniques; MPLS; video-on-demand systems; information infrastructure; OFDM; multicasting; broadband networks; network management; QoS for next generation Internet; wideband wireless local networks; interference mitigation and equalization; scheduling and queuing; service management of evolving telecommunication networks; enterprise networking; access protocols; turbo coding for magnetic recording; smart antennas; computer communications; Diffserv solutions for the next generation Internet; next generation network operations and management; communications theory; wireless IP networking; multicast routing and flow classification; satellite communications; signal processing for data storage; packet switching and packet scheduling; ATM network switches and routing; transport control protocol; communications software",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=936259,no,undetermined,0
"Curve evolution implementation of the Mumford-Shah functional for image segmentation, denoising, interpolation, and magnification","We first address the problem of simultaneous image segmentation and smoothing by approaching the Mumford-Shah (1989) paradigm from a curve evolution perspective. In particular, we let a set of deformable contours define the boundaries between regions in an image where we model the data via piecewise smooth functions and employ a gradient flow to evolve these contours. Each gradient step involves solving an optimal estimation problem for the data within each region, connecting curve evolution and the Mumford-Shah functional with the theory of boundary-value stochastic processes. The resulting active contour model offers a tractable implementation of the original Mumford-Shah model (i.e., without resorting to elliptic approximations which have traditionally been favored for greater ease in implementation) to simultaneously segment and smoothly reconstruct the data within a given image in a coupled manner. Various implementations of this algorithm are introduced to increase its speed of convergence. We also outline a hierarchical implementation of this algorithm to handle important image features such as triple points and other multiple junctions. Next, by generalizing the data fidelity term of the original Mumford-Shah functional to incorporate a spatially varying penalty, we extend our method to problems in which data quality varies across the image and to images in which sets of pixel measurements are missing. This more general model leads us to a novel PDE-based approach for simultaneous image magnification, segmentation, and smoothing, thereby extending the traditional applications of the Mumford-Shah functional which only considers simultaneous segmentation and smoothing",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=935033,no,undetermined,0
Formalizing COSMIC-FFP using ROOM,"We propose a formalization of the COSMIC Full Function Point (COSMIC-FFP) measure for the Real-time Object Oriented Modeling (ROOM) language. COSMIC-FFP is a measure of the functional size of software. It has been proposed by the COSMIC group as an adaptation of the function point measure for real-time systems. The definition of COSMIC-FFP is general and can be applied to any specification language. The benefits of our formalization are twofold. First it eliminates measurement variance, because the COSMIC informal definition is subject to interpretation by COSMIC-FFP raters, which may lead to different counts for the same specification, depending on the interpretation made by each rater. Second it allows the automation of COSMIC-FFP measurement for ROOM specifications, which reduces measurement costs. Finally, the formal definition of COSMIC-FFP can provide a clear and unambiguous characterization of COSMIC-FFP concepts which is helpful for measuring COSMIC-FFP for other object-oriented notations like UML",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=934002,no,undetermined,0
A general prognostic tracking algorithm for predictive maintenance,"Prognostic health management (PHIM) is a technology that uses objective measurements of condition and failure hazard to adaptively optimize a combination of availability, reliability, and total cost of ownership of a particular asset. Prognostic utility for the signature features are determined by transitional failure experiments. Such experiments provide evidence for the failure alert threshold and of the likely advance warning one can expect by tracking the feature(s) continuously. Kalman filters are used to track changes in features like vibration levels, mode frequencies, or other waveform signature features. This information is then functionally associated with load conditions using fuzzy logic and expert human knowledge of the physics and the underlying mechanical systems. Herein is the greatest challenge to engineering. However, it is straightforward to track the progress of relevant features over time using techniques such as Kalman filtering. Using the predicted states, one can then estimate the future failure hazard, probability of survival, and remaining useful life in an automated and objective methodology",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=931317,no,undetermined,0
Evaluating capture-recapture models with two inspectors,"Capture-recapture (CR) models have been proposed as an objective method for controlling software inspections. CR models were originally developed to estimate the size of animal populations. In software, they have been used to estimate the number of defects in an inspected artifact. This estimate can be another source of information for deciding whether the artifact requires a reinspection to ensure that a minimal inspection effectiveness level has been attained. Little evaluative research has been performed thus far on the utility of CR models for inspections with two inspectors. We report on an extensive Monte Carlo simulation that evaluated capture-recapture models suitable for two inspectors assuming a code inspections context. We evaluate the relative error of the CR estimates as well as the accuracy of the reinspection decision made using the CR model. Our results indicate that the most appropriate capture-recapture model for two inspectors is an estimator that allows for inspectors with different capabilities. This model always produces an estimate (i.e., does not fail), has a predictable behavior (i.e., works well when its assumptions are met), will have a relatively high decision accuracy, and will perform better than the default decision of no reinspections. Furthermore, we identify the conditions under which this estimator will perform best",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=950319,no,undetermined,0
Advanced test cell diagnostics for gas turbine engines,"Improved test cell diagnostics capable of detecting and classifying engine mechanical and performance faults as well as instrumentation problems is critical to reducing engine operating and maintenance costs while optimizing test cell effectiveness. Proven anomaly detection and fault classification techniques utilizing engine Gas Path Analysis (GPA) and statistical/empirical models of structural and performance related engine areas can now be implemented for real-time and post-test diagnostic assessments. Integration and implementation of these proven technologies into existing USAF engine test cells presents a great opportunity to significantly improve existing engine test cell capabilities to better meet today's challenges. A suite of advanced diagnostic and troubleshooting tools have been developed and implemented for gas turbine engine test cells as part of the Automated Jet Engine Test Strategy (AJETS) program. AJETS is an innovative USAF program for improving existing engine test cells by providing more efficient and advanced monitoring, diagnostic and troubleshooting capabilities. This paper describes the basic design features of the AJETS system; including the associated data network, sensor validation and anomaly detection/diagnostic software that was implemented in both a real-time and post-test analysis mode. These advanced design features of AJETS are currently being evaluated and advanced utilizing data from TF39 test cell installations at Travis AFB and Dover AFB",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=931313,no,undetermined,0
The effect of faults on plasma particle detector data reduction,"Missions in NASA's Solar-Terrestrial Probe Line feature challenges such as multiple spacecraft and high data production rates. An important class of scientific instruments that have for years strained against limits on communications are the particle detectors used to measure space plasma density, temperature, and flow. The Plasma Moment Application (PMA) software is being developed for the NASA Remote Exploration and Experimentation (REE) Program's series of Flight Processor Testbeds. REE seeks to enable instrument science teams to move data analyses such as PMA on board the spacecraft thereby reducing communication downlink requirements. Here we describe the PMA for the first time and examine its behavior under single bit faults in its static state. We find that ~90% of the faults lead to tolerable behavior, while the remainder cause either program failure or nonsensical results. These results help guide the development of fault tolerant, non-hardened flight/science processors",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=931202,no,undetermined,0
A new uncertainty measure for belief networks with applications to optimal evidential inferencing,"We are concerned with the problem of measuring the uncertainty in a broad class of belief networks, as encountered in evidential reasoning applications. In our discussion, we give an explicit account of the networks concerned, and call them the Dempster-Shafer (D-S) belief networks. We examine the essence and the requirement of such an uncertainty measure based on well-defined discrete event dynamical systems concepts. Furthermore, we extend the notion of entropy for the D-S belief networks in order to obtain an improved optimal dynamical observer. The significance and generality of the proposed dynamical observer of measuring uncertainty for the D-S belief networks lie in that it can serve as a performance estimator as well as a feedback for improving both the efficiency and the quality of the D-S belief network-based evidential inferencing. We demonstrate, with Monte Carlo simulation, the implementation and the effectiveness of the proposed dynamical observer in solving the problem of evidential inferencing with optimal evidence node selection",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=929899,no,undetermined,0
Run-time upgradable software in a large real-time telecommunication system,"For large real-time systems it is often important to achieve non-stop availability. Software updates must therefore be made during program operation. We consider a number of techniques for run-time software updates, and concentrate on one particular technique called Dynamic C++ Classes that allows run-time updates of C++ programs. The Dynamic C++ Classes approach is evaluated on a large real-time telecommunication system, the Ericsson Billing Gateway. The evaluation showed that there were no significant performance problems generated by the Dynamic C++ Classes approach. We have identified and discussed a number of problems with the approach and we have also implemented some improvements of the approach",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=929889,no,undetermined,0
"Low-cost, software-based self-test methodologies for performance faults in processor control subsystems","A software-based testing methodology for processor control subsystems, targeting hard-to-test performance faults in high-end embedded and general-purpose processors, is presented. An algorithm for directly controlling, using the instruction-set architecture only, the branch-prediction logic, a representative example of the class of processor control subsystems particularly prone to such performance faults, is outlined. Experimental results confirm the viability of the proposed methodology as a low-cost and effective answer to the problem of hard-to-test performance faults in processor architectures",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=929769,no,undetermined,0
A frame-level measurement apparatus for performance testing of ATM equipment,"Performance testing of ATM equipment is here dealt with. In particular, the attention is paid to frame-level metrics, recently proposed by the ATM forum because of their suitability to reflect user-perceived performance better than traditional cell-level metrics. Following the suggestions of the ATM forum, more and more network engineers and production managers are nowadays interested in these metrics, thus increasing the need of instruments and measurement solutions appropriate to their estimation. Trying to satisfy this exigency, a new VXI-based measurement apparatus is proposed in the paper. The apparatus features a suitable software, developed by the authors, which allows the evaluation of the aforementioned metrics by making simply use of common ATM analyzers; only two VXI line interfaces, capable of managing both the physical and ATM layer, are, in fact, adopted. At first, some details about the hierarchical structure of the ATM technology as used as the main differences between frames, peculiar to the ATM adaptation layer, and cells characterizing the lower ATM layer are given. Then, both the hardware and software solutions of the measurement apparatus are described in detail with a particular attention to the measurement procedures implemented. At the end the performance of a new ATM device, developed by Ericsson, is assessed in terms of frame-level metrics by means of the proposed apparatus",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=929479,no,undetermined,0
A Bayesian predictive software reliability model with pseudo-failures,"In our previous paper (2000), a Bayesian software reliability model with stochastically decreasing hazard rate was presented. Within any given failure time interval, the hazard rate is a function of both total testing time as well as number of encountered encountered failures. In this paper, to improve the predictive performance of our previously proposed model, a pseudo-failure is inserted whenever there is a period of failure-free execution equals (1-Î±)th percentile of the predictive distribution for time until the next failure has passed. We apply the enhanced model with pseudo-failures inserted to actual software failure data and show it gives better results under the sum of square errors criteria compared to previous Bayesian models and other existing times between failures models",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=925663,no,undetermined,0
In-situ plasma etch process endpoint control in integrated circuit manufacturing,"In-situ noninvasive etch process endpoint control with repeatable accuracy is essential for producing the kinds of yields, product quality, and throughput necessary for IC manufacturers to remain competitive. A unique in-situ endpoint detection system with the capability to accept different, multiple sensors based on various optical analytical techniques is described here. When combined in the same touchscreen instrument platform, these different techniques increase instrument flexibility in handling a larger variety of applications, directly enhancing the ability to increase process yields. Weak optical emission spectroscopy (OES) wavelength intensities resulting from either weak transitions or low emitting species concentrations from low exposed area and/or low vapor pressure species, requires a highly sensitive endpoint detection system containing highly sensitive and efficient sensors, transducers, signal conditioning components and software, and flexible algorithms to call process endpoint. These same requirements are necessary in other optical analytical techniques such as optical reflectometry or optical interferometry using single or multiple wavelength static or scanning transmission and detection. The paper describes how the optical sensor signal is subsequently processed into an electronic signal upon which proprietary window triggering software endpoint algorithms are applied to call an end to the process condition. This endpoint system is more than a process control tool, as it also offers data analysis capability, making it an in-situ diagnostic instrument for process troubleshooting and process development",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=925615,no,undetermined,0
A simple method for extracting models from protocol code,"The use of model checking for validation requires that models of the underlying system be created. Creating such models is both difficult and error prone and as a result, verification is rarely used despite its advantages. In this paper we present a method for automatically extracting models from low level software implementations. Our method is based on the use of an extensible compiler system, xg++, to perform the extraction. The extracted model is combined with a model of the hardware, a description of correctness, and an initial state. The whole model is then checked with the MurÏ† model checker. As a case study, we apply our method to the cache coherence protocols of the Stanford FLASH multiprocessor. Our system has a number of advantages. First, it reduces the cost of creating models, which allows model checking to be used more frequently. Second, it increases the effectiveness of model checking since the automatically extracted models are more accurate and faithful to the underlying implementation. We found a total of 8 errors using our system. Two errors were global resource errors, which would be difficult to find through any other means. We feel the approach is applicable to other low level systems",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=937448,no,undetermined,0
Learning visual models of social engagement,"We introduce a face detector for wearable computers that exploits constraints in face scale and orientation imposed by the proximity of participants in near social interactions. Using this method we describe a wearable system that perceives â€œsocial engagement,â€?i.e., when the wearer begins to interact with other individuals. Our experimental system proved >90% accurate when tested on wearable video data captured at a professional conference. Over 300 individuals were captured during social engagement, and the data was separated into independent training and test sets. A metric for balancing the performance of face detection, localization, and recognition in the context of a wearable interface is discussed. Recognizing social engagement with a user's wearable computer provides context data that can be useful in determining when the user is interruptible. In addition, social engagement detection may be incorporated into a user interface to improve the quality of mobile face recognition software. For example, the user may cue the face recognition system in a socially graceful way by turning slightly away and then toward a speaker when conditions for recognition are favorable",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=938923,no,undetermined,0
An approach for analysing the propagation of data errors in software,"We present a novel approach for analysing the propagation of data errors in software. The concept of error permeability is introduced as a basic measure upon which we define a set of related measures. These measures guide us in the process of analysing the vulnerability of software to find the modules that are most likely exposed to propagating errors. Based on the analysis performed with error permeability and its related measures, we describe how to select suitable locations for error detection mechanisms (EDMs) and error recovery mechanisms (ERMs). A method for experimental estimation of error permeability, based on fault injection, is described and the software of a real embedded control system analysed to show the type of results obtainable by the analysis framework. The results show that the developed framework is very useful for analysing error propagation and software vulnerability and for deciding where to place EDMs and ERMs.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=941402,no,undetermined,0
FATOMAS-a fault-tolerant mobile agent system based on the agent-dependent approach,"Fault tolerance is fundamental to the further development of mobile agent applications. In the context of mobile agents, fault-tolerance prevents a partial or complete loss of the agent, i.e., it ensures that the agent arrives at its destination. We present FATOMAS, a Java-based fault-tolerant mobile agent system based on an algorithm presented in an earlier paper (2000). Contrary to the standard ""place-dependent"" architectural approach, FATOMAS uses the novel ""agent-dependent"" approach. In this approach, the protocol that provides fault tolerance travels with the agent. This has the important advantage to allow fault-tolerant mobile agent execution without the need to modify the underlying mobile agent platform (in our case ObjectSpace's Voyager). In our performance evaluation, we show the costs of our approach relative to the single, non-replicated agent execution. Pipelined mode and optimized agent forwarding are two optimizations that reduce the overhead of a fault-tolerant mobile agent execution.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=941407,no,undetermined,0
A multi-sensor based temperature measuring system with self-diagnosis,"A new multi-sensor based temperature measuring system with self-diagnosis is developed to replace a conventional system that uses only a single sensor. Controlled by a 16-bit microprocessor, each sensor output from the sensor array is compared with a randomly selected quantised reference voltage at a voltage comparator and the result is a binary ""one"" or ""zero"". The number of ""ones"" and ""zeroes"" is counted and the temperature can be estimated using statistical estimation and successive approximation. A software diagnostic algorithm was developed to detect and isolate the faulty sensors that may be present in the sensor array and to recalibrate the system. Experimental results show that temperature measurements obtained are accurate with acceptable variances. With the self-diagnostic algorithm, the accuracy of the system in the presence faulty sensors is significantly improved and a more robust measuring system is produced",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=949727,no,undetermined,0
Reliability modeling incorporating error processes for Internet-distributed software,"The paper proposes several improvements to conventional software reliability growth models (SRGMs) to describe actual software development processes by eliminating an unrealistic assumption that detected errors are immediately corrected. A key part of the proposed models is the ""delay-effect factor"", which measures the expected time lag in correcting the detected faults during software development. To establish the proposed model, we first determine the delay-effect factor to be included In the actual correction process. For the conventional SRGMs, the delay-effect factor is basically non-decreasing. This means that the delayed effect becomes more significant as time moves forward. Since this phenomenon may not be reasonable for some applications, we adopt a bell-shaped curve to reflect the human learning process in our proposed model. Experiments on a real data set for Internet-distributed software has been performed, and the results show that the proposed new model gives better performance in estimating the number of initial faults than previous approaches",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=949540,no,undetermined,0
Fault prognosis using dynamic wavelet neural networks,"Prognostic algorithms for condition based maintenance of critical machine components are presenting major challenges to software designers and control engineers. Predicting time-to-failure accurately and reliably is absolutely essential if such maintenance practices are to find their way into the industrial floor. Moreover, means are required to assess the performance and effectiveness of these algorithms. This paper introduces a prognostic framework based upon concepts from dynamic wavelet neural networks and virtual sensors and demonstrates its feasibility via a bearing failure example. Statistical methods to assess the performance of prognostic routines are suggested that are intended to assist the user in comparing candidate algorithms. The prognostic and assessment methodology proposed here may be combined with diagnostic and maintenance scheduling methods and implemented on a conventional computing platform to serve the needs of industrial and other critical processes",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=949467,no,undetermined,0
"Integrated reliability analysis, diagnostics and prognostics for critical power systems","Critical power systems, such as data centers and communication switching facilities, have very high availability requirements (5 min./year downtime). A data center that consumes electricity at a rate of 3 MW can have a downtime cost of $300,000 an hour. Even a momentary interruption of two seconds may cause a loss of two hours of data processing. Consequently, power quality has emerged as an issue of significant importance in the operation of these systems. In this paper, we address three issues of power quality: real-time detection and diagnosis of power quality problems, reliability and availability evaluation, and capacity margin analysis. The objective of real-time detection and diagnosis is to provide a seamless on-line monitoring and off-line maintenance process. The techniques are being applied to monitor the power quality of a few facilities at the University of Connecticut. Reliability analysis, based on a computationally efficient sum of disjoint products, enables analysts to decide on the optimum levels of redundancy, aids operators in prioritizing the maintenance options within a given budget, and in monitoring the system for capacity margin. Capacity margin analysis helps operators to plan for additional loads and to schedule repair/replacement activities. The resulting analytical and software tool is demonstrated on a sample data center",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=949035,no,undetermined,0
Tools and procedures for successful TPS management,"The enormous costs associated with Test Program Sets are well known throughout the industry. Therefore, the successful management of a TPS development program is of crucial importance. This paper will attempt to encompass the various management issues involved in the TPS development process. To begin with, a careful review of the TPS must be effected during the bid process. Task evaluation tools are a distinct aid in management's ability to scope and estimate a TPS development task. Old fashioned engineering experience is used to check the results provided by these automatic tools. Once a job is won, rigid management controls must be kept in place to track the percentage completion against the formal plan (e.g. how often have we seen 90% of the available funds expended halfway through the technical effort). Software evaluation tools are a significant aid in keeping a development program on track. Management must try to balance its TPS staff between experienced engineers, middle range people and newer hires. The transfer of knowledge with such a mix is indeed beneficial. Management must look to create a broad experience base on different level UUT (i.e. systems, subsystems, modules, boards, etc.) spanning the overall circuitry spectrum (i.e. digital, analog, RF, etc.). Providing their designers with truly modem ATE software is a must. The operating system and test language must contribute features including incremental compilation, advanced edit/debug aids, links to major simulators and ATPGs, advanced fault isolation techniques, automated instrument programming, extensive use of graphics and more. Newer aids including analog simulators must also be made available by the TPS management team. Today another major factor to be addressed by management is the need to deal with TPS conversion from legacy ATE systems. The large investment in TPSs must be converted to modem ATE systems as the older ATE machines become totally unsupportable. These and other aspects of successful TPS management are the focus of this paper",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=948974,no,undetermined,0
Plain end-to-end measurement for local area network voice transmission feasibility,"It is well known that company Intranets are growing into ubiquitous communications media for everything. As a consequence, network traffic is notoriously dynamic, and unpredictable. Unfortunately, local area networks were designed for scalability and robustness, not for sophisticated traffic monitoring. This paper introduces a performance measurement method based on widely used IP protocol elements, which allows measurement of network performance criteria to predict the voice transmission feasibility of a given local area network. The measurement does neither depend on special VoIP equipment, nor does it need network monitoring hardware. Rather it uses special payload samples to detect unloaded network conditions to receive reference values. These samples are followed by typical VoIP application payload to obtain real-world measurement conditions. The validation of our method was done within a local area network and showed convincing results",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=948873,no,undetermined,0
Consistency management of product line requirements,"Contemporary software engineering utilizes product lines for reducing time to market and development cost of a single product variant, for improving quality of the products, and for creating better estimations of the development process. Most product line development processes rely on performing a domain analysis to find out commonalities among proposed family members and to estimate how they will vary. On the other hand, most requirements engineering methods focus on the specification of a single system. Despite active research efforts to close this, gap there is still no effective method that allows product specifications in arbitrary levels of detail for a hierarchical product family. In particular, it is not possible to combine different specification mechanisms to produce a complete family specification. The authors approach these problems by presenting a method that allows system specifications both in the product line variant as well as the product family level. This exposes many problems in managing consistency between different methods to specify families of systems. To achieve this, our method offers derivation of consistency management support between different specification levels and among family variants",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=948542,no,undetermined,0
The Quadrics network (QsNet): high-performance clustering technology,"The Quadrics interconnection network (QsNet) contributes two novel innovations to the field of high-performance interconnects: (I) integration of the virtual-address spaces of individual nodes into a single, global, virtual-address space and (2) network fault tolerance via link-level and end-to-end protocols that can detect faults and automatically re-transmit packets. QsNet achieves these feats by extending the native operating system in the nodes with a network operating system and specialized hardware support in the network interface. As these and other important features of QsNet can be found in the InfiniBand specification, QsNet can be viewed as a precursor to InfiniBand. In this paper, we present an initial performance evaluation of QsNet. We first describe the main hardware and software features of QsNet, followed by the results of benchmarks that we ran on our experimental, Intel-based, Linux cluster built around QsNet. Our initial analysis indicates that QsNet performs remarkably well, e.g., user-level latency under 2 Î¼s and bandwidth over 300 MB/s",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=946704,no,undetermined,0
An FDI approach for sampled-data systems,"In this paper, problems related to fault detection and isolation (FDI) in sampled-data (SD) systems are studied. A tool to analyze SD systems from the viewpoint of FDI and based on it a direct design approach of FDI system are developed. Key of these studies is the introduction of an operator which is used to describe the sampling effect. With the aid of the developed tool, we also study the perfect decoupling problem, the influence of sampling on the performance of FDI system and the relationship between the direct and indirect design approaches. The application of the approach proposed is finally illustrated and compared with the indirect design approaches through examples",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=946289,no,undetermined,0
Benchmarking of advanced technologies for process control: an industrial perspective,"Global competition is forcing industrial plants to continuously improve product quality and reduce costs in order to be more profitable. This scenario doesn't allow producing in less than excellent performance. Combining higher, more consistent product quality with a larger production volume and an increased flexibility however places special stresses on plant assets and equipment jeopardizing safety and environmental compliance. Consequently, process industries are called to operate on a very narrow, constrained path that needs to be continuously monitored, assessed and adjusted. The talk aims at reviewing the main points of the problem and at discussing how benchmarking practices should include and take advantage of advanced automation technologies. It will briefly consider basics for project justification and what an automation vendor may (really should) do in order to help process industries customer to select the best solutions for their plants. A particular emphasis will be placed on sometimes neglected aspects, such as hardware-software integration issues; life-cycle cost-benefit analysis; and operator acceptance and living with the APC tools and strategies. Finally some basic suggestions taken out of field experience will be given",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=945656,no,undetermined,0
Grid information services for distributed resource sharing,"Grid technologies enable large-scale sharing of resources within formal or informal consortia of individuals and/or institutions: what are sometimes called virtual organizations. In these settings, the discovery, characterization, and monitoring of resources, services, and computations are challenging problems due to the considerable diversity; large numbers, dynamic behavior, and geographical distribution of the entities in which a user might be interested. Consequently, information services are a vital part of any Grid software infrastructure, providing fundamental mechanisms for discovery and monitoring, and hence for planning and adapting application behavior. We present an information services architecture that addresses performance, security, scalability, and robustness requirements. Our architecture defines simple low-level enquiry and registration protocols that make it easy to incorporate individual entities into various information structures, such as aggregate directories that support a variety of different query languages and discovery strategies. These protocols can also be combined with other Grid protocols to construct additional higher-level services and capabilities such as brokering, monitoring, fault detection, and troubleshooting. Our architecture has been implemented as MDS-2, which forms part of the Globus Grid toolkit and has been widely deployed and applied",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=945188,no,undetermined,0
Power quality assessment from a wave-power station,This paper describes the development and testing of a software based flickermeter used in order to assess the supply quality from the LIMPET wave-power station on Islay. It describes the phenomenon of voltage flicker and the effect that a wave-power station has on this quantity. The paper also explains techniques developed in order to improve flickermeter performance when used with pre-recorded data. It also shows that the standard flickermeter sample frequency may be reduced for wave-station applications. Finally the paper presents flicker results from preliminary data collected from the LIMPET station and shows that the device is operating well within acceptable limits,2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=942975,no,undetermined,0
Model-aided diagnosis: an inexpensive combination of model-based and case-based condition assessment,"Online condition monitoring and diagnosis are being utilized more and more for increasing the reliability and availability of technical systems and to reduce their maintenance costs. Today's model-based diagnosis (MBD) tools are able to detect and identify incipient and sudden faults very reliably. For application to cost-sensitive equipment, such as high-voltage circuit breakers (HVCBs), however, the presently available MBD systems are not feasible for economic reasons. In this paper, a novel combination of the model-based with the case-based approach to condition diagnosis is presented, which can be implemented on a low-cost computer and which offers satisfactory performance. The technique is divided into two parts: (1) preparation and (2) diagnosis. The diagnosis part can be executed on an inexpensive low-performance computer. Successful tests on real HVCBs confirm the usefulness of this new approach to condition diagnosis",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=941838,no,undetermined,0
Criteria for developing clinical decision support systems,"The use of archived information and knowledge derived from data-driven system, both at the point of care and retrospectively, is critical to improving the balance between healthcare expenditure and healthcare quality. Data-driven clinical decision support, augmented by performance feedback and education, is a logical addition to consensus- and evidence-based approaches on the path to widespread use of intelligent search agents, expert recognition and warning systems. We believe that these initial applications should (a) capture and archive, with identifiable end-points, complete episode-of-care information for high-complexity, high-cost illnesses, and (b) utilize large numbers of these cases to drive risk-adjusted â€œindividualizedâ€?probabilities for patients requiring care at the time of intervention",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=941732,no,undetermined,0
The use of a multi-agent paradigm in electrical plant condition monitoring,"Electrical utilities need to operate their equipment closer to their design limits and require to extend their operating life through automatic condition monitoring systems. This paper introduces a multi agent paradigm for data interpretation in electrical plant monitoring. Data interpretation is of significant importance to infer the state of the equipment by converting the condition monitoring data into appropriate information. The vast amount of data and the complex processes behind on-line fault detection indicate the need for an automated solution. The classification of partial discharge signatures from gas insulated substations, as a result of applying different artificial intelligence techniques within a multi agent system, is described in this paper. A multi agent system that views the problem as an interaction of simple independent software entities, for effective use of the available data, is presented. The overall solution is derived from the combination of solutions provided by the components of the multi-agent system. This multi-agent system can employ various intelligent system techniques and has been implemented using the ZEUS Agent Building Toolkit",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=941621,no,undetermined,0
Performance validation of fault-tolerance software: a compositional approach,"Discusses the lessons learned in the modeling of a software fault tolerance solution built by a consortium of universities and industrial companies for an Esprit project called TIRAN (TaIlorable fault-toleRANce framework for embedded applications). The requirements of high flexibility and modularity for the software have lead to a modeling approach that is strongly based on compositionality. Since the interest was in assessing both the correctness and the performance of the proposed solution, we have cared for these two aspects at the same time, and, by means of an example, we show how this was a central aspect of our analysis.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=941422,no,undetermined,0
Experimental evaluation of the fail-silent behavior of a distributed real-time run-time support built from COTS components,"Mainly for economic and maintainability reasons, more and more dependable real-time systems are being built from commercial off-the-shelf (COTS) components. To build these systems, a commonly-used assumption is that computers are fail-silent. The goal of our work is so determine the coverage of the fail-silence assumption for computers executing a real-time run-time support system built exclusively from COTS components, in the presence of physical faults. The evaluation of fail-silence has been performed on the HADES (Highly Available Distributed Embedded System) run-time support system, aimed at executing distributed hard real-time dependable applications. The main result of the evaluation is a fail-silence coverage of 99.1%. Moreover, we evaluate the error detection mechanisms embedded in HADES according to a rich set of metrics which provides guidance for choosing the set of error detection mechanisms that is best suited to the system needs (e.g. find the best trade-off between fail-silence coverage and overhead caused by error detection).",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=941415,no,undetermined,0
State estimation methods applied to transformer monitoring,"State estimation methods are valuable for monitoring complex systems with not directly measurable states. One case in point is the monitoring of power transformers. The thermal condition of the transformer is very crucial in determining the life expectancy of the transformer, yet the hot spot temperature of the transformer cannot be directly measured. Other events such as inter-coil discharges cannot be directly measured unless they develop into a power fault. This paper describes the application of state estimation methods in a transformer diagnostic system. This system consists of hardware and software that continuously monitor the transformer electro-thermal states. The transformer states are obtained from redundant measurements of transformer terminal voltages, currents, tank temperature, oil temperature and ambient temperature. This data are fitted into the electro-thermal model of the transformer via state estimation methods that provide the full range of the transformer electro-thermal states. The paper describes the electrothermal model, the unique features of the data acquisition system and focuses on the application of the state estimation methods to derive important transformer quantities such as: (a) loss of life; and (b) transformer coil integrity. The system has been implemented on a trial basis on four Entergy System transformers. In the present implementation, the minimum input data requirements are: (1) phase currents and voltages at both ends of the transformer (a total of twelve); (2) top of oil and bottom of oil temperatures; and (3) load tap changer position. Actual data and the performance of the state estimation methods are presented.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=970060,no,undetermined,0
Combined use of intelligent partial discharge analysis in evaluating high voltage dielectric condition,"This paper describes the results of synthesised high voltage impulse tests, conducted on surrogate dielectric samples. The tests conducted under laboratory conditions, were performed using contoured electrodes submersed under technical grade insulating oil. An escalating level of artificial degradation within surrogate samples was assessed, this correlated against magnitude and frequency of events. Withstand of partial discharge activity up to a point of insulation breakdown was observed using a conventional elliptical display partial discharge detector. Measurements of PD activity were simultaneously captured by virtual scope relaying data array captures to a desktop computer. The captured data arrays were duly processed by an artificial neural network program, the net result of which indicated harmony between human-guided opinion and the software aptitude. This paper describes work currently being undertaken for the identification and diagnosis of faults in high voltage dielectrics in furthering development of AI techniques",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=971345,no,undetermined,0
Modeling and quantification of security attributes of software systems,"Quite often failures in network based services and server systems may not be accidental, but rather caused by deliberate security intrusions. We would like such systems to either completely preclude the possibility of a security intrusion or design them to be robust enough to continue functioning despite security attacks. Not only is it important to prevent or tolerate security intrusions, it is equally important to treat security as a QoS attribute at par with, if not more important than other QoS attributes such as availability and performability. This paper deals with various issues related to quantifying the security attribute of an intrusion tolerant system, such as the SITAR system. A security intrusion and the response of an intrusion tolerant system to the attack is modeled as a random process. This facilitates the use of stochastic modeling techniques to capture the attacker behavior as well as the system's response to a security intrusion. This model is used to analyze and quantify the security attributes of the system. The security quantification analysis is first carried out for steady-state behavior leading to measures like steady-state availability. By transforming this model to a model with absorbing states, we compute a security measure called the ""mean time (or effort) to security failure"" and also compute probabilities of security failure due to violations of different security attributes.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1028941,no,undetermined,0
Software-based weighted random testing for IP cores in bus-based programmable SoCs,"Presents a software-based weighted random pattern scheme for testing delay faults in IP cores of programmable SoCs. We describe a method for determining static and transition probabilities (profiles) at the inputs of circuits with full-scan using testability metrics based on the targeted fault model, We use a genetic algorithm (GA) based search procedure to determine optimal profiles. We use these optimal profiles to generate a test program that runs on the processor core. This program applies test patterns to the target IP cores in the SoC and analyzes the test responses. This provides the flexibility of applying multiple profiles to the IP core under test to maximize fault coverage. This scheme does not incur the hardware overhead of logic BIST, since the pattern generation and analysis is done by software. We use a probabilistic approach to finding the profiles. We describe our method on transition and path-delay fault models, for both enhanced full-scan and normal full-scan circuits. We present experimental results using the ISCAS 89 benchmarks as IP cores.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1011125,no,undetermined,0
Release date prediction for telecommunication software using Bayesian Belief Networks,"Many techniques are used for cost, quality and schedule estimation in the context of software risk management. Application of Bayesian Belief Networks (BBN) in this area permits process metrics and product metrics (static code metrics) to be considered in a causal way (i.e. each variable within the model has a cause-effect relationship with other variables) and, in addition, current observations can be used to update estimates based on historical data. However, the real situation that researchers face is that process data is often inadequately, or inappropriately, collected and organized by the development organization. In this paper, we explore if BBN could be used to predict appropriate release dates for a new set of products from a telecommunication company based on static code metrics data and limited process information collected from a earlier set of the same products. Two models are evaluated with different methods involved to analyze the available metrics data.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1013033,no,undetermined,0
A quality assessment model for Java code,"Quality measures are extremely difficult to quantify because they depend on many parameters and factors, some of which cannot be identified or measured readily. Java is the language of choice for interoperable code segments that constitute an effective interface layer between Web servers and the user. Realizing those code segments, however, is a challenge. Reusability criteria do not apply. This paper describes a quality model that can be used directly on code, and thus during light development and in rapid development cycles. The model is based on nonquantifiable attributes of quality that then are related to specific measures found using a structured method. The measures identify statistical clusters that can be used to categorize the quality of each Java class file. The relation between quality factors and measures is proven at the mathematical level, using the representational theory of measurement, and then at the empirical level, using an independent assessment. The preliminary results collected seem to indicate that the quality model is effective in classifying Java programs. An important indication can then be obtained by the quality analysis.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1013024,no,undetermined,0
"An integrated approach to flow, thermal and mechanical modeling of electronics devices","The future success of many electronics companies will depend to a large extent on their ability to initiate techniques that bring schedules, performance, tests, support, production, life-cycle-costs, reliability prediction and quality control into the earliest stages of the product creation process. Earlier papers have discussed the benefits of an integrated analysis environment for system-level thermal, stress and EMC prediction. This paper focuses on developments made to the stress analysis module and presents results obtained for an SMT resistor. Lifetime predictions are made using the Coffin-Manson equation. Comparison with the creep strain energy based models of Darveaux (1997) shows the shear strain based method to underestimate the solder joint life. Conclusions are also made about the capabilities of both approaches to predict the qualitative and quantitative impact of design changes.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1012545,no,undetermined,0
How valuable is company-specific data compared to multi-company data for software cost estimation?,"This paper investigates the pertinent question whether multi-organizational data is valuable for software project cost estimation. Local, company-specific data is widely believed to provide a better basis for accurate estimates. On the other hand, multi-organizational databases provide an opportunity for fast data accumulation and shared. information benefits. Therefore, this paper trades off the potential advantages and drawbacks of using local data as compared to multi-organizational data. Motivated by the results from previous investigations, we further analyzed a large cost database from Finland that collects standard cost factors and includes information on six individual companies. Each of these companies provided data for more than ten projects. This information was used to compare the accuracy between company-specific (local) and company-external (global) cost models. They show that company-specific models seem not to yield better results than the company external models. Our results are based on applying two standard statistical estimation methods (OLS-regression, analysis of variance) and analogy-based estimation.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1011342,no,undetermined,0
An empirical study of the impact of count models predictions on module-order models,"Software quality prediction models are used to achieve high software reliability. A module-order model (MOM) uses an underlying quantitative prediction model to predict this rank-order. This paper compares performances of module-order models of two different count models which are used as the underlying prediction models. They are the Poisson regression model and the zero-inflated Poisson regression model. It is demonstrated that improving a count model for prediction does not ensure a better MOM performance. A case study of a full-scale industrial software system is used to compare performances of module-order models of the two count models. It was observed that improving prediction of the Poisson count model by using zero-inflated Poisson regression did not yield module-order models with better performance. Thus, it was concluded that the degree of prediction accuracy of the underlying model did not influence the results of the subsequent module-order model. Module-order modeling is proven to be a robust and effective method even though both underlying prediction may sometimes lack acceptable prediction accuracy.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1011335,no,undetermined,0
A generic model and tool support for assessing and improving Web processes,"We discuss a generic quality framework, based on a generic model, for evaluating Web processes. The aim is to perform assessment and improvement of web processes by using techniques from empirical software engineering. A web development process can be broadly classified into two almost independent sub-processes: the authoring process (AUTH process) and the process of developing the infrastructure (INF process). The AUTH process concerns the creation and management of the contents of a set of nodes and the way they are linked to produce a web application, whereas the INF development process provides technological support and involves creation of databases, integration of the web application to legacy systems etc. In this paper, we instantiate our generic quality model to the AUTH process and present a measurement framework for this process. We also present a tool support to provide effective guidance to software personnel including developers, managers and quality assurance engineers.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1011333,no,undetermined,0
Investigating the influence of inspector capability factors with four inspection techniques on inspection performance,"We report on a controlled experiment with over 170 student subjects to investigate the influence of inspection process, i.e., the defect detection technique applied, and inspector capability factors on the effectiveness and efficiency of inspections on individual and team level. The inspector capability factors include measures on the inspector's experience, as well as a pre-test with a mini-inspection. We use sampling to quantify the gain of defects detected from selecting the best inspectors according to the pre-test results compared to the performance of an average team of inspectors. Main findings are that inspector development and quality assurance capability and experience factors do not significantly distinguish inspector groups with different inspection performance. On the other hand the mini-inspection pre-test has considerable correlation to later inspection performance. The sampling of teams shows that selecting inspectors according to the mini-inspection pretest considerably improves average inspection effectiveness by up to one third.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1011330,no,undetermined,0
An industrial case study to examine a non-traditional inspection implementation for requirements specifications,"Software inspection is one of the key enablers for quality improvement and defect cost reduction. Although its benefits are shown in many studies, a major obstacle to implement and use inspection technologies in software projects is the costs associated with it. Companies therefore constantly ask for the most cost-effective inspection implementation. Answering this question requires a discussion about the design of the inspection process as well as the question of how the selected process activities are to be performed. As a consequence, the tailored process and activities often result in an inspection implementation that perfectly fits into a project or environment but is different to the traditional ones presented in the existing inspection literature. We present and examine a non-traditional inspection implementation at DaimlerChrysler AG. The design of this inspection approach evolved over time as part of a continuous improvement effort and therefore integrates specific issues of the project environment as well as recent research findings. In addition to the description of the inspection approach, the paper presents quantitative results to characterize the suggested inspection implementation and investigates some of the essential hypotheses in the inspection area. Both, the technical description as well as its quantitative underpinning serves as an example for other companies that pursue improvement or adaptation efforts in the software inspection area.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1011329,no,undetermined,0
A delay/Doppler-mapping receiver system for GPS-reflection remote sensing,"A delay/Doppler-mapping receiver system, developed specifically for global positioning system (GPS)-reflection remote sensing, is described, and example delay/Doppler waveforms are presented. The high-quality data obtained with this system provide a more accurate and detailed examination of ground-based and aircraft GPS-reflection phenomenology than has been available to date. As an example, systematic effects in the reflected signal delay waveform, due to nonideal behavior of the C/A-code auto-correlation function, are presented for the first time. Both a single-channel open-loop recording system and a recently developed 16-channel recorder are presented. The open-loop data from either recorder are postprocessed with a software GPS receiver that performs the following functions: signal detection; phase and delay tracking; delay, Doppler, and delay/Doppler waveform mapping; dual-frequency (L1 and L2) processing; C/A-code and Y-code waveform extraction; coherent integrations as short as 125 Î¼s; navigation message decoding; and precise observable time tagging. The software can perform these functions on all detectable satellite signals without dead time, and custom signal-processing features can easily be included into the system",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1010901,no,undetermined,0
FPGA resource and timing estimation from Matlab execution traces,"We present a simulation-based technique to estimate area and latency of an FPGA implementation of a Matlab specification. During simulation of the Matlab model, a trace is generated that can be used for multiple estimations. For estimation the user provides some design constraints such as the rate and bit width of data streams. In our experience the runtime of the estimator is approximately only 1/10 of the simulation time, which is typically fast enough to generate dozens of estimates within a few hours and to build cost-performance trade-off curves for a particular algorithm and input data. In addition, the estimator reports on the scheduling and resource binding used for estimation. This information can be utilized not only to assess the estimation quality, but also as first starting point for the final implementation.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1003597,no,undetermined,0
CASCADE - configurable and scalable DSP environment,"As the complexity of embedded systems grows rapidly, it is common to accelerate critical tasks with hardware. Designers usually use off-the-shelf components or licensed IP cores to shorten the time to market, but the hardware/software interfacing is tedious, error-prone and usually not portable. Besides, the existing hardware seldom matches the requirements perfectly, CASCADE, the proposed design environment as an alternative, generates coprocessing datapaths from the executing algorithms specified in C/C++ and attaches these datapaths to the embedded processor with an auto-generated software driver. The number of datapaths and their internal parallel functional units are scaled to fit the application. It seamlessly integrates the design tools of the embedded processor to reduce the re-training/design efforts and maintains short product development time as the pure software approaches. A JPEG encoder is built in CASCADE successfully with an auto-generated four-MAC accelerator to achieve 623% performance boost for our video application.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1010596,no,undetermined,0
Recent advances in digital halftoning and inverse halftoning methods,"Halftoning is the rendition of continuous-tone pictures on displays, paper or other media that are capable of producing only two levels. In digital halftoning, we perform the gray scale to bilevel conversion digitally using software or hardware. In the last three decades, several algorithms have evolved for halftoning. Examples of algorithms include ordered dither, error diffusion, blue noise masks, green noise halftoning, direct binary search (DBS), and dot diffusion. In this paper, we first review some of the algorithms which have a direct bearing on our paper and then describe some of the more recent advances in the field. The dot-diffusion method for digital halftoning has the advantage of pixel-level parallelism unlike the error-diffusion method, which is a popular halftoning method. However, the image quality offered by error diffusion is still regarded as superior to most of the other known methods. We first review error diffusion and dot diffusion, and describe a recent method to improve the image quality of the dot-diffusion algorithm which takes advantage of the Human Visual System (HVS) function. Then, we discuss the inverse halftoning problem",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1010034,no,undetermined,0
Data mining technology for failure prognostic of avionics,Adverse environmental conditions have combined cumulative effects leading to performance degradation and failures of avionics. Classical reliability addresses statistically-generic devices and is less suitable for the situations when failures are not traced to manufacturing but rather to unique operational conditions of particular hardware units. An approach aimed at the accurate assessment of the probability of failure of any avionics unit utilizing the known history-of-abuse from environmental and operational factors is presented herein. The suggested prognostic model utilizes information downloaded from dedicated monitoring systems of flight-critical hardware and stored in a database. Such a database can be established from the laboratory testing of hardware and supplemented with real operational data. This approach results in a novel knowledge discovery from data technology that can be efficiently used in a wide area of applications and provide a quantitative basis for the modern maintenance concept known as service-when-needed. An illustrative numerical example is provided,2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1008974,no,undetermined,0
Experiences in assessing product family software architecture for evolution,"Software architecture assessments are a means to detect architectural problems before the bulk of development work is done. They facilitate planning of improvement activities early in the lifecycle and allow limiting the changes on any existing software. This is particularly beneficial when the architecture has been planned to (or already does) support a whole product family, or a set of products that share common requirements, architecture, components or code. As the family requirements evolve and new products are added, the need to assess the evolvability of the existing architecture is vital. The author illustrates two assessment case studies in the mobile telephone software domain: the Symbian operating system platform and the network resource access control software system. By means of simple experimental data, evidence is shown of the usefulness of architectural assessment as rated by the participating stakeholders. Both assessments have led to the identification of previously unknown architectural defects, and to the consequent planning of improvement initiatives. In both cases, stakeholders noted that a number of side benefits, including improvement of communication and architectural documentation, were also of considerable importance. The lessons learned and suggestions for future research and experimentation are outlined.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1008003,no,undetermined,0
Software quality prediction using median-adjusted class labels,Software metrics aid project managers in predicting the quality of software systems. A method is proposed using a neural network classifier with metric inputs and subjective quality assessments as class labels. The labels are adjusted using fuzzy measures of the distances from each class center computed using robust multivariate medians,2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1007518,no,undetermined,0
Experiences with evaluating network QoS for IP telephony,"Successful deployment of networked multimedia applications such as IP telephony depends on the performance of the underlying data network. QoS requirements of these applications are different from those of traditional data applications. For example, while IP telephony is very sensitive to delay and jitter, traditional data applications are more tolerant of these performance metrics. Consequently, assessing a network to determine whether it can accommodate the stringent QoS requirements of IP telephony becomes critical. We describe a technique for evaluating a network for IP telephony readiness. Our technique relies on the data collection and analysis support of our prototype tool, ExamiNetâ„? It automatically discovers the topology of a given network and collects and integrates network device performance and voice quality metrics. We report the results of assessing the IP telephony readiness of a real network of 31 network devices (routers/switches) and 23 hosts via ExamiNetâ„? Our evaluation identified links in the network that were over utilized to the point at which they could not handle IP telephony.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1006594,no,undetermined,0
Test compaction for at-speed testing of scan circuits based on nonscan test. sequences and removal of transfer sequences,"Proposes a procedure for generating compact test sets with enhanced at-speed testing capabilities for scan circuits. Compaction refers here to a reduction in the test application time, while at-speed testing refers to the application of primary input sequences that contribute to the detection. of delay defects. The proposed procedure generates an initial test set that has a low test application time and consists of long sequences of primary input vectors applied consecutively. To construct this test set, the proposed procedure transforms a test sequence T<sub>o</sub> for the nonscan circuit into a scan-based test by selecting an appropriate scan-in state and removing primary input vectors from T<sub>o</sub> if they do not contribute to the fault coverage. If T<sub>o</sub> contains long transfer sequences, several scan-based tests with long primary input sequences may be obtained by replacing transfer sequences in T<sub>o</sub> with scan operations. This helps reduce the test application time further. We demonstrate through experimental results the advantages of this approach over earlier ones as a method for generating test sets with minimal test application time and long primary input sequences",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1004314,no,undetermined,0
End-to-end latency of a fault-tolerant CORBA infrastructure,"This paper presents measured probability density functions (pdfs) for the end-to-end latency, of two-way, remote method invocations from a CORBA client to a replicated CORBA server in a fault-tolerance infrastructure. The infrastructure uses a multicast group-communication protocol based on a logical token-passing ring imposed on a single local-area network. The measurements show that the peaks of the pd/s for the latency are affected by the presence of duplicate messages for active replication, and by the position of the primary server replica on the ring for semi-active and passive replication. Because a node cannot broadcast a user message until it receives the token, up to two complete token rotations can contribute to the end-to-end latency seen by the client for synchronous remote method invocations, depending on the server processing time and the interval between two consecutive client invocations. For semi-active and passive replication, careful placement of the primary server replica is necessary to alleviate this broadcast delay to achieve the best possible end-to-end latency. The client invocation patterns and the server processing time must be considered together to determine the most favorable position for the primary replica. Assuming that an effective sending-side duplicate suppression mechanism is implemented, active replication can be more advantageous than semi-active and passive replication because all replicas compete for sending and, therefore, the replica at the most favorable position will have the opportunity to send first",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1003697,no,undetermined,0
A probably approximately correct framework to estimate performance degradation in embedded systems,"Future design environments for embedded systems will require the development of sophisticated computer-aided design tools for compiling the high-level specifications of an application down to a final low-level language describing the embedded solution. This requires abstraction of technology-dependent aspects and requirements into behavioral entities. The paper takes a first step in this direction by introducing a high-level methodology for estimating the performance degradation of an application affected by perturbations; a special emphasis is given to accuracy performance. To grant generality it is uniquely assumed that the performance degradation function and the mathematical formulation describing the application are Lebesgue measurable. Perturbations affecting the application abstract details related to physical sources of uncertainties such as finite precision representation, faults, fluctuations of physical parameters, battery power variations, and aging effects whose impact on the computation can be treated within a high-level homogenous framework. A novel stochastic theory based on randomization is suggested to quantify the approximated nature of the perturbed environment. The outcomes are two algorithms which estimate in polynomial time the performance degradation of the application once affected by perturbations. Such information can then be exploited by HW/SW codesign methodologies to guide the subsequent partitioning between HW and SW, analog versus digital, fixed versus floating point, or used to validate architectural choices before any low-level design step takes place. The proposed method is finally applied to real designs involving neural and wavelet-based applications",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1013889,no,undetermined,0
"Trace: an open platform for high-layer protocols, services and networked applications management","This paper presents the Trace management platform, an extension of the SNMP infrastructure based on the IETF Script MIB to support integrated, distributed and flexible management of high-layer protocols, services and networked applications. The platform is specifically geared towards running and analyzing protocol interactions, and triggering custom scripts when certain conditions are met.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1015630,no,undetermined,0
Heaps and stacks in distributed shared memory,"Software-based distributed shared memory (DSM) systems do usually not provide any means to use shared memory regions as stacks or via an efficient heap memory allocator. Instead DSM users are forced to work with very rudimentary and coarse grain memory (de-)allocation primitives. As a consequence most DSM applications have to ""reinvent the wheel"", that is to implement simple stack or heap semantics within the shared regions. Obviously, this has several disadvantages. It is error-prone, timeconsuming and inefficient. This paper presents an all in software DSM that does not suffer from these drawbacks. Stack and heap organization is adapted to the changed requirements in DSM environments and both, stacks and heaps, are transparently placed in DSM space by the operating system.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1016494,no,undetermined,0
Bond and electron beam welding quality control of the aluminum stabilized and reinforced CMS conductor by means of ultrasonic phased-array technology,"The Compact Muon Solenoid (CMS) is one of the general-purpose detectors to be provided for the LHC project at CERN. The design field of the CMS superconducting magnet is 4 T, the magnetic length is 12.5 m and the free bore is 6 m. The coils for CNIS are wound of aluminum-stabilized Rutherford type superconductors reinforced with high-strength aluminum alloy. For optimum performance of the conductor a void-free metallic bonding between the high-purity aluminum and the Rutherford type cable as well as between the electron beam welded reinforcement and the high-purity aluminum must be guaranteed. It is the main task of this development work to assess continuously the bond quality over the whole width and the total length of the conductors during manufacture. To achieve this goal we use the ultrasonic phased-array technology. The application of multi-element transducers allows an electronic scanning perpendicular to the direction of production. Such a testing is sufficiently fast in order to allow a continuous analysis of the complete bond. A highly sophisticated software allows the on-line monitoring of the bond and weld quality.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1018616,no,undetermined,0
Advanced pattern recognition for detection of complex software aging phenomena in online transaction processing servers,"Software aging phenomena have been recently studied; one particularly complex type is shared memory pool latch contention in large OLTP servers. Latch contention onset leads to severe performance degradation until a manual rejuvenation of the DBMS shared memory pool is triggered. Conventional approaches to automated rejuvenation have failed for latch contention because no single resource metric has been identified that can be monitored to alert the onset of this complex mechanism. The current investigation explores the feasibility of applying an advanced pattern recognition method that is embodied in a commercially available equipment condition monitoring system (SmartSignal eCMâ„? for proactive annunciation of software-aging faults. One hundred data signals are monitored from a large OLTP server, collected at 20-60 sec. intervals over a 5-month period. Results show 13 variables consistently deviate from normal operation prior to a latch event, providing up to 2 hours early warning.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1028933,no,undetermined,0
A software-reliability growth model for N-version programming systems,"This paper presents a NHPP-based SRGM (software reliability growth model) for NVP (N-version programming) systems (NVP-SRGM) based on the NHPP (nonhomogeneous Poisson process). Although many papers have been devoted to modeling NVP-system reliability, most of them consider only the stable reliability, i.e., they do not consider the reliability growth in NVP systems due to continuous removal of faults from software versions. The model in this paper is the first reliability-growth model for NVP systems which considers the error-introduction rate and the error-removal efficiency. During testing and debugging, when a software fault is found, a debugging effort is devoted to remove this fault. Due to the high complexity of the software, this fault might not be successfully removed, and new faults might be introduced into the software. By applying a generalized NHPP model into the NVP system, a new NVP-SRGM is established, in which the multi-version coincident failures are well modeled. A simplified software control logic for a water-reservoir control system illustrates how to apply this new software reliability model. The s-confidence bounds are provided for system-reliability estimation. This software reliability model can be used to evaluate the reliability and to predict the performance of NVP systems. More application is needed to validate fully the proposed NVP-SRGM for quantifying the reliability of fault-tolerant software systems in a general industrial setting. As the first model of its kind in NVP reliability-growth modeling, the proposed NVP SRGM can be used to overcome the shortcomings of the independent reliability model. It predicts the system reliability more accurately than the independent model and can be used to help determine when to stop testing, which is a key question in the testing and debugging phase of the NVP system-development life cycle",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1028403,no,undetermined,0
Built-in-test for processor-based modules,"This article describes a software methodology for built-in-test (BIT) for processor based modules. Firstly, an example of the hardware comprising a common processor module is given. This is followed by a description of the BIT software architecture and details of the component elements: the test executive, which integrates all the components and provides the interface to external applications and boot programs; the BIT reconfiguration table, for test control flexibility; test result handling; and the individual test functions, written in both assembly language and C. This BIT product is extremely flexible, portable, and very thorough are high fault detection, high isolation coverage, and support for unique test requirements",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1028371,no,undetermined,0
Fast software implementation of MPEG advanced audio encoder,"An optimized software implementation of a high quality MPEG AAC-LC (low complexity) audio encoder is presented in this paper. The standard reference encoder is improved by utilizing several algorithmic optimizations (fast psycho-acoustic model, new tonality estimation, new time domain block switching, optimized quantizer and Huffman coder) and very careful code optimizations for PC CPU architectures with SIMD (single-instruction-multiple-data) instruction set. The psychoacoustic model used the MDCT filterbank for energy estimation and peak detection as a measure of tonality. Block size decision is based on local perceptual entropies as well as LPC analysis of the time signal. Algorithmic optimizations in the quantizer include loop control module modification and optimized Huffman search. Code optimization is based on parallel processing by replacing vector algebra and math junctions with their optimized equivalents with Intel<sup>Â®</sup> Signal Processing Library (SPL). The implemented codec outperforms consumer MP3 encoders at 30% less bitrate at the same time achieving encoding times several times faster than real-time.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1028221,no,undetermined,0
Automatic detection and exploitation of branch constraints for timing analysis,"Predicting the worst-case execution time (WCET) and best-case execution time (BCET) of a real-time program is a challenging task. Though much progress has been made in obtaining tighter timing predictions by using techniques that model the architectural features of a machine, significant overestimations of WCET and underestimations of GCET can still occur. Even with perfect architectural modeling, dependencies on data values can constrain the outcome of conditional branches and the corresponding set of paths that can be taken in a program. While branch constraint information has been used in the past by some timing analyzers, it has typically been specified manually, which is both tedious and error prone. This paper describes efficient techniques for automatically detecting branch constraints by a compiler and automatically exploiting these constraints within a timing analyzer. The result is significantly tighter timing analysis predictions without requiring additional interaction with a user.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1027799,no,undetermined,0
Fair triangle mesh generation with discrete elastica,"Surface fairing, generating free-form surfaces satisfying aesthetic requirements, is important for many computer graphics and geometric modeling applications. A common approach for fair surface design consists of minimization of fairness measures penalizing large curvature values and curvature oscillations. The paper develops a numerical approach for fair surface modeling via curvature-driven evolutions of triangle meshes. Consider a smooth surface each point of which moves in the normal direction with speed equal to a function of curvature and curvature derivatives. Chosen the speed function properly, the evolving surface converges to a desired shape minimizing a given fairness measure. Smooth surface evolutions are approximated by evolutions of triangle meshes. A tangent speed component is used to improve the quality of the evolving mesh and to increase computational stability. Contributions of the paper include also art improved method for estimating the mean curvature.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1027502,no,undetermined,0
"Estimation of net primary productivity using Boreal Ecosystem Productivity Simulator - a case study for Hokkaido Island, Japan","This paper describes a method for the estimation of the net primary productivity (NPP) by integrating remotely sensed and GIS data with a process,based ecosystem model (BEPS: Boreal Ecosystem Productivity Simulator) for Hokkaido Island, Japan. Using this method, we calculated the mean and total NPP for the study area in 1998 was 644 g C m<sup>-2</sup> year<sup>-1</sup> and 0.078 Pg C year<sup>-1</sup>, respectively. The effect of the quality of the model input requirements on accurate NPP estimation using a process-based model was also checked. The results show that the higher quality input data obtained from GIS data sets for a process-based model improved the NPP estimation accuracy for Hokkaido Island by about 16.6-39.7%.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1026540,no,undetermined,0
Needs for communications and onboard processing in the vision era,"The NASA New Millennium Program (NMP), in conjunction with the Earth Science Enterprise Technology Office, has examined the capability needs of future NASA Earth Science missions and defined a set of high priority technologies that offer broad benefits to future missions, which would benefit from validation in space before their use in a science mission. In the area of spacecraft communications, the need for high and ultra-high data rates is driving development of communications technologies. This paper describes the current vision and roadmaps of the NMP for the technology needed to support ultra-high data rate downlink to Earth. Hyperspectral land imaging, radar imaging and multi-instrument platforms represent the most demanding classes of instruments in which large data flows place limitations upon the performance of the instrument and systems. The existing and prospective data distribution (DD) modes employ various types of links, such as DD from low-Earth-orbit (LEO) spacecraft direct to the ground, DD from geosynchronous (GEO) spacecraft, LEO to GEO relays, multi-spacecraft links, and sensor webs. Depending on the type of link, the current data rate requirements vary from 2 Mbps (LEO to GEO relay) to 150 Mbps (DD from LEO spacecraft). It is expected that in the 20-year timeframe, the link data rates may increase to 100 Gbps. To ensure such capabilities, the aggressive development of communication technologies in the optical frequency region is necessary. Current technology readiness levels (TRL) of the technology components for the space segment of communications hardware varies from 3 (proof of concept) to 5 (validation in relevant environment). Development of onboard processing represents another area driven by increasing data rates of spaceborne experiments. The technologies that need further development include data compression, event recognition and response, as well as specific hyperspectral and radar data processing. Aspects of onboard processing technologies requiring flight validation include: fault-tolerant computing and processor stability, autonomous event detection and response, situation-based data compression and processing. The required technology validation missions can be divided in two categories: hardware-related missions and softwar- e-related missions. Objectives of the first kind of missions include radiation-tolerant processors and radiation-tolerant package switching communications node/network interface. Objectives of the second kind of missions include autonomous spacecraft operations and payload (instrument-specific) system operations.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1025044,no,undetermined,0
Calibration and estimation of redundant signals,This paper presents an adaptive filter for real-time calibration of redundant signals consisting of sensor data and/or analytically derived measurements. The measurement noise covariance matrix is adjusted as a function of the a posteriori probabilities of failure of the individual signals. An estimate of the measured variable is obtained as a weighted average of the calibrated signals. The weighting matrix is recursively updated in real time instead of being fixed a priori. The filter software is presently hosted in a Pentium platform and is portable to other commercial platforms. The filter can be used to enhance the Instrumentation and Control System Software in large-scale dynamical systems.,2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1023223,no,undetermined,0
Assessing the quality of Web-based applications via navigational structures,"We study the link validity of a Web site's navigational structure to enhance Web quality. Our approach employs the principle of statistical usage testing to develop an efficient and effective testing mechanism. Some advantages of our approach include generating test scripts systematically, providing coverage metrics, and executing hyperlinks only once.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1022855,no,undetermined,0
An adaptive distance relay and its performance comparison with a fixed data window distance relay,"This paper describes the design, implementation and testing of an adaptive distance relay. The relay uses a fault detector to determine the inception of a fault and then uses data windows of appropriate length for estimating phasors and seen impedances. Hardware and software of the proposed adaptive relay are also described in this paper. The relay was tested using a model power system and a real-time playback simulator. Performance of the relay was compared with a fixed data window distance relay. Some results are reported in this paper. These results indicate that the adaptive distance relay provides faster tripping in comparison to the fixed data window distance relay.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1022788,no,undetermined,0
The application of a distributed system-level diagnosis algorithm in dynamic positioning system,"This paper introduces the application of a distributed system-level fault diagnosis algorithm for detecting and diagnosing faulty processors in dynamic positioning system (DPS) of an offshore vessel. The system architecture of DPS is a loose coupling distributed multiprocessor system, which adopts the technique of Intel's MULTIBUS II and develops a software application on the platform of iRMX OS. In this paper a new approach to the diagnosis problem is presented, including an adaptive PMC model, distributed diagnosis including self-diagnosis and interactive-diagnosis, and system graph-theoretic model. The self-diagnosis fully utilises the individual results of built-in self-tests as a part of diagnosis work. Interactive-diagnosis means that in the system fault-free units perform simple periodic tests on one another under the direction of the graph-theoretic model by interactively communicating, and every unit can only send the diagnosis information to its considered fault-free units. Finally, we illustrate the procedure of diagnosis verification. The results obtained show that the adaptive PMC model is applicable, the distributed system-level diagnosis algorithm is proper, and the applications of diagnosis and verification are reliable and practicable.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1021494,no,undetermined,0
Comprehension of object-oriented software cohesion: the empirical quagmire,"Chidamber and Kemerer (1991) proposed an object-oriented (OO) metric suite which included the Lack of Cohesion Of Methods (LCOM) metric. Despite considerable effort both theoretically and empirically since then, the software engineering community is still no nearer finding a generally accepted definition or measure of OO cohesion. Yet, achieving highly cohesive software is a cornerstone of software comprehension and hence, maintainability. In this paper, we suggest a number of suppositions as to why a definition has eluded (and we feel will continue to elude) us. We support these suppositions with empirical evidence from three large C++ systems and a cohesion metric based on the parameters of the class methods; we also draw from other related work. Two major conclusions emerge from the study. Firstly, any sensible cohesion metric does at least provide insight into the features of the systems being analysed. Secondly however, and less reassuringly, the deeper the investigative search for a definitive measure of cohesion, the more problematic its understanding becomes; this casts serious doubt on the use of cohesion as a meaningful feature of object-orientation and its viability as a tool for software comprehension.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1021308,no,undetermined,0
Software measurement: uncertainty and causal modeling,"Software measurement can play an important risk management role during product development. For example, metrics incorporated into predictive models can give advance warning of potential risks. The authors show how to use Bayesian networks, a graphical modeling technique, to predict software defects and-perform ""what if"" scenarios.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1020298,no,undetermined,0
A survey on software architecture analysis methods,"The purpose of the architecture evaluation of a software system is to analyze the architecture to identify potential risks and to verify that the quality requirements have been addressed in the design. This survey shows the state of the research at this moment, in this domain, by presenting and discussing eight of the most representative architecture analysis methods. The selection of the studied methods tries to cover as many particular views of objective reflections as possible to be derived from the general goal. The role of the discussion is to offer guidelines related to the use of the most suitable method for an architecture assessment process. We will concentrate on discovering similarities and differences between these eight available methods by making classifications, comparisons and appropriateness studies.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1019479,no,undetermined,0
Using version control data to evaluate the impact of software tools: a case study of the Version Editor,"Software tools can improve the quality and maintainability of software, but are expensive to acquire, deploy, and maintain, especially in large organizations. We explore how to quantify the effects of a software tool once it has been deployed in a development environment. We present an effort-analysis method that derives tool usage statistics and developer actions from a project's change history (version control system) and uses a novel effort estimation algorithm to quantify the effort savings attributable to tool usage. We apply this method to assess the impact of a software tool called VE, a version-sensitive editor used in Bell Labs. VE aids software developers in coping with the rampant use of certain preprocessor directives (similar to #if/#endif in C source files). Our analysis found that developers were approximately 40 percent more productive when using VE than when using standard text editors.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1019478,no,undetermined,0
Predicting TCP throughput from non-invasive network sampling,"In this paper, we wish to derive analytic models that predict the performance of TCP flows between specified endpoints using routinely observed network characteristics such as loss and delay. The ultimate goal of our approach is to convert network observables into representative user and application relevant performance metrics. The main contributions of this paper are in studying which network performance data sources are most reflective of session characteristics, and then in thoroughly investigating a new TCP model based on Padhye et al. (2000) that uses non-invasive network samples to predict the throughput of representative TCP flows between given end-points.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1019259,no,undetermined,0
Distributed real-time simulation of the group manager executing the multicast protocol RFRM,"Group communication in real-time computing systems is useful for building fault-tolerant distributed real-time applications. The purpose of this paper is to show the efficiency of the group membership manager executing the multicast protocol RFRM (Release-time based Fault-tolerant Real-time Multicast) which is based on the idea of attaching the official release time to each group view send message. In this approach, the group membership manager interacts with the fault detector and the client application do not send view-ack messages to the group membership manager. A real-time simulation based on the TMO structuring scheme is conducted to study its performance for both the proposed approach and the group management scheme which receives view-ack messages. Simulation results show that with a proper value of the official release time interval, our approach improves the delay in completing the group view change event",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1003688,no,undetermined,0
Exploiting agent mobility for large-scale network monitoring,"As networks become pervasive, the importance of efficient information gathering for purposes such as monitoring, fault diagnosis, and performance evaluation increases. Distributed monitoring systems based on either management protocols such as SNMP or distributed object technologies such as CORBA can cope with scalability problems only to a limited extent. They are not well suited to systems that are both very large and highly dynamic because the monitoring logic, although possibly distributed, is statically predefined at design time. This article presents an active distributed monitoring system based on mobile agents. Agents act as area monitors not bound to any particular network node that can ""sense"" the network, estimate better locations, and migrate in order to pursue location optimality. Simulations demonstrate the capability of this approach to cope with large-scale systems and changing network conditions",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1002994,no,undetermined,0
"Design of integrated software for reconfiguration, reliability, and protection system analysis","Interdependencies among software components for distribution network reconfiguration, reliability and protection system analysis are considered. Software interface specifications are presented. Required functionalities of reconfiguration for restoration are detailed. Two algorithms for reconfiguration for restoration are reviewed and compared. Use of outage analysis data to locate circuit sections in need of reliability improvements and to track predicted improvements in reliability is discussed",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=971385,no,undetermined,0
Dependability analysis of fault-tolerant multiprocessor systems by probabilistic simulation,"The objective of this research is to develop a new approach for evaluating the dependability of fault-tolerant computer systems. Dependability has traditionally been evaluated through combinatorial and Markov modelling. These analytical techniques have several limitations, which can restrict their applicability. Simulation avoids many of the limitations, allowing for more precise representation of system attributes than feasible with analytical modelling. However, the computational demands of simulating a system in detail, at a low abstraction level, currently prohibit evaluation of high-level dependability metrics such as reliability and availability. The new approach abstracts a system at the architectural level, and employs life testing through simulated fault-injection to accurately and efficiently measure dependability. The simulation models needed to implement this approach are derived, in part, from the published results of computer performance studies and low-level fault-injection experiments. The developed probabilistic models of processor, memory and fault-tolerant mechanisms take such properties of real systems, as error propagation, different modes of failures, event dependency and concurrency. They have been integrated with a workload model and statistical analysis module into a generalised software tool. The effectiveness of such an approach was demonstrated through the analysis of several multiprocessor architectures",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=975079,no,undetermined,0
Reliability estimation for a software system with sequential independent reviews,"Suppose that several sequential test and correction cycles have been completed for the purpose of improving the reliability of a given software system. One way to quantify the success of these efforts is to estimate the probability that all faults are found by the end of the last cycle, We describe how to evaluate this probability both prior to and after observing the numbers of faults detected in each cycle and we show when these two evaluations would be the same",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=988707,no,undetermined,0
Modelling of grounding systems for better protection of communication installations against effects from electric power system and lightning,"This paper addresses three important issues in modelling of grounding systems: modelling of high frequency and transient behavior, of complex and extended grounding systems, and use of software methods. First, modelling of frequency dependent and transient behavior of grounding systems is considered. When simple grounding arrangements are considered then simple steps for improved design for better transient performance may be followed. In cases when they are subjected to fast fronted current impulses, that is, with high frequency content, generation of large peaks of the transient voltages between excitation point and neutral ground during the rise of the current impulse is possible. Described design procedures are aimed at reducing such excessive transient voltages. More complex grounding arrangements may be optimised using suitable computer software. Next, modelling of very complex grounding systems is described. As an example, protection against dangerous voltages between telephone subscriber lines and local ground near high voltage substations due to ground potential rise in case of ground faults is analysed. A computer model of the substation grounding system and connected and near buried metallic structures in an urban environment is used for estimation of the ground potential rise zone of influence on the subscription wire-line installations. Finally, computer software method for analysis of low and high frequency and transient analysis of grounding systems of arbitrary geometry are described.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=988604,no,undetermined,0
Development of a dynamic power system load model,"The paper addresses the issue of measurement based power system load model development. The majority of power system loads respond dynamically to voltage disturbances and such contribute to overall system dynamics. Induction motors represent a major portion of system loads that exhibit dynamic behaviour following the disturbance. In this paper, the dynamic behaviours of an induction motor and a combination of induction motor and static load were investigated under different disturbances and operating conditions in the laboratory. A first order generic dynamic, load model is developed based on the test results. The model proposed is in a transfer function form and it is suitable for direct inclusion in the existing power system stability software. The robustness of the proposed model is also assessed.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=988434,no,undetermined,0
"RHIC insertion region, shunt power supply current errors",The Relativistic Heavy Ion Collider (RHIC) was commissioned in 1999 and 2000. RHIC requires power supplies to supply currents to highly inductive superconducting magnets. The RHIC Insertion Region contains many shunt power supplies to trim the current of different magnet elements in a large superconducting magnet circuit. Power Supply current error measurements were performed during the commissioning of RHIC. Models of these power supply systems were produced to predict and improve these power supply current errors using the circuit analysis program MicroCap V by Spectrum Software (TM). Results of the power supply current errors are presented from the models and from the measurements performed during the commissioning of RHIC,2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=988211,no,undetermined,0
Combinatorial designs in multiple faults localization for battlefield networks,We present an application of combinatorial designs and variance analysis to correlating events in the midst of multiple network faults. The network fault model is based on the probabilistic dependency graph that accounts for the uncertainty about the state of network elements. Orthogonal arrays help reduce the exponential number of failure configurations to a small subset on which further analysis is performed. The preliminary results show that statistical analysis can pinpoint the probable causes of the observed symptoms with high accuracy and significant level of confidence. An example demonstrates how multiple soft link failures are localized in MIL-STD 188-220's datalink layer to explain the end-to-end connectivity problems in the network layer This technique can be utilized for the networks operating in an unreliable environment such as wireless and/or military networks.,2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=985975,no,undetermined,0
Reliability of fault tolerant control systems: Part I,"The reliability analysis of fault-tolerant control systems is performed using Markov models. Reliability properties peculiar to fault-tolerant control systems are emphasized. As a consequence, coverage of failures through redundancy management can be severely limited. It is shown that in the early life of a system composed of highly reliable subsystems, the reliability of the overall system is affine with respect to coverage, and inadequate coverage induces dominant single point failures. The utility of some existing software tools for assessing the reliability of fault tolerant control systems is also discussed",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=981100,no,undetermined,0
A block algorithm for the Sylvester-observer equation arising in state-estimation,"We propose an algorithm for solving the Sylvester-observer equation arising in the construction of the Luenberger observer. The algorithm is a block-generalization of P. Van Dooren's (1981) scalar algorithm. It is more efficient than Van Dooren's algorithm and the recent block algorithm by B.N. Datta and D. Sarkissian (2000). Furthermore, the algorithm is well-suited for implementation on some of today's powerful high performance computers using the high-quality software package LAPACK",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=980481,no,undetermined,0
"The application of remote sensing, geographic information systems, and Global Positioning System technology to improve water quality in northern Alabama","Recently, the water quality status in northern Alabama has been declining due to urban and agricultural growth. Throughout the years, the application of remote sensing and geographic information system technology has undergone numerous modifications and revisions to enhance their ability to control, reduce, and estimate the origin of non-point source pollution. Yet, there is still a considerable amount of uncertainty surrounding the use of this technology as well as its modifications. This research demonstrates how the application of remote sensing, geographic information system, and global positioning system technologies can be used to assess water quality in the Wheeler Lake watershed. In an effort to construct a GIS based water quality database of the study area for future use, a land use cover of the study area will be derived from LANDSAT Thermatic Mapper (TM) imagery using ERDAS IMAGINE image processing software. A Digital Elevation Model of the Wheeler Lake watershed was also from an Environmental Protection Agency Basins database. Physical and chemical properties of water samples including pH, Total Suspended Solids (TSS), Total Fecal Coliform (TC), Total Nitrogen (TN), Total Phosphorus (TP), Biological Oxygen Demand (BOD), Dissolved Oxygen (DO), and selected metal concentrations were measured",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=976822,no,undetermined,0
A fault diagnosis system for heat pumps,"During the operation of heat pumps, faults like heat exchanger fouling, component failure, or refrigerant leakage reduce the system performance. In order to recognize these faults early, a fault diagnosis system has been developed and verified on a test bench. The parameters of a heat pump model are identified sequentially and classified during operation. For this classification, several `hard' and `soft' clustering methods have been investigated, while fuzzy inference systems or neural networks are created automatically by newly developed software. Choosing a simple black-box model structure, the number of sensors can be minimized, whereas a more advanced grey-box model yields better classification results",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=973840,no,undetermined,0
Fault detection and location on electrical distribution system,"This case study summarizes the efforts undertaken at CP and L-A Progress Energy Company over the past five years to improve distribution reliability via detection of distribution faults and determination of their location. The analysis methods used by CP and L have changed over the years as improvements were made to the various tools used. The tools used to analyze distribution faults included a feeder monitoring system (FMS), an automated outage management system (OMS), a distribution SCADA system (DSCADA), and several Excel spreadsheet applications. The latest fault detection system involves an integration of FMS, OMS, and DSCADA systems to provide distribution dispatchers with a graphical display of possible locations for faults that have locked out feeder circuit breakers",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1002288,no,undetermined,0
Real-time suppression of stochastic pulse shaped noise for on-site PD measurements,Partial Discharge (PD) diagnosis and monitoring systems are state of the art for performing quality assurance or fault identification of high voltage components. However their on-site application is a rather difficult task because PD information is widely superimposed with electromagnetic disturbances. These disturbances affect very negatively all known PD diagnosis systems. Especially the detection and suppression of stochastic pulse shaped noise is a major problem. To determine such noise a fast machine intelligent signal recognition system (NeuroTEK II) was developed. This system is able to distinguish between PD pulses and noise up to a signal to noise ratio of 1:10. Therefore a measuring system acquires the input data in the VHF range (20..100MHz). The discrimination of the pulses is performed in real time in time domain using fast neural network hardware. With that signal recognition system a noise suppressed Phase Resolved Pulse Sequence (PRPS) data set in the CIGRE data format is generated that can be the input source of the most modern PD evaluation software,2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=973550,no,undetermined,0
"Managing the maintenance of ported, outsourced, and legacy software via orthogonal defect classification","From the perspective of maintenance, software systems that include COTS software, legacy, ported or outsourced code pose a major challenge. The dynamics of enhancing or adapting a product to address evolving customer usage and the inadequate documentation of these changes over a period of time (and several generations) are just two of the factors which may have a debilitating effect on the maintenance effort. While many approaches and solutions have been offered to address the underlying problems, few offer methods which directly affect a team's ability to quickly identify and prioritize actions targeting the product which is already in front of them. The paper describes a method to analyze the information contained in the form of defect data and arrive at technical actions to address explicit product and process weaknesses which can be feasibly addressed in the current effort. The defects are classified using Orthogonal Defect Classification (ODC) and actual case studies are used to illustrate the key points",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=972791,no,undetermined,0
Dynamic and static views of software evolution,"In addition to managing day-to-day maintenance, information system managers need to be able to predict and plan the longer-term evolution of software systems on an objective, quantified basis. Currently this is a difficult task, since the dynamics of software evolution, and the characteristics of evolvable software are not clearly understood. In this paper we present an approach to understanding software evolution. The approach looks at software evolution from two different points of view. The dynamic viewpoint investigates how to model software evolution trends and the static viewpoint studies the characteristics of software artefacts to see what makes software systems more evolvable. The former will help engineers to foresee the actions to be taken in the evolution process, while the latter provides an objective, quantified basis to evaluate the software with respect to its ability to evolve and will help to produce more evolvable software systems",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=972776,no,undetermined,0
Modeling clones evolution through time series,"The actual effort to evolve and maintain a software system is likely to vary depending on the amount of clones (i.e., duplicated or slightly different code fragments) present in the system. This paper presents a method for monitoring and predicting clones evolution across subsequent versions of a software system. Clones are firstly identified using a metric-based approach, then they are modeled in terms of time series identifying a predictive model. The proposed method has been validated with an experimental activity performed on 27 subsequent versions of mSQL, a medium-size software system written in C. The time span period of the analyzed mSQL releases covers four years, from May 1995 (mSQL 1.0.6) to May 1999 (mSQL 2. 0. 10). For any given software release, the identified models was able to predict the clone percentage of the subsequent release with an average error below 4 %. A higher prediction error was observed only in correspondence of major system redesign",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=972740,no,undetermined,0
Impact analysis of maintenance tasks for a distributed object-oriented system,"The work described in this paper is part of an ongoing project to improve the maintenance process in a Vienna Software-House. A repository has been constructed on the basis of a relational database and populated with metadata on a wide variety of software artifacts at each semantic level of development-concept, code and test. Now the repository is being used to perform impact analysis and cost estimation of change requests prior to implementing them. For this, hypertext techniques and ripple effects are used to identify all interdependencies. A tool has been constructed to navigate through the repository, select the impacted entities and pick up their size, complexity and quality metrics for effort estimation",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=972728,no,undetermined,0
Investigation of the risk to software reliability and maintainability of requirements changes,"In order to continue to make progress in software measurement, as it pertains to reliability and maintainability, we must shift the emphasis from design and code metrics to metrics that characterize the risk of making requirements changes. Although these software attributes can be difficult to deal with due to the fuzzy requirements from which they are derived, the advantage of having early indicators of future software problems outweighs this inconvenience. We developed an approach for identifying requirements change risk factors as predictors of reliability and maintainability problems. Our case example consists of twenty-four Space Shuttle change requests, nineteen risk factors, and the associated failures and software metrics. The approach can be generalized to other domains with numerical results that would vary according to application",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=972723,no,undetermined,0
Empiric validation of the person to role allocation process,"A Capacities-Centered Integral Software Process Model (CCISPM) has been designed, covering and modeling all the relevant elements of the software process. In this paper an empirical validation of one of the processes is presented: the Person to Role Allocation Process. In this process the allocation of persons to fulfill roles is made according to the capacities or behavioral competencies that the persons possess and those required by the roles in the CCISPM. A set of experiments are carried out dealing with the development of the Initiation, Planning and Estimation Process, Domain Study Process, Requirements Analysis Process and Design Process of eight projects. It was proved that the estimated time deviation, as well as the errors found in the technical reviews of requirements specification, were less when the persons fulfilling the roles of planning engineer, domain analyst, requirements specificator and designer were allocated, according to the proposed model, considering the set of critical capacities",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=972645,no,undetermined,0
Empirical validation of class diagram complexity metrics,"One of the principal objectives of software engineering is to improve the quality of software products. It is widely recognised that the quality assurance of software products must be guaranteed from the early phases of development. As a key artifact produced in the early development of object-oriented (OO) information systems (OOISs), class diagram quality has a great impact on the quality of the software product which is finally delivered. Hence, class diagram quality is a crucial issue that must be evaluated (and improved if necessary) in order to obtain quality OOISs, which is the main concern of present-day software development organisations. After a thorough review of the existing OO measures that are applicable to class diagrams at a high-level design stage, M. Genero et al. (2000) presented in a set of metrics for the structural complexity of class diagrams built using the Unified Modelling Language (UML). We focus on class diagram structural complexity, an internal quality attribute which we believe could be closely correlated with one of the most critical external quality attributes, such as class diagram maintainability. Since the main goal of this paper is the empirical validation of those metrics, we present two controlled experiments carried out to corroborate if those metrics are closed to class diagram maintainability and thus could be used as early maintainability indicators. Based on data collected in the experiments, we build a prediction model for class diagram maintainability using a method for the induction of fuzzy rules",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=972637,no,undetermined,0
Feedback control of the software test process through measurements of software reliability,"A closed-loop feedback control model of the software test process (STP) is described. The model is grounded in the well established theory of automatic control. It offers a formal and novel procedure for using product reliability or failure intensity as a basis for closed loop control of the STP. The reliability or the failure intensity of the product is compared against the desired reliability at each checkpoint and the difference fed back to a controller. The controller uses this difference to compute changes necessary in the process parameters to meet the reliability, or failure intensity objective at the terminal checkpoint (the deadline). The STP continues beyond a checkpoint with a revised set of parameters. This procedure is repeated at each checkpoint until the termination of the STP. The procedure accounts for the possibility of changes (during testing), in reliability or failure intensity objective, the checkpoints, and the parameters that characterize the STP. The effectiveness of this procedure was studied using commercial data available in the public domain and also from the data generated through simulation. In all cases, the use of feedback control produces adequate results allowing the achievement of the objectives.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=989477,no,undetermined,0
An empirical evaluation of statistical testing designed from UML state diagrams: the flight guidance system case study,"This paper presents an empirical study of the effectiveness of test cases generated from UML state diagrams using transition coverage as the testing criterion. The test cases production is mainly based on an adaptation of a probabilistic method, called statistical testing based on testing criteria. This technique was automated with the aid of the Rational Software Corporation's Rose RealTime tool. The test strategy investigated combines statistical test cases with (few) deterministic test cases focused on domain boundary values. Its feasibility is exemplified on a research version of an avionics system implemented in Java: the Flight Guidance System case study (14 concurrent state diagrams). Then, the results of an empirical evaluation of the effectiveness of the created test cases are presented. The evaluation was performed using mutation analysis to assess the error detection power of the test cases on more than 1500 faults seeded one by one in the Java source code (115 classes, 6500 LOC). A detailed analysis of the test results allows us to draw first conclusions on the expected strengths and weaknesses of the proposed test strategy.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=989479,no,undetermined,0
Evaluating the software test strategy for the 2000 Sydney Olympics,"The 2000 summer Olympic Games event was a major information technology challenge. With a fixed deadline for completion, its inevitable dependency on software systems and immense scope, the testing and verification effort was critical to its success. One way in which success was assured was the use of innovative techniques using ODC based analysis to evaluate planned and executed test activities. These techniques were used to verify that the plan was comprehensive, yet efficient, and ensured that progress could be accurately measured. This paper describes some of these techniques and provides examples of the benefits derived. We also discuss the applicability, of the techniques to other software projects.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=989480,no,undetermined,0
Accounting for realities when estimating the field failure rate of software,"A realistic estimate of the field failure rate of software is essential in order to decide when to release the software while maintaining an appropriate balance between reliability, time-to-market and development cost. Typically, software reliability models are applied to system test data with the hope of obtaining an estimate of the software failure rate that will be observed in the field. Unfortunately, test environments are usually quite different from field environments. In this paper, we use a calibration factor to characterize the mismatch between the system test environment and the field environment, and then incorporate the factor into a widely used software reliability model. For projects that have both system test data and field data for one or more previous releases, the calibration factor can be empirically evaluated and used to estimate the field failure rate of a new release based on its system test data. For new projects, the calibration factor can be estimated by matching the software to related projects that have both system test data and field data. In practice, isolating and removing a software fault is a complicated process. As a result, a fault may be encountered more than once before it is ultimately removed. Most software reliability growth models assume instantaneous fault removal. We relax this assumption by relating non-zero fault removal times to imperfect debugging. Finally, we distinguish between two types of faults based on whether their observed occurrence would precipitate a fix in the current or future release, respectively. Type-F faults, which are fixed in the current release, contribute to a growth component of the overall failure rate. Type-D faults, whose fix is deferred to a subsequent release, contribute to a constant component of the overall software failure rate. The aggregate software failure rate is thus the sum of a decreasing failure rate and a constant failure rate.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=989487,no,undetermined,0
Using simulation to facilitate effective workflow adaptation,"In order to support realistic real-world processes, workflow systems need to be able to adapt to changes. Detecting the need to change and deciding what changes to carry out are very difficult. Simulation analysis can play an important role in this. It can be used in tuning quality of service metrics and exploring ""what-if"" questions. Before a change is actually made, its possible effects can be explored with simulation. To facilitate rapid feedback, the workflow system (METEOR) and simulation system (JSIM) need to interoperate. In particular, workflow specification documents need to be translated into simulation model specification documents so that the new model can be executed/animated on-the-fly. Fortunately, modern Web technology (e.g., XML, DTD, XSLT) make this relatively straightforward. The utility of using simulation in adapting a workflow is illustrated with an example from a genome workflow.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1000151,no,undetermined,0
Assessing multi-version systems through fault injection,"Multi-version design (MVD) has been proposed as a method for increasing the dependability, of critical systems beyond current levels. However, a major obstacle to large-scale commercial usage of this approach is the lack of quantitative characterizations available. We seek to help answer this problem using fault injection. This approach has the potential for yielding highly useful metrics with regard to MVD systems, as well as giving developers a greater insight into the behaviour of each channel within the system. In this research, we develop an automatic fault injection system for multi-version systems called FITMVS. We use this si,stem to test a multi-version system, and then analyze the results produced. We conclude that this approach can yield useful metrics, including metrics related to channel sensitivity, code scope sensitivity, and the likelihood of common-mode failure occurring within a system",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1000042,no,undetermined,0
Real-time software based MPEG-4 video encoder,"Rapid improvements in general-purpose processors are making software-based video encoding solutions increasingly feasible. Software encoders for H.263 and MPEG-2 video standards are well documented, reporting close to real-time performance. However, MPEG-4 video, due to its higher complexity, requires more computational power, making its real-time encoding speed rather infeasible. Design of a fully standard-compliant MPEG-4 encoder with real-time speed on a PC entails optimizations at all levels. This includes designing efficient encoding algorithms, software implementation with efficient data structures, and enhancing computation speed by all possible methods such as taking advantage of the machine architecture. We report a software-based real-time MPEG-4 video encoder on a single-processor PC, without any frame skipping, profile simplifying tricks, or quality loss compromise. The encoder is a quintessence of a number of novel algorithms. Specifically, we have designed a fast motion estimation algorithm. We have also designed an algorithm for the detection of all-zero quantized blocks, which reduces the complexity of the DCT and quantization. To enhance the computation speed, we harness Intel's MMX technology to implement these algorithms in an SIMD (single instruction stream, multiple data stream) fashion within the same processor. On the 800 MHz Pentium III, our encoder yields up to 70 frames per second for CIF resolution video, with the similar picture quality as the reference VM software",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=996451,no,undetermined,0
A simulation based approach for estimating the reliability of distributed real-time systems,"Designers of safety-critical real-time systems are often mandated by requirements on reliability as well as timing guarantees. For guaranteeing timing properties, the standard practice is to use various analysis techniques provided by hard real-time scheduling theory. The paper presents analysis based on simulation, that considers the effects of faults and timing parameter variations on schedulability analysis, and its impact on the reliability estimation of the system. We look at a wider set of scenarios than just the worst case considered in hard real-time schedulability analysis. The ideas have general applicability, but the method has been developed with modelling the effects of external interferences on the controller area network (CAN) in mind. We illustrate the method by showing that a CAN interconnected distributed system, subjected to external interference, may be proven to satisfy its timing requirements with a sufficiently high probability, even in cases when the worst-case analysis has deemed it non-schedulable.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=996375,no,undetermined,0
Experiences with EtheReal: a fault-tolerant real-time Ethernet switch,"We present our experiences with the implementation of a real-time Ethernet switch called EtheReal. EtheReal provides three innovations for real-time traffic over switched Ethernet networks. First, EtheReal delivers connection oriented hard bandwidth guarantees without requiring any changes to the end host operating system and network hardware/software. For ease of deployment by commercial vendors, EtheReal is implemented in software over Ethernet switches, with no special hardware requirements. QoS support is contained within two modules, switches and end-host user level libraries that expose a socket like API to real time applications. Secondly, EtheReal provides automatic fault detection and recovery mechanisms that operate within the constraints of a real-time network. Finally EtheReal supports server-side push applications with a guaranteed bandwidth link-layer multicast scheme. Performance results from the implementation show that EtheReal switches deliver bandwidth guarantees to real time-applications within 0.6% of the contracted value, even in the presence of interfering best-effort traffic between the same pair of communicating hosts.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=996368,no,undetermined,0
Evaluating low-cost fault-tolerance mechanism for microprocessors on multimedia applications,"We evaluate a low-cost fault-tolerance mechanism for microprocessors, which can detect and recover from transient faults, using multimedia applications. There are two driving forces to study fault-tolerance techniques for microprocessors. One is deep submicron fabrication technologies. Future semiconductor technologies could become more susceptible to alpha particles and other cosmic radiation. The other is the increasing popularity of mobile platforms. Recently cell phones have been used for applications which are critical to our financial security, such as flight ticket reservation, mobile banking, and mobile trading. In such applications, it is expected that computer systems will always work correctly. From these observations, we propose a mechanism which is based on an instruction reissue technique for incorrect data speculation recovery which utilizes time redundancy. Unfortunately, we found significant performance loss when we evaluated the proposal using the SPEC2000 benchmark suite. We evaluate it using MediaBench which contains more practical mobile applications than SPEC2000",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=992702,no,undetermined,0
Practical automated filter generation to explicitly enforce implicit input assumptions,"Vulnerabilities in distributed applications are being uncovered and exploited faster than software engineers can, patch the security holes. All too often these weaknesses result from implicit assumptions made by an application about its inputs. One approach to defending against their exploitation is to interpose a filter between the input source and the application that verifies that the application's assumptions about its inputs actually hold. However, ad hoc design of such filters is nearly as tedious and error-prone as patching the original application itself. We have automated the filter generation process based on a simple formal description of a broad class of assumptions about the inputs to an application. Focusing on the back-end server application case, we have prototyped an easy-to-use tool that generates server-side filtering scripts. These can then be quickly installed on a front-end webs server (either in concert with the application or., when a vulnerability is uncovered), thus shielding the server application from a variety of existing and exploited, attacks, as solutions requiring changes to the applications are developed and tested. Our measurements suggest that input filtering can be done efficiently and should not be a performance concern for moderately loaded web servers. The overall approach may be generalizable to other domains, such as firewall filter generation and API wrapper filter generation.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=991551,no,undetermined,0
Using variable-MHz microprocessors to efficiently handle uncertainty in real-time systems,"Guaranteed performance is critical in real-time systems because correct operation requires tasks complete on time. Meanwhile, as software complexity increases and deadlines tighten, embedded processors inherit high-performance techniques such as pipelining, caches, and branch prediction. Guaranteeing the performance of complex pipelines is difficult and worst-case analysis often under-estimates the microarchitecture for correctness. Ultimately, the designer must turn to clock frequency as a reliable source of performance. The chosen processor has a higher frequency than is needed most of the time, to compensate for uncertain hardware enhancements-partly defeating their intended purpose. We propose using microarchitecture simulation to produce accurate but not guaranteed-correct worst-case performance bounds. The primary clock frequency is chosen based on simulated-worst-case performance. Since static analysis cannot confirm simulated-worst-case bounds, the microarchitecture is also backed up by clock frequency reserves. When running a task, the processor periodically checks for interim microarchitecture performance failures. These are expected to be rare, but frequency reserves are available to guarantee the final deadline is met in spite of interim failures. Experiments demonstrate significant frequency reductions, e.g., -100 MHz for a peak 300 MHz processor. The more conservative worst-case analysis is, the larger the frequency reduction. The shorter the deadline, the larger the frequency reduction. And reserve frequency is generally no worse than the high frequency produced by conventional worst-case analysis, i.e., the system degrades gracefully in the presence of transient performance faults.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=991103,no,undetermined,0
Quality-assuring scheduling-using stochastic behavior to improve resource utilization,"We present a unified model for admission and scheduling, applicable for various active resources such as CPU or disk to assure a requested quality in situations of temporary overload. The model allows us to predict and control the behavior of applications based on given quality requirements. It uses the variations in the execution time, i.e., the time any active resource is needed We split resource requirements into a mandatory part which must be available and an optional part which should be available as often as possible but at least with a certain percentage. In combination with a given distribution for the execution time we can move away from worst-case reservations and drastically reduce the amount of reserved resources for applications which can tolerate occasional deadline misses. This increases the number of admittable applications. For example, with negligible loss of quality our system can admit more than two times the disk bandwidth than a system based on the worst-case. Finally, we validated the predictions of our model by measurements using a prototype real-time system and observed a high accuracy between predicted and measured values.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=990603,no,undetermined,0
Dimension recognition and geometry reconstruction in vectorization of engineering drawings,"This paper presents a novel approach for recognizing and interpreting dimensions in engineering drawings. It starts by detecting potential dimension frames, each comprising only the line and text components of a dimension, then verifies them by detecting the dimension symbols. By removing the prerequisite of symbol recognition from detection of dimension sets, our method is capable of handling low quality drawings. We also propose a reconstruction algorithm for rebuilding the drawing entities based on the recognized dimension annotations. A coordinate grid structure is introduced to represent and analyze two-dimensional spatial constraints between entities; this simplifies and unifies the process of rectifying deviations of entity dimensions induced during scanning and vectorization.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=990545,no,undetermined,0
A P1500 compliant BIST-based approach to embedded RAM diagnosis,"This paper deals with the diagnosis of faulty embedded RAMs and outlines the solution which is currently under evaluation within STMicroelectronics. The proposed solution exploits a BIST module implementing a March algorithm, defines a wrapper allowing its interface with a TAP controller, and describes a diagnostic procedure running in the external ATE software environment. The approach allows one to test multiple modules in the same chip through a single TAP interface and is compliant with the proposed P1500 standard for Embedded Core Test. Some preliminary experimental results gathered using a sample circuit are reported, showing the effectiveness of the proposed solution in terms of area and time requirements",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=990266,no,undetermined,0
The case for resilient overlay networks,"This paper makes the case for Resilient Overlay Networks (RONs), an application-level routing and packet forwarding service that gives end-hosts and applications the ability to take advantage of network paths that traditional Internet routing cannot make use of, thereby improving their end-to-end reliability and performance. Using RON, nodes participating in a distributed Internet application configure themselves into an overlay network and cooperatively forward packets for each other. Each RON node monitors the quality of the links in the underlying Internet and propagates this information to the other nodes; this enables a RON to detect and react to path failures within several seconds rather than several minutes, and allows it to select application-specific paths based on performance. We argue that RON has the potential to substantially improve the resilience of distributed Internet applications to path outages and sustained overload.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=990076,no,undetermined,0
Self-tuned remote execution for pervasive computing,"Pervasive computing creates environments saturated with computing and communication capability, yet gracefully integrated with human users. Remote execution has a natural role to play, in such environments, since it lets applications simultaneously leverage the mobility of small devices and the greater resources of large devices. In this paper, we describe Spectra, a remote execution system designed for pervasive environments. Spectra monitors resources such as battery, energy and file cache state which are especially important for mobile clients. It also dynamically balances energy use and quality goals with traditional performance concerns to decide where to locate functionality. Finally, Spectra is self-tuning-it does not require applications to explicitly specify intended resource usage. Instead, it monitors application behavior, learns functions predicting their resource usage, and uses the information to anticipate future behavior.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=990062,no,undetermined,0
Using abstraction to improve fault tolerance,"Software errors are a major cause of outages and they are increasingly exploited in malicious attacks. Byzantine fault tolerance allows replicated systems to mask some software errors but it is expensive to deploy. The paper describes a replication technique, BFTA, which uses abstraction to reduce the cost of Byzantine fault tolerance and to improve its ability to mask software errors. BFTA reduces cost because it enables reuse of off-the-shelf service implementations. It improves availability because each replica can be repaired periodically using an abstract view of the state stored by correct replicas, and because each replica can run distinct or non-deterministic service implementations, which reduces the probability of common mode failures. We built an NFS service that allows each replica to run a different operating system. This example suggests that BFTA can be used in practice; the replicated file system required only a modest amount of new code, and preliminary performance results indicate that it performs comparably to the off-the-shelf implementations that it wraps.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=990057,no,undetermined,0
QUIM: a framework for quantifying usability metrics in software quality models,"The paper examines current approaches to usability metrics and proposes a new approach for quantifying software quality in use, based on modelling the dynamic relationships of the attributes that affect software usability. The Quality in Use Integrated Map (QUIM) is proposed for specifying and identifying quality in use components, which brings together different factors, criteria, metrics and data defined in different human computer interface and software engineering models. The Graphical Dynamic Quality Assessment (GDQA) model is used to analyse interaction of these components into a systematic structure. The paper first introduces a new classification scheme into a graphical logic based framework using QUIM components (factors, criteria metrics and data) to assess quality in use of interactive systems. Then we illustrate how QUIM and GDQA may be used to assess software usability using subjective measures of quality characteristics as defined in ISO/IEC 9126",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=990036,no,undetermined,0
Assurance of conceptual data model quality based on early measures,"The increasing demand for quality information systems (IS), has become quality the most pressing challenge facing IS development organisations. In the IS development field it is generally accepted that the quality of an IS is highly dependent on decisions made early in its development. Given the relevant role that data itself plays in an IS, conceptual data models are a key artifact of the IS design: Therefore, in order to build ""better quality "" IS it is necessary to assess and to improve the quality of conceptual data models based on quantitative criteria. It is in this context where software measurement can help IS designers to make better decision during design activities. We focus this work on the empirical validation of the metrics proposed by Genero et al. for measuring the structural complexity of entity relationship diagrams (ERDs). Through a controlled experiment we will demonstrate that these metrics seem to be heavily correlated with three of the sub-factors that characterise the maintainability of an ERD, such as understandability, analysability and modifiability",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=990007,no,undetermined,0
An analysis of software correctness prediction methods,"Reliability is one of the most important aspects of software systems of any kind. Software development is a complex and complicated process in which software faults are inserted into the code by mistakes during the development process or maintenance. It has been shown that the pattern of the faults insertion phenomena is related to measurable attributes of the software. We introduce some methods for reliability prediction based on software metrics, present the results of using these methods in the particular industrial software environment for which we have a large database of modules in C language. Finally we compare the results and methods and give some directions and ideas for future work",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=989999,no,undetermined,0
Diagnostic Maintenance: A Technique Using a Computer,"A possible technique and the attending software needs for advanced computer controlled checkout equipment, based on transfer properties, are presented in this paper. By employing the transfer function, single measurements can be used to designate the status of relatively complex entities and equipment descriptive matrices can be readily formed. In many cases, such an analysis allows for deeper piece-part fault localization than would be possible from test terminals alone. Matrix manipulations are used to perform tests, localize faults and predict performance. A hierarchical listing of matrices is used to give the computer a description of the unit under test. In addition, the problem of providing a practical method of solving problems under access points restrictions has also been taken into account. Based on these restrictions and on the proposed mathematical tools, a program flow chart is shown to outline these functions. Table-updating (learning) features are incorporated into the program. Essentially, the program consists of a man-written executive program and subroutines which are employed by a computer to derive the actual checkout program used by the checkout computer. This goal is accomplished by applying, under computer control, specific parameters to general routines. The stimuli selection is accomplished by the computer by cross-referencing given restrictions against circuit type and functional data. In formulating the checkout technique, both man-performed and machine-performed checkout processes were examined in order to combine the desirable features of each into an optimum technique.",1963,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4319462,no,undetermined,0

Document Title,Year,Abstract,PDF Link,label
Can Cohesion Predict Fault Density?,2006,"<div style=""font-variant: small-caps; font-size: .9em;"">First Page of the Article</div><img class=""img-abs-container"" style=""width: 95%; border: 1px solid #808080;"" src=""/xploreAssets/images/absImages/01618458.png"" border=""0"">",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1618458,yes
Empirical Analysis of Object-Oriented Design Metrics for Predicting High and Low Severity Faults,2006,"In the last decade, empirical studies on object-oriented design metrics have shown some of them to be useful for predicting the fault-proneness of classes in object-oriented software systems. This research did not, however, distinguish among faults according to the severity of impact. It would be valuable to know how object-oriented design metrics and class fault-proneness are related when fault severity is taken into account. In this paper, we use logistic regression and machine learning methods to empirically investigate the usefulness of object-oriented design metrics, specifically, a subset of the Chidamber and Kemerer suite, in predicting fault-proneness when taking fault severity into account. Our results, based on a public domain NASA data set, indicate that 1) most of these design metrics are statistically related to fault-proneness of classes across fault severity, and 2) the prediction capabilities of the investigated metrics greatly depend on the severity of faults. More specifically, these design metrics are able to predict low severity faults in fault-prone classes better than high severity faults in fault-prone classes",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1717471,yes
Using Historical In-Process and Product Metrics for Early Estimation of Software Failures,2006,"The benefits that a software organization obtains from estimates of product quality are dependent upon how early in the product cycle that these estimates are available. Early estimation of software quality can help organizations make informed decisions about corrective actions. To provide such early estimates we present an empirical case study of two large scale commercial operating systems, Windows XP and Windows Server 2003. In particular, we leverage various historical in-process and product metrics from Windows XP binaries to create statistical predictors to estimate the post-release failures/failure-proneness of Windows Server 2003 binaries. These models estimate the failures and failure-proneness of Windows Server 2003 binaries at statistically significant levels. Our study is unique in showing that historical predictors for a software product line can be useful, even at the very large scale of the Windows operating system",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021972,yes
Rate-distortion performance of H.264/AVC compared to state-of-the-art video codecs,2006,"In the domain of digital video coding, new technologies and solutions are emerging in a fast pace, targeting the needs of the evolving multimedia landscape. One of the questions that arises is how to assess these different video coding technologies in terms of compression efficiency. In this paper, several compression schemes are compared by means of peak signal-to-noise ratio (PSNR) and just noticeable difference (JND). The codecs examined are XviD 0.9.1 (conform to the MPEG-4 Visual Simple Profile), DivX 5.1 (implementing the MPEG-4 Visual Advanced Simple Profile), Windows Media Video 9, MC-EZBC and H.264/AVC AHM 2.0 (version JM 6.1 of the reference software, extended with rate control). The latter plays a key role in this comparison because the H.264/AVC standard can be considered as the de facto benchmark in the field of digital video coding. The obtained results show that H.264/AVC AHM 2.0 outperforms current proprietary and standards-based implementations in almost all cases. Another observation is that the choice of a particular quality metric can influence general statements about the relation between the different codecs.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1564130,no
A new hybrid fault detection technique for systems-on-a-chip,2006,"Hardening SoCs against transient faults requires new techniques able to combine high fault detection capabilities with the usual requirements of SoC design flow, e.g., reduced design-time, low area overhead, and reduced (or ) accessibility to source core descriptions. This paper proposes a new hybrid approach which combines hardening software transformations with the introduction of an Infrastructure IP with reduced memory and performance overheads. The proposed approach targets faults affecting the memory elements storing both the code and the data, independently of their location (inside or outside the processor). Extensive experimental results, including comparisons with previous approaches, are reported, which allow practically evaluating the characteristics of the method in terms of fault detection capabilities and area, memory, and performance overheads.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1566579,no
Measurement Framework for Assessing Risks in Component-Based Software Development,2006,"As Component-based software development (CBSD) is getting popular and is being considered as both efficient and effective approach to build large software applications and systems, potential risks within component-based practicing areas such as quality assessment, complexity estimation, performance prediction, configuration, and application management should not be taken lightly. In the existing literature there is lack of systematic work in identifying and assessing these risks. In particular, there is lack of a structuring framework that could be helpful for related CBSD stakeholders to measure these risks. In this research we examine prior related research work in software measurement and aim to develop a practical risk measurement framework that classifies potential CBSD risks and related metrics and provides a practical guidance for CBSD stakeholders.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1579763,no
CrossTalk: cross-layer decision support based on global knowledge,2006,"The dynamic nature of ad hoc networks makes system design a challenging task. Mobile ad hoc networks suffer from severe performance problems due to the shared, interference-prone, and unreliable medium. Routes can be unstable due to mobility and energy can be a limiting factor for typical devices such as PDAs, mobile phones, and sensor nodes. In such environments cross-layer architectures are a promising new approach, as they can adapt protocol behavior to changing networking conditions. This article introduces CrossTalk, a cross-layer architecture that aims at achieving global objectives with local behavior. It further compares CrossTalk with other cross-layer architectures proposed. Finally, it analyzes the quality of the information provided by the architecture and presents a reference application to demonstrate the effectiveness of the general approach.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1580938,no
Static code analysis of functional descriptions in SystemC,2006,"The co-design of hardware and software systems with object oriented design languages like SystemC has become very popular Static analysis of those descriptions allows to conduct the design process with metrics regarding quality of the code as well as with estimations of the properties of the final design. This paper shows the utilization of software metrics and the computation of high level metrics for SystemC, whose generation is embedded into a complete design methodology. The performance of this analysis process is demonstrated on a UMTS cell searching unit",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1581220,no
A new insight into postsurgical objective voice quality evaluation: application to thyroplastic medialization,2006,"This paper aims at providing new objective parameters and plots, easily understandable and usable by clinicians and logopaedicians, in order to assess voice quality recovering after vocal fold surgery. The proposed software tool performs presurgical and postsurgical comparison of main voice characteristics (fundamental frequency, noise, formants) by means of robust analysis tools, specifically devoted to deal with highly degraded speech signals as those under study. Specifically, we address the problem of quantifying voice quality, before and after medialization thyroplasty, for patients affected by glottis incompetence. Functional evaluation after thyroplastic medialization is commonly based on several approaches: videolaryngostroboscopy (VLS), for morphological aspects evaluation, GRBAS scale and Voice Handicap Index (VHI), relative to perceptive and subjective voice analysis respectively, and Multi-Dimensional Voice Program (MDVP), that provides objective acoustic parameters. While GRBAS has the drawback to entirely rely on perceptive evaluation of trained professionals, MDVP often fails in performing analysis of highly degraded signals, thus preventing from presurgical/postsurgical comparison in such cases. On the contrary, the new tool, being capable to deal with severely corrupted signals, always allows a complete objective analysis. The new parameters are compared to scores obtained with the GRBAS scale and to some MDVP parameters, suitably modified, showing good correlation with them. Hence, the new tool could successfully replace or integrate existing ones. With the proposed approach, deeper insight into voice recovering and its possible changes after surgery can thus be obtained and easily evaluated by the clinician.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1597494,no
Telephony-based voice pathology assessment using automated speech analysis,2006,"A system for remotely detecting vocal fold pathologies using telephone-quality speech is presented. The system uses a linear classifier, processing measurements of pitch perturbation, amplitude perturbation and harmonic-to-noise ratio derived from digitized speech recordings. Voice recordings from the Disordered Voice Database Model 4337 system were used to develop and validate the system. Results show that while a sustained phonation, recorded in a controlled environment, can be classified as normal or pathologic with accuracy of 89.1%, telephone-quality speech can be classified as normal or pathologic with an accuracy of 74.2%, using the same scheme. Amplitude perturbation features prove most robust for telephone-quality speech. The pathologic recordings were then subcategorized into four groups, comprising normal, neuromuscular pathologic, physical pathologic and mixed (neuromuscular with physical) pathologic. A separate classifier was developed for classifying the normal group from each pathologic subcategory. Results show that neuromuscular disorders could be detected remotely with an accuracy of 87%, physical abnormalities with an accuracy of 78% and mixed pathology voice with an accuracy of 61%. This study highlights the real possibility for remote detection and diagnosis of voice pathology.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1597497,no
Motion estimation and temporal up-conversion on the TM3270 media-processor,2006,"We present a qualitative performance evaluation of several components of a video format conversion algorithm (referred to as natural motion (NM)). The implementation platform is a new programmable media-processor, the TM3270, combined with dedicated hardware support. The performance of two compute-intense NM components, motion estimation (ME) and temporal up-conversion (TU), is evaluated. The impact of new TM3270 features, such as new video-processing operations and data prefetching, is quantified. We show that a real-time implementation of the ME and TU algorithms is achievable in a fraction of the available compute performance, when operating on standard definition video",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1598437,no
Optimal project feature weights in analogy-based cost estimation: improvement and limitations,2006,"Cost estimation is a vital task in most important software project decisions such as resource allocation and bidding. Analogy-based cost estimation is particularly transparent, as it relies on historical information from similar past projects, whereby similarities are determined by comparing the projects' key attributes and features. However, one crucial aspect of the analogy-based method is not yet fully accounted for: the different impact or weighting of a project's various features. Current approaches either try to find the dominant features or require experts to weight the features. Neither of these yields optimal estimation performance. Therefore, we propose to allocate separate weights to each project feature and to find the optimal weights by extensive search. We test this approach on several real-world data sets and measure the improvements with commonly used quality metrics. We find that this method 1) increases estimation accuracy and reliability, 2) reduces the model's volatility and, thus, is likely to increase its acceptance in practice, and 3) indicates upper limits for analogy-based estimation quality as measured by standard metrics.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1599418,no
Semi-Automatic Business Process Performance Optimization Based On Redundant Control Flow Detection,2006,"Composite web services are often defined in terms of business processes. Therefore, their performance depends on the effectiveness of the said processes. This paper describes an approach allowing for business process performance optimization that eventually leads to improved composite web service performance. The approach is based on the discovery of the redundant control flow. It is argued that such is the flow which is not backed up by data flow. After detection, redundant control flow is replaced with nonredundant.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1602279,no
Evaluating architectural stability using a metric-based approach,2006,Architectural stability refers to the extent software architecture is flexible to endure evolutionary changes while leaving the architecture intact. Approaches to evaluate software architectures for stability can be retrospective or predictive. Retrospective evaluation looks at successive releases of a software system to analyze how smoothly the evolution has taken place. Predictive evaluation examines a set of likely changes and shows the architecture can endure these changes. This paper proposes a metric-based approach to evaluate architectural stability of a software system by combining these two traditional analysis techniques. Such an approach performs on the fact bases extracted from the source code by reverse engineering techniques. We also present experimental results by applying the proposed approach to analyze the architectural stability across different versions of two spreadsheet systems,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1602377,no
Non-intrusive monitoring of software quality,2006,"Measurement based software process improvement needs a non-intrusive approach to determine what and where improvement is needed without knowing anything about the methods and techniques used during project execution. Beside, it is necessary for obtaining successful business management, an accurate process behavior prediction. In order to obtain these results we proposed to use statistical process control (SPC) tailored to the software process point of view. The paper proposes an appropriate SPC-Framework and presents two industrial experiences in order to validate the framework in two different software contexts: recalibration of effort estimation models; monitoring of the primary processes through the supporting ones. These experiences validate the framework and show how it can be successfully used as a decision support tool in software process improvement",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1602387,no
Software test cases: is one ever enough?,2006,"In this paper, software testing theory was examined as it pertains to one test at a time. In doing so, the author hopes to highlight some useful facts about testing theory that are somewhat obvious but often overlooked. Some precise statements about how bad the one-test policy can be were also made",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1603473,no
Study for Classification of Quality Attributes in Argentinean ECommerce Sites,2006,"This work starts with results obtained in [4] where quality attributes to be considered in operative web projects were analyzed and evaluated, mainly for the ""Functionality and Content"" feature of sites and applications of electronic commerce (e-commerce). The evaluation process applied to sites from the studies of 2000 and 2004 [2,4] produced a concentrated binary matrix, which has been processed in previous works [3] with multivariate approaches that in turn have provided useful information that allowed the identification of a precise relationship between attributes. Aiming to obtain a group of quality attributes where it is possible to identify inter-relations or common features between the variables that compose each class, the binary table from the previous study has been revisited, and has been processed with statistical cluster analysis. Ultimately, hypothesis tests are proposed to infer the behavior of the population of Argentinean e-commerce sites.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1604743,no
Mission dependability modeling and evaluation of repairable systems considering maintenance capacity,2006,"The mission dependability of repairable systems not only depends on mission reliability and capacity of the system, but also the maintenance capacity during the whole mission. The probability of mission successful completion is one of the important performance measures. For the complex mission that has many sub-missions of kinds, its success probability is associated with the ready and execution duration, maintenance conditions in the working field and success requirements of each sub-mission. Maintenance conditions in the sub-mission working field mainly include replacement and repair of the failed components. According to these different maintenance conditions, we classify all sub-mission into three classes. By analyzing the state transition during the ready period and execution period of each sub-mission, this paper presents a dependability model to evaluate the probability of mixed multi-mission success of repairable systems considering maintenance capacity. A simple example is provided to show the application of the model",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607382,no
A routing methodology for achieving fault tolerance in direct networks,2006,"Massively parallel computing systems are being built with thousands of nodes. The interconnection network plays a key role for the performance of such systems. However, the high number of components significantly increases the probability of failure. Additionally, failures in the interconnection network may isolate a large fraction of the machine. It is therefore critical to provide an efficient fault-tolerant mechanism to keep the system running, even in the presence of faults. This paper presents a new fault-tolerant routing methodology that does not degrade performance in the absence of faults and tolerates a reasonably large number of faults without disabling any healthy node. In order to avoid faults, for some source-destination pairs, packets are first sent to an intermediate node and then from this node to the destination node. Fully adaptive routing is used along both subpaths. The methodology assumes a static fault model and the use of a checkpoint/restart mechanism. However, there are scenarios where the faults cannot be avoided solely by using an intermediate node. Thus, we also provide some extensions to the methodology. Specifically, we propose disabling adaptive routing and/or using misrouting on a per-packet basis. We also propose the use of more than one intermediate node for some paths. The proposed fault-tolerant routing methodology is extensively evaluated in terms of fault tolerance, complexity, and performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1608003,no
Performance analysis of the FastICA algorithm and CrameÂ´r-rao bounds for linear independent component analysis,2006,"The FastICA or fixed-point algorithm is one of the most successful algorithms for linear independent component analysis (ICA) in terms of accuracy and computational complexity. Two versions of the algorithm are available in literature and software: a one-unit (deflation) algorithm and a symmetric algorithm. The main result of this paper are analytic closed-form expressions that characterize the separating ability of both versions of the algorithm in a local sense, assuming a ""good"" initialization of the algorithms and long data records. Based on the analysis, it is possible to combine the advantages of the symmetric and one-unit version algorithms and predict their performance. To validate the analysis, a simple check of saddle points of the cost function is proposed that allows to find a global minimum of the cost function in almost 100% simulation runs. Second, the CrameÂ´r-Rao lower bound for linear ICA is derived as an algorithm independent limit of the achievable separation quality. The FastICA algorithm is shown to approach this limit in certain scenarios. Extensive computer simulations supporting the theoretical findings are included.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1608537,no
"Electronics at Nanoscale: Fundamental and Practical Challenges, and Emerging Directions",2006,"In electronics, i.e. when using charge transport and change of electromagnetic fields in devices and systems, non-linearity, collective effects, and a hierarchy of design across length and time scales is central to efficient information processing through manipulation and transmission of bits. Silicon-based electronics brings together a systematic interdependent framework that connects software and hardware to reproducibility, speed, power, noise margin, reliability, signal restoration and communication, low defect count, and an ability to do predictive design across the scales. In the limits of nanometer scale, the dominant practical constraints arise from power dissipation in ever smaller volumes and of efficient signal interconnectivity commensurate with the large density of devices. These limitations are tied to the physical basis in charge transport and changes of fields, and equally apply to other materials â€?hard, soft or molecular. At the largest scale, the limitations arise from partitioning and hierarchical apportionment for system performance, ease of design and manufacturing. Power management, behavioral encapsulation, fault tolerance, congestion avoidance, timing, placement, routing, electromagnetic cross-talk, etc. all need to be addressed from the perspective of centimeter scale. We take a hierarchical view of the underlying fundamental and practical challenges of the conventional and unconventional approaches using the analytic framework appropriate to the length scale to distinguish between fact and fantasy, and to point to practical emerging directions with a system-scale perspective.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1609776,no
Agent-based self-healing protection system,2006,"This paper proposes an agent-based paradigm for self-healing protection systems. Numerical relays implemented with intelligent electronic devices are designed as a relay agent to perform a protective relaying function in cooperation with other relay agents. A graph-theory-based expert system, which can be integrated with supervisory control and a data acquisition system, has been developed to divide the power grid into primary and backup protection zones online and all relay agents are assigned to specific zones according to system topological configuration. In order to facilitate a more robust, less vulnerable protection system, predictive and corrective self-healing strategies are implemented as guideline regulations of the relay agent, and the relay agents within the same protection zone communicate and cooperate to detect, locate, and trip fault precisely with primary and backup protection. Performance of the proposed protection system has been simulated with cascading fault, failures in communication and protection units, and compared with a coordinated directional overcurrent protection system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1610669,no
PET reconstruction with system matrix derived from point source measurements,2006,"The quality of images reconstructed by statistical iterative methods depends on an accurate model of the relationship between image space and projection space through the system matrix. The elements of a system matrix on the HiRez scanner from CPS Innovations were acquired by positioning a point source at different positions in the scanner field of view. Then, a whole system matrix was derived by processing the responses in projection space. Such responses included both geometrical and detection physics components of the system matrix. The response was parameterized to correct for point source location, smooth projection noise and the whole system matrix was derived. The model accounts for axial compression (span) used on the scanner. The projection operator for iterative reconstruction was constructed using the estimated response parameters. Computer-generated and acquired data were used to compare reconstruction obtained by the HiRez standard software and that produced by better modeling. Results showed that better resolution and noise property can be achieved with the modeled system matrix.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1610966,no
Software-based transparent and comprehensive control-flow error detection,2006,"Shrinking microprocessor feature size and growing transistor density may increase the soft-error rates to unacceptable levels in the near future. While reliable systems typically employ hardware techniques to address soft-errors, software-based techniques can provide a less expensive and more flexible alternative. This paper presents a control-flow error classification and proposes two new software-based comprehensive control-flow error detection techniques. The new techniques are better than the previous ones in the sense that they detect errors in all the branch-error categories. We implemented the techniques in our dynamic binary translator so that the techniques can be applied to existing x86 binaries transparently. We compared our new techniques with the previous ones and we show that our methods cover more errors while has similar performance overhead.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1611552,no
Algorithmic implementation and efficiency maintenance of real-time environment using low-bitrate wireless communication,2006,"This paper presents different issues of the real-time compression algorithms without compromising the video quality in the distributed environment. The theme of this research is to manage the critical processing stages (speed, information lost, redundancy, distortion) having better encoded ratio, without the fluctuation of quantization scale by using IP configuration. In this paper, different techniques such as distortion measure with searching method cover the block phenomenon with motion estimation process while passing technique and floating measurement is configured by discrete cosine transform (DCT) to reduce computational complexity which is implemented in this video codec. While delay of bits in encoded buffer side especially in real-time state is being controlled to produce the video with high quality and maintenance a low buffering delay. Our results show the performance accuracy gain with better achievement in all the above processes in an encouraging mode",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1611718,no
Influence of adaptive data layouts on performance in dynamically changing storage environments,2006,"For most of today's IT environments, the tremendous need for storage capacity in combination with a required minimum I/O performance has become highly critical. In dynamically growing environments, a storage management solution's underlying data distribution scheme has great impact to the overall system I/O performance. The evaluation of a number of open system storage visualization solutions and volume managers has shown that all of them lack the ability to automatically adapt to changing access patterns and storage infrastructures; many of them require an error prone manual re-layout of the data blocks, or rely on a very time consuming re-striping of all available data. This paper evaluates the performance of conventional data distribution approaches compared to the adaptive virtualization solution V:DRIVE in dynamically changing storage environments. Changes of the storage infrastructure are normally not considered in benchmark results, but can have a significant impact on storage performance. Using synthetic benchmarks, V:DRIVE is compared in such changing environments with the non-adaptive Linux logical volume manager (LVM). The performance results of our tests clearly outline the necessity of adaptive data distribution schemes.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1613268,no
A framework to support run-time assured dynamic reconfiguration for pervasive computing environments,2006,"With the increasing use of pervasive computing environments (PCEs), developing dynamic reconfigurable software in such environments becomes an important issue. The ability to change software components in running systems has advantages such as building adaptive, long-life, and self-reconfigurable software as well as increasing invisibility in PCEs. As dynamic reconfiguration is performed in error-prone wireless mobile systems frequently, it can threaten system safety. Therefore, a mechanism to support assured dynamic reconfiguration at run-time for PCEs is required. In this paper, we propose an assured dynamic reconfiguration framework (ADRF) with emphasis on assurance analysis. The framework is implemented and is available for further research. To evaluate the framework, an abstract case study including reconfigurations has been applied using our own developed simulator for PCEs. Our experience shows that ADRF properly preserves reconfiguration assurance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1613631,no
Will Johnny/Joanie Make a Good Software Engineer? Are Course Grades Showing the Whole Picture?,2006,"Predicting future success of students as software engineers is an open research area. We posit that current grading means do not capture all the information that may predict whether students will become good software engineers. We use one such piece of information, traceability of project artifacts, to illustrate our argument. Traceability has been shown to be an indicator of software project quality in industry. We present the results of a case study of a University of Waterloo graduate-level software engineering course where traceability was examined as well as course grades (such as mid-term, project grade, etc.). We found no correlation between the presence of good traceability and any of the course grades, lending support to our argument",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1617344,no
Software Defect Prediction Using Regression via Classification,2006,"<div style=""font-variant: small-caps; font-size: .9em;"">First Page of the Article</div><img class=""img-abs-container"" style=""width: 95%; border: 1px solid #808080;"" src=""/xploreAssets/images/absImages/01618375.png"" border=""0"">",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1618375,no
Toward trustworthy software systems,2006,"Organizations such as Microsoft's Trusted Computing Group and Sun Microsystems' Liberty Alliance are currently leading the debate on ""trustworthy computing."" However, these and other initiatives primarily focus on security, and trustworthiness depends on many other attributes. To address this problem, the University of Oldenburg's TrustSoft Graduate School aims to provide a holistic view of trustworthiness in software - one that considers system construction, evaluation/analysis, and certification - in an interdisciplinary setting. Component technology is the foundation of our research program. The choice of a component architecture greatly influences the resulting software systems' nonfunctional properties. We are developing new methods for the rigorous design of trustworthy software systems with predictable, provable, and ultimately legally certifiable system properties. We are well aware that it is impossible to build completely error-free complex software systems. We therefore complement fault-prevention and fault-removal techniques with fault-tolerance methods that introduce redundancy and diversity into software systems. Quantifiable attributes such as availability, reliability, and performance call for analytical prediction models, which require empirical studies for calibration and validation. To consider the legal aspects of software certification and liability, TrustSoft integrates the disciplines of computer science and computer law.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1621008,no
Detecting anomaly and failure in Web applications,2006,"Improving Web application quality will require automated evaluation tools. Many such tools are already available either as commercial products or research prototypes. The authors use their automated evaluation tools, ReWeb and TestWeb, for Web analysis and testing that improves Web pages and applications and to find some anomalies and failures in four case studies.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1621033,no
Effects of hardware imperfection on six-port direct digital receivers calibrated with three and four signal standards,2006,"Online calibration is essential for the proper operation of six-port digital receivers in communication systems, as such calibration cancels out receiver ageing and manufacturing defects. Simple calibration methods, using only three or four signal standards (SS), were reported in a previous paper, where these methods were assessed using an ADS software simulation for an ideal six-port circuit. A unified and general theory is presented for examining the effects of hardware imperfection on the performance of a six-port receiver (SPR) calibrated using these simplified techniques. This can be used to establish permissible hardware tolerances for proper operation of SPRs in different digital modulations.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1621518,no
Monitoring Computer Interactions to Detect Early Cognitive Impairment in Elders,2006,Maintaining cognitive performance is a key factor influencing elders' ability to live independently with a high quality of life. We have been developing unobtrusive measures to monitor cognitive performance and potentially predict decline using information from routine computer interactions in the home. Early detection of cognitive decline offers the potential for intervention at a point when it is likely to be more successful. This paper describes recommendations for the conduct of studies monitoring cognitive function based on routine computer interactions in elders' home environments,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1624801,no
Reliability forecasting in complex hardware/software systems,2006,"We describe a class of models used to evaluate and forecast the reliability of complex hardware/software systems. We assume the system may be described through a reliability block diagram. Blocks referring to hardware components are dealt with 'pending' continuous time Markov chain models, whereas blocks referring to software components are dealt with 'pending' software reliability growth models. Inference and forecasting tasks with such models are described.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1625324,no
A methodological framework for conceptual modeling of optical networks,2006,"We present a methodological framework for conceptual modeling of optical networks. The framework is targeted towards a larger scale initiative by us for designing a unified modeling language (UML) profile that can help with conceptual modeling of typical optical network design problems. While there are pieces of literature available that describe profiles for other application areas, there are no profiles for use in networking problems, in general. Our work is an important step forward to address this deficiency. Our proposed profile takes the existing UML notations and extends them to support the modeling of optical networks",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1625548,no
An efficient low complexity encoder for MPEG advanced audio coding,2006,"MPEG advanced audio coding (AAC) is the current state of the art in perceptual audio coding technology standardized by MPEG. Nevertheless the real-time constraint of reference software implementation for the AAC encoder provided by MPEG leads to a heavy computational bottleneck on today's portable devices. In this paper we review briefly the algorithms specified in the MPEG AAC standard and analyze the algorithmic performance. Then we carry out algorithm level optimizations in order to get a low complexity as well as high quality audio encoder that can be easily ported onto multiple platforms. In psychoacoustic model, a MDCT-based psychoacoustic analysis and block switching algorithm in time domain and a tonality detection measure are presented. Moreover, we employ improved pre-processing and adjustment of scale factor in quantization module. Experimental results show the optimized encoder can save about half of overall computational complexity. According to subject evaluation result, the perceptual audio quality of improved encoder is better than the AAC LC reference encoder",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1625874,no
Design of a smart sensor for load insulation failure detection in DC supply system of UAV,2006,"This paper introduces an embedded smart sensor in the DC supply system of an unmanned aerial vehicle (UAV) based on AT89C2051, which can measure load insulation failure in direct current supply system. The measuring principles are analyzed first in this paper and the system design schemes of software and hardware are given out as well. Finally, the measure error has been analyzed and counted. This sensor possesses characteristics of low-cost, low-power consumption, high measurement precision, and high reliability. Practical experiments show that the sensor system, of which the measurement precision achieves 0.01mA, could meet the demands of detection accuracy, achieve the expected criteria of design, and could be spread",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1627541,no
SHARP: a new real-time scheduling algorithm to improve security of parallel applications on heterogeneous clusters,2006,"This paper addresses the problem of improving quality of security for real-time parallel applications on heterogeneous clusters. We propose a new security- and heterogeneity-driven scheduling algorithm (SHARP for short), which strives to maximize the probability that parallel applications are executed in time without any risk of being attacked. Because of high security overhead in existing clusters, an important step in scheduling is to guarantee jobs' security requirements while minimizing overall execution times. The SHARP algorithm accounts for security constraints in addition to different processing capabilities of each node in a cluster. We introduce two novel performance metrics, degree of security deficiency and risk-free probability, to quantitatively measure quality of security provided by a heterogeneous cluster. Both security and performance of SHARP are compared with two well-known scheduling algorithms. Extensive experimental studies using real-world traces confirm that the proposed SHARP algorithm significantly improves security and performance of parallel applications on heterogeneous clusters",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1629388,no
A QoS-negotiable middleware system for reliably multicasting messages of arbitrary size,2006,"E-business organizations commonly trade services together with quality of service (QoS) guarantees that are often dynamically agreed upon prior to service provisioning. Violating agreed QoS levels incurs penalties and hence service providers agree to QoS requests only after assessing the resource availability. Thus the system should, in addition to providing the services: (i) monitor resource availability, (ii) assess the affordability of a requested QoS level, and (iii) adapt autonomically to QoS perturbations which might undermine any assumptions made during assessment. This paper will focus on building such a system for reliably multicasting messages of arbitrary size over a loss-prone network of arbitrary topology such as the Internet. The QoS metrics of interest will be reliability, latency and relative latency. We meet the objectives (i)-(iii) by describing a network monitoring scheme, developing two multicast protocols, and by analytically estimating the achievable latencies and reliability in terms of controllable protocol parameters. Protocol development involves extending in two distinct ways an existing QoS-adaptive protocol designed for a single packet. Analytical estimation makes use of experimentally justified approximations and their impact is evaluated through simulations. As the protocol extension approaches are complementary in nature, so are the application contexts they are found best suited to; e.g., one is suited to small messages while the other to large messages",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1630487,no
Exploit failure prediction for adaptive fault-tolerance in cluster computing,2006,"As the scale of cluster computing grows, it is becoming hard for long-running applications to complete without facing failures on large-scale clusters. To address this issue, checkpointing/restart is widely used to provide the basic fault-tolerant functionality, yet it suffers from high overhead and its reactive characteristic. In this work, we propose FT-Pro, an adaptive fault management mechanism that optimally chooses migration, checkpointing or no action to reduce the application execution time in the presence of failures based on the failure prediction. A cost-based evaluation model is presented for dynamic decision at run-time. Using the actual failure log from a production cluster at NCSA, we demonstrate that even with modest failure prediction accuracy, FT-Pro outperforms the traditional checkpointing/restart strategy by 13%-30% in terms of reducing the application execution time despite failures, which is a significant performance improvement for long-running applications.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1630866,no
Byzantine Anomaly Testing for Charm++: Providing Fault Tolerance and Survivability for Charm++ Empowered Clusters,2006,"Recently shifts in high-performance computing have increased the use of clusters built around cheap commodity processors. A typical cluster consists of individual nodes, containing one or several processors, connected together with a high-bandwidth, low-latency interconnect. There are many benefits to using clusters for computation, but also some drawbacks, including a tendency to exhibit low Mean Time To Failure (MTTF) due to the sheer number of components involved. Recently, a number of fault-tolerance techniques have been proposed and developed to mitigate the inherent unreliability of clusters. These techniques, however, fail to address the issue of detecting non-obvious faults, particularly Byzantine faults. At present, effectively detecting Byzantine faults is an open problem. We describe the operation of ByzwATCh, a module for run-time detecting Byzantine hardware errors as part of the Charm++ parallel programming framework",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1630925,no
Survival of the Internet applications: a cluster recovery model,2006,"Internet applications become increasingly widely used for millions of people in the world and on the other hand the accidents or disruptions of service are also dramatically increasing. Accidents or disruptions occur either because of disasters or because of malicious attacks. The disasters could not be completely prevented. Prevention is a necessary but not a sufficient component of disaster. In this case, we have to prepare thoroughly for reducing the recovery time and get the users back to work faster. In this paper, we present a cluster recovery model to increase the survivability level of Internet applications. We construct a state transition model to describe the behaviors of cluster systems. By mapping through recovery actions to this transition model with stochastic process, we capture system behaviors as well as we get mathematical steady-state solutions of that chain. We first carry out for steady-state behaviors leading to measures like steady-state availability. By transforming this model with the system states we compute a system measure, the mean time to repair (MTTR) and also compute probabilities of cluster systems failures due in face of disruptions. Our model with the recovery actions have several benefits, which include reducing the time to get the users back to work and making recovery performance insensitive to the selection of a failure treatment parameter",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1630928,no
Leveraged Quality Assessment using Information Retrieval Techniques,2006,"The goal of this research is to apply language processing techniques to extend human judgment into situations where obtaining direct human judgment is impractical due to the volume of information that must be considered. On aspect of this is leveraged quality assessments, which can be used to evaluate third-party coded subsystems, to track quality across the versions of a program, to assess the compression effort (and subsequent cost) required to make a change, and to identify parts of a program in need of preventative maintenance. A description of the QALP tool, its output from just under two million lines of code, and an experiment aimed at evaluating the tool's use in leveraged quality assessment are presented. Statistically significant results from this experiment validate the use of the QALP tool in human leverage quality assessment",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1631117,no
A Metric-Based Heuristic Framework to Detect Object-Oriented Design Flaws,2006,"One of the important activities in re-engineering process is detecting design flaws. Such design flaws prevent an efficient maintenance, and further development of a system. This research proposes a novel metric-based heuristic framework to detect and locate object-oriented design flaws from the source code. It is accomplished by evaluating design quality of an object-oriented system through quantifying deviations from good design heuristics and principles. While design flaws can occur at any level, the proposed approach assesses the design quality of internal and external structure of a system at the class level which is the most fundamental level of a system. In a nutshell, design flaws are detected and located systematically in two phases using a generic OO design knowledge-base. In the first phase, hotspots are detected by primitive classifiers via measuring metrics indicating a design feature (e.g. complexity). In the second phase, individual design flaws are detected by composite classifiers using a proper set of metrics. We have chosen JBoss application server as the case study, due to its pure OO large size structure, and its success as an open source J2EE platform among developers",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1631118,no
Towards a Client Driven Characterization of Class Hierarchies,2006,"Object-oriented legacy systems are hard to maintain because they are hard to understand. One of the main understanding problems is revealed by the so-called ""yo-yo effect"" that appears when a developer or maintainer wants to track a polymorphic method call. At least part of this understanding problem is due to the dual nature of the inheritance relation i.e., the fact that it can he used both as a code and/or as an interface reuse mechanism. Unfortunately, in order to find out the original intention for a particular hierarchy it is not enough to look at the hierarchy itself; rather than that, an in-depth analysis of the hierarchy's clients is required. In this paper we introduce a new metrics-based approach that helps us characterize the extent to which a base class was intended for interface reuse, by analyzing how clients use the interface of that base class. The idea of the approach is to quantify the extent to which clients treat uniformly the instances of the descendants of the base class, when invoking methods belonging to this common interface, We have evaluated our approach on two medium-sized case studies and we have found that the approach does indeed help to characterize the nature of a base class with respect to interface reuse. Additionally, the approach can be used to detect some interesting patterns in the way clients actually use the descendants through the interface of the base class",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1631136,no
QoS assessment via stochastic analysis,2006,"Using a stochastic modeling approach based on the Unified Modeling Language and enriched with annotations that conform to the UML profile for schedulability performance, and time, the authors propose a method for assessing quality of service (QoS) in fault-tolerant (FT) distributed systems. From the UML system specification, they produce a generalized stochastic Petri net (GSPN) performance model for assessing an FT application's QoS via stochastic analysis. The ArgoSPE tool provides support for the proposed technique, helping to automatically produce the GSPN model",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1631976,no
Automatic Instruction-Level Software-Only Recovery,2006,"As chip densities and clock rates increase, processors are becoming more susceptible to transient faults that can affect program correctness. Computer architects have typically addressed reliability issues by adding redundant hardware, but these techniques are often too expensive to be used widely. Software-only reliability techniques have shown promise in their ability to protect against soft-errors without any hardware overhead. However, existing low-level software-only fault tolerance techniques have only addressed the problem of detecting faults, leaving recovery largely unaddressed. In this paper, we present the concept, implementation, and evaluation of automatic, instruction-level, software-only recovery techniques, as well as various specific techniques representing different trade-offs between reliability and performance. Our evaluation shows that these techniques fulfill the promises of instruction-level, software-only fault tolerance by offering a wide range of flexible recovery options",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1633498,no
Assessment of the Effect of Memory Page Retirement on System RAS Against Hardware Faults,2006,"The Solaris 10 operating system includes a number of new features for predictive self-healing. One such feature is the ability of the fault management software to diagnose memory errors and drive automatic memory page retirement (MPR), intended to reduce the negative impact of permanent memory faults that generate either correctable or uncorrectable errors on system reliability, availability, and serviceability (RAS). The MPR technique allows memory pages suffering from correctable errors and relocatable clean pages suffering from uncorrectable errors to be removed from use in the virtual memory system without interrupting user applications. It also allows relocatable dirty pages associated with uncorrectable errors to be isolated with limited impact on affected user processes, avoiding an outage for the entire system. This study applies analytical models, with parameters calibrated by field experience, to quantify the reduction that can be made by this operating system self-healing technique on the system interruptions, yearly downtime, and number of services introduced by hardware permanent faults, for typical low-end and mid-range server systems. The results show that significant improvements can be made on these three system RAS metrics by deploying the MPR capability",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1633525,no
"Performance Assurance via Software Rejuvenation: Monitoring, Statistics and Algorithms",2006,"We present three algorithms for detecting the need for software rejuvenation by monitoring the changing values of a customer-affecting performance metric, such as response time. Applying these algorithms can improve the values of this customer-affecting metric by triggering rejuvenation before performance degradation becomes severe. The algorithms differ in the way they gather and use sample values to arrive at a rejuvenation decision. Their effectiveness is evaluated for different sets of control parameters, including sample size, using simulation. The results show that applying the algorithms with suitable choices of control parameters can significantly improve system performance as measured by the response time",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1633532,no
Climbing the commercialization hill the four phase of product development,2006,"The article tried to give some sense of the activities and the amount of work still remaining after the creation of the first working product breadboard. It is estimated that typically only 10-50% of the work has been done by the time this first breadboard is done. The rest of the effort will typically require significant cooperation between employees in each functional area. All of these activities are designed to deliver a product that will allow the organization to manufacture the product repeatedly, with predictable delivery, quality, cost, and performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1634952,no
Smart laser vision sensors simplify inspection,2006,"For all in-process and finished product applications, laser sensors are used in the rubber and tire industry to enhance competitiveness by improving productivity. The basic benefits of using laser sensors for quality control include increasing yield and productivity, increasing quality by providing 100% product inspection, reducing scrap production and rejects, and in-process inspection to detect and correct trends quickly before production of scrap. New developments in laser-based measuring systems can now provide high-speed digital data communications, eliminating the effects of errors from electrical noise and eliminating the need for A/D converters. New smart sensor developments allow application specific analysis software to run inside the sensor, simplifying operation, improving reliability, and reducing cost by eliminating the need for external signal processing hardware.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1634956,no
Industry-oriented software-based system for quality evaluation of vehicle audio environments,2006,"A new set of integrated software tools are proposed for the evaluation of vehicle audio quality for industrial purposes, taking advantage of the auralization approach that allows to simulate the binaural listening experience outside the cockpit. Two main cooperating tools are implemented. The first fulfills the function of acquiring relevant data for system modeling and for canceling the undesired effects of the acquisition chain. The second offers a user-friendly interface for real-time simulation of different car audio systems and the consequent evaluation of both objective and subjective performances. In the latter case, the listening procedure is directly experienced at the PC workplace, leading to a significant simplification of the audio-quality assessing task for comparing the selected systems. Moreover, such kind of subjective evaluation allowed to validate the proposed approach through a complete set of experiments (developed by means of a dedicated software environment) based on appropriate ITU recommendations.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1637827,no
SIP intrusion detection and prevention: recommendations and prototype implementation,2006,"As VoIP deployment are expected to grow, intrusion problems similar to those of which data networks experience will become very critical. In the early stages of deployment, the intrusion and security problems have not been seriously considered, although they could have a negative impact on VoIP deployment. In the paper, SIP intrusion detection and prevention requirements are analyzed and an IDS/IPS architecture is proposed. A prototype of the proposed architecture was implemented using as a basis the very popular open-source software Snort, a network-based intrusion detection and prevention system. The prototype of the proposed architecture extends the basic functionality of Snort, making use of the preprocessing feature that permits analyzing protocols of layers above the TCP/UDP one. The preprocessors block is a very powerful one since it permits to implement both knowledge and behavior based intrusion detection and prevention techniques in Snort that basically adopts a network based technique. An important requirement of an IPS is that legitimate traffic should be forwarded to the recipient with no apparent disruption or delay of service. Hence, the performance of the proposed architecture has been evaluated in terms of impact that its operation has on the QoS experienced by the VoIP users.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1638122,no
A configurable framework for stream programming exploration in baseband applications,2006,"This paper presents a configurable framework to be used for rapid prototyping of stream based languages. The framework is based on a set of design patterns defining the elementary structure of a domain specific language for high-performance signal processing. A stream language prototype for baseband processing has been implemented using the framework. We introduce language constructs to efficiently handle dynamic reconfiguration of distributed processing parameters. It is also demonstrated how new language specific primitive data types and operators can be used to efficiently and machine independently express computations on bitfields and data-parallel vectors. These types and operators yield code that is readable, compact and amenable to a stricter type checking than is common practice. They make it possible for a programmer to explicitly express parallelism to be exploited by a compiler. In short, they provide a programming style that is less error prone and has the potential to lead to more efficient implementations",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1639502,no
Facing the challenges of multicore processor technologies using autonomic system software,2006,"Summary form only given. Multicore processor technologies, which appear to dominate the processor design landscape, require a shift of paradigm in the development of programming models and supporting environments for scientific and engineering applications. System software for multicore processors needs to exploit fine-grain concurrent execution capabilities and cope with deep, non- uniform memory hierarchies. Software adaptation to multicore technologies needs to happen even as hardware platforms change underneath the software. Last but not least, due to the extremely high compute density of chip multiprocessing components, system software needs to increase its energy-awareness and treat energy and temperature distribution as first-class optimization targets. Unfortunately, energy awareness is most often at odds with high performance. In the first part of this talk, the author discusses some of the major challenges of software adaptation to multicore technologies and motivate the use of autonomic, self-optimizing system software, as a vehicle for both high performance portability and energy-efficient program execution. In the second part of the talk, the author presents ongoing research in runtime environments for dense parallel systems built from multicore and SMT components, and focus on two topics, polymorphic multithreading, and power-aware concurrency control with quality-of-service guarantees. In the same context, the author discusses enabling technologies for improved software autonomy via dynamic runtime optimization, including continuous hardware profilers, and online power-efficiency predictors",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1639604,no
Analysis of checksum-based execution schemes for pipelined processors,2006,"The performance requirements for contemporary microprocessors are increasing as rapidly as their number of applications grows. By accelerating the clock, performance can be gained easily but only with high additional power consumption. The electrical potential between logic `0' and `1' is decreased as integration and clock rates grow, leading to a higher susceptibility for transient faults, caused e.g. by power fluctuations or single event upsets (SEUs). We introduce a technique which is based on the well-known cyclic redundancy check codes (CRCs) to secure the pipelined execution of common microprocessors against transient faults. This is done by computing signatures over the control signals of each pipeline stage including dynamic out-of-order scheduling. To correctly compute the checksums, we resolve the time-dependency of instructions in the pipeline. We first discuss important physical properties of single event upsets (SEUs). Then we present a model of a simple processor with the applied scheme as an example. The scheme is extended to support n-way simultaneous multithreaded systems, resulting in two basic schemes. A cost analysis of the proposed SEU-detection schemes leads to the conclusion that both schemes are applicable at reasonable costs for pipelines with 5 to 10 stages and maximal 4 hardware threads. A worst-case simulation using software fault-injection of transient faults in the processor model showed that errors can be detected with an average of 83% even at a fault rate of 10<sup>-2</sup>. Furthermore, the scheme is able to detect an error within an average of only 5.05 cycles",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1639664,no
Predicting failures of computer systems: a case study for a telecommunication system,2006,"The goal of online failure prediction is to forecast imminent failures while the system is running. This paper compares similar events prediction (SEP) with two other well-known techniques for online failure prediction: a straightforward method that is based on a reliability model and dispersion frame technique (DFT). SEP is based on recognition of failure-prone patterns utilizing a semi-Markov chain in combination with clustering. We applied the approaches to real data of a commercial telecommunication system. Results are presented in terms of precision, recall, F-measure and accumulated runtime-cost. The results suggest a significantly improved forecasting performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1639672,no
Evaluating cooperative checkpointing for supercomputing systems,2006,"Cooperative checkpointing, in which the system dynamically skips checkpoints requested by applications at runtime, can exploit system-level information to improve performance and reliability in the face of failures. We evaluate the applicability of cooperative checkpointing to large-scale systems through simulation studies considering real workloads, failure logs, and different network topologies. We consider two cooperative checkpointing algorithms: work-based cooperative checkpointing uses a heuristic based on the amount of unsaved work and risk-based cooperative checkpointing leverages failure event prediction. Our results demonstrate that, compared to periodic checkpointing, risk-based checkpointing with event prediction accuracy as low as 10% is able to significantly improve system utilization and reduce average bounded slowdown by a factor of 9, without losing any additional work to failures. Similarly, work-based checkpointing conferred tremendous performance benefits in the face of large checkpoint overheads",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1639693,no
Soft real-time aspects for service-oriented architectures,2006,"In today's businesses we can see the trend that service-oriented architectures (SOA) represent the main paradigm for IT infrastructures. In this setting, a software offers its functionality as an electronic service to other software in a network. In order to realise more complex tasks or business processes that are comprised of individual services, compositions of these are formed. For specific application cases such as the time-dependent trading of goods or the IT-based management of natural disasters, service compositions are anticipated to provide soft real-time characteristics. Soft real-time characteristics for service compositions involve the QoS-based trading of services as well as a real-time conforming trading process that is capable of optimising the QoS in predictable and feasible amount of time. This paper discusses the two aspects and introduce two heuristics for the QoS-aware trading that show feasible computational efforts. Based on a software simulation, the performance of these algorithms is discussed",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1640260,no
Comparison of Super-Resolution Algorithms Using Image Quality Measures,2006,"This paper presents comparisons of two learning-based super-resolution algorithms as well as standard interpolation methods. To allow quality assessment of results, a comparison of a variety of image quality measures is also performed. Results show that a MRF-based super-resolution algorithm improves a previously interpolated image. The estimated degree of improvement varies both according to the quality measure chosen for the comparison as well as the image class.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1640427,no
Design Phase Analysis of Software Qualities Using Aspect-Oriented Programming,2006,"If we can analyze software qualities during the design phase of development without waiting until the implementation is completed and tested, the total development cost and time will be significantly saved. Therefore in the past many design analysis methods have been proposed but either they are hard-to-learn and use or, in the case of simulation-based analysis, functionality concerns and quality concerns were intermingled in the design as well as in the implementation thereby making development and maintenance more complicated. In this paper, we propose a simulation-based design phase analysis method based on aspect-oriented programming. In our method, quality aspects remain separate from functionality aspect in the design model and the implementation code for simulation is automatically obtained by injecting quality requirements into the skeleton code generated from the design level functionality model. Our method has advantages over the conventional approach in reducing both the development cost and the maintenance costly",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1640663,no
Application of set membership identification for fault detection of MEMS,2006,"In this article, a set membership (SM) identification technique is tailored to detect faults in microelectromechanical systems. The SM-identifier estimates an orthotope which contains the system's parameter vector. Based on this orthotope, the system's output interval is predicted. If the actual output is outside of this interval, then a fault is detected. Utilization of this scheme can discriminate mechanical-component faults from electronic component variations frequently encountered in MEMS. For testing the suggested algorithm's performance in simulation studies, an interface between classical control-software (MATLAB) and circuit emulation (HSPICE) is developed",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1641783,no
Establishing software product quality requirements according to international standards,2006,"Software product quality is an important concern in the computer environment and whose immediate results are appreciated in all the activities where computers are used. The ISO/IEC 9126 standard series settle a software product quality model, for example, in the annex, shows the identification of the quality requirements like a necessary step for product quality. However, the standard does not included the way to get quality requirements, neither how to establish metrics levels. Establishing quality requirements and metric levels seems to be simple activities but they could be annoying and prone to errors if there is not a systematic approach for the process. This article presents a proposal for establishing product quality requirements according to the ISO/IEC 9126 standard.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1642457,no
Usability measures for software components,2006,"The last decade marked the first real attempt to turn software development into engineering through the concepts of Component-Based Software Development (CBSD) and Commercial Off-The-Shelf (COTS) components, with the goal of creating high-quality parts that could be joined together to form a functioning system. One of the most critical processes in CBSD is the selection of the software components (from either in-house or external repositories) that fulfill some architectural and user-defined requirements. However, there is currently a lack of quality models and metrics that can help evaluate the quality characteristics of software components during this selection process. This paper presents a set of measures to assess the Usability of software components, and describes the method followed to obtain and validate them.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1642462,no
Detecting computer-induced errors in remote-sensing JPEG compression algorithms,2006,"The JPEG image compression standard is very sensitive to errors. Even though it contains error resilience features, it cannot easily cope with induced errors from computer soft faults prevalent in remote-sensing applications. Hence, new fault tolerance detection methods are developed to sense the soft errors in major parts of the system while also protecting data across the boundaries where data flow from one subsystem to the other. The design goal is to guarantee no compressed or decompressed data contain computer-induced errors without detection. Detection methods are expressed at the algorithm level so that a wide range of hardware and software implementation techniques can be covered by the fault tolerance procedures while still maintaining the JPEG output format. The major subsystems to be addressed are the discrete cosine transform, quantizer, entropy coding, and packet assembly. Each error detection method is determined by the data representations within the subsystem or across the boundaries. They vary from real number parities in the DCT to bit-level residue codes in the quantizer, cyclic redundancy check parities for entropy coding, and packet assembly. The simulation results verify detection performances even across boundaries while also examining roundoff noise effects in detecting computer-induced errors in processing steps.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1643684,no
Fully 3-D PET reconstruction with system matrix derived from point source measurements,2006,"The quality of images reconstructed by statistical iterative methods depends on an accurate model of the relationship between image space and projection space through the system matrix. The elements of the system matrix for the clinical Hi-Rez scanner were derived by processing the data measured for a point source at different positions in a portion of the field of view. These measured data included axial compression and azimuthal interleaving of adjacent projections. Measured data were corrected for crystal and geometrical efficiency. Then, a whole system matrix was derived by processing the responses in projection space. Such responses included both geometrical and detection physics components of the system matrix. The response was parameterized to correct for point source location and to smooth for projection noise. The model also accounts for axial compression (span) used on the scanner. The forward projector for iterative reconstruction was constructed using the estimated response parameters. This paper extends our previous work to fully three-dimensional. Experimental data were used to compare images reconstructed by the standard iterative reconstruction software and the one modeling the response function. The results showed that the modeling of the response function improves both spatial resolution and noise properties",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1644806,no
Production performance of the ATLAS semiconductor tracker readout system,2006,"The ATLAS Semiconductor Tracker (SCT) together with the pixel and the transition radiation detectors will form the tracking system of the ATLAS experiment at LHC. It will consist of 20000 single-sided silicon microstrip sensors assembled back-to-back into modules mounted on four concentric barrels and two end-cap detectors formed by nine disks each. The SCT module production and testing has finished while the macro-assembly is well under way. After an overview of the layout and the operating environment of the SCT, a description of the readout electronics design and operation requirements will be given. The quality control procedure and the DAQ software for assuring the electrical functionality of hybrids and modules will be discussed. The focus will be on the electrical performance results obtained during the assembly and testing of the end-cap SCT modules.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1644934,no
Benchmarks and implementation of the ALICE high level trigger,2006,"The ALICE High Level Trigger combines and processes the full information from all major detectors in a large computer cluster. Data rate reduction is achieved by reducing the event rate by selecting interesting events (software trigger) and by reducing the event size by selecting sub-events and by advanced data compression. Reconstruction chains for the barrel detectors and the forward muon spectrometer have been benchmarked. The HLT receives a replica of the raw data via the standard ALICE DDL link into a custom PCI receiver card (HLT-RORC). These boards also provide a FPGA co-processor for data-intensive tasks of pattern recognition. Some of the pattern recognition algorithms (cluster finder, Hough transformation) have been re-designed in VHDL to be executed in the Virtex-4 FPGA on the HLT-RORC. HLT prototypes were operated during the beam tests of the TPC and TRD detectors. The input and output interfaces to DAQ and the data flow inside of HLT were successfully tested. A full-scale prototype of the dimuon-HLT achieved the expected data flow performance. This system was finally embedded in a GRID-like system of several distributed clusters demonstrating the scalability and fault-tolerance of the HLT.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1644953,no
Design and development of a graphical setup software for the CMS global trigger,2006,"The CMS experiment at CERN's Large Hadron Collider will search for new physics at the TeV energy scale. Its trigger system is an essential component in the selection process of potentially interesting events. The Global Trigger is the final stage of the first-level selection process. It is implemented as a complex electronic system containing logic devices, which need to be programmed according to physics requirements. It has to reject or accept events for further processing based on coarse measurements of particle properties such as energies, momenta, and location. Algorithms similar to the ones used in the physics analysis are executed in parallel during the event selection process. A graphical setup program to define these algorithms and to subsequently configure the hardware has been developed. The design and implementation of the program, guided by the principal requirements of flexibility, quality assurance, platform-independence and extensibility, are described.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1645032,no
GNAM: a low-level monitoring program for the ATLAS experiment,2006,"During the last years many test-beam sessions were carried out on each ATLAS subdetector in order to assess the performances in standalone mode. During these tests, different monitoring programs were developed to ease the setup of correct running conditions and the assessment of data quality. The experience has converged into a common effort to develop a monitoring program, which aims to be exploitable by various subdetector groups. The requirements which drove the design of the program as well as its architecture are discussed in this paper. Characteristic features of the application are a modular software based on a Finite State Machine core to implement the synchronization with the data acquisition system and exploiting the ROOT Tree as transient data store. The first version of this monitoring program was used for the 2004 ATLAS Combined Test Beam.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1645037,no
A Hierarchical Approach to Internet Distance Prediction,2006,"Internet distance prediction gives pair-wise latency information with limited measurements. Recent studies have revealed that the quality of existing prediction mechanisms from the application perspective is short of satisfactory. In this paper, we explore the root causes and remedies for this problem. Our experience with different landmark selection schemes shows that although selecting nearby landmarks can increase the prediction accuracy for short distances, it can cause the prediction accuracy for longer distances to degrade. Such uneven prediction quality significantly impacts application performance. Instead of trying to select the landmark nodes in some ""intelligent"" fashion, we propose a hierarchical prediction approach with straightforward landmark selection. Hierarchical prediction utilizes multiple coordinate sets at multiple distance scales, with the ""right"" scale being chosen for prediction each time. Experiments with Internet measurement datasets show that this hierarchical approach is extremely promising for increasing the accuracy of network distance prediction.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1648860,no
The Power of the Defender,2006,"We consider a security problem on a distributed network. We assume a network whose nodes are vulnerable to infection by threats (e.g. viruses), the attackers. A system security software, the defender, is available in the system. However, due to the networkÂ’s size, economic and performance reasons, it is capable to provide safety, i.e. clean nodes from the possible presence of attackers, only to a limited part of it. The objective of the defender is to place itself in such a way as to maximize the number of attackers caught, while each attacker aims not to be caught. In [7], a basic case of this problem was modeled as a non-cooperative game, called the Edge model. There, the defender could protect a single link of the network. Here, we consider a more general case of the problem where the defender is able to scan and protect a set of k links of the network, which we call the Tuple model. It is natural to expect that this increased power of the defender should result in a better quality of protection for the network. Ideally, this would be achieved at little expense on the existence and complexity of Nash equilibria (profiles where no entity can improve its local objective unilaterally by switching placements on the network). In this paper we study pure and mixed Nash equilibria in the model. In particular, we propose algorithms for computing such equilibria in polynomial time and we provide a polynomial-time transformation of a special class of Nash equilibria, called matching equilibria, between the Edge model and the Tuple model, and vice versa. Finally, we establish that the increased power of the defender results in higher-quality protection of the network.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1648926,no
Productivity and code quality improvement of mixed-signal test software by applying software engineering methods,2006,Typical nowadays mixed-signal ICs are approaching 1000 or even more parametric tests. These tests are usually coded in a procedural or a semi-object oriented language. The huge code base of the programs is a significant challenge for maintaining code quality which inherently translates into outgoing quality. The paper presents software metrics of typical mixed-signal power management and audio devices with regard to the number of tests conducted. It is shown that classical ways to handle test programs are error prone and tend to systematically repeat known mistakes. The adoption of selected software engineering methods can avoid such mistakes and improves the productivity of the mixed-signal test generation. Results of a pilot project show significant productivity improvement. Open-source based software is employed to provide the necessary tool support. They establish a potential roadmap to get away from proprietary tester specific tool sets,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1649593,no
A Classification Scheme for Evaluating Management Instrumentation in Distributed Middleware Infrastructure,2006,"Management instrumentation is an integrated capability of a software system that enables an external observer to monitor the system's availability, performance, and reliability during operation. It is highly useful for taking both proactive and reactive actions to keep a software system operational in mission-critical environments where tolerance for an unavailable or poor-performing system is very low. Middleware infrastructure components have taken important positions in distributed software systems due to various benefits related to the development, deployment, and runtime operations. Keeping these components highly available and up to the expected performance requires integrated capabilities that allow regular monitoring of critical functionality, measurement of Quality of Service (QoS), debugging and troubleshooting, and health-checks in the context of actual business processes.. Yet, currently there is no approach that enables systematic evaluation of the relative strengths and weaknesses of a middleware component's management instrumentation. In this paper, we will present an approach to evaluating management instrumentation of middleware infrastructure components. We use a classification-based scheme that has a functional dimension called Capability and two main quality dimensions called Usability and Precision. We further categorize each dimension into smaller, more precise instrumentation features, such as Tracing, Distributed Correlation and Granularity. In presenting our approach, we hope to achieve the following: i) educate middleware users on how to systematically assess or compare the overall manageability of a MidIn component using the classification scheme, and ii) share with middleware researchers on the importance of good integrated manageability in middleware infrastructure.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1651279,no
Improving Accuracy of Multiple Regression Analysis for Effort Prediction Model,2006,"In this paper, we outline the effort prediction model and the evaluation experiment. In addition we explore the parameters in the model. The model predicts effort of embedded software developments via multiple regression analysis using the collaborative filtering. Because companies, recently, focus on methods to predict effort of projects, which prevent project failures such as exceeding deadline and cost, due to more complex embedded software, which brings the evolution of the performance and function enhancement. In the model, we have fixed two parameters named k and ampmax, which would influence the accuracy of predicting effort. Hence, we investigate a tendency of them in the model and find the optimum value",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1651969,no
Topology Discovery for Coexisting IPv6 and IPv4 Networks,2006,"Having an accurate network topology is vital for network performance optimization, configuration control, and fault monitoring...etc. In this paper, two network layer topology discovery algorithms for IPv6-only and IPv4-only networks, a data link layer topology discovery algorithm, and a transition detection algorithm of coexisting networks are presented. The network layer topology is constructed using the routing information from ip6RouteTable and ipRouteTable, while the data-link layer map is built using data from atTable and dotIdTpFdbTable. The transition detection is carried out by collecting and analyzing all IP addresses of both IP versions, through making the use of multicasting inverse IPv6 neighbor discovery messages and broadcasting IPv4 RARP messages. It is useful for finding out which transition mechanism is deployed in a coexisting network. Consequently, overlapped routers and hosts can be determined to complete the IP network topology discovery",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1651975,no
PalProtect: A Collaborative Security Approach to Comment Spam,2006,"Collaborative security is a promising solution to many types of security problems. Organizations and individuals often have a limited amount of resources to detect and respond to the threat of automated attacks. Enabling them to take advantage of the resources of their peers by sharing information related to such threats is a major step towards automating defense systems. In particular, comment spam posted on blogs as a way for attackers to do search engine optimization (SEO) is a major annoyance. Many measures have been proposed to thwart such spam, but all such measures are currently enacted and operate within one administrative domain. We propose and implement a system for cross-domain information sharing to improve the quality and speed of defense against such spam",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1652092,no
Resource Availability Prediction in Fine-Grained Cycle Sharing Systems,2006,"Fine-grained cycle sharing (FGCS) systems aim at utilizing the large amount of computational resources available on the Internet. In FGCS, host computers allow guest jobs to utilize the CPU cycles if the jobs do not significantly impact the local users of a host. A characteristic of such resources is that they are generally provided voluntarily and their availability fluctuates highly. Guest jobs may fail because of unexpected resource unavailability. To provide fault tolerance to guest jobs without adding significant computational overhead, it requires to predict future resource availability. This paper presents a method for resource availability prediction in FGCS systems. It applies a semi-Markov Process and is based on a novel resource availability model, combining generic hardware-software failures with domain-specific resource behavior in FGCS. We describe the prediction framework and its implementation in a production FGCS system named iShare. Through the experiments on an iShare testbed, we demonstrate that the prediction achieves accuracy above 86% on average and outperforms linear time series models, while the computational cost is negligible. Our experimental results also show that the prediction is robust in the presence of irregular resource unavailability",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1652140,no
Market-Based Resource Allocation using Price Prediction in a High Performance Computing Grid for Scientific Applications,2006,"We present the implementation and analysis of a market-based resource allocation system for computational grids. Although grids provide a way to share resources and take advantage of statistical multiplexing, a variety of challenges remain. One is the economically efficient allocation of resources to users from disparate organizations who have their own and sometimes conflicting requirements for both the quantity and quality of services. Another is secure and scalable authorization despite rapidly changing allocations. Our solution to both of these challenges is to use a market-based resource allocation system. This system allows users to express diverse quantity- and quality-of-service requirements, yet prevents them from denying service to other users. It does this by providing tools to the user to predict and tradeoff risk and expected return in the computational market. In addition, the system enables secure and scalable authorization by using signed money-transfer tokens instead of identity-based authorization. This removes the overhead of maintaining and updating access control lists, while restricting usage based on the amount of money transferred. We examine the performance of the system by running a bioinformatics application on a fully operational implementation of an integrated grid market",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1652144,no
Ensuring numerical quality in grid computing,2006,"We propose an approach which gives the user valuable information on the various platforms avail able in a grid in order to assess the numerical quality of an algorithm run on each of these platforms. In this manner, the user is provided with at least very strong hints whether a program performs reliably in a grid before actually executing it. Our approach extends IeeeCC754 by two ""grid-enabled"" modes: The first mode calculates a ""numerical checksum"" on a specific grid host and executes the job only if the check sum is identical to a locally generated one. The second mode provides the user with information on the reliability and IEEE 754-conformity of the underlying floating-point implementation of various platforms. In addition, it can help to find a set of compiler options to optimize the application's performance while retaining numerical stability",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1652171,no
CEDA: control-flow error detection through assertions,2006,"This paper presents an efficient software technique, control flow error detection through assertions (CEDA), for online detection of control flow errors. Extra instructions are automatically embedded into the program at compile time to continuously update run-time signatures and to compare them against pre-assigned values. The novel method of computing run-time signatures results in a huge reduction in the performance overhead, as well as the ability to deal with complex programs and the capability to detect subtle control flow errors. The widely used C compiler, GCC, has been modified to implement CEDA, and the SPEC benchmark programs were used as the target to compare with earlier techniques. Fault injection experiments were used to evaluate the fault detection capabilities. Based on a new comparison metric, method efficiency, which takes into account both error coverage and performance overhead, CEDA is found to be much better than previously proposed methods",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1655535,no
A low-cost SEU fault emulation platform for SRAM-based FPGAs,2006,"In this paper, we introduce a fully automated low cost hardware/software platform for efficiently performing fault emulation experiments targeting SEUs in the configuration bits of FPGA devices, without the need for expensive radiation experiments. We propose a method for significantly reducing the fault list by removing the faults on unused LUT bit positions. We also target the design flip-flops found in the configurable logic blocks (CLBs) inside the FPGA. Run-time reconfigurability of Virtex devices using JBits is exploited to provide the means not only for fault injection but fault detection as well. First, we consider five possible application scenarios for evaluating different self-test schemes. Then, we apply the least favorite and most time consuming of these scenarios on two 32times32 multiplier designs, demonstrating that transferring the simulation processing workload to FPGA hardware can allow for acceleration of simulation time of more than two orders of magnitude",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1655554,no
Software-based adaptive and concurrent self-testing in programmable network interfaces,2006,"Emerging network technologies have complex network interfaces that have renewed concerns about network reliability. In this paper, we present an effective low-overhead failure detection technique, which is based on a software watchdog timer that detects network processor hangs and a self-testing scheme that detects interface failures other than processor hangs. The proposed adaptive and concurrent self-testing scheme achieves failure detection by periodically directing the control flow to go through only active software modules in order to detect errors that affect instructions in the local memory of the network interface. The paper shows how this technique can be made to minimize the performance impact on the host system and be completely transparent to the user",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1655700,no
The Mars Exploration Rover surface mobility flight software driving ambition,2006,"NASA's Mars exploration rovers' (MER) onboard mobility flight software was designed to provide robust and flexible operation. The MER vehicles can be commanded directly, or given autonomous control over multiple aspects of mobility: which motions to drive, measurement of actual motion, terrain interpretation, even the selection of targets of interest (although this mode remains largely underused). Vehicle motion can be commanded using multiple layers of control: motor control, direct drive operations (arc, turn in place), and goal-based driving (goto waypoint). Multiple layers of safety checks ensure vehicle performance: command limits (command timeout, time of day limit, software enable, activity constraints), reactive checks (e.g., motor current limit, vehicle tilt limit), and predictive checks (e.g., step, tilt, roughness hazards). From January 2004 through October 2005, Spirit accumulated over 5000 meters and Opportunity 6000 meters of odometry, often covering more than 100 meters in a single day. In this paper we describe the software that has driven these rovers more than a combined 11,000 meters over the Martian surface, including its design and implementation, and summarize current mobility performance results from Mars",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1655723,no
Coverall algorithm for test case reduction,2006,"This paper proposes a technique called Coverall algorithm, which is based on a conventional attempt to reduce cases that have to be tested for any given software. The approach utilizes the advantage of regression testing where fewer test cases would lessen time consumption of the testing as a whole. The technique also offers a means to perform test case generation automatically. Compared to most of the techniques in the literature where the tester has no option but to perform the test case generation manually, the proposed technique provides a better option. As for the test cases reduction, the technique uses simple algebraic conditions to assign fixed values to variables (maximum, minimum and constant variables). By doing this, the variables values would be limited within a definite range, resulting in fewer numbers of possible test cases to process. The technique can also be used in program loops and arrays. After a comparative assessment of the technique, it has been confirmed that the technique could reduce number of test cases by more than 99%. As for the other features of the technique, automatic test cases generation, all four step of test cases generation in the proposed technique have been converted into an operational program. The success of the program in performing these steps is indeed significant since it represents a practical means for performing test cases generation automatically by a computer algorithm",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1656028,no
Fixing BIT on the V-22 Osprey,2006,"The V-22 Osprey measured an unsatisfactory high built-in-test (BIT) false alarm rate of 92% (threshold les 25%) during its first Operational Test And Evaluation Phase (OPEVAL) in 2000. On a good note, the V-22 did exceed its operational objectives for BIT fault detection and fault isolation rates. Afterwards, the Blue Ribbon Panel report identified the need to fix false alarms: ""Expedite the plan to reduce the V-22 false-alarm rate in both the aircraft and ground systems, with priority on aircraft software"". Correction of false alarms then became a high priority issue on the program. Therefore, a success-oriented engineering approach was developed and implemented to mature the diagnostics system in order to meet the operational requirements",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1656076,no
OpenMP extension to SMP clusters,2006,"This article discusses the approaches to apply the OpenMP programming model to symmetric multiprocessor (SMP) clusters using software distributed shared memory (SDSM). The major focus of this article is on the challenges that the prior studies faced and on their solution techniques. Exploiting message-passing primitives explicitly for the openMP synchronization and work-sharing directives enables light interprocess synchronizations. The studies on loop scheduling for SMP clusters will promise significant improvement in system performance. Finally, OpenMP is considered as promising programming model",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1657761,no
Decision support system for software project management,2006,"Decision support systems combine individuals' and computers' capabilities to improve the quality of decisions. Usually adopted in manufacturing to design floor plans and optimize resource allocation and performance, DSSs are penetrating increasingly complex application areas, from insurance fraud detection to military system procurement to emergency planning. Although researchers have suggested many approaches, DSSs haven't yet entered the main stream of software engineering tools. The complexity of the software process and its sociotechnical nature are often mentioned as the main obstacles to their adoption. As DSSs are developed for other equally complex application areas, we need to identify approaches that can overcome these difficulties and enable project managers to exploit DSSs' capabilities in their daily activities. A hybrid two-level modeling approach is a step in this direction",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1657943,no
Partition-based vector filtering technique for suppression of noise in digital color images,2006,"A partition-based adaptive vector filter is proposed for the restoration of corrupted digital color images. The novelty of the filter lies in its unique three-stage adaptive estimation. The local image structure is first estimated by a series of center-weighted reference filters. Then the distances between the observed central pixel and estimated references are utilized to classify the local inputs into one of preset structure partition cells. Finally, a weighted filtering operation, indexed by the partition cell, is applied to the estimated references in order to restore the central pixel value. The weighted filtering operation is optimized off-line for each partition cell to achieve the best tradeoff between noise suppression and structure preservation. Recursive filtering operation and recursive weight training are also investigated to further boost the restoration performance. The proposed filter has demonstrated satisfactory results in suppressing many distinct types of noise in natural color images. Noticeable performance gains are demonstrated over other prior-art methods in terms of standard objective measurements, the visual image quality and the computational complexity.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1658096,no
Near-Optimal Low-Cost Distortion Estimation Technique for JPEG2000 Encoder,2006,"Optimal rate-control is a very important feature of JPEG2000 which allows simple truncation of compressed bit-stream to achieve best image quality at a given target bit-rate. Accurate distortion estimation with respect to allowed bit-stream truncation points, is essential for rate-control performance. In this paper, we address the issues involved in accurate distortion estimation for hardware oriented implementation of JPEG2000 encoding systems with generic block coding capabilities. We propose a novel hardware friendly distortion estimation technique. Rate control based on the proposed technique results in only an average 0.02 dB PSNR degradation with respect to the optimal distortion estimation approach used in the software implementations of JPEG2000. This is the best performance reported in comparison to existing techniques. The proposed technique requires only an additional 4096 bits per block coder which is 80% less than the memory requirements of optimal approach",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1660813,no
Data Warehousing Process Maturity: An Exploratory Study of Factors Influencing User Perceptions,2006,"This paper explores the factors influencing perceptions of data warehousing process maturity. Data warehousing, like software development, is a process, which can be expressed in terms of components such as artifacts and workflows. In software engineering, the Capability Maturity Model (CMM) was developed to define different levels of software process maturity. We draw upon the concepts underlying CMM to define different maturity levels for a data warehousing process (DWP). Based on the literature in software development and maturity, we identify a set of features for characterizing the levels of data warehousing process maturity and conduct an exploratory field study to empirically examine if those indeed are factors influencing perceptions of maturity. Our focus in this paper is on managerial perceptions of DWP. The results of this exploratory study indicate that several factors-data quality, alignment of architecture, change management, organizational readiness, and data warehouse size-have an impact on DWP maturity, as perceived by IT professionals. From a practical standpoint, the results provide useful pointers, both managerial and technological, to organizations aspiring to elevate their data warehousing processes to more mature levels. This paper also opens up several areas for future research, including instrument development for assessing DWP maturity",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1661915,no
Building statistical test-cases for smart device software - an example,2006,"Statistical testing (ST) of software or logic-based components can produce dependability information on such components by yielding an estimate for their probability of failure on demand. An example of software-based components that are increasingly used within safety-related systems e.g. in the nuclear industry, are smart devices. Smart devices are devices with intelligence, capable of more than merely representing correctly a sensed quantity but of functionality such as processing data, self-diagnosis and possibly exchange of data with other devices. Examples are smart transmitters or smart sensors. If such devices are used in a safety-related context, it is crucial to assess whether they fulfil the dependability requirements posed on them to ensure they are dependable enough to be used within the specific safety-related context. This involves making a case for the probability of systematic failure of the smart device. This failure probability is related to faults present in the logic or software-based part of the device. In this paper we look at a technique that can be used to establish a probability of failure for the software part of a smart monitoring unit. This technique is ""statistical testing"" (ST). Our aim is to share our own experience with ST and to describe some of the issues we have encountered so far on the way to perform ST on this device software.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1662253,no
QMON: QoS- and Utility-Aware Monitoring in Enterprise Systems,2006,"The scale, reliability and cost requirements of enterprise data centers require automation of center management. Examples include provisioning, scheduling, capacity planning, logging and auditing. A key component of such automation functions is online monitoring. In contrast to monitoring systems designed for human users, a particular concern for online enterprise monitoring is Quality of Service (QoS). Since breaking service level agreements (SLAs) has direct financial and legal implications, enterprise monitoring must be conducted so as to maintain SLAs. This includes the ability to differentiate the QoS of monitoring itself for different classes of users or more generally, for software components subject to different SLAs. Thus, without embedding notions of QoS into the monitoring systems used in next generation data centers, it will not be possible to accomplish the desired automation of their operation. This paper both demonstrates the importance of QoS in monitoring and it presents a QoS-capable monitoring system, termed QMON. QMON supports utility-aware monitoring while also able to differentiate between different classes of monitoring, corresponding to classes of SLAs. The implementation of QMON offers high levels of predictability for service delivery (i.e., predictable performance) and it is dynamically configurable to deal with changes in enterprise needs or variations in services and applications. We demonstrate the importance of QoS in monitoring and the QoS capabilities of QMON in a series of case studies and experiments, using a multi-tier web service benchmark.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1662390,no
Service System Resource Management Based on a Tracked Layered Performance Model,2006,"Autonomic computer systems adapt themselves to cope with changes in the operating conditions and to meet the service-level agreements with a minimum of resources. Changes in operating conditions include hardware and software failures, load variation and variations in user interaction with the system. The self adaptation can be achieved by tuning the software, balancing the load or through hardware provisioning. This paper investigates a feed-forward adaptation scheme in which tuning and provisioning decisions are based on a dynamic predictive performance model of the system and the software. The model consists of a layered queuing network whose parameters are tuned by tracking the system with an Extended Kalman Filter. An optimization algorithm searches the system configuration space by using the predictive performance model to evaluate every configuration.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1662396,no
Continuous geodetic time-transfer analysis methods,2006,"We address two issues that limit the quality of time and frequency transfer by carrier phase measurements from the Global Positioning System (GPS). The first issue is related to inconsistencies between code and phase observations. We describe and classify several types of events that can cause inconsistencies and observe that some of them are related to the internal clock of the GPS receiver. Strategies to detect and overcome time-code inconsistencies have been developed and implemented into the Bernese GPS software package. For the moment, only inconsistencies larger than the 20 ns code measurement noise level can be detected automatically. The second issue is related to discontinuities at the day boundaries that stem from the processing of the data in daily batches. Two new methods are discussed: clock handover and ambiguity stacking. The two approaches are tested on data obtained from a network of stations, and the results are compared with an independent time-transfer method. Both methods improve the stability of the transfer for short averaging times, but there is no benefit for averaging times longer than 8 days. We show that continuous solutions are sufficiently robust against modeling and preprocessing errors to prevent the solution from accumulating a permanent bias.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1665073,no
An Efficient Radio Admission Control Algorithm for 2.5G/3G Cellular Networks,2006,"We design an efficient radio admission control algorithm that minimizes blocking probability subject to the condition that the overload probability is smaller than a pre-specified threshold. Our algorithm is quite general and can be applied to both TDMA-based cellular technologies, such as GPRS and EDGE, and CDMA-based technologies, such as UMTS and CDMA2000. We extend prior work in measurement-based admission control in wireline networks to wireless cellular networks and to heterogeneous users. We take the variance of the resource requirement into account while making the admission decision. Using simulation results, we show that our admission control algorithm is able to meet the target overload probability over a range of call arrival rates and radio conditions. We also compare our scheme with a simple admission control algorithm and also show how to use our approach for the carrier selection problem",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1665171,no
VoIP service quality monitoring using active and passive probes,2006,"Service providers and enterprises all over the world are rapidly deploying Voice over IP (VoIP) networks because of reduced capital and operational expenditure, and easy creation of new services. Voice traffic has stringement requirements on the quality of service, like strict delay and loss requirements, and 99.999% network availability. However, IP networks have not been designed to easily meet the above requirements. Thus, service providers need service quality management tools that can proactively detect and mitigate service quality degradation of VoIP traffic. In this paper, we present active and passive probes that enable service providers to detect service impairments. We use the probes to compute the network parameters (delay, loss and jitter) that can be used to compute the call quality as a Mean Opinion Score using a voice quality metric, E-model. These tools can be used by service providers and enterprises to identify network impairments that cause service quality degradation and take corrective measures in real time so that the impact on the degradation perceived by end-users is minimal",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1665188,no
Performance of Real-Time Traffic in EDCA-BASED IEEE 802.11 b/g WLANS,2006,"The IEEE 802.11e draft specification is intended to solve the performance problems of WLANs for real time traffic by extending the original 802.11 medium access control (MAC) protocol and introducing priority access mechanisms in the form of the enhanced distributed channel access mechanism (EDCA) and hybrid coordination channel access (HCCA). The draft standard comes with a lot of configurable parameters for channel access, admission control, etc. but it is not very clear how real time traffic actually performs with respect to capacity and throughput in such WLANs that deploy this upcoming standard. In this report we have provided detailed simulation results on the performance of enterprise-anticipated real time VoIP application and collaborative video conferencing in presence of background traffic in EDCA-based IEEE 802.11 WLANs. We estimate the channel capacity and acceptable load conditions for some important enterprise usage scenarios. Subsequently, admission control limits are experimentally derived for these usage scenarios for voice and video traffic. Our simulations show that admission control greatly helps in maintaining the quality of admitted voice calls and video conferencing sessions that are prioritized as per EDCA mechanisms within acceptable channel load conditions. The use of admission control allows admitted voice calls and video sessions to retain their throughput and delay characteristics while unadmitted traffic (voice/video streams) greatly suffer from poor quality (delays, packet drops, etc.) as the channel load increases",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1665193,no
Application Aware Overlay One-to-Many Data Dissemination Protocol for High-Bandwidth Sensor Actuator Networks,2006,"An application-aware deterministic overlay one-to-many (DOOM) protocol is proposed for meeting heterogeneous QoS requirements of multiple end users of high-bandwidth sensor actuator network (HB-SAN) applications. Although DOOM is initially targeted for use in collaborative adaptive systems of weather radars, it has been designed for use in wider class of sensing systems. DOOM protocol performs rate-based application aware congestion control by selecting end user specific subset of the sensor data for transmission thus adapting to available network infrastructure under dynamic network conditions. Performance of DOOM is evaluated for radar networking using a combination of Planetlab as well as an emulation based test-bed. It is shown that DOOM protocol is able to meet individual end user QoS requirements as well as aggregate QoS requirements of different end users. Moreover, multiple DOOM streams are friendly to each other as well as to TCP cross- traffic sharing the bottleneck link",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1665209,no
A Low-level Simulation Study of Prioritization in IEEE 802.11e Contention-based Networks,2006,"This work deals with the performance evaluation of the IEEE 802.11e EDCA proposal for service prioritization in wireless LANs. A large amount of study has been carried out in the scientific community to evaluate the performance of the EDCA proposal, mainly in terms of throughput and access delay differentiation. However, we argue that further performance insights are needed in order to fully understand the principles behind the EDCA prioritization mechanisms. To this purpose, rather than limit our investigation on throughput and delay performance figures, we take a closer look to their operation also in terms of low-level performance metrics (such as probability of accessing specific channel slots). The paper contribution is threefold: first, we specify a detailed NS2 simulation model by enlightening the typical mis-configuration and errors that may occur when NS2 is used as simulation platform for WLANs and we cross-validate the simulation results with our custom-made C++ simulation tool; second, we describe some performance figures related to the different forms of prioritization provided by the EDCA mechanisms; finally, we verify some assumptions commonly used in the EDCA analytical models",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1665237,no
Photovoltaic Power Conditioning System With Line Connection,2006,"A photovoltaic (PV) power conditioning system (PCS) with line connection is proposed. Using the power slope versus voltage of the PV array, the maximum power point tracking (MPPT) controller that produces a smooth transition to the maximum power point is proposed. The dc current of the PV array is estimated without using a dc current sensor. A current controller is suggested to provide power to the line with an almost-unity power factor that is derived using the feedback linearization concept. The disturbance of the line voltage is detected using a fast sensing technique. All control functions are implemented in software with a single-chip microcontroller. Experimental results obtained on a 2-kW prototype show high performance such as an almost-unity power factor, a power efficiency of 94%, and a total harmonic distortion (THD) of 3.6%",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1667902,no
ReStore: Symptom-Based Soft Error Detection in Microprocessors,2006,"Device scaling and large-scale integration have led to growing concerns about soft errors in microprocessors. To date, in all but the most demanding applications, implementing parity and ECC for caches and other large, regular SRAM structures have been sufficient to stem the growing soft error tide. This will not be the case for long and questions remain as to the best way to detect and recover from soft errors in the remainder of the processor - in particular, the less structured execution core. In this work, we propose the ReStore architecture, which leverages existing performance enhancing checkpointing hardware to recover from soft error events in a low cost fashion. Error detection in the ReStore architecture is novel: symptoms that hint at the presence of soft errors trigger restoration of a previous checkpoint. Example symptoms include exceptions, control flow misspeculations, and cache or translation look-aside buffer misses. Compared to conventional soft error detection via full replication, the ReStore framework incurs little overhead, but sacrifices some amount of error coverage. These attributes make it an ideal means to provide very cost effective error coverage for processor applications that can tolerate a nonzero, but small, soft error failure rate. Our evaluation of an example ReStore implementation exhibits a 2times increase in MTBF (mean time between failures) over a standard pipeline with minimal hardware and performance overheads. The MTBF increases by 20times if ReStore is coupled with protection for certain particularly vulnerable pipeline structures",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1673379,no
A behavior-based process for evaluating availability achievement risk using stochastic activity networks,2006,"With the increased focus on the availability of complex, multifunction systems, modeling processes and analysis tools are needed that help the availability systems engineer understand the impact of architectural and logistics design choices concerning system availability. Because many fielded systems are required to achieve a specified minimal availability over a short measurement period, a modeling methodology must also support computation of the distribution of operational availability for the specified measurement period. This paper describes a two-part behavior-based availability achievement risk methodology that starts with a description of the system's availability related behavior followed by a stochastic activity network-based simulation to obtain numeric estimate of expected availability and the distribution of availability over a selected time frame. The process shows how the system engineer freed to explore complex behavior not possible with combinatorial estimation methods in wide use today",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1677344,no
Better software reliability by getting the requirements right,2006,"For too long software has been produced using processes that result in a product that is completed late, over-budget, and below the quality expectations of end users. Over the last 25 years a great many incremental, evolutionary, and iterative software development process models have been introduced with the intention of improving software development; yet a recent study by the Standish Group reported that fewer than 28% of software projects are completed on schedule within budget and with most of the originally required features. We believe that getting the requirements right is the key to building successful and reliable software products. In the requirements process the product features are defined, the users are identified, and the user interactions with the system are designed. In addition, usage profiles are developed and estimates of the anticipated software reliability based on those profiles and on formal models of the system are obtained. Thus, reliability becomes an integral part of the requirement development process rather than an after thought of testing at the end of the development process. Although, defining the requirements may be an iterative process, a complete and correct set of requirements must be completed before starting to build the product",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1677359,no
Risk assessment of real time digital control systems,2006,"This paper describes stochastic methods for assessing risk in integrated hardware and software systems. The methods assess evaluate availability, outage probabilities, and effectiveness-weighted degraded states based on data from measurements with a specified confidence level. System-level reliability/availability models can also identify the elements where failure rate, recovery probability, or recovery time improvement will provide the greatest benefit. The validity of this approach is determined by the extent to which the system failure behavior conforms to a stochastic process (i.e., random, non-deterministic failures). Evidence from large studies of other high availability computer systems provides substantial evidence of such behavior in mature systems. The approach is limited to the systems with failure rates higher than 10<sup>-6</sup>per hour and the availability below 0.999999, i.e., below safety grade. To assess safety critical systems, the risk assessment method described here can be used as an adjunct for other approaches described in various industry standards that intended to minimize the likelihood that deterministic defects are introduced into the system design",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1677409,no
Safety assessment for safety-critical systems including physical faults and design faults,2006,"Two types of faults, design faults and physical faults, are discussed in this paper. Since they are two mutually exclusive and complete fault types on the fault space, the safety assessment of safety-critical computer systems in this paper considers the hazard contribution from both types. A three-state Markov model is introduced to model safety-critical systems. Steady state safety and mean time to unsafe failure (MTTUF) are the two most important metrics for safety assessment. Two homogenous Markov models are derived from the three-state Markov model to estimate the steady state safety and the MTTUF. The estimation results are generalized given the fault space is divided by M mutually exclusive and complete types of faults",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1677437,no
Estimation of Nonlinear Errors-in-Variables Models for Computer Vision Applications,2006,"In an errors-in-variables (EIV) model, all the measurements are corrupted by noise. The class of EIV models with constraints separable into the product of two nonlinear functions, one solely in the variables and one solely in the parameters, is general enough to represent most computer vision problems. We show that the estimation of such nonlinear EIV models can be reduced to iteratively estimating a linear model having point dependent, i.e., heteroscedastic, noise process. Particular cases of the proposed heteroscedastic errors-in-variables (HEIV) estimator are related to other techniques described in the vision literature: the Sampson method, renormalization, and the fundamental numerical scheme. In a wide variety of tasks, the HEIV estimator exhibits the same, or superior, performance as these techniques and has a weaker dependence on the quality of the initial solution than the Levenberg-Marquardt method, the standard approach toward estimating nonlinear models",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1677513,no
Performance Modeling and Evaluation of Distributed Component-Based Systems Using Queueing Petri Nets,2006,"Performance models are used increasingly throughout the phases of the software engineering lifecycle of distributed component-based systems. However, as systems grow in size and complexity, building models that accurately capture the different aspects of their behavior becomes a more and more challenging task. In this paper, we present a novel case study of a realistic distributed component-based system, showing how queueing Petri net models can be exploited as a powerful performance prediction tool in the software engineering process. A detailed system model is built in a step-by-step fashion, validated, and then used to evaluate the system performance and scalability. Along with the case study, a practical performance modeling methodology is presented which helps to construct models that accurately reflect the system performance and scalability characteristics. Taking advantage of the modeling power and expressiveness of queueing Petri nets, our approach makes it possible to model the system at a higher degree of accuracy, providing a number of important benefits",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1677534,no
Data Mining Based Fuzzy Classification Algorithm for Imbalanced Data,2006,"The elegant fuzzy classification algorithm proposed by Ishibuchi et al. (I-algorithm) has achieved satisfactory performance on many well-known test data sets that have usually been carefully preprocessed. However, the algorithm does not provide satisfactory performance for the problems with imbalanced data that are often encountered in real-world applications. This paper presents an extension of the I-algorithm to E-algorithm to alleviate the effect of data imbalance. Both the I-algorithm and the E-algorithm are applied to Duke Energy outage data for power distribution systems fault cause identification. Their performance on this real-world imbalanced data set is presented, compared, and analyzed to demonstrate the improvement of the extended algorithm.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1681806,no
"""Word-of-Mouth"" in Radio Access Markets",2006,"Opening up interoperability between wide and local area networks seems to be a very promising solution for delivering improved user experience while reducing overall service costs. In a scenario, where a single operator owns different types of networks, QoS provision can be achieved by introducing complex multi-system resource management. On the contrary, if local area networks are deployed by different entities, due to the lack of both coordination and centralized RRM management, the experienced QoS may drastically fluctuate, ranging from SLAs in wide area, to only ""best effort"" expectations in local area networks. In order for ""nomadic"" terminal agents to perform ""informed"" access selection decisions, we propose the adoption of ""word-of-mouth"" (WoM), a novel scheme for sharing, in a peer-to-peer fashion, information about the service quality experienced with different networks. The performances of our proposed WoM scheme have been evaluated for a file pre-fetching service, considering information characterized by various degrees of time criticality, when different RRM strategies are implemented in the local area networks. The results show that if a critical mass of terminal agents exchange experienced QoS information, the overall network selection decision is improved: terminal agents can estimate, on beforehand, which type of performances to expect with different candidate networks, and avoid to select those not satisfying service requirements. This, in turn brings two main positive effects: on one hand, user perceived performance is improved, and, on the other hand, the adoption of RRM strategies providing some degree QoS is incentivated",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1682927,no
Toward Formal Verification of 802.11 MAC Protocols: a Case Study of Applying Petri-nets to Modeling the 802.11 PCF,2006,"Centralized control functions for the IEEE 802.11 family of WLAN standards are vital for the distribution of traffic with stringent quality of service (QoS) requirements. These centralized control functions overlay a time-based organizational ""super-frame"" structure on the medium, allocating part of the super-frame to polling traffic and part to contending traffic. This allocation directly determines how well the two forms of traffic are supported. Given the vital role of this allocation in the success of a system, we must have confidence in the configuration used, beyond that provided by empirical simulation results. Formal mathematical methods are a means to conduct rigorous analysis that will permit us such confidence, and the Petri-net formalism offers an intuitive representation with formal semantics. We present an extended Petri-net model of the super-frame, and use this model to assess the performance of different super-frame configurations and the effects of different traffic patterns. We believe that using such a model to analyze performance in this manner is new in itself",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1683019,no
Fast Variable Block Size Motion Estimation by Adaptive Early Termination,2006,"This paper presents a fast motion estimation algorithm by adaptively changing the early termination threshold for the current accumulated partial sum of absolute difference (SAD) value. The simulation results show that the proposed algorithm can provide the similar quality while saving 77.9% and 50.6% of SAD computation when comparing with early termination methods in MPEG-4 VM18.0 and H.264 JM9.0, respectively",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1683827,no
Component Reusability and Cohesion Measures in Object-Oriented Systems,2006,"In software component reuse processing, the success of software systems is decided by the quality of components. One important characteristic to measure quality of components is component reusability. Component reusability measures how easily the component can be reused in a new environment. This paper provides a new measure of cohesion developed to assess the reusability of Java components retrieved from the Internet by a search engine. This measure differs from the majority of established metrics in two respects: it reflects the degree of similarity between classes quantitatively, and they also take account of indirect similarities. An empirical comparison of the new measure with the established metrics is described. The new measures are shown to be consistently superior at ranking components according to their reusability",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1684869,no
Fault Tolerance in Mobile Agent Systems by Cooperating the Witness Agents,2006,"Mobile agents travel through servers for perform their programs and fault tolerance is fundamental and important in their itinerary. In the paper, are considering and described existent methods of fault tolerance in mobile agents. Then the method is considered that which uses cooperating agents to fault tolerance and to detect server and agent failure, meaning three type of agents involved: actual agent which performs programs for its owner, witness agent which monitors the actual agent and the witness agent after itself, probe which is sent for recovery the actual agent or the witness agent on the side of the witness agent. Traveling agent through servers, the witness is created by actual agent. Scenarios of failure and recovery of server and agent are discussed in the method. During performing the actual agent, the witness agents are increased by addition the servers. Proposed scheme is that minimizes the witness agents as far as possible, because with considering and comparing could concluded that existing all of witness agent is not necessary on the initial servers. Simulation of this method is done by C-Sim",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1684897,no
Toward Exception-Handling Best Practices and Patterns,2006,"An exception condition occurs when an object or component, for some reason, can't fulfil a responsibility. Poor exception-handling implementations can thwart even the best design. It's high time we recognize exception handling's importance to an implementation's overall quality. Agreeing on a reasonable exception-handling style for your application and following a consistent set of exception-handling practices is crucial to implementing software that's easy to comprehend, evolve, and refactor. The longer you avoid exceptions, the harder it is to wedge cleanly designed exception-handling code into working software. To demystify exception-handling design, we must write about - and more widely disseminate - proven techniques, guidelines, and patterns",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1687854,no
moPGA: Towards a New Generation of Multi-objective Genetic Algorithms,2006,"This paper describes a multi-objective parameter-less genetic algorithm (moPGA), which combines several recent developments including efficient non-dominated sorting, linkage learning, isin-Dominance, building-block mutation and convergence detection. Additionally, a novel method of clustering in the objective space using an isin-Pareto Set is introduced. Comparisons with well-known multi-objective GAs on scalable benchmark problems indicate that the algorithm scales well with problem size in terms of number of function evaluations and quality of solutions found. moPGA was built for easy usage and hence, in addition to the problem function and encoding, there are only two required user defined parameters; (1) the maximum running time or generations and (2) the precision of the desired solutions (isin).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1688513,no
An Anomaly Detection-Based Classification System,2006,"In this paper, we describe the construction of a classification system based on an anomaly detection system that employs constraint-based detectors, which are generated using a genetic algorithm. The performance of the classification system was evaluated using two benchmark datasets including the Wisconsin breast cancer dataset and the Fisher's iris dataset.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1688584,no
IMPRES: integrated monitoring for processor reliability and security,2006,"Security and reliability in processor based systems are concerns requiring adroit solutions. Security is often compromised by code injection attacks, jeopardizing even 'trusted software'. Reliability is of concern where unintended code is executed in modern processors with ever smaller feature sizes and low voltage swings causing bit flips. Countermeasures by software-only approaches increase code size by large amounts and therefore significantly reduce performance. Hardware assisted approaches add extensive amounts of hardware monitors and thus incur unacceptably high hardware cost. This paper presents a novel hardware/software technique at the granularity of micro-instructions to reduce overheads considerably. Experiments show that our technique incurs an additional hardware overhead of 0.91% and clock period increase of 0.06%. Average clock cycle and code size overheads are just 11.9% and 10.6% for five industry standard application benchmarks. These overheads are far smaller than have been previously encountered",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1688849,no
Signature-based workload estimation for mobile 3D graphics,2006,"Until recently, most 3D graphics applications had been regarded as too computationally intensive for devices other than desktop computers and gaming consoles. This notion is rapidly changing due to improving screen resolutions and computing capabilities of mass-market handheld devices such as cellular phones and PDAs. As the mobile 3D gaming industry is poised to expand, significant innovations are required to provide users with high-quality 3D experience under limited processing, memory and energy budgets that are characteristic of the mobile domain. Energy saving schemes such as dynamic voltage and frequency scaling (DVFS), as well as system-level power and performance optimization methods for mobile devices require accurate and fast workload prediction. In this paper, we address the problem of workload prediction for mobile 3D graphics. We propose and describe a signature-based estimation technique for predicting 3D graphics workloads. By analyzing a gaming benchmark, we show that monitoring specific parameters of the 3D pipeline provides better prediction accuracy over conventional approaches. We describe how signatures capture such parameters concisely to make accurate workload predictions. Signature-based prediction is computationally efficient because first, signatures are compact, and second, they do not require elaborate model evaluations. Thus, they are amenable to efficient, real-time prediction. A fundamental difference between signatures and standard history-based predictors is that signatures capture previous outcomes as well as the cause that led to the outcome, and use both to predict future outcomes. We illustrate the utility of signature-based workload estimation technique by using it as a basis for DVFS in 3D graphics pipelines",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1688866,no
A Software Component Quality Model: A Preliminary Evaluation,2006,"Component-based software development is becoming more generalized, representing a considerable market for the software industry. The perspective of reduced development costs and shorter life cycles acts as a motivation for this expansion. However, several technical issues remain unsolved before software component's industry reaches the maturity exhibited by other component industries. Problems such as the component selection by their integrators and the uncertain quality of third-party developed components, bring new challenges to the software engineering community. By the other hand, the software components certification area is still immature and further research is needed in order to obtain well-defined standards for certification. In this way, we aim to propose a component quality model, describing consistent and well-defined characteristics, quality attributes and related metrics for the components evaluation. A preliminary evaluation to analyze the results of using the component quality model proposed is also presented",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1690122,no
State of the Art and Practice of OpenSource Component Integration,2006,"The open source software (OSS) development approach has become a remarkable option to consider for cost-efficient, high quality software development. Utilizing OSS as part of an in-house software application requires the software company to take the role of a component integrator. In addition, integrating OSS as part of in-house software has a few differences compared to integrating closed source software and in-house software, such as access to source code and the fact that OSS evolves differently than closed source software. This paper describes the current state of the art and practice of open source integration techniques. The main observations are that the lack of documentation and heterogeneity of platforms are problems that neither the state of the art or practice could solve. In addition, although literature provides techniques and methods for predicting and solving both architecture and component level integration problems, these were not used in practice. Instead, companies relied on experience and rules of thumb",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1690138,no
Software Defect Identification Using Machine Learning Techniques,2006,"Software engineering is a tedious job that includes people, tight deadlines and limited budgets. Delivering what customer wants involves minimizing the defects in the programs. Hence, it is important to establish quality measures early on in the project life cycle. The main objective of this research is to analyze problems in software code and propose a model that will help catching those problems earlier in the project life cycle. Our proposed model uses machine learning methods. Principal component analysis is used for dimensionality reduction, and decision tree, multi layer perceptron and radial basis functions are used for defect prediction. The experiments in this research are carried out with different software metric datasets that are obtained from real-life projects of three big software companies in Turkey. We can say that, the improved method that we proposed brings out satisfactory results in terms of defect prediction",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1690146,no
Managing Risk of Inaccurate Runtime Estimates for Deadline Constrained Job Admission Control in Clusters,2006,"The advent of service-oriented grid computing has resulted in the need for grid resources such as clusters to enforce user-specific service needs and expectations. Service level agreements (SLAs) define conditions which a cluster needs to fulfill for various jobs. An example of SLA requirement is the deadline by which a job has to be completed. In addition, these clusters implement job admission control so that overall service performance does not deteriorate due to accepting exceeding amount of jobs. However, their effectiveness is highly dependent on accurate runtime estimates of jobs. This paper thus examines the adverse impact of inaccurate runtime estimates for deadline constrained job admission control in clusters using the earliest deadline first (EDF) strategy and a deadline-based proportional processor share strategy called Libra. Simulation results show that an enhancement called LibraRisk can manage the risk of inaccurate runtime estimates better than EDF and Libra by considering the risk of deadline delay",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1690649,no
Finite Horizon QoS Prediction of Reconfigurable Firm Real-Time Sytems,2006,"Updating real-time system software is often needed in response to errors and added requirements to the software. Stopping a running application, updating the software, and then restarting the application is not suitable for systems with high availability requirements. On the other hand, dynamically updating a system may increase the execution time of the tasks, thus, degrading the performance of the system. Degradation is not acceptable for performance-critical real-time systems as there are strict requirements on the performance. In this paper we present an approach that enables dynamic reconfiguration of a real-time system, where the performance of the system during a reconfiguration satisfies a given worst-case performance specification. Evaluation shows that the presented method is efficient in guaranteeing the worst-case performance of dynamically reconfigurable firm real-time systems",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1691318,no
Applying System Execution Modeling Tools to Evaluate Enterprise Distributed Real-time and Embedded System QoS,2006,"Component middleware is popular for enterprise distributed systems because it provides effective reuse of the core intellectual property (i.e., the ""business logic""). Component-based enterprise distributed real-time and embedded (DRE) systems, however, incur new integration problems associated with component configuration and deployment. New research is therefore needed to minimize the gap between the development and deployment/configuration of components, so that deployment and configuration strategies can be evaluated well before system integration. This paper uses an industrial case study from the domain of shipboard computing to show how system execution modeling tools can provide software and system engineers with quantitative estimates of system bottlenecks and performance characteristics to help evaluate the performance of component-based enterprise DRE systems and reduce time/effort in the integration phase. The results from our case study show the benefits of system execution modeling tools and pinpoint where more work is needed",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1691334,no
"Monitoring and Improving the Quality of ODC Data using the ""ODC Harmony Matrices"": A Case Study",2006,"Orthogonal defect classification (ODC) is an advanced software engineering technique to provide in-process feedback to developers and testers using defect data. ODC institutionalization in a large organization involves some challenging roadblocks such as the poor quality of the collected data leading to wrong analysis. In this paper, we have proposed a technique ('Harmony Matrix') to improve the data collection process. The ODC Harmony Matrix has useful applications. At the individual defect level, results can be used to raise alerts to practitioners at the point of data collection if a low probability combination is chosen. At the higher level, the ODC Harmony Matrix helps in monitoring the quality of the collected ODC data. The ODC Harmony Matrix complements other approaches to monitor and enhances the ODC data collection process and helps in successful ODC institutionalization, ultimately improving both the product and the process. The paper also describes precautions to take while using this approach",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1691408,no
Experiences with product line development of embedded systems at Testo AG,2006,"Product line practices are increasingly becoming popular in the domain of embedded software systems. This paper presents results of assessing success, consistency, and quality of Testo's product line of climate and flue gas measurement devices after its construction and the delivery of three commercial products. The results of the assessment showed that the incremental introduction of architecture-centric product line development can be considered successful even though there is no quantifiable reduction of time-to-market as well as development and maintenance costs so far. The success is mainly shown by the ability of Testo to develop more complex products and the satisfaction of the involved developers. A major issue encountered is ensuring the quality of reusable components and the conformance of the products to the architecture during development and maintenance",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1691589,no
Transient fault-tolerance through algorithms,2006,"This article describes that single-version enhanced processing logic or algorithms can be very effective in gaining dependable computing through hardware transient fault tolerance (FT) in an application system. Transients often cause soft errors in a processing system resulting in mission failure. Errors in program flow, instruction codes, and application data are often caused by electrical fast transients. However, firmware and software fixes can have an important role in designing an ESD, or EMP-resistant system and are more cost effective than hardware. This technique is useful for detecting and recovering transient hardware faults or random bit errors in memory while an application is in execution. The proposed single-version software fix is a practical, useful, and economic tool for both offline and online memory scrubbing of an application system without using conventional N versions of software (NVS) and hardware redundancy in an application like a frequency measurement system",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1692282,no
An efficient SNR scalability coding framework hybrid open-close loop FGS coding,2006,"This paper presents a novel high-efficient hybrid open-close loop based fine granularity scalable (HOCFGS) coding framework supporting different decoding complexity applications. The open-loop motion compensation for inter-pictures is introduced to efficiently exploit the temporal correlation among adjacent pictures for both base layer and FGS enhancement layer within a wide bit-rate range. An efficient rate-distortion optimized macro-block mode decision rule is used to reduce drifting error and achieve comparable coding performance at the lowest bit-rate point (base layer) with non-scalable coding. An approach like MPEG-4 FGS coding with close-loop motion compensation only at base layer is used for some inter-pictures to stop the drifting error propagation. Furthermore, to achieve better coding performance for these inter-pictures, block based progressive fine granularity scalable (BLPFGS) is introduced, in which leaky prediction is used to generate high-quality reference for FGS enhancement layer. In BLPFGS coding, efficient bit-plane coding and de-blocking techniques are investigated to improve the coding performance for FGS enhancement layer, especially at low bit-rate points. The coding performance for the proposed method is verified by integrating it into MPEG SVC reference software",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1693225,no
A case-study on multimedia applications for the XiRisc reconfigurable processor,2006,"Embedded real-time multimedia applications pose several challenges in order to satisfy increasing quality of service (QoS) and energy consumption constraints, that are hardly matched by the capabilities of general-purpose standard processors. Reconfigurable processors, coupling the flexibility of software-programmable devices with the computational efficiency of application specific architectures, represent an appealing trade-off for next generation devices in the digital signal processing application domain. In this paper, we present a benchmarking application for the XiRisc reconfigurable processor, based on a public release of the MPEG-2 video encoder. The introduction of the reconfigurable logic gives a 5times performance improvement and a 66% energy saving",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1693719,no
Improved refinement search for H.263 to H.264/AVC transcoding based on the minimum cost tendency search,2006,"An improved refinement search method for transcoding from H.263 to H.264/AVC is proposed in this paper. Many existing motion re-estimation methods refine the input motion vector (MV) with a small search range, which is usually input MV biased. Motion estimation (ME) in H.263 usually does not consider the rate required for coding the MV, and hence, the input MV may incur a large cost in H.264/AVC. To overcome this problem, we introduce a refinement search method, called minimum cost tendency search (MCTS), which takes the difference between the cost functions for ME in H.263 and H.264/AVC into consideration. The input MV and the predictor MV are used as two anchor points. The proposed MCTS starts searching from the anchor point with a higher cost to another. Finally, the best point is chosen as the center for further refinement. The performance of MCTS is evaluated by comparing with full search, FME in JM software and refinement scheme using small diamond pattern around the input MV (RSD). Experimental results show the proposed MCTS performs more stable than FME and RSD over a wide range of output video quality",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1693823,no
Measuring Instrument of Parameters of Quality of Electric Energy,2006,"In electric power industry, the important role is played with quality of electric energy as suitability for use of electric energy depends on its quality. Within the limits of the given work development and research of an electro technical complex of an estimation of quality of electric energy is carried out",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1694121,no
Performance evaluation of maximal ratio combining diversity over the Weibull fading channel in presence of co-channel interference,2006,"The Weibull distribution has recently attracted much attention among the radio community as a statistical model that better describes the fading phenomenon on wireless channels. In this paper, we consider a multiple access system in which each of the desired signal as well as the co-channel interferers are subject to Weibull fading. We analyze the performance of the L-branch maximal ratio combining (MRC) receiver in terms of the outage probability in such scenario in the two cases where the diversity branches are assumed to be independent or correlated. The analysis is also applicable to the cases where the diversity branches and/or the interferers fading amplitudes are non-identically distributed. Due to the difficulty of handling the resulting outage probability expressions numerically using the currently available mathematical software packages, we alternatively propose using Pade approximation (PA) to make the results numerically tractable. We provide numerical results for different number of interferers, different number of diversity branches as well as different degrees of correlation and power unbalancing between diversity branches. All our numerical results are verified by means of Monte-Carlo simulations and excellent agreement between the two sets is noticed",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1696510,no
A Software Simulation Study of a MD DS/SSMA Communication System with Adaptive Channel Coding,2006,"Studies have shown that adaptive forward error correction (FEC) coding schemes enable a communication system to take advantage of varying channel conditions by switching to less powerful and/or less redundant FEC channel codes when conditions are good, thus enabling an increase in system throughput. The focus of this study is the simulation performance of a complete simulated multi-dimensional (MD) direct-sequence spread spectrum multiple access (DS/SSMA) communication system that employs an advanced adaptive channel coding scheme. The system is simulated and evaluated over a fully user-definable software-based multi-user (MU) multipath fading channel simulator (MFCS). Channel conditions are varied and the switching and adaptation performance of the system is monitored and evaluated. Sensing for adaptation is made possible by a sophisticated quality-of-service monitoring unit (QoSMU) that uses a sophisticated pseudo-error-rate (PER) extrapolation technique to estimate the system's true probability-of-error in real-time, without the need for known transmitted data. The system attempts to keep the estimated bit-error-rate (BER) performance within a predetermined range by switching between different FEC codes as conditions change. This paper commences with a short overview of each of the functional units of the system. Lastly, the simulation results for the coded and uncoded BER performances, as well as the real-time adaptation performance of the system are presented and discussed. This paper conclusively proves that adaptive coded systems have large throughput utilization advantages over that of fixed coded systems",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1698491,no
Prediction Strategies for Proactive Management in Dynamic Distributed Systems,2006,"In real-life distributed systems, dynamic changes are unavoidable properties because of the degeneration or improvement in system performance. Hence to understand the dynamic changes and to ""catch the trend"" of the changes in distributed systems will be very important for distributed systems management. In order to model the dynamic changes in distributed systems, temporal extensions of Bayesian networks are employed to address the temporal factors and to model the dynamic changes of managed entities and the dependencies between them. Furthermore, the prediction capabilities are investigated by means of the relevant inference techniques when the imprecise and dynamic management information occurs in the distributed system",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1698521,no
Simple Shadow Remova,2006,"Given the location of shadows, how can we obtain high-quality shadow-free images? Several methods have been proposed so far, but they either introduce artifacts or can be difficult to implement. We propose here a simple method that results in virtually error and shadow-free images in a very short time. Our approach is based on the insight that shadow regions differ from their shadow-free counterparts by a single scaling factor. We derive a robust method to obtain that factor. We show that for complex scenes - containing many disjointed shadow regions - our new method is faster and more robust than others previously published. The method delivers good performance on a variety of outdoor images",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1699019,no
Analysis of Restart Mechanisms in Software Systems,2006,"Restarts or retries are a common phenomenon in computing systems, for instance, in preventive maintenance, software rejuvenation, or when a failure is suspected. Typically, one sets a time-out to trigger the restart. We analyze and optimize time-out strategies for scenarios in which the expected required remaining time of a task is not always decreasing with the time invested in it. Examples of such tasks include the download of Web pages, randomized algorithms, distributed queries, and jobs subject to network or other failures. Assuming the independence of the completion time of successive tries, we derive computationally attractive expressions for the moments of the completion time, as well as for the probability that a task is able to meet a deadline. These expressions facilitate efficient algorithms to compute optimal restart strategies and are promising candidates for pragmatic online optimization of restart timers",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1703386,no
Design by Contract to Improve Software Vigilance,2006,"Design by contract is a lightweight technique for embedding elements of formal specification (such as invariants, pre and postconditions) into an object-oriented design. When contracts are made executable, they can play the role of embedded, online oracles. Executable contracts allow components to be responsive to erroneous states and, thus, may help in detecting and locating faults. In this paper, we define vigilance as the degree to which a program is able to detect an erroneous state at runtime. Diagnosability represents the effort needed to locate a fault once it has been detected. In order to estimate the benefit of using design by contract, we formalize both notions of vigilance and diagnosability as software quality measures. The main steps of measure elaboration are given, from informal definitions of the factors to be measured to the mathematical model of the measures. As is the standard in this domain, the parameters are then fixed through actual measures, based on a mutation analysis in our case. Several measures are presented that reveal and estimate the contribution of contracts to the overall quality of a system in terms of vigilance and diagnosability",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1703388,no
Development of circuit models for extractor components in high power microwave sources,2006,"Summary form only given. The state-of-the-art in high power microwave (HPM) sources has greatly improved in recent years, in part due to advances in the computational tools available to analyze such devices. Chief among these advances is the widespread use of parallel particle-in-cell (PIC) techniques. Parallel PIC software allows high fidelity, three-dimensional, electromagnetic simulations of these complex devices to be performed. Despite these advances, however, parallel PIC software could be greatly supplemented by fast-running parametric codes specifically designed to mimic the behavior of the source in question. These tools can then be used to develop zero-order point designs for eventual assessment via full PIC simulation. One promising technique for these parametric formulations is circuit models, where in the full field description is reduced to capacitances, inductances, and resistances that can be quickly solved to yield small signal growth rates, resonant frequencies, quality factors, and potentially efficiencies. Building on the extensive literature from the vacuum electronics community, this poster will investigate the circuit models associated with the purely electromagnetic components of the extractor in the absence of space charge. Specifically, three-dimensional time-domain computational electromagnetics (AFRL's ICEPIC software) will be used to investigate the modification of the resonant frequencies and mode quality factors as a function of slot and load geometry. These field calculations will be reduced to circuit parameters for potential inclusion in parametric models, and the fidelity of the resulting description will be assessed",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1707288,no
On the Use of Mutation Faults in Empirical Assessments of Test Case Prioritization Techniques,2006,"Regression testing is an important activity in the software life cycle, but it can also be very expensive. To reduce the cost of regression testing, software testers may prioritize their test cases so that those which are more important, by some measure, are run earlier in the regression testing process. One potential goal of test case prioritization techniques is to increase a test suite's rate of fault detection (how quickly, in a run of its test cases, that test suite can detect faults). Previous work has shown that prioritization can improve a test suite's rate of fault detection, but the assessment of prioritization techniques has been limited primarily to hand-seeded faults, largely due to the belief that such faults are more realistic than automatically generated (mutation) faults. A recent empirical study, however, suggests that mutation faults can be representative of real faults and that the use of hand-seeded faults can be problematic for the validity of empirical results focusing on fault detection. We have therefore designed and performed two controlled experiments assessing the ability of prioritization techniques to improve the rate of fault detection of test case prioritization techniques, measured relative to mutation faults. Our results show that prioritization can be effective relative to the faults considered, and they expose ways in which that effectiveness can vary with characteristics of faults and test suites. More importantly, a comparison of our results with those collected using hand-seeded faults reveals several implications for researchers performing empirical studies of test case prioritization techniques in particular and testing techniques in general",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1707670,no
A case study of supply chain management in oil industry,2006,"The paper deals with business renovation, the effective utilisation of information technology and the role of business process modelling in supply chain integration project in oil industry. The main idea is to show how the performance of the supply chain can be improved with the integration of various tiers in the chain. Integration is a prerequisite for the effective sharing and utilisation of information between different companies in the chain. Simulation-based methodology for measuring the benefits combines the simulation of business processes with the simulation of supply and demand. A case study of the procurement process in a petrol company is shown with old and renewed business process models and changes in lead-times, process execution costs, quality of the process and inventory costs are estimated",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1708537,no
Dependability analysis: performance evaluation of environment configurations,2006,Prototyping-based fault injection environments are employed to perform dependability analysis and thus predict the behavior of circuits in presence of faults. A novel environment has been recently proposed to perform several types of dependability analyses in a common optimized framework. The approach takes advantage of hardware speed and of software flexibility to achieve optimized trade-offs between experiment duration and processing complexity. This paper discusses the possible repartition of tasks between hardware and embedded software with respect to the type of circuit to analyze and to the instrumentation achieved. The performances of the approach are evaluated for each configuration of the environment,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1708652,no
Practical application of probabilistic reliability analyses,2006,"Liberalization of the energy markets has increased the cost pressure on network operators significantly. Corresponding cost saving measures in general will have negative effects on quality of supply, especially on supply reliability. On the other hand, a decrease of supply reliability is not acceptable for customers and politicians - and the public awareness was focused by several blackouts in the European and American transmission systems. In order to handle this delicate question of balancing network costs and supply reliability, detailed and above all quantitative information is required in the network planning process. A suitable tool for this task is probabilistic reliability analysis, which has already been in use for several years successfully. The method and possible applications are briefly described here and application is demonstrated with various examples from practical studies focusing on distribution systems. The results prove that reliability analyses are becoming an indispensable component of customer-oriented network planning",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1709155,no
Investigation of radiometric partial discharge detection for use in switched HVDC testing,2006,"This paper reports on initial trials of a non-contact radio frequency partial discharge detection technique that has potential for use within fast switching HVDC test systems. Electromagnetic environments of this type can arise within important electrical transmission nodes such converter stations, so the methods described could in future be useful for condition monitoring purposes. The radiometric technique is outlined and the measurement system and its components are described. Preliminary field trials are reported and results presented for a discharge detected in part of the HV test system during set-up for long-term testing of a reactor. The calculated and observed locations of the discharge were in agreement to within 60 cm inside a test housing of diameter 5 m and height 8 m. Techniques for improving the location accuracy are discussed. The issue of data volume presents a considerable challenge for the RF measurement techniques. On the basis of observations, strategies for moving towards automated interpretation of the partial discharge signals are proposed, which will make use of intelligent software techniques",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1709200,no
Fault Tolerant Control Research for High-Speed Maglev System with Sensor Failure,2006,"The faults of sensors occurred in Maglev train's suspension system reduce the running performance even disable the suspension system. Especially in the high-speed Maglev train, the loss is unpredictable. This paper focuses on the fault tolerant control problem of the suspension system. And the methods of control strategy reconstruction and state estimator were adopted. Based on the model of the simplified suspension system, the reconstructed controller and state estimator were designed. Furthermore, the simulated analysis for the static and dynamic levitation process was done by the software of Simulink. It is clearly observed that the FTC scheme can remain the static and dynamic performance in conformity with the original system. At last, the new FTC index used to judge the FTC problem of the unstable system is brought forwarded",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1712766,no
Continuous Measurement of Aggregate Size and Shape by Image Analysis of a Falling Stream,2006,"Quality control of aggregate is very important for mining, mineral processing and aggregate production. In last decade, several authors presented a system for automatic image analysis of aggregates on a moving conveyor belt. The test results showed that the system was capable of estimating size distributions of aggregates in real time, but could not give any information about shape(s) because of the difficulties of handling overlapping aggregates. As a consequence we initiated a study using an on-line computer system to analyse aggregate in a gravitational flow, i.e. a falling stream of particles. Compared with images from a moving conveyor belt, in gravity flow both aggregates and background appear more and less afflicted by overlaps. This, apart from alleviating the problems of overlapping, implies that the size and shape of aggregates can be measured more accurately. The system hardware includes a CCD camera, mounted at the end of a conveyor belt, an image frame grabber and a PC computer. The system software consists of image acquisition and selection, grey level image binarization, splitting of touching aggregates based on polygon approximation and size and shape analysis based on an oriented Feret box. The system has been tested in a quarry for aggregate production at Vasteras, Central Sweden",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1713360,no
Multi-function Intelligent Instrument of Vehicle Detection Based on Embedded System,2006,"By analyzing all kinds of application situation about speed and sideslip test-bed, a multi-function intelligent instrument of speed and sideslip detection based on embedded system was designed. Its operation principle, hardware constitution and design method of system software was described. It has such functions as data acquisition, data processing, fault diagnosis and etc. This instrument suits to three kinds of sensor. It is fit for different applications, and the application results indicate that it is not only a low cost and highly universal detection instrument of speed and sideslip, but also the only kind of small-scale multi-function instrument up until now",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1713372,no
Sonar Power Amplifier Testing System Based on Virtual Instrument,2006,"This paper presents an intelligent test system for power amplifiers by using virtual instrument technology. The automatic range switching circuit and Hall current transducers are applied in this system, realizing effectively the measurement of wide-voltage signal and large current. LabVIEW, a graphical programming language, and expert system technology are employed to develop testing software, implementing performance test of sonar power amplifier parts, and measurement of technique index and fault diagnosis. The proposed system has been used in the certain type sonar power amplifier system. The test results show that flexibility and data processing capacity of test instruments are improved upon greatly, which can satisfy more needs of measurement. And the proposed system can detect and locate the fault position quickly",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1714084,no
Utilizing Computational Intelligence in Estimating Software Readiness,2006,"Defect tracking using computational intelligence methods is used to predict software readiness in this study. By comparing predicted number of faults and number of faults discovered in testing, software managers can decide whether the software are ready to be released or not. Our predictive models can predict: (i) the number of faults (defects), (ii) the amount of code changes required to correct a fault and (iii) the amount of time (in minutes) to make the changes in respective object classes using software metrics as independent variables. The use of neural network model with a genetic training strategy is introduced to improve prediction results for estimating software readiness in this study. Existing object-oriented metrics and complexity software metrics are used in the Business Tier neural network based prediction model. New sets of metrics have been defined for the Presentation Logic Tier and Data Access Tier.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1716506,no
A Computational Intelligence Strategy for Software Complexity Prediction,2006,"The automated prediction of software module complexity using quantitative measures is a desirable goal in the area of software engineering. A computational intelligence based strategy, stochastic feature selection, is investigated as a classification system to determine the subset of software measures that yields the greatest predictive power for module complexity. This strategy stochastically examines subsets of software measures for predictive power. Its effectiveness is measured against a conventional artificial neural network benchmark.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1716756,no
"A High-Precision NDIR <formula formulatype=""inline""> <img src=""/images/tex/535.gif"" alt=""\hbox {CO}_{2}""> </formula> Gas Sensor for Automotive Applications",2006,"A new high-precision spectroscopic gas sensor measuring carbon dioxide (CO<sub>2</sub>) for harsh environmental conditions of automotive applications is presented. The carbon dioxide concentration is the primary parameter for sensing in cabin air quality, as well as an important safety parameter when R744 (carbon dioxide) is used as the refrigerant in the air conditioning system. The automotive environment challenges the potential sensor principles because of the wide temperature range from -40degC to +85degC, the atmospheric pressure from 700 to 1050 mbar, and relative humidity from 0% to 95%. The presented sensor system is based on the nondispersive infrared principle with new features for reaching high precision criteria and for enhancing long-term stability. A second IR source is used for internal recalibration of the primary IR source, redundancy purposes, and software plausibility checks. The CO<sub>2</sub> sensor system achieves an accuracy of better than plusmn5.5% over the whole temperature, pressure, and humidity ranges, with a resolution below 15 ppm and a response time shorter than 5 s. The operating time of the sensor system is more than 6000 h over a corresponding lifetime of more than 15 years. Experimental results show outstanding results for the intended automotive applications",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4014157,no
A Modular Software System to Assist Interpretation of Medical Imagesâ€”Application to Vascular Ultrasound Images,2006,"Improvements in medical imaging technology have greatly contributed to early disease detection and diagnosis. However, the accuracy of an examination depends on both the quality of the images and the ability of the physician to interpret those images. Use of output from computerized analysis of an image may facilitate the diagnostic tasks and, potentially improve the overall interpretation of images and the subsequent patient care. In this paper, Analysis, a modular software system designed to assist interpretation of medical images, is described in detail. Analysis allows texture and motion estimation of selected regions of interest (ROIs). Texture features can be estimated using first-order statistics, second-order statistics, Laws' texture energy, neighborhood gray-tone difference matrix, gray level difference statistics, and the fractal dimension. Motion can be estimated from temporal image sequences using block matching or optical flow. Image preprocessing, manual and automatic definition of ROIs, and dimensionality reduction and clustering using fuzzy c-means, are also possible within Analysis. An important feature of Analysis is the possibility for online telecollaboration between health care professionals under a secure framework. To demonstrate the applicability and usefulness of the system in clinical practice, Analysis was applied to B-mode ultrasound images of the carotid artery. Diagnostic tasks included automatic segmentation of the arterial wall in transverse sections, selection of wall and plaque ROIs in longitudinal sections, estimation of texture features in different image areas, motion analysis of tissue ROIs, and clustering of the extracted features. It is concluded that Analysis can provide a useful platform for computerized analysis of medical images and support of diagnosis",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4014737,no
Emulation of Software Faults: A Field Data Study and a Practical Approach,2006,"The injection of faults has been widely used to evaluate fault tolerance mechanisms and to assess the impact of faults in computer systems. However, the injection of software faults is not as well understood as other classes of faults (e.g., hardware faults). In this paper, we analyze how software faults can be injected (emulated) in a source-code independent manner. We specifically address important emulation requirements such as fault representativeness and emulation accuracy. We start with the analysis of an extensive collection of real software faults. We observed that a large percentage of faults falls into well-defined classes and can be characterized in a very precise way, allowing accurate emulation of software faults through a small set of emulation operators. A new software fault injection technique (G-SWFIT) based on emulation operators derived from the field study is proposed. This technique consists of finding key programming structures at the machine code-level where high-level software faults can be emulated. The fault-emulation accuracy of this technique is shown. This work also includes a study on the key aspects that may impact the technique accuracy. The portability of the technique is also discussed and it is shown that a high degree of portability can be achieved",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015509,no
User Interface Design for VCMMs: an Approach to Increase Fidelity and Usability,2006,"In recent years the declaration of uncertainties according to the ISO GUM (1995) has pushed the development of the so called Virtual Coordinate Measuring Machines - VCMMs as a software tool to generate uncertainty estimates for industrial applications. That is, a tool to calculate measurement uncertainty estimates based on real metrology information to be used in the context of quality systems. Recent tests on commercial VCMMs have shown that the fidelity of the simulation process to the real world is as important as the numerical correctness of uncertainty calculations. In this paper a VCMM interface design is proposed, focusing on techniques to increase the software fidelity and usability in industrial metrology applications. Results show that the quality of the simulation can be improved by proper user guidance as much as the software usability by the application of basic principles of software prototyping using the MVC model",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4016680,no
Improving a 3 Data-Source Diagnostic Method,2006,"In this paper, we present improvements to a diagnosis method for bridging faults combining three different data sources. The first data source is a set of I<sub>DDQ</sub> measurements used to identify the most probable fault type. The second source is a list of parasitic capacitances extracted from layout and used to create a list of realistic potential bridging fault sites. The third source is logical faults detectable at the primary outputs (including scan flip flops), used to limit the number of suspected gates. Combining these data significantly reduces the number of potential fault sites to consider in the diagnosis process. The modifications proposed in this paper allow the technique to be even more suitable for very large devices. Results obtained with different circuits are provided",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4016930,no
Weighted Proportional Sampling : AGeneralization for Sampling Strategies in Software Testing,2006,"Current activities to measure the quality of software products rely on software testing. The size and complexity of software systems make it almost impossible to perform complete coverage testing. During the past several years, many techniques to improve the test effectiveness (i.e., the ability to find faults) have been proposed to address this issue. Two examples of such strategies are random testing and partition testing. Both strategies follow an input domain sampling to perform the testing process. The procedure and assumptions for selecting these points seem to be different for both strategies: random testing considers only the probability of each sub-domain (i.e. uniform sampling) while partition testing considers only the sampling rate of each sub-domain (i.e., proportional sampling). This paper describes a more general sampling strategy, named weighted proportional sampling strategy. This strategy unifies both strategies into a general model that encompasses both of them as special cases. This paper also proposes an optimization model to determine the number of sampled points depending on the sampling strategy",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4017950,no
Estimating the Heavy-Tail Index for WWW Traces,2006,"Heavy-tailed behavior of WWW traffic has serious implications for the design and performance analysis of computer networks. This behavior gives rise to rare events which could be catastrophic for the QoS of an application. Thus, an accurate detection and quantification of the degree of thickness of a distribution is required. In this paper we detect and quantify the degree of tail-thickness for the file size and transfer times distributions of several WWW traffic traces. For accomplishing the above, the behavior of four estimators in real WWW traces characteristics is studied. We show that Hill-class estimators present varying degrees of accuracy and should be used as a first step towards the estimation of the tail-index. The QQ estimator, on the other hand, is shown to be more robust and adaptable, thus giving rise to more confident point estimates",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4018018,no
Developing an Effective Validation Strategy for Genetic Programming Models Based on Multiple Datasets,2006,"Genetic programming (GP) is a parallel searching technique where many solutions can be obtained simultaneously in the searching process. However, when applied to real-world classification tasks, some of the obtained solutions may have poor predictive performances. One of the reasons is that these solutions only match the shape of the training dataset, failing to learn and generalize the patterns hidden in the dataset. Therefore, unexpected poor results are obtained when the solutions are applied to the test dataset. This paper addresses how to remove the solutions which will have unacceptable performances on the test dataset. The proposed method in this paper applies a multi-dataset validation phase as a filter in GP-based classification tasks. By comparing our proposed method with a standard GP classifier based on the datasets from seven different NASA software projects, we demonstrate that the multi-dataset validation is effective, and can significantly improve the performance of GP-based software quality classification models",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4018495,no
An FPGA-Based Application-Specific Processor for Efficient Reduction of Multiple Variable-Length Floating-Point Data Sets,2006,"Reconfigurable computers (RCs) that combine generalpurpose processors with field-programmable gate arrays (FPGAs) are now available. In these exciting systems, the FPGAs become reconfigurable application-specific processors (ASPs). Specialized high-level language (HLL) to hardware description language (HDL) compilers allow these ASPs to be reconfigured using HLLs. In our research we describe a novel toroidal data structure and scheduling algorithm that allows us to use an HLL-to-HDL environment to implement a high-performance ASP that reduces multiple, variable-length sets of 64-bit floating-point data. We demonstrate the effectiveness of our ASP by using it to accelerate a sparse matrix iterative solver. We compare actual wall clock run times of a production-quality software iterative solver with an ASP-augmented version of the same solver on a current generation RC. Our ASP-augmented solver runs up to 2.4 times faster than software. Estimates show that this same design can run over 6.4 times faster on a next-generation RC.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4019536,no
Automated Information Aggregation for Scaling Scale-Resistant Services,2006,"Machine learning provides techniques to monitor system behavior and predict failures from sensor data. However, such algorithms are ""scale resistant"" $high computational complexity and not parallelizable. The problem then becomes identifying and delivering the relevant subset of the vast amount of sensor data to each monitoring node, despite the lack of explicit ""relevance"" labels. The simplest solution is to deliver only the ""closest"" data items under some distance metric. We demonstrate a better approach using a more sophisticated architecture: a scalable data aggregation and dissemination overlay network uses an influence metric reflecting the relative influence of one node's data on another, to efficiently deliver a mix of raw and aggregated data to the monitoring components, enabling the application of machine learning tools on real-world problems. We term our architecture level of detail after an analogous computer graphics technique",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4019558,no
Multilevel Modelling Software Development,2006,"Different from other engineering areas, the level of reuse in software engineering is very low. Also, developing large-scale applications which involve thousands of software elements such as classes and thousands of interactions among them is a complex and error-prone task. Industry currently lacks modelling practices and modelling tool support to tackle these issues. Model driven development (MDD) has emerged as an approach to diminishing software development complexity. We claim that models alone are not enough to tackle low reuse and complexity. Our contribution is a multilevel modelling development (MMD) framework whereby models are defined at different abstraction levels. A modelling level is constructed out by assembling software elements defined at the adjacent lower-level. MMD effectively diminish development complexity and facilitates large-scale reuse",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4019748,no
PAFBV: A Novel Parallel Aggregated and Folded Bit Vector Packet Classification Scheme for IPv6 Routers,2006,"Packet classification is a central function for a number of network applications, such as routing and firewalls. Most existing algorithms for packet classification scale poorly in either time or space when the databases grow in size. The scalable algorithm Aggregated Bit Vector (ABV) is an improvement on the Lucent bit vector scheme (BV), but has some limitations such as large variance in performance, rule mapping back and preprocessing cost. Our paradigm, Parallel Aggregated and Folded bit vector (PAFBV) seeks to reduce false matches while keeping the benefits of bit vector aggregation and avoiding rearrangement. This model also uses multi-ary trie structures to reduce the seek time of bit vectors and thereby increasing the speed of packet classification. The objective of this paper is to propose a scalable packet classification algorithm with increased speed for even large database size for IPv6 addresses",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4019922,no
Risk: A Good System Security Measure,2006,"What gets measured gets done. Security engineering as a discipline is still in its infancy. The field is hampered by its lack of adequate measures of goodness. Without such a measure, it is difficult to judge progress and it is particularly difficult to make engineering trade-off decisions when designing systems. The qualities of a good metric include that it: (1) measures the right thing, (2) is quantitatively measurable, (3) can be measured accurately, (4) can be validated against ground truth, and (5) be repeatable. By ""measures the right thing"", the author means that it measures some set of attributes that directly correlates to closeness to meeting some stated goal. For system security, the author sees the right goal as ""freedom from the possibility of suffering damage or loss from malicious attack."" Damage or loss applies to the mission effectiveness of the information infrastructure of a system. The mission can be maximizing profits while making quality cars or it could be defending an entire nation against foreign incursion",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020053,no
On the Distribution of Property Violations in Formal Models: An Initial Study,2006,"Model-checking techniques are successfully used in the verification of both hardware and software systems of industrial relevance. Unfortunately, the capability of current techniques is still limited and the effort required for verification can be prohibitive (if verification is possible at all). As a complement, fast, but incomplete, search tools may provide practical benefits not attainable with full verification tools, for example, reduced need for manual abstraction and fast detection of property violations during model development. In this report we investigate the performance of a simple random search technique. We conducted an experiment on a production-sized formal model of the mode-logic of a flight guidance system. Our results indicate that random search quickly finds the vast majority of property violations in our case-example. In addition, the times to detect various property violations follow an acutely right-skewed distribution and are highly biased toward the easy side. We hypothesize that the observations reported here are related to the phase transition phenomenon seen in Boolean satisfiability and other NP-complete problems. If so, these observations could be revealing some of the fundamental aspects of software (model) faults and have implications on how software engineering activities, such as analysis, testing, and reliability modeling, should be performed",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020073,no
Scale Free in Software Metrics,2006,"Software has become a complex piece of work by the collective efforts of many. And it is often hard to predict what the final outcome will be. This transition poses new challenge to the software engineering (SE) community. By employing methods from the study of complex network, we investigate the object oriented (OO) software metrics from a different perspective. We incorporate the weighted methods per class (WMC) metric into our definition of the weighted OO software coupling network as the node weight. Empirical results from four open source OO software demonstrate power law distribution of weight and a clear correlation between the weight and the out degree. According to its definition, it suggests uneven distribution of function among classes and a close correlation between the functionality of a class and the number of classes it depending on. Further experiment shows similar distribution also exists between average LCOM and WMC as well as out degree. These discoveries will help uncover the underlying mechanisms of software evolution and will be useful for SE to cope with the emerged complexity in software as well as efficient test cases design",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020082,no
Proportional Intensity-Based Software Reliability Modeling with Time-Dependent Metrics,2006,"The black-box approach based on stochastic software reliability models is a simple methodology with only software fault data in order to describe the temporal behavior of fault-detection processes, but fails to incorporate some significant development metrics data observed in the development process. In this paper we develop proportional intensity-based software reliability models with time-dependent metrics, and propose a statistical framework to assess the software reliability with the time-dependent covariate as well as the software fault data. The resulting models are similar to the usual proportional hazard model, but possess somewhat different covariate structure from the existing one. We compare these metrics-based software reliability models with some typical non-homogeneous Poisson process models, which are the special cases of our models, and evaluate quantitatively the goodness-of-fit from the viewpoint of information criteria. As an important result, the accuracy on reliability assessment strongly depends on the kind of software metrics used for analysis and can be improved by incorporating the time-dependent metrics data in modeling",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020098,no
Reference Models and Automatic Oracles for the Testing of Mesh Simplification Software for Graphics Rendering,2006,"Software with graphics rendering is an important class of applications. Many of them use polygonal models to represent the graphics. Mesh simplification is a vital technique to vary the levels of object details and, hence, improve the overall performance of the rendering process. It progressively enhances the effectiveness of rendering from initial reference systems. As such, the quality of its implementation affects that of the associated graphics rendering application. Testing of mesh simplification is essential towards assuring the quality of the applications. Is it feasible to use the reference systems to serve as automated test oracles for mesh simplification programs? If so, how well are they useful for this purpose? We present a novel approach in this paper. We propose to use pattern classification techniques to address the above problem. We generate training samples from the reference system to test samples from the implementation. Our experimentation shows that the approach is promising",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020105,no
A Technique to Reduce the Test Case Suites for Regression Testing Based on a Self-Organizing Neural Network Architecture,2006,"This paper presents a technique to select subsets of the test cases, reducing the time consumed during the evaluation of a new software version and maintaining the ability to detect defects introduced. Our technique is based on a model to classify test case suites by using an ART-2A self-organizing neural network architecture. Each test case is summarized in a feature vector, which contains all the relevant information about the software behavior. The neural network classifies feature vectors into clusters, which are labeled according to software behavior. The source code of a new software version is analyzed to determine the most adequate clusters from which the test case subset will be selected. Experiments compared feature vectors obtained from all-uses code coverage information to a random selection approach. Results confirm the new technique has improved the precision and recall metrics adopted",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020148,no
ARIMAmmse: An Improved ARIMA-based,2006,"Productivity is a critical performance index of process resources. As successive history productivity data tends to be auto-correlated, time series prediction method based on auto-regressive integrated moving average (ARIMA) model was introduced into software productivity prediction by Humphrey et al. In this paper, a variant of their prediction method named ARIMAmmse is proposed. This variant formulates the ARIMA parameter estimation issue as a minimum mean square error (MMSE) based constrained optimization problem. The ARIMA model is used to describe constraints of the parameter estimation problem, while MMSE is used as the objective function of the constrained optimization problem. According to the optimization theory, ARIMAmmse will definitely achieve a higher MMSE prediction precision than Humphrey et al's which is based on the Yule-Walk estimation technique. Two comparative experiments are also presented. The experimental results further confirm the theoretical superiority of ARIMAmmse",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020157,no
Automated Health-Assessment of Software Components using Management Instrumentatio,2006,"Software components are regularly reused in many large-scale, mission-critical systems where the tolerance for poor performance is quite low. As new components are integrated within an organization's computing infrastructure, it becomes critical to ensure that these components continue to meet the expected quality of service (QoS) requirements. Management instrumentation is an integrated capability of a software system that enables an external entity to assess that system's internals, such as its operational states, execution traces, and various quality attributes during runtime. In this paper, we present an approach that enables the efficient generation, measurement, and assessment of various QoS attributes of software components during runtime using management instrumentation. Monitoring the quality of a component in this fashion has many benefits, including the ability to proactively detect potential QoS-related issues within a component to avoid potentially expensive downtime of the overall environment. The main contributions of our approach consist of three parts: a lightweight component instrumentation framework that transparently generates a pre-defined set of QoS-related diagnostic data when integrated within a component, a method to formally define the health state of a component in terms of the expected QoS set forth by the target environment, and finally a method for publishing the QoS-related diagnostic data during runtime so that an external entity can measure the current health of a component and take appropriate actions. The main QoS types that we consider are: performance, reliability, availability, throughput, and resource usage. Experimentation results show that our approach can be efficiently utilized in large mission-critical systems",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020164,no
Consensus ontology generation in a socially interacting multiagent system,2006,"This paper presents an approach for building consensus ontologies from the individual ontologies of a network of socially interacting agents. Each agent has its own conceptualization of the world. The interactions between agents are modeled by sending queries and receiving responses and later assessing each other's performance based on the results. This model enables us to measure the quality of the societal beliefs in the resources which we represent as the expertise in each domain. The dynamic nature of our system allows us to model the emergence of consensus that mimics the evolution of language. We present an algorithm for generating the consensus ontologies which makes use of the authoritative agent's conceptualization in a given domain. As the expertise of agents change after a number of interactions, the consensus ontology that we build based on the agents' individual views evolves. We evaluate the consensus ontologies by using different heuristic measures of similarity based on the component ontologies",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020180,no
QoS-Based Service Composition,2006,"QoS has been one of the major challenges in Web services area. Though negotiated in the contract, service quality usually can not be guaranteed by providers. Therefore, the service composer is obligated to detect real quality status of component services. Local monitoring cannot fulfil this task. We propose a Probe-based architecture to address this problem. By running light weighted test cases, the Probe can collect accurate quality data to support runtime service composition and composition re-planning",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020196,no
Measurement-Based Analysis: A Key to Experimental Research in Dependability,2006,"This paper reflects on years of our experience in measurement-based dependability analysis of operational systems. In particular, the discussion brings examples illustrating a key importance of operational data analysis in designing and experimental assessment of dependable computing systems",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020834,no
Dynamic Derivation of Application-Specific Error Detectors and their Implementation in Hardware,2006,"This paper proposes a novel technique for preventing a wide range of data errors from corrupting the execution of applications. The proposed technique enables automated derivation of fine-grained, application-specific error detectors. An algorithm based on dynamic traces of application execution is developed for extracting the set of error detector classes, parameters, and locations in order to maximize the error detection coverage for a target application. The paper also presents an automatic framework for synthesizing the set of detectors in hardware to enable low-overhead runtime checking of the application execution. Coverage (evaluated using fault injection) of the error detectors derived using the proposed methodology, the additional hardware resources needed, and performance overhead for several benchmark programs are also reported",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020839,no
Rephrasing Rules for Off-The-Shelf SQL Database Servers,2006,"We have reported previously (Gashi et al., 2004) results of a study with a sample of bug reports from four off-the-shelf SQL servers. We checked whether these bugs caused failures in more than one server. We found that very few bugs caused failures in two servers and none caused failures in more than two. This would suggest a fault-tolerant server built with diverse off-the-shelf servers would be a prudent choice for improving failure detection. To study other aspects of fault tolerance, namely failure diagnosis and state recovery, we have studied the ""data diversity"" mechanism and we defined a number of SQL rephrasing rules. These rules transform a client sent statement to an additional logically equivalent statement, leading to more results being returned to an adjudicator. These rules therefore help to increase the probability of a correct response being returned to a client and maintain a correct state in the database",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020843,no
Model-Based Testing of Community-Driven Open-Source GUI Applications,2006,"Although the World-Wide-Web (WWW) has significantly enhanced open-source software (OSS) development, it has also created new challenges for quality assurance (QA), especially for OSS with a graphical-user interface (GUI) front-end. Distributed communities of developers, connected by the WWW, work concurrently on loosely-coupled parts of the OSS and the corresponding GUI code. Due to the unprecedented code churn rates enabled by the WWW, developers may not have time to determine whether their recent modifications have caused integration problems with the overall OSS; these problems can often be detected via GUI integration testing. However, the resource-intensive nature of GUI testing prevents the application of existing automated QA techniques used during conventional OSS evolution. In this paper we develop new process support for three nested techniques that leverage developer communities interconnected by the WWW to automate model-based testing of evolving GUI-based OSS. The ""innermost"" technique (crash testing) operates on each code check-in of the GUI software and performs a quick and fully automatic integration test. The second technique {smoke testing) operates on each day's GUI build and performs functional ""reference testing"" of the newly integrated version of the GUI. The third (outermost) technique (comprehensive GUI testing) conducts detailed integration testing of a major GUI release. An empirical study involving four popular OSS shows that (1) the overall approach is useful to detect severe faults in GUI-based OSS and (2) the nesting paradigm helps to target feedback and makes effective use of the WWW by implicitly distributing QA",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021332,no
Regression Testing UML Designs,2006,"As model driven architectures (MDAs) gain in popularity, several techniques that test the UML models have been proposed. These techniques aim at early detection and correction of faults to reduce the overall cost of correcting them later in the software life-cycle. Recently, Pilskalns et al., 2003 proposed an approach to test the UML design models to check for inconsistencies. They create an aggregate model which merges information from class diagrams, sequence diagrams and OCL statements, then generate test cases to identify inconsistencies. Since designs change often in the early stages of the software life-cycle, we need a regression testing approach that can be performed on the UML model. By classifying design changes, and then further classifying the test cases, we provide a set of rules about how to reuse part of the existing test cases, and generate new ones to ensure all affected parts of the system are tested adequately. The approach is a safe and efficient selective retest strategy. A case-study is reported to demonstrate the benefits",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021344,no
The Conceptual Coupling Metrics for Object-Oriented Systems,2006,"Coupling in software has been linked with maintainability and existing metrics are used as predictors of external software quality attributes such as fault-proneness, impact analysis, ripple effects of changes, changeability, etc. Many coupling measures for object-oriented (OO) software have been proposed, each of them capturing specific dimensions of coupling. This paper presents a new set of coupling measures for OO systems - named conceptual coupling, based on the semantic information obtained from the source code, encoded in identifiers and comments. A case study on open source software systems is performed to compare the new measures with existing structural coupling measures. The case study shows that the conceptual coupling captures new dimensions of coupling, which are not captured by existing coupling measures; hence it can be used to complement the existing metrics",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021375,no
Towards Portable Metrics-based Models for Software Maintenance Problems,2006,"The usage of software metrics for various purposes has become a hot research topic in academia and industry (e.g. detecting design patterns and bad smells, studying change-proneness, quality and maintainability, predicting faults). Most of these topics have one thing in common: they are all using some kind of metrics-based models to achieve their goal. Unfortunately, only few researchers have tested these models on unknown software systems so far. This paper tackles the question, which metrics are suitable for preparing portable models (which can be efficiently applied to unknown software systems). We have assessed several metrics on four large software systems and we found that the well-known RFC and WMC metrics differentiate the analyzed systems fairly well. Consequently, these metrics cannot be used to build portable models, while the CBO, LCOM and LOC metrics behave similarly on all systems, so they seem to be suitable for this purpose",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021377,no
A Method for an Accurate Early Prediction of Faults in Modified Classes,2006,"In this paper we suggest and evaluate a method for predicting fault densities in modified classes early in the development process, i.e., before the modifications are implemented. We start by establishing methods that according to literature are considered the best for predicting fault densities of modified classes. We find that these methods can not be used until the system is implemented. We suggest our own methods, which are based on the same concept as the methods suggested in the literature, with the difference that our methods are applicable before the coding has started. We evaluate our methods using three large telecommunication systems produced by Ericsson. We find that our methods provide predictions that are of similar quality to the predictions based on metrics available after the code is implemented. Our predictions are, however, available much earlier in the development process. Therefore, they enable better planning of efficient fault prevention and fault detection activities",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021378,no
Delay Time Identification and Dynamic Characteristics Study on ANN Soft Sensor,2006,"Soft sensor software based on ANN (artificial neural network) using BP or RBF was developed to estimate unmeasured variables such as product quality online. Some important topics including how to determine the delay time, how to simulate the dynamic system were discussed and solved. We applied a 3 layers BP network to identify the delay time of nonlinear system, feedback output variables to input layer, and weight of all the input variables to describe dynamic characteristics of the system. This makes the ANN soft sensor reflect truly both the static and dynamic characteristics of the system and provide more adaptability",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021406,no
Notice of Violation of IEEE Publication Principles<BR>An Artificial Immune Recognition System-based Approach to Software Engineering Management: with Software Metrics Selection,2006,"Notice of Violation of IEEE Publication Principles<BR><BR>""An Artificial Immune Recognition System-based Approach to Software Engineering Management: with Software Metrics Selection""<BR>by Xin Jin, Rongfang Bie, and X.Z. Gao<BR>in the Proceedings of the Sixth International Conference on Intelligent Systems Design and Applications, 2006, pp. 523-528<BR><BR>After careful and considered review of the content and authorship of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>This paper improperly paraphrased portions of original text from the paper cited below. The original text was paraphrased without attribution (including appropriate references to the original author(s) and/or paper title) and without permission.<BR><BR>Due to the nature of this violation, reasonable effort should be made to remove all past references to this paper, and future references should be made to the following article:<BR><BR>""Artificial Immune Recognition System (AIRS): A Review and Analysis""<BR>by Jason Brownlee,<BR>in Technical Report 1-02, Center for Intelligent Systems and Complex Processes (CISCP), Faculty of Information and Communication Technologies, Swinburne University of Technology, January 2005Artificial immune systems (AIS) are emerging machine learners, which embody the principles of natural immune systems for tackling complex real-world problems. The artificial immune recognition system (AIRS) is a new kind of supervised learning AIS. Improving the quality of software products is one of the principal objectives of software engineering. It is well known that software metrics are the key tools in the software quality management. In this paper, we propose an AIRS-based method for software quality classification. We also compare our scheme with other conventional classification techniques. In addition, the gain ratio is employed to select relevant software metrics for classifiers. Results on t- he MDP benchmark dataset using the error rate (ER) and average sensitivity (AS) as the performance measures demonstrate that the AIRS is a promising method for software quality classification and the gain ratio-based metrics selection can considerably improve the performance of classifiers",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021493,no
Empirically Validating Software Metrics for Risk Prediction Based on Intelligent Methods,2006,"The software systems which are related to national projects are always very crucial. This kind of systems always involves hi-tech factors and has to spend a large amount of money, so the quality and reliability of the software deserve to be further studied. Hence, we propose to apply three classification techniques most used in data mining fields: Bayesian belief networks (BBN), nearest neighbor (NN) and decision tree (DT), to validate the usefulness of software metrics for risk prediction. Results show that comparing with metrics such as Lines of code (LOQ and Cyclomatic complexity (V(G)) which are traditionally used for risk prediction, Halstead program difficulty (D), Number of executable statements (EXEC) and Halstead program volume (V) are the more effective metrics as risk predictors. By analyzing we also found that BBN was more effective than the other two methods in risk prediction",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021584,no
Stabilization Time - A Quality Metric for Software Products,2006,"In software products, often the failure rate decreases after installation, eventually reaching a steady state. The time it takes for a product to reach its steady state reliability depends on different product parameters. In this paper we propose a new metric for software products called stabilization time which is the time taken after installation for the reliability of the product to stabilize. This metric can be used for comparing products, and can be useful for organizations and individuals using the product as well as for the product vendor. We also present an approach for determining the stabilization time of a product from its failure and sales data. We apply the approach to three real life products using their failure and sales data after release",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021970,no
Metrics-Based Software Reliability Models Using Non-homogeneous Poisson Processes,2006,"The traditional software reliability models aim to describe the temporal behavior of software fault-detection processes with only the fault data, but fail to incorporate some significant test-metrics data observed in software testing. In this paper we develop a useful modeling framework to assess the quantitative software reliability with time-dependent covariate as well as software-fault data. The basic ideas employed here are to introduce the discrete proportional hazard model on a cumulative Bernoulli trial process, and to represent a generalized fault-detection processes having time-dependent covariate structure. The resulting stochastic models are regarded as combinations of the proportional hazard models and the familiar non-homogeneous Poisson processes. We compare these metrics-based software reliability models with some typical non-homogeneous Poisson process models, and evaluate quantitatively both goodness-of-fit and predictive performances from the viewpoint of information criteria. As an important result, the accuracy on reliability assessment strongly depends on the kind of software metrics used for analysis and can be improved by incorporating time-dependent metrics data in modeling",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021971,no
"Studying the Characteristics of a ""Good"" GUI Test Suite",2006,"The widespread deployment of graphical-user interfaces (GUIs) has increased the overall complexity of testing. A GUI test designer needs to perform the daunting task of adequately testing the GUI, which typically has very large input interaction spaces, while considering tradeoffs between GUI test suite characteristics such as the number of test cases (each modeled as a sequence of events), their lengths, and the event composition of each test case. There are no published empirical studies on GUI testing that a GUI test designer may reference to make decisions about these characteristics. Consequently, in practice, very few GUI testers know how to design their test suites. This paper takes the first step towards assisting in GUI test design by presenting an empirical study that evaluates the effect of these characteristics on testing cost and fault detection effectiveness. The results show that two factors significantly effect the fault-detection effectiveness of a test suite: (1) the diversity of states in which an event executes and (2) the event coverage of the suite. Test designers need to improve the diversity of states in which each event executes by developing a large number of short test cases to detect the majority of ""shallow"" faults, which are artifacts of modern GUI design. Additional resources should be used to develop a small number of long test cases to detect a small number of ""deep"" faults",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021981,no
Assessing the Relationship between Software Assertions and Faults: An Empirical Investigation,2006,"The use of assertions in software development is thought to help produce quality software. Unfortunately, there is scant empirical evidence in commercial software systems for this argument to date. This paper presents an empirical case study of two commercial software components at Microsoft Corporation. The developers of these components systematically employed assertions, which allowed us to investigate the relationship between software assertions and code quality. We also compare the efficacy of assertions against that of popular bug finding techniques like source code static analysis tools. We observe from our case study that with an increase in the assertion density in a file there is a statistically significant decrease in fault density. Further, the usage of software assertions in these components found a large percentage of the faults in the bug database",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021986,no
Testing During Refactoring: Adding Aspects to Legacy Systems,2006,"Moving program code that implements cross-cutting concerns into aspects can improve the maintainability of legacy systems. This kind of refactoring, called aspectualization, can also introduce faults into a system. A test driven approach can identify these faults during the refactoring process so that they can be removed. We perform systematic testing as we aspectualize commercial VLSI CAD applications. The process of refactoring these applications revealed the kinds of faults that can arise during aspectualization, and helped us to develop techniques to reduce their occurrences",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021988,no
Model Simulation for Test Execution Capacity Estimation,2006,"It is important for a company to know if it is able to attend the clients' requests on time and according to their expectations, mainly in competitive markets. A usual activity performed to ensure quality is the test activity. Software testing is been considered so important that organizations can allocate teams exclusively for test activities. In this context, it is important that test managers know the capacity of their test team. This paper defines a capacity model for test execution teams and presents the use of simulation technique for estimating teams' capacities for executing tests in mobile applications. We also report our experiences using this model in an organizational setting",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021989,no
On the Effect of Fault Removal in Software Testing - Bayesian Reliability Estimation Approach,2006,"In this paper, we propose some reliability estimation methods in software testing. The proposed methods are based on the familiar Bayesian statistics, and can be characterized by using test outcomes in input domain models. It is shown that the resulting approaches are capable of estimating software reliability in the case where the detected software faults are removed. In numerical examples, we compare the proposed methods with the existing method, and investigate the effect of fault removal on the reliability estimation in software testing. We show that the proposed methods can give more accurate estimates of software reliability",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021991,no
"Reliability and Performance of Component Based Software Systems with Restarts, Retries, Reboots and Repairs",2006,"High reliability and performance are vital for software systems handling diverse mission critical applications. Such software systems are usually component based and may possess multiple levels of fault recovery. A number of parameters, including the software architecture, behavior of individual components, underlying hardware, and the fault recovery measures, affect the behavior of such systems, and there is a need for an approach to study them. In this paper we present an integrated approach for modeling and analysis of component based systems with multiple levels of failures and fault recovery both at the software, as well as the hardware level. The approach is useful to analyze attributes such as overall reliability, performance, and machine availabilities for such systems, wherein failures may happen at the software components, the operating system, or at the hardware, and corresponding restarts, retries, reboots or repairs are used for mitigation. Our approach encompasses Markov chain, and queueing network modeling, for estimating system reliability, machine availabilities and performance. The approach is helpful for designing and building better systems and also while improving existing systems",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021996,no
Performance Evaluation of Contour Determination Algorithm Using Conventional and Compounded Ultrasound Medical Images,2006,"Performance evaluation of contour determination algorithm using both conventional ultrasound images and ultrasound images, generated by multi-angle compounding imaging (MACI) is presented. A method for modelling different contour shapes with known true positions and a method for error estimation are used to compare the results in both cases. A probabilistic data association combined with interacting multiple model (IMMPDA) approach is used for contour determination algorithm. The results are presented in figures and show that the contour determination algorithm works more robust and more precisely with compound images",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4022048,no
Automatic Usability Evaluation Using AOP,2006,Usability is one of the most important qualities of a software system. Designing for usability is a complex task and sometimes even expensive. Automatic usability evaluation can ease the evaluation process. In this paper we try to analyze where and how can aspect oriented programming be used to develop modules to support automatic usability evaluation. A small family budget application is used to test the module we have designed and implemented using aspect oriented programming,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4022928,no
Data acquisition equipment for measurement of the parameters for electric energy quality,2006,"The paper presents the equipment used to measure some parameters that characterize the electric energy quality. The proposed equipment performs test and acquisition of analogue data (U and I) and numerical data. The sampled data are recorded when preset thresholds are exceeded by the analogical inputs or when the digital inputs states change. The fixed variant is supplementary provided with 2 analogue outputs and 8 numerical outputs. The operation of equipment is simulated and the corresponding software are exemplified for the case of a highly distorting consumer, a set of electric energy quality parameters being determined for this case",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4022935,no
An Approach for Evaluating Trust in IT Infrastructure,2006,Trustworthiness of an IT infrastructure can be justified using the concept of trust case which denotes a complete and explicit structure encompassing all the evidence and argumentation supporting trust within a given context. A trust case is developed by making an explicit set of claims about the system of interest and showing how the claims are interrelated and supported by evidence. The approach uses Dempster-Shafer belief function framework to quantify the trust case. We demonstrate how recommendations issued by different stakeholders enable stakeholder-specific views of the trust case and reasoning about the level of trust in a given IT infrastructure,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4024037,no
Measurement Techniques in On-Demand Overlays for Reliable Enterprise IP Telephony,2006,"Maintaining good quality of service for real-time applications like IP Telephony requires quick detection and reaction to network impairments. In this paper, we propose and study novel measurement techniques in ORBIT, which is a simple, easily deployable architecture that uses single-hop overlays implemented with intelligent endpoints and independent relays. The measurement techniques provide rapid detection and recovery of IP Telephony during periods of network trouble. We study our techniques via detailed simulations of several multi-site enterprise topologies of varying sizes and three typical fault models. We show that our proposed techniques can detect network impairments rapidly and rescue IP Telephony calls in sub-second intervals. We observed that all impacted calls were rescued with only a few relays in the network and the run-time overhead was low. Furthermore, the relay sites needed to be provisioned with minimal additional bandwidth to support the redirected calls.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4024198,no
Modeling and Performance Analysis of Beyond 3G Integrated Wireless Networks,2006,"Next-generation wireless networking is evolving towards a multi-service heterogeneous paradigm that converges different pervasive access technologies and provides a large set of novel revenue generating applications. Hence, system complexity increases due to its embedded heterogeneity, which can not be accounted by the existing modeling and performance evaluation techniques. Consequently, the development of new modeling approaches becomes as a crucial requirement for proper system design and performance evaluation. This paper presents a novel mobility model for a two-tier integrated wireless system using a new modeling approach that accommodates the aforementioned complexity. Additionally, a novel session model is developed as an adapted version of the proposed mobility model. These models use phase-type distributions that are known to approximate any generic probability laws. Using the proposed session model, a novel generic analytical framework is developed to obtain several salient performance metrics such as network utilization times and handoff rates. Simulation and analysis results prove the proposed model validity and demonstrate the accuracy of the novel modeling approach when compared with traditional modeling techniques.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4024417,no
Performance Analysis and Enhancement for Priority Based IEEE 802.11 Network,2006,"In this paper, a novel non-saturation analytical model for priority based IEEE 802.11 network is introduced. Unlike previous work that is focused on MAC backoff for saturation stations, this model uses Markov and M/ M/1/K theories to predict MAC and queuing service time and loss. Then a performance prediction based enhancement scheme is proposed. By dynamic tuning of protocol options, this proposed scheme limits end-to-end delay and loss rate of real-time traffic and maximizes throughput. Consequently, call admission control is taken to protect existing traffics when the channel is saturated. Simulations validate this model and the comparison with IEEE802.11e EDCA shows that our mechanism can guarantee quality of service more efficiently.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4024792,no
Field Data-based Study on Electric Arc Furnace Flicker Mitigation,2006,"As an industry customer of utility, electrical arc furnace (EAF), acting as a varying and reactive power sink, is the major flicker source to influence the grid power quality. In this paper, based on the field data recorded at the electric system surrounding EAF, the flicker issues and reactive power compensation requirement are quantitatively analyzed. The possible compensation solutions, including passive filter, SVC (static Var compensator), STATCOM (static synchronous compensator) are discussed. Furthermore, a field data-based EAF model is proposed and implemented in the simulation software. The fast-bandwidth STATCOM solution with or without capacitor bank are designed and implemented. With the validated EAF and STATCOM model, the extensive simulations are conducted to study compensator performance and cost-effectiveness for EAF flicker mitigation. Finally, the guideline for compensator selection and performance estimation is obtained",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4025198,no
Development of a Controller Virtual Maintenance System Through the Internet,2006,"There is a great deal of benefits for control systems to adopt the Internet to controller maintenance. This paper aims at developing a controller virtual maintenance system via the Internet to solve controller performance monitoring, controller fault detection and remote upgrading controller issues. The basic model of Internet-based virtual maintenance system has been proposed, which is very suitable for remote monitoring and controller maintenance applications. Several essential implementation issues of the system, including system architecture, network communication architecture and Web-based user interface, have also been discussed. A water tank system with a simple PID controller has been used as a case study to illustrate the solution for the remote controller maintenance",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4026124,no
Research on Test-platform and Condition Monitoring Method for AUV,2006,"To improve the reliability and intelligence of autonomous underwater vehicle, an AUV test-platform named ""Beaver"" is developed. The hardware and software system structure are introduced in detail. By analyzing the performance and the fault mechanism of thruster, it establishes the condition monitoring system for thrusters and sensors based on the double closed-loop PID controller, which includes the performance model of thruster based on RBF neural network and forward model of AUV based on improved dynamic recursive Elman neural network, and it probes into the method of combine fault detection. The results of experiment indicate that the ""Beaver"" can achieve the basic motion control and meet the requirement in test, and the combine fault detection method by parallel connected performance model and forward model can detect the typical fault of thrusters and sensors, which certificates the reliability and the effectiveness of condition monitoring system",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4026343,no
Validating Requirements Engineering Process Improvements - A Case Study,2006,"The quality of the Requirements Engineering (RE) process plays a critical role in successfully developing software systems. Often, in software organizations, RE processes are assessed and improvements are applied to overcome their deficiency. However, such improvements may not yield desired results for two reasons. First, the assessed deficiency may be inaccurate because of ambiguities in measurement. Second, the improvements are not validated to ascertain their correctness to overcome the process deficiency. Therefore, a Requirements Engineering Process Improvement (REPI) exercise may fail to establish its purpose. A major shortfall in validating RE processes is the difficulty in representing process parameters in some cognitive form. We address this issue with an REPI framework that has both measurement and visual validation properties. The REPI validation method presented is empirically tested based on a case study in a large software organization. The results are promising towards considering this REPI validation method in practice by organizations.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4026790,no
Bug Classification Using Program Slicing Metrics,2006,"In this paper, we introduce 13 program slicing metrics for C language programs. These metrics use program slice information to measure the size, complexity, coupling, and cohesion properties of programs. Compared with traditional code metrics based on code statements or code structure, program slicing metrics involve measures for program behaviors. To evaluate the program slicing metrics, we compare them with the Understand for C++ suite of metrics, a set of widely-used traditional code metrics, in a series of bug classification experiments. We used the program slicing and the Understand for C++ metrics computed for 887 revisions of the Apache HTTP project and 76 revisions of the Latex2rtf project to classify source code files or functions as either buggy or bug-free. We then compared their classification prediction accuracy. Program slicing metrics have slightly better performance than the Understand for C++ metrics in classifying buggy/bug-free source code. Program slicing metrics have an overall 82.6% (Apache) and 92% (Latex2rtf) accuracy at the file level, better than the Understand for C++ metrics with an overall 80.4% (Apache) and 88% (Latex2rtf) accuracy. The experiments illustrate that the program slicing metrics have at least the same bug classification performance as the Understand for C++ metrics.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4026853,no
Contextual Spectrums in Technology and Information Platform,2006,"Summary form only given. The technology and information platform (TIP) is a three-dimensional architecture framework to effectively cope with the architecture complexity and manage the architectural assets of information system applications in a service-oriented paradigm. This holistic framework comprises the generic architecture stack (GAS), which is made up of a series of architecture layers, and the contextual spectrums, which consist of the process, abstraction, latitude, and maturity (PALM) dimensions. This paper describes the detailed attributes in the four dimensions of the contextual spectrums. The process dimension covers operations, risk, financial, resources, estimation, planning, execution, policies, governance, compliance, organizational politics, etc. The abstraction dimension deals with what, why, who, where, when, which and how (6W+1H). The latitude dimension includes principles, functional, logical, physical, interface, integration & interoperability, access & delivery, security, quality of services, patterns, standards, tools, skills, and so forth. Finally the maturity dimension is about performance, metrics, competitive assessment, scorecards, capacity maturity, benchmarks, service management, productivity, gap analysis, transition, etc",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4026972,no
Performance Modeling of WS-BPEL-Based Web Service Compositions,2006,This paper addresses quality of service aspects of Web service orchestrations created using WS-BPEL from the standpoint of a Web service integrator. A mathematical model based on operations research techniques and formal semantics of WS-BPEL is proposed to estimate and forecast the influence of the execution of orchestrated processes on utilization and throughput of individual involved nodes and of the whole system. This model is applied to the optimization of service levels agreement process between the involved parties,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4027028,no
Modeling Request Routing in Web Applications,2006,"For Web applications, determining how requests from a Web page are routed through server components can be time-consuming and error-prone due to the complex set of rules and mechanisms used in a platform such as J2EE. We define request routing to be the possible sequences of server-side components that handle requests. Many maintenance tasks require the developer to understand the request routing, so this complexity increases maintenance costs. However, viewing this problem at the architecture level provides some insight. The request routing in these Web applications is an example of a pipeline architectural pattern: each request is processed by a sequence of components that form a pipeline. Communication between pipeline stages is event-based, which increases flexibility but obscures the pipeline structure because communication is indirect. Our approach for improving the maintainability of J2EE Web applications is to provide a model that exposes this architectural information. We use Z to formally specify request routing models and analysis operations that can be performed on them, then provide tools to extract request routing information from an application's source code, create the request routing model, and analyze it automatically. We have applied this approach to a number of existing applications up to 34K LOC, showing improvement via typical maintenance scenarios. Since this particular combination of patterns is not unique to Web applications, a model such as our request routing model could provide similar benefits for these systems",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4027212,no
An Efficient Test Pattern Selection Method for Improving Defect Coverage with Reduced Test Data Volume and Test Application Time,2006,"Testing using n-detection test sets, in which a fault is detected by n (n > 1) input patterns, is being increasingly advocated to increase defect coverage. However, the data volume for an n-detection test set is often too large, resulting in high testing time and tester memory requirements. Test set selection is necessary to ensure that the most effective patterns are chosen from large test sets in a high-volume production testing environment. Test selection is also useful in a time-constrained wafer-sort environment. The authors use a probabilistic fault model and the theory of output deviations for test set selection - the metric of output deviation is used to rank candidate test patterns without resorting to fault grading. To demonstrate the quality of the selected patterns, experimental results were presented for resistive bridging faults and non-feedback zero-resistance bridging faults in the ISCAS benchmark circuits. Our results show that for the same test length, patterns selected on the basis of output deviations are more effective than patterns selected using several other methods",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4030788,no
Design Structural Stability Metrics and Post-Release Defect Density: An Empirical Study,2006,"This paper empirically explores the correlations between a suite of structural stability metrics for object-oriented designs and post-release defect density. The investigated stability metrics measure the extent to which the structure of a design is preserved throughout the evolution of the software from one release to the next. As a case study, thirteen successive releases of Apache Ant were analyzed. The results indicate that some of the stability metrics are significantly correlated with post-release defect density. It was possible to construct statistically significant regression models to estimate post-release defect density from subsets of these metrics. The results reveal the practical significance and usefulness of some of the investigated stability metrics as early indicators of one of the important software quality outcomes, which is post-release defect density.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4030809,no
Combined software and hardware techniques for the design of reliable IP processors,2006,"In the recent years both software and hardware techniques have been adopted to carry out reliable designs, aimed at autonomously detecting the occurrence of faults, to allow discarding erroneous data and possibly performing the recovery of the system. The aim of this paper is the introduction of a combined use of software and hardware approaches to achieve complete fault coverage in generic IP processors, with respect to SEU faults. Software techniques are preferably adopted to reduce the necessity and costs of modifying the processor architecture; since a complete fault coverage cannot be achieved, partial hardware redundancy techniques are then introduced to deal with the remaining, not covered, faults. The paper presents the methodological approach adopted to achieve the complete fault coverage, the proposed resulting architecture, and the experimental results gathered from the fault injection analysis campaign",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4030937,no
A Software-Based Error Detection Technique Using Encoded Signatures,2006,"In this paper, a software-based control flow checking technique called SWTES (software-based error detection technique using encoded signatures) is presented and evaluated. This technique is processor independent and can be applied to any kind of processors and microcontrollers. To implement this technique, the program is partitioned to a set of blocks and the encoded signatures are assigned during the compile time. In the run-time, the signatures are compared with the expected ones by a monitoring routine. The proposed technique is experimentally evaluated on an ATMEL MCS51 microcontroller using software implemented fault injection (SWIFI). The results show that this technique detects about 90% of the injected errors. The memory overhead is about 135% on average, and the performance overhead varies between 11% and 191% depending on the workload used",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4030951,no
Load Board Designs Using Compound Dot Technique and Phase Detector for Hierarchical ATE Calibrations,2006,This paper presents two load board designs for hierarchical calibration of largely populated ATE. Compound dot technique and phase detector are used on both boards to provide automatic and low cost calibration of ATE with or without a single reference clock. Two different relay tree structures are implemented on the two boards with advanced board design techniques for group offset calibration. Various error sources have been identified and analyzed on both boards based on SPICE simulations and real measurements. TDR measurement compares the two approaches and shows that the two load boards give a maximum of 37ps group timing skew and can be calibrated out by the calibration software,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4030961,no
The Filter Checker: An Active Verification Management Approach,2006,"Dynamic verification architectures provide fault detection by employing a simple checker processor that dynamically checks the computations of a complex processor. For dynamic verification to be viable, the checker processor must keep up with the retirement throughput of the core processor. However, the overall throughput would be limited if the checker processor is neither fast nor wide enough to keep up with the core processor. The authors investigate the impact of checker bandwidth on performance. As a solution for the checker's congestion, the authors propose an active verification management (AVM) approach with a filter checker. The goal of AVM is to reduce overloaded verification in the checker with a congestion avoidance policy and to minimize the performance degradation caused by congestion. Before the verification process starts at the checker processor, a filter checker marks a correctness non-criticality indicator (CNI) bit in advance to indicate how likely these pre-computed results are to be unimportant for reliability. Then AVM decides how to deal with the marked instructions by using a congestion avoidance policy. Both reactive and proactive congestion avoidance policies are proposed to skip the verification process at the checker. Results show that the proposed AVM has the potential to solve the verification congestion problem when perfect fault coverage is not needed. With no AVM, congestion at the checker badly affects performance, to the tune of 57%, when compared to that of a non-fault-tolerant processor. With good marking by AVM, the performance of a reliable processor approaches 95% of that of a non-fault-tolerant processor. Although instructions can be skipped on a random basis, such an approach reduces the fault coverage. A filter checker with a marking policy correlated with the correctness non-criticality metric, on the other hand, significantly reduces the soft error rate. Finally, the authors also present results showing the trade-off be- - tween performance and reliability",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4030964,no
Extending an SSI Cluster for Resource Discovery in Grid Computing,2006,"Grid technologies enable large-scale sharing of resources within formal or informal consortia of individuals and/or virtual organizations. In these settings, the discovery, characterization, and monitoring of resources, services, and computations can be challenging due to the considerable diversity, large numbers, dynamic behavior, and geographical distribution of the entities in which a user might be interested. Hence, information services are a vital part of any grid software infrastructure, providing fundamental mechanisms for discovery and monitoring, and thus for planning and adapting application behavior. This paper proposes a resource discovery system for grid computing with fault-tolerant capabilities starting from an SSI clustering operating system. The proposed system uses dynamic leader-determination and registration mechanisms to automatically recover from nodes and network failures. The system is centralized and uses dynamic (or soft-state) registration to detect and recover from failures. Provisional or backup leader determination provides tolerance and recovery in the event of the leader node failing. The system was tested against a control network modeled after existing grid computing resource discovery components, such as Globus monitoring and discovery system (MDS). In various failure scenarios, the proposed system showed better resilience and performance than the control system",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4031470,no
Data Processing Workflow Automation in Grid Architecture,2006,"Because of the poor performance and the expensive license cost, traditional relational database management systems are no longer good choices for processing huge amount of data. Grid computing is replacing the place of RDBMS in data processing. Traditionally a workflow is generated by data experts, which is time consuming, labor intensive and error prone. More over, it becomes the bottleneck of the overall performance of data processing in the grid architecture. This paper proposes a multi-layer workflow automation strategy that can automatically generate a workflow from a business language. A prototype has been implemented and a simulation has been designed",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4031551,no
On the Assessment of the Mean Failure Frequency of Software in Late Testing,2006,"We propose an approach for assessing the mean failure frequency of a program, based on the statistical test of hypotheses. The approach can be used to establish stopping rules and evaluate the quality of a program based on its mean failure frequency during the late testing phases. Our proposal shows how to set and satisfy conservative bounds for the minimum number of test executions that are needed to achieve a target mean failure frequency with a specified level of statistical significance, based on the quality goal of testing and the specific test execution profile chosen. We relax a few assumptions of the literature, so our approach can be used in a larger set of real-life cases.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4031800,no
Estimating Software Quality with Advanced Data Mining Techniques,2006,"Current software quality estimation models often involve the use of supervised learning methods for building a software fault prediction models. In such models, dependent variable usually represents a software quality measurement indicating the quality of a module by risk-basked class membership, or the number of faults. Independent variables include various software metrics as McCabe, Error Count, Halstead, Line of Code, etc... In this paper we present the use of advanced tool for data mining called Multimethod on the case of building software fault prediction model. Multimethod combines different aspects of supervised learning methods in dynamical environment and therefore can improve accuracy of generated prediction model. We demonstrate the use Multimethod tool on the real data from the Metrics Data Project Data (MDP) Repository. Our preliminary empirical results show promising potentials of this approach in predicting software quality in a software measurement and quality dataset.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4031804,no
DuoTracker: Tool Support for Software Defect Data Collection and Analysis,2006,"In today software industry defect tracking tools either help to improve an organization's software development process or an individual's software development process. No defect tracking tool currently exists that help both processes. In this paper we present DuoTracker, a tool that makes possible to track and analyze software defects for organizational and individual software process decision making. To accomplish this, DuoTracker has capabilities to classify defects in a manner that makes analysis at both organizational and individual software processes meaningful. The benefit of this approach is that software engineers are able to see how their personal software process improvement impacts their organization and vice versa. This paper shows why software engineers need to keep track of their program defects, how this is currently done, and how DuoTracker offers a new way of keeping track of software errors. Furthermore, DuoTracker is compared to other tracking tools that enable software developers to record program defects that occur during their individual software processes.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4031807,no
Towards an Identification Framework for Software Drifts: A Case Study,2006,"Software drift is a common phenomenon in software development processes, which may lead to process deviation and then affect software quality. Effectively controlling negative software drifts is the key to achieving acceptable, predictable, and dependable software evolution in the model-driven development. In this paper we put forward a general taxonomy to identify different drifts in development processes according to their effects on software development. Based on the taxonomy, categories of process drift and quality drift are detailedly introduced. Moreover, we propose an integration framework for different drifts to realize the integration between development process and product quality. Eventually, a case study from practical project development is shown to prove the validity of our identification framework.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4031810,no
Application of Computational Redundancy in Dangling Pointers Detection,2006,"Many programmers manipulate dynamic data improperly, which may produce dynamic memory problems, such as dangling pointer. Dangling pointers can occur when a function returns a pointer to an automatic variable, or when trying to access a deleted object. The existence of dangling pointers causes the programs to behave incorrectly. Dangling pointers are common defect that are easy to commit, but are difficult to discover. In this paper we propose a dynamic approach that detects dangling pointers in computer programs. Redundant computation is an execution of a program statement(s) that does not contribute to the program output. The notion of redundant computation is introduced as a potential indicator of defects in programs. We investigate the application of redundant computation in dangling pointers detection. The results of the initial experiment show that, the redundant computation detection can help the debuggers to localize the source(s) of the dangling pointers. During the experiment, we find that, our approach may be capable of detecting other types of dynamic memory problems such as memory leaks and inaccessible objects, which we plan to investigate in our future research.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4031815,no
Performance of MPI Parallel Applications,2006,"The study of the performance of parallel applications may have different reasons. One of them is the planning of the execution architecture to guaranty a quality of service. In this case, it is also necessary to study the computer hardware, the operating system, the network and the users behavior. This paper proposes different models for those based on simulation. First results of this work show the impact of the network and the necessary precision of communication model in fine-grained parallel applications.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4031844,no
A New Intelligent Traffic Shaper for High Speed Networks,2006,"In this paper, a new intelligent traffic shaper is proposed to obtain a reasonable utilization of bandwidth while preventing traffic overload in other part of the network and as a result, reducing total number of packet dropping in the whole network. This approach trains an intelligent agent to learn an appropriate value for token generation rate of a Token Bucket at various states of the network. This method shows satisfactory results in simulations from the aspects of keeping dropping probability low while injecting as many packets as possible into the network by minimization of used buffer size at each router in order to keep the delay occurred by packets waiting in long buffers to be sent, as small as possible",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4031945,no
Bootstrapping Performance and Dependability Attributes ofWeb Services,2006,"Web services gain momentum for developing flexible service-oriented architectures. Quality of service (QoS) issues are not part of the Web service standard stack, although non-functional attributes like performance, dependability or cost and payment play an important role for service discovery, selection, and composition. A lot of research is dedicated to different QoS models, at the same time omitting a way to specify how QoS parameters (esp. the performance related aspects) are assessed, evaluated and constantly monitored. Our contribution in this paper comprises: a) an evaluation approach for QoS attributes of Web services, which works completely service-and provider independent, b) a method to analyze Web service interactions by using our evaluation tool and extract important QoS information without any knowledge about the service implementation. Furthermore, our implementation allows assessing performance specific values (such as latency or service processing time) that usually require access to the server which hosts the service. The result of the evaluation process can be used to enrich existing Web service descriptions with a set of up-to-date QoS attributes, therefore, making it a valuable instrument for Web service selection",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4032029,no
A Lightweight Software Design Process for Web Services Workflows,2006,"Service-oriented computing (SOC) suggests that many open, network-accessible services will be available over the Internet for organizations to incorporate into their own processes. Developing new software systems by composing an organization's local services and externally-available Web services is conceptually different from system development supported by traditional software engineering lifecycles. Consumer organizations typically have no control over the quality and/or consistency of the external services that they incorporate, thus top-down software development lifecycles are impractical. Software architects and designers will require agile, lightweight processes to evaluate tradeoffs in system design based on the ""estimated"" responsiveness of external services coupled with known performance of local services. We introduce a model-driven software engineering approach for designing systems (i.e. workflows of Web services) under these circumstances and a corresponding simulation-based evaluation tool",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4032052,no
Application of a Statistical Methodology to Simplify Software Quality Metric Models Constructed Using Incomplete Data Samples,2006,"During the construction of a software metric model, incomplete data often appear in the data sample used for the construction. Moreover, the decision on whether a particular predictor metric should be included is most likely based on an intuitive or experience-based assumption that the predictor metric has an impact on the target metric with a statistical significance. However, this assumption is usually not verifiable ""retrospectively"" after the model is constructed, leading to redundant predictor metric(s) and/or unnecessary predictor metric complexity. To solve all these problems, the authors have earlier derived a methodology consisting of the k-nearest neighbors (k-NN) imputation method, statistical hypothesis testing, and a ""goodness-of fit"" criterion. Whilst the methodology has been applied successfully to software effort metric models, it is applied only recently to software quality metric models which usually suffer from far more serious incomplete data. This paper documents the latter application based on a successful case study",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4032264,no
Object-Relational Database Metrics Formalization,2006,"In this paper the formalization of a set of metrics for assessing the complexity of ORBDs is presented. An ontology for the SQL:2003 standard was produced, as a framework for representing the SQL schema definitions. It was represented using UML. The metrics were defined with OCL, upon the SQL:2003 ontology",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4032266,no
An Empirical Study on the Likelihood of Adoption in Practice of a Size Measurement Procedure for Requirements Specification,2006,"Software size is one of the essential parameters of the estimation models used for project management purposes, and therefore being able to measure the size of software at an early stage of the development lifecycle is in theory of crucial importance. However, although some proposals for functional size measurement in industry do currently exist, there is as yet little validating evidence for such proposals. This paper describes an empirical study, based on the method evaluation model (MEM), of users' perceptions resulting from actual experience of use of a measurement procedure called RmFFP. This procedure was designed in accordance with the COSMIC-FFP standard method for estimating the functional size of object-oriented systems from requirements specifications obtained in the context of the OO-Method approach. In addition, an analysis of MEM relationships was also carried out using the regression analysis technique",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4032278,no
Quality Assessment of Mutation Operators Dedicated for C# Programs,2006,"The mutation technique inserts faults in a program under test in order to assess or generate test cases, or evaluate the reliability of the program. Faults introduced into the source code are defined using mutation operators. They should be related to different, also object-oriented features of a program. The most research on OO mutations was devoted to Java programs. This paper describes analytical and empirical study performed to evaluate the quality of advanced mutation operators for C# programs. Experimental results demonstrate effectiveness of different mutation operators. Unit tests suites and functional tests were used in experiments. A detailed analysis was conducted on mutation operators dealing with delegates and exception handling",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4032289,no
Generating Optimal Test Set for Neighbor Factors Combinatorial Testing,2006,"Combinatorial testing is a specification-based testing method, which can detect the faults triggered by interaction of factors. For one kind of software in which the interactions only exist between neighbor factors, this paper proposes the concept of neighbor factors combinatorial testing, presents the covering array generation algorithms for neighbor factors pair-wise (N=2) coverage, neighbor factors N-way (Nges2) coverage and variable strength neighbor factors coverage, and proves that the covering arrays generated by these three algorithms are optimal. Finally we analyze an application scenario, which shows that this approach is very practical",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4032293,no
Probabilistic Adaptive Random Testing,2006,"Adaptive random testing (ART) methods are software testing methods which are based on random testing, but which use additional mechanisms to ensure more even and widespread distributions of test cases over an input domain. Restricted random testing (RRT) is a version of ART which uses exclusion regions and restricts test case generation to outside of these regions. RRT has been found to perform very well, but its use of strict exclusion regions (from within which test cases cannot be generated) has prompted an investigation into the possibility of modifying the RRT method such that all portions of the input domain remain available for test case generation throughout the duration of the algorithm. In this paper, we present a probabilistic approach, probabilistic ART (PART), and explain two different implementations. Preliminary empirical data supporting the methods is also examined",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4032295,no
Detecting Malicious Manipulation in Grid Environments,2006,"Malicious manipulation of jobs results endangers the efficiency and performance of grid computing applications. The presence of nodes interested in depreciating jobs results may be detected and minimized with the usage of fault tolerance techniques. In order to detect this kind of nodes, this paper presents a distributed and hierarchical diagnosis model based on comparison and reputation, which can be applied to both public and private grids. This strategy defines the status of a node according to its level of confidence, measured through its behavior. The proposed model was submitted to simulations to evaluate its effectiveness under different quota of malicious nodes. The results reveals that 8 test rounds can detect practically all malicious nodes and even with less rounds, the correctness remains high without a significant overhead increase",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4032413,no
Hybrid Fault Detection Technique: A Case Study on Virtex-II Pro's PowerPC 405,2006,"Hardening processor-based systems against transient faults requires new techniques able to combine high fault detection capabilities with the usual design requirements, e.g., reduced design-time, low area overhead, reduced (or null) accessibility to processor internal hardware. This paper proposes the adoption of a hybrid approach, which combines ideas from previous techniques based on software transformations with the introduction of an Infrastructure IP with reduced memory and performance overheads, to harden system based on the PowerPC 405 core available in Virtex-II Pro FPGAs. The proposed approach targets faults affecting the memory elements storing both the code and the data, independently of their location (inside or outside the processor). Extensive experimental results including comparisons with previous approaches are reported, which allow practically evaluating the characteristics of the method in terms of fault detection capabilities and area, memory and performance overheads",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4033903,no
Internet Traffic Classification for Scalable QOS Provision,2006,"A new scheme that classifies the Internet traffic according to their application types for scalable QoS provision is proposed in this work. The traditional port-based classification method does not yield satisfactory performance, since the same port can be shared by multiple applications. Furthermore, asymmetric routing and errors of modern measurement tools such as PCF and NetFlow degrades the classification performance. To address these issues, the proposed classification process consists of two steps: feature selection and classification. Candidate features that can be obtained easily by ISP are examined. Then, we perform feature reduction so as to balance the performance and complexity. As to classification, the REPTree and the bagging schemes are adopted and compared. It is demonstrated by simulations with real data that the proposed classification scheme outperforms existing techniques",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4036826,no
Comparison and Assessment of Improved Grey Relation Analysis for Software Development Effort Estimation,2006,"The goal of software project planning is to provide a framework that allows project manager to make reasonable estimates of the resources. In fact, software development is highly unpredictable - only 10% of projects on time and budget. Thus, it is very important for software project managers to accurately and precisely estimate software development effort since the resources are limited. One of the most widely used approaches of software effort estimation is the analogy method. Since the method of analogy is constructed on the foundation of distance-based similarity, there are still some drawbacks and restrictions for application. For example, the anomalistic and outlying values will influence the function to determine similarity. Contrarily, grey relational analysis (GRA) is a distinct measurement from the traditional distance scale and can dig out the realistic law from small-sample data. In this paper, we show how to apply GRA to evaluate the effort estimation results for different data sequences and to compare its accuracy with that of Analogy method. Experimental result shows that the GRA provides a better predictive performance than other methods. We can see that the GRA is more suitable for predicting software development effort with unbalanced dataset",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4037100,no
An Analysis Model for Software Project Development,2006,"The goal of software development is to build quality software that meets customers' needs on time and on budget. However, several kinds of risks, complexity and uncertainties make software development hard to control. If no appropriate project management is applied, the software project will go nowhere. In this paper, we build an analysis model for software project development from the perspective of people, tools, and materials. The analysis model can be used to reveal project status in each development phase and this helps to investigate and manage particular attributes of software project, including cost, effort, schedule, risk, and complexity",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4037102,no
Predicting the Viterbi Score Distribution for a Hidden Markov Model and Application to Speech Recognition,2006,"Hidden Markov models are used in many important applications such as speech recognition and handwriting recognition. Finding an effective HMM to fit the data is important for successful operation. Typically, the Baum-Welch algorithm is used to train an HMM, and seeks to maximize the likelihood probability of the model, which is closely related to the Viterbi score. However, random initialization causes the final model quality to vary due to locally optimum solutions. Conventionally, in speech recognition systems, models are selected from a collection of already trained models using some performance criterion. In this paper, we investigate an alternative method of selecting models using the Viterbi score distribution. A method to determine the Viterbi score distribution is described based on Viterbi score variation with respect to the number of states (N) in the model. Our tests show that the distribution is approximately Gaussian when the number of states is greater than 3. The paper also investigates the relationship between performance and the Viterbi score's percentile value and discusses several interesting implications for Baum-Welch training",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4041064,no
Two-Dimensional Software Reliability Models and Their Application,2006,"In general, the software-testing time may be measured by two kinds of time scales: calendar time and test-execution time. In this paper, we develop two-dimensional software reliability models with two-time measures and incorporate both of them to assess the software reliability with higher accuracy. Since the resulting software reliability models are based on the familiar non-homogeneous Poisson processes with two-time scales, which are the natural extensions of one-dimensional models, it is possible to treat both the time data simultaneously and effectively. We investigate the dependence of test-execution time as a testing effort on the software reliability assessment, and validate quantitatively the software reliability models with two-time scales. We also consider an optimization problem when to stop the software testing in terms of two-time measurements",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4041882,no
A Strategy for Verification of Decomposable SCR Models,2006,"Formal methods for verification of software systems often face the problem of state explosion and complexity. We propose a divide and conquer methodology which leads to component-based verification and analysis of formal requirements specifications expressed using software cost reduction (SCR) models. This paper presents a novel decomposition methodology which identifies components in the given SCR specification and automates related abstraction methods. Further, we propose a verification strategy for modular and decomposable software models. Efficient verification of SCR models is achieved through the use of invariants and proof compositions. Experimental validation of our methodology brought to light the importance of modularity, encapsulation, information hiding and the avoidance of global variables in the context of formal specification models. The advantages of the compositional verification strategy are demonstrated in the analysis of the personnel access control system. Our approach offers significant savings in terms of time and memory requirements needed to perform formal system verification",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4041883,no
An Evaluation of Similarity Coefficients for Software Fault Localization,2006,"Automated diagnosis of software faults can improve the efficiency of the debugging process, and is therefore an important technique for the development of dependable software. In this paper we study different similarity coefficients that are applied in the context of a program spectral approach to software fault localization (single programming mistakes). The coefficients studied are taken from the systems diagnosis/automated debugging tools Pinpoint, Tarantula, and AMPLE, and from the molecular biology domain (the Ochiai coefficient). We evaluate these coefficients on the Siemens Suite of benchmark faults, and assess their effectiveness in terms of the position of the actual fault in the probability ranking of fault candidates produced by the diagnosis technique. Our experiments indicate that the Ochiai coefficient consistently outperforms the coefficients currently used by the tools mentioned. In terms of the amount of code that needs to be inspected, this coefficient improves 5% on average over the next best technique, and up to 30% in specific cases",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4041886,no
An OS-level Framework for Providing Application-Aware Reliability,2006,"The paper describes the reliability microkernel framework (RMK), a loadable kernel module for providing application-aware reliability and dynamically configuring reliability mechanisms installed in RMK. The RMK prototype is implemented in Linux and supports detection of application/OS failures and transparent application checkpointing. Experiment results show that the OS hang detection, which exploits characteristics of application and system behavior, can achieve high coverage (100% in our experiments) and low false positive rate. Moreover, the performance overhead is negligible because instruction counting is performed in hardware",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4041888,no
A Best Practice Guide to Resources Forecasting for the Apache Webserver,2006,"Recently, measurement based studies of software systems proliferated, reflecting an increasingly empirical focus on system availability, reliability, aging and fault tolerance. However, it is a non-trivial, error-prone, arduous, and time-consuming task even for experienced system administrators and statistical analysis to know what a reasonable set of steps should include to model and successfully predict performance variables or system failures of a complex software system. Reported results are fragmented and focus on applying statistical regression techniques to captured numerical system data. In this paper, we propose a best practice guide for building empirical models based on our experience with forecasting Apache Web server performance variables and forecasting call availability of a real world telecommunication system. To substantiate the presented guide and to demonstrate our approach step-by-step we model and predict the response time and the amount of free physical memory of an Apache Web server system. Additionally, we present concrete results for a) variable selection where we cross benchmark three procedures, b) empirical model building where we cross benchmark four techniques and c) sensitivity analysis. This best practice guide intends to assist in configuring modeling approaches systematically for best estimation and prediction results",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4041903,no
Early Software Reliability Prediction with ANN Models,2006,"It is well-known that accurate reliability estimates can be obtained by using software reliability models only in the later phase of software testing. However, prediction in the early phase is important for cost-effective and timely management. Also this requirement can be achieved with information from previous releases or similar projects. This basic idea has been implemented with nonhomogeneous Poisson process (NHPP) models by assuming the same testing/debugging environment for similar projects or successive releases. In this paper we study an approach to using past fault-related data with artificial neural network (ANN) models to improve reliability predictions in the early testing phase. Numerical examples are shown with both actual and simulated datasets. Better performance of early prediction is observed compared with original ANN model with no such historical fault-related data incorporated. Also, the problem of optimal switching point from the proposed approach to original ANN model is studied, with three numerical examples",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4041906,no
Concave and Convex Area on Planar Curve for Shape Defect Detection and Recognition,2006,"A shape representation based on concave and convex area along a closed curve is presented. Curvature estimation is done to the input curve and searched for its critical points. Splitting the critical points into concave and convex critical points, the concave and convex area is computed. This technique is tested on shape defect detection of starfruit and also to shape recognition. In the first case, defect is measured with concave energy and obtained a stable measure, which is proportional with the defect. In shape recognition, starfruit's stem is identified to remove it from the starfruit shape, as it will contribute to false computation in defect measurement",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4042288,no
Joint Analysis and Simulation of Time Synchronization-Relative Navigation Performance of a kind of wireless TDMA Network,2006,"In this paper we analyze the joint performance of time synchronization and relative navigation of a kind of wireless TDMA network. The network is based on spread spectrum communication system and can support relative navigation service by continuously receiving and parsing the navigation information in position information (PI) messages. By the calculation of the relative position of other users according to the transmission delay of PI messages, user can estimates its own position within a reasonable bias. On the other hand, users with poor time synchronization situation can improve their synchronization quality by PI messages received from other nodes in network. With the calculation of relative position and transmission delay of PI messages by Kalman filter, users can also estimate the clock bias from other users who have better synchronization situation. So we can find out that time synchronization and relative navigation quality in this TDMA network have very close and complex relationship. Based on network simulation software OPNET, we validate the relationship between time synchronization and relative navigation and also show that geodetic reference (GR) will greatly helpful for the time synchronization and relative navigation performance of users",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4053778,no
Research and Establishment of Quality Cost Oriented Software Testing Model,2006,"In a competitive market, quality is essential to the software. Software test plays an important role during the software-developing period. Any saving on software test will greatly reduce the total cost of software. The key point of this paper is to build a software test process with the cost control management and to make tradeoff between the quality and the cost. Typical software testing model are analyzed and compared. A novel QC 3-D test model is introduced. The model combined with software test process, software quality cost and testing type. The computational formula is defined to calculate software quality cost. A software quality balance law is introduced to balance control cost and failure cost. By adjusting software test method and strategy, organization can reach the objective of the best balance between software quality and cost. An enterprise case is studied and the result is analyzed to prove the basic law. The result shows an optimistic point that will prove the accurate of the model and the law",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4054588,no
Test Selection and Coverage Based on CTM and Metric Spaces,2006,"One of the main open issues in testing and in particular in conformance testing is finding a good way of measuring the quality of the testing activity. With other words after performing a given quantity of tests, how much it is covered from the number of potential errors of the implementation under test (IUT). The present paper tries to build up a solution for this necessity. In (L. Feijs et al., 2002, N. Goga et al., 2004) we proposed a general framework for defining a coverage measure based on a distance measure between different behaviors of a system under test. In the current paper we apply this general theory to another domain, namely to CTM (classification tree method) that is usually used for the selection of the data for testing. As a case study we apply this technology for testing a software tool used for computer music generation",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4054615,no
Parity Prediction of S-Box for AES,2006,"In this paper, we present the parity prediction approach of the S-Box for designing high performance and fault detection structures of the AES. Unlike the traditional scheme which is based on using look-up tables, we use the logical gates implementation based on the composite fields for fault detection of S-Box in AES. We find closed formulations for the output parity bits of S-Box considering the composite-field transformation matrix and its inverse in GF(2<sup>8</sup>) as well as the affine transformation. To the best of our knowledge, no closed formulations for parity prediction of the S-Box have been proposed in the open literature",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4054882,no
"Uniform Crime Report ""SuperClean"" Data Cleaning Tool",2006,"The analysis of UCR data provides a basis for crime prevention in the United States as well as a sound decision making tool for policy makers. The decisions made with the use of UCR data range from major funding for resource allocation down to patrol distribution by local police departments. The FBI collects and maintains the database of the Uniform Crime Reports (UCR), from 18,000 reporting police agencies nationwide. However, many of these data sets have missing, incomplete, or incorrect data points that render crime analysis less effective. UCR experts have stated that in the current form UCR data is unreliable and sporadic. Efforts have previously been made to design a software application to correct these necessary problems, but the application was deemed insufficient due to limited portability and usability. Software requirements restricted potential users and the user interface was ineffective. However, this previous work describes the functions needed to effectively clean and assess UCR data. This paper describes the design of an application used to clean, process, and correct UCR data so that ideal policy decisions can be made. Erroneous portions of the data will be found using the outlier detection function that is based on a statistical model of anomalous behavior. These methods incorporate sponsor specifications and user requirements. This project builds upon the GRASP (geospatial repository for analysis and safety planning) project's goal of sharing information between law enforcement agencies. Eventually this application could be integrated with GRASP to form a single repository for UCR and spatial crime data. This paper describes how the new stand alone application will allow users to clean, correct, and process UCR data in an efficient, user-friendly manner. Formal testing provides a basis to assess the effectiveness of the application based on the metrics of time, cost, and quality. The results are the basis for improvements to the application",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4055105,no
Notions of Diagnosability for Timed Failure Propagation Graphs,2006,"Timed failure propagation graphs (TFPG) are causal models that capture the temporal aspects of failure propagation in dynamic systems. This paper presents several notions of diagnosability for timed failure propagation models. Diagnosability characteristics of TFPG models are defined based on three metrics; failure detectability, distinguishability, and predictability. The paper identifies the modeling and run-time parameters of the diagnosability metrics. The application of diagnosability analysis to the problem of alarm allocation is discussed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4062453,no
Diagnositc Reasoner Model Production from Atlas Test Program Sets,2006,"Model-based diagnostic reasoning provides improvements in fault isolation and repair time as well as cost savings provided by a reduction in false pull rates. To support legacy systems that use ATLAS test program sets for fault isolation and repair, Honeywell has developed a method for extracting diagnostic fault models from ATLAS source code. We will describe the content of the fault models and highlight model features that use data collected in a netcentric maintenance environment to optimize reasoner performance. We will then present a description of the method used to create bootstrap fault models, including a configurable ATLAS source code parser that uses regular text expressions to extract test and fault information from Test Program Sets and create fault models.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4062454,no
Development of Oil Immersed Transformer Management Technologies by Using Dissolved Gas Analysis,2006,"A newly developed diagnosis system using DGA (dissolved gas analysis) was studied for the effective management of oil-immersed transformers. It consists of the measurement system of dissolved hydrogen gas in oil and DGA software based on dissolved gases such as acetylene (C<sub>2</sub>H<sub>2</sub>), hydrogen (H<sub>2</sub>), ethylene (C<sub>2</sub>H<sub>4</sub>), methane (CH<sub>4</sub>), ethane (C<sub>2</sub>H<sub>6</sub>), carbon monoxide (CO) and carbon dioxide (CO<sub>2</sub>). The system for hydrogen gas measurement was designed using semiconductor gas sensor and polymer membrane. Artificial neural network and rule based DGA possibility in the software were used for recognition of fault causes and decision of fault",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4062758,no
Optimal QoS-aware Sleep/Wake Scheduling for Time-Synchronized Sensor Networks,2006,"We study the sleep/wake scheduling problem in the context of clustered sensor networks. We conclude that the design of any sleep/wake scheduling algorithm must take into account the impact of the synchronization error. Our work includes two parts. In the first part, we show that there is an inherent tradeoff between energy consumption and message delivery performance (defined as the message capture probability in this work). We formulate an optimization problem to minimize the expected energy consumption, with the constraint that the message capture probability should be no less than a threshold. In the first part, we assume the threshold is already given. However, by investigating the unique structure of the problem, we transform the non-convex problem into a convex equivalent, and solve it using an efficient search method. In the second part, we remove the assumption that the capture probability threshold is already given, and study how to decide it to meet the quality of services (QoS) requirement of the application. We observe that in many sensor network applications, a group of sensors collaborate to perform common task(s). Therefore, the QoS is usually not decided by the performance of any individual node, but by the collective performance of all the related nodes. To achieve the collective performance with minimum energy consumption, intuitively we should provide differentiated services for the nodes and favor more important ones. We thus formulate an optimization problem, which aims to set the capture probability threshold for messages from each individual node such that the expected energy consumption is minimized, while the collective performance is guaranteed. The problem turns out to be non-convex and hard to solve exactly. Therefore, we use approximation techniques to obtain a suboptimal solution that approximates the optimum. Simulations show that our approximate solution significantly outperforms a scheme without differentiated treatment of the nodes.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4067940,no
New Tools for Blackout Prevention,2006,"Recent power system blackouts have heightened the concern for power system security and, therefore, reliability. However, potential security improvements which could be achieved through transmission system facility reinforcement and generation supply expansion are long-term, costly, and uncertain. As a result, more immediate and cost effective solutions to the security issue have been pursued including the development of specialized software tools which can assess power system security in near-real-time and assist operators in maintaining adequate security margins at all times thus lowering the risk of blackouts. Such software tools have been implemented in numerous power systems world-wide and are gaining popularity as a result of demonstrated benefits to system performance",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4075765,no
Extraction of Harmonics and Reactive Current for Power Quality Enhancement,2006,"A detection technique for extraction of total harmonics, individual harmonics, and reactive current components is introduced and its performance is evaluated. The detection algorithm is then adopted as part of the control system of a single-phase active power filter (APF) to provide the required signals for harmonic filtering and reactive power compensation. Performance of the overall system is evaluated based on digital time-domain simulation studies. The APF control system including the signal processing algorithms are implemented in Matlab/Simulink fixed-point blockset to accommodate bit-length limitation which is a crucial factor in digital implementation. The power system including the APF, load and the supply system are simulated with the PSCAD/EMTDC software to which the Matlab-based control model is interfaced. The simulation results reflect the desired performance of the detection algorithm",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4078496,no
Harmonization of usability measurements in ISO9126 software engineering standards,2006,"The measurement of software usability is recommended in ISO 9126-2 to assess the external quality of software by allowing the user to test its usefulness before it is delivered to the client. Later, during the operation and maintenance phases, usability should be maintained, otherwise the software will have to be retired. This then raises harmonization issues about the proper positioning of the usability characteristic: does usability really belong to the external quality view of ISO 9126-2 and should the external quality characteristic of usability be harmonized with that of the quality in use model defined in ISO 9126-1 and ISO 9126-4? This paper analyzes these two questions: first, we identify and analyze the subset of ISO 9126-2 quality subcharacteristics and measures of usability that can be useful for quality in use, and then we recommend improvements to the harmonization of these ISO 9126 models",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4078913,no
Distributed Anomaly Detection in Wireless Sensor Networks,2006,"Identifying misbehaviors is an important challenge for monitoring, fault diagnosis and intrusion detection in wireless sensor networks. A key problem is how to minimize the communication overhead and energy consumption in the network when identifying misbehaviors. Our approach to this problem is based on a distributed, cluster-based anomaly detection algorithm. We minimize the communication overhead by clustering the sensor measurements and merging clusters before sending a description of the clusters to the other nodes. In order to evaluate our distributed scheme, we implemented our algorithm in a simulation based on the sensor data gathered from the Great Duck Island project. We demonstrate that our scheme achieves comparable accuracy compared to a centralized scheme with a significant reduction in communication overhead",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4085803,no
"""Warehouse-Sized Workloads""",2006,"<div style=""font-variant: small-caps; font-size: .9em;"">First Page of the Article</div><img class=""img-abs-container"" style=""width: 95%; border: 1px solid #808080;"" src=""/xploreAssets/images/absImages/04086128.png"" border=""0"">",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4086128,no
Implementation Aspects of Software Development Projects,2006,"The software industry has been plagued by the staggering failure rate of projects, which have resulted in the loss of billions of dollars. The industry has realized that the solution lies in fixing the software process. Hence the processes have been built for development of the systems. The implementation process has still not gained that much importance. Hence problems are faced in moving the systems to production. This paper talks of the implementation/operational aspects. The failure of projects is now being owed to customer not having the correct vision & strategy for the developed software product. The customer blames it to the IT company for developing the 'not so good' product. The fix lies in the IT companies to handhold with the customers on this aspect, guide them throughout on the process of implementation, extend its support for implementation of the product with getting the right involvement and support from the customer during implementation. The implementation of the developed software product is a collaborative effort between the customer and the developer",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4086221,no
StegoBreaker: Audio Steganalysis using Ensemble Autonomous Multi-Agent and Genetic Algorithm,2006,"The goal of steganography is to avoid drawing suspicion to the transmission of a hidden message in multi-medium. This creates a potential problem when this technology is misused for planning criminal activities. Differentiating anomalous audio document (stego audio) from pure audio document (cover audio) is difficult and tedious. Steganalytic techniques strive to detect whether an audio contains a hidden message or not. This paper investigates the use of genetic algorithm (GA) to aid autonomous intelligent software agents capable of detecting any hidden information in audio files, automatically. This agent would make up the detection agent in an architecture comprising of several different agents that collaborate together to detect the hidden information. The basic idea is that, the various audio quality metrics (AQMs) calculated on cover audio signals and on stego-audio signals vis-a-vis their denoised versions, are statistically different. GA employs these AQMs to steganalyse the audio data. The overall agent architecture will operate as an automatic target detection (ATD) system. The architecture of ATD system is presented in this paper and it is shown how the detection agent fits into the overall system. The design of ATD based audio steganalyzer relies on the choice of these audio quality measures and the construction of a GA based rule generator, which spawns a set of rules that discriminates between the adulterated and the untouched audio samples. Experimental results show that the proposed technique provides promising detection rates",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4086242,no
Unsupervised Contextual Keyword Relevance Learning and Measurement using PLSA,2006,"In this paper, we have developed a probabilistic approach using PLSA for the discovery and analysis of contextual keyword relevance based on the distribution of keywords across a training text corpus. We have shown experimentally, the flexibility of this approach in classifying keywords into different domains based on their context. We have developed a prototype system that allows us to project keyword queries on the loaded PLSA model and returns keywords that are closely correlated. The keyword query is vectorized using the PLSA model in the reduce aspect space and correlation is derived by calculating a dot product. We also discuss the parameters that control PLSA performance including a) number of aspects, b) number of EM iterations c) weighting functions on TDM (pre-weighting). We have estimated the quality through computation of precision-recall scores. We have presented our experiments on PLSA application towards document classification",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4086258,no
Modified Smith Predictor for Network Congestion Control,2006,"In this paper, a novel active queue management (AQM) control law using modified Smith predictor is proposed. The proposed method provides a stable response for a TCP flow dynamics which is highly oscillatory or unstable under practical conditions. The controller decouples the setpoint response from the load response. The proposed control scheme can track the desired buffer level and reject any additional load disturbance such as unknown buffer depletion rate. Simulation results show high performance for the setpoint response when the parameters of TCP flow dynamics are known",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4086260,no
Proactive Admission Control for IP Networks using Teletraffic Engineering Model,2006,"The paper proposes an admission control scheme which reserves network resources on a proactive basis. The current flow arrival and departure patterns are used to determine the future demand for network resources using the telephone networks Erlang-B model. The Erlang model derives the traffic-QoS relationship in terms of the blocking probability of the network resource given the network capacity, its current utilization and future demand. The same principle is remodeled here to suit the IP network. The blocking probability thus measured is used as a flow admission decision parameter. The effectiveness of the scheme over the other flow admission control algorithms is determined here. Detailed simulations of the proposed algorithm result in a higher admission rate and higher bottleneck link utilization at a comparatively lower overhead traffic",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4087748,no
Range Detection Approach in Interactive Virtual Heritage Walkthrough,2006,"Exploring virtual world will give a memorable experience to the users. Generally, they are expecting smooth navigation with high quality image drawn into their eyes. Nevertheless, when the virtual world is growing simultaneously with the number of objects, the navigation will be detained. Yet, the frame rates can become unacceptable. In this paper, we present and examine range detection approach for view frustum culling to effectively speed-up the interactive virtual heritage project namely Ancient Malacca Virtual Walkthrough. Normal six plane view frustum culling approach use the plane equation to test whether objects is within the visible region. However, range detection approach is different with normal six plane view frustum. It is based on camera referential point and tests the objects if they are in the viewing range. The results show that this approach is able to increase the rendering performance of the virtual walkthrough without sacrificing the visual quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4089321,no
Blocking vs. Non-Blocking Coordinated Checkpointing for Large-Scale Fault Tolerant MPI,2006,"A long-term trend in high-performance computing is the increasing number of nodes in parallel computing platforms, which entails a higher failure probability. Fault programming environments should be used to guarantee the safe execution of critical applications. Research in fault tolerant MPI has led to the development of several fault tolerant MPI environments. Different approaches are being proposed using a variety of fault tolerant message passing protocols based on coordinated checkpointing or message logging. The most popular approach is with coordinated checkpointing. In the literature, two different concepts of coordinated checkpointing have been proposed: blocking and non-blocking. However they have never been compared quantitatively and their respective scalability remains unknown. The contribution of this paper is to provide the first comparison between these two approaches and a study of their scalability. We have implemented the two approaches within the MPICH environments and evaluate their performance using the NAS parallel benchmarks",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4090192,no
SPDW: A Software Development Process Performance Data Warehousing Environment,2006,"Metrics are essential in the assessment of the quality of software development processes (SDP). However, the adoption of a metrics program requires an information system for collecting, analyzing, and disseminating measures of software processes, products and services. This paper describes SPDW, an SPD data warehousing environment developed in the context of the metrics program of a leading software operation in Latin America, currently assessed as CMM Level 3. SDPW architecture encompasses: 1) automatic project data capturing, considering different types of heterogeneity present in the software development environment; 2) the representation of project metrics according to a standard organizational view; and 3) analytical functionality that supports process analysis. The paper also describes current implementations, and reports experiences on the use of SPDW by the organization",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4090251,no
Design Space Exploration for an Adaptive Noise Cancellation Algorithm,2006,"The most difficult aspect of system-level design consists in make a good selection between the multiplicities of options in the task level specification. This process relies heavily on the quality of the area, execution time and power consumption measures; the usefulness of the generated partition depends on how accurate these estimation measures are. The aim of this paper is to probe a general sequence of actions to obtain these values. The approximation has consisted of the specification of the problem in ANSI C and its transformation into assembler code for several DSP platforms and into RTL VHDL synthesizable code as input to automatic synthesis tools. The outputs of the estimates measures obtained when the sequence of actions is completed feed the partitioning phase. Also, the assembler and RTL VHDL code chosen may be reused in the cosynthesis stage. A noise canceller filter as example of application is presented",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4099989,no
Predictive Performance Analysis of a Parallel Pipelined Synchronous Wavefront Application for Commodity Processor Cluster Systems,2006,"This paper details the development and application of a model for predictive performance analysis of a pipelined synchronous wavefront application running on commodity processor cluster systems. The performance model builds on existing work (Cao et al.) by including extensions for modern commodity processor architectures. These extensions, including coarser hardware benchmarking, prove to be essential in countering the effects of modern superscalar processors (e.g. multiple operation pipelines and on-the-fly optimisations), complex memory hierarchies, and the impact of applying modern optimising compilers. The process of application modelling is also extended, combining static source code analysis with run-time profiling results for increased accuracy. The model is validated on several high performance SMP systems and the results show a high predictive accuracy (les 10% error). Additionally, the use of the performance model to speculate on the performance and scalability of this application on a hypothetical cluster with two different problem sizes is demonstrated. It is shown that such speculative techniques can be used to support system procurement, run-time verification and system maintenance and upgrading",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4100394,no
Application of Static Transfer Switch for Feeder Reconfiguration to Improve Voltage at Critical Locations,2006,"The main objective of this work was to assess and evaluate the performance of static transfer switch (STS) for feeder reconfiguration. Two particular network feeders namely preferred and alternate were selected for simulation studies. Both feeders belong to IESCO system (Islamabad Electric Supply Company, Pakistan). The sensitive loads are fed by preferred feeder but in case of disturbances, the loads are transferred to alternate feeder. Different simulation cases were performed for optimum installation of STS to obtain the required voltage quality. The simulations are performed using the PSCAD/EMTDC package",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4104661,no
On the Use of Behavioral Models for the Integrated Performance and Reliability Evaluation of Fault-Tolerant Avionics Systems,2006,"In this paper, the authors propose an integrated methodology for the reliability and performance analysis of fault-tolerant systems. This methodology uses a behavioral model of the system dynamics, similar to the ones used by control engineers when designing the control system, but incorporates additional artifacts to model the failure behavior of the system components. These artifacts include component failure modes (and associated failure rates) and how those failure modes affect the dynamic behavior of the component. The methodology bases the system evaluation on the analysis of the dynamics of the different configurations the system can reach after component failures occur. For each of the possible system configurations, a performance evaluation of its dynamic behavior is carried out to check whether its properties, e.g., accuracy, overshoot, or settling time, which are called performance metrics, meet system requirements. After all system configurations have been evaluated, the values of the performance metrics for each configuration and the probabilities of going from the nominal configuration (no component failures) to any other configuration are merged into a set of probabilistic measures of performance. To illustrate the methodology, and to introduce a tool that the authors developed in MATLAB/SIMULINKreg that supports this methodology, the authors present a case-study of a lateral-directional flight control system for a fighter aircraft",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4106294,no
Multiple Description Scalar Quantization Based 3D Mesh Coding,2006,"In this paper, we address the problem of 3D model transmission over error-prone channels using multiple description coding (MDC). The objective of MDC is to encode a source into multiple bitstreams, called descriptions, supporting multiple quality levels of decoding. Compared to layered coding techniques, each description can be decoded independently to approximate the model. In the proposed approach, the mesh geometry is compressed using multiresolution geometry compression. Then multiple descriptions are obtained by applying multiple description scalar quantization (MDSQ) to the obtained wavelet coefficients. Experimental results show that, the proposed approach achieves competitive compression performance compared with existing multiple description methods.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4106589,no
2D Frequency Selective Extrapolation for Spatial Error Concealment in H.264/AVC Video Coding,2006,"The frequency selective extrapolation extends an image signal beyond a limited number of known samples. This problem arises in image and video communication in error prone environments where transmission errors may lead to data losses. In order to estimate the lost image areas, the missing pixels are extrapolated from the available correctly received surrounding area which is approximated by a weighted linear combination of basis functions. In this contribution, we integrate the frequency selective extrapolation into the H.264/AVC coder as spatial concealment method. The decoder reference software uses spatial concealment only for I frames. Therefore, we investigate the performance of our concealment scheme for I frames and its impact on following P frames caused by error propagation due to predictive coding. Further, we compare the performance for coded video sequences in TV quality against the non-normative concealment feature of the decoder reference software. The investigations are done for slice patterns causing chequerboard and raster scan losses enabled by flexible macroblock ordering (FMO).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4107009,no
Development of Defect Classification Algorithm for POSCO Rolling Strip Surface Inspection System,2006,"Surface inspection system (SIS) is an integrated hardware-software system which automatically inspects the surface of the steel strip. It is equipped with several cameras and illumination over and under the steel strip roll and automatically detects and classifies defects on the surface. The performance of the inspection algorithm plays an important role in not only quality assurance of the rolled steel product, but also improvement of the strip production process control. Current implementation of POSCO SIS has good ability to detect defects, however, classification performance is not satisfactory. In this paper, we introduce POSCO SIS and suggest a new defect classification algorithm which is based on support vector machine technique. The suggested classification algorithm shows good classification ability and generalization performance",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4108062,no
A New Approach for Induction Motor Broken Bar Diagnosis by Using Vibration Spectrum,2006,"Different methods for detecting broken bars in induction motors can be found in literature. Many of these methods are based on evaluating special frequency magnitudes in machine signals spectrums. Current, power, flux, etc are among these signals. Frequencies related to broken rotor fault are dependent on slip; therefore, correct diagnosis of fault depends on accurate determination of motor velocity and slip. The traditional methods typically require several sensors that should be pre-installed in some cases. A practical diagnosis method should be easily performed in site and does not take too much time. This paper presents a diagnosing method based on only a vibration sensor. Motor velocity oscillation due to broken rotor causes frequency components at twice slip frequency difference around speed frequency in vibration spectrum. Speed frequency and its harmonics as well as twice supply frequency, can easily and accurately be found in vibration spectrum, therefore the motor slip can be computed. Now components related to rotor fault can be found. According to this method, an apparatus consisting necessary hardware and software has been designed. Experimental tests have confirmed the efficiency of the method",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4108512,no
Characterization of Diagnosis Algorithms for Real-time Scheduling on Distributed Processors in Support to e-Maintenance Services,2006,"This paper presents a characterization study of agent algorithms of distributed diagnosis, for real-time scheduling and on-line implementation. These parameters will be used to optimize the strategies of real-time distributed diagnosis. The characterization of execution parameters of an agent algorithm must satisfy the diagnosis time constraints related to the availability objective of the each equipment, defined in the predictive maintenance service policy. Hence, we propose to define the time characteristics and constraints, in the case of networked distributed system architecture. The diagnosis agent algorithms are then characterized for evaluating their performances, following criteria relating to the real-time execution",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4114505,no
Traffic and Network Engineering in Emerging Generation IP Networks: A Bandwidth on Demand Model,2006,"This paper assesses the performance of a network management scheme where network engineering (NE) is used to complement traffic engineering (TE) in a multi-layer setting where a data network is layered above an optical network. We present a TE strategy which is based on a multi-constraint optimization model consisting of finding bandwidth-guaranteed IP tunnels subject to contention avoidance minimization and bandwidth usage maximization constraints. The TE model is complemented by a NE model which uses a bandwidth trading mechanism to rapidly re-size and re-optimize the established tunnels (LSPs/lambdaSPs) under quality of service (QoS) mismatches between the traffic carried by the tunnels and the resources available for carrying the traffic. The resulting TE+NE strategy can be used to achieve bandwidth on demand (BoD) in emerging generation IP networks using a (G)MPLS- like integrated architecture in a cost effective way. We evaluate the performance of this hybrid strategy when routing, re-routing and re-sizing the tunnels carrying the traffic offered to a 23-node test network.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4114884,no
Software Quality in Ladder Programming,2006,"This paper aims to measure the software quality for programmable logic controllers (PLCs) especially in ladder programming. The proposed quality metrics involve the criteria of simplicity, reconfigurability, reliability, and flexibility. A fuzzy inference algorithm is developed to select the best controller design among different ladder programs for the same application. A single tone membership function is used to represent the quality metric per each controller. The fitness of each controller is represented by the minimum value of all evaluated criteria. Thereafter, a min-max fuzzy inference is applied to take the decision (which controller is the best). The developed fuzzy assessment algorithm is applied to a conveyor belt module connected to a PLC to perform a repeated sequence. The decision making to select the best ladder program is obtained using the fuzzy assessment algorithm. The obtained results affirmed the potential of the proposed algorithm to assess the quality of the designed ladder programs",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4115500,no
Forming Groups for Collaborative Learning in Introductory Computer Programming Courses Based on Students' Programming Styles: An Empirical Study,2006,"This paper describes and evaluates an approach for constructing groups for collaborative learning of computer programming. Groups are formed based on students' programming styles. The style of a program is characterized by simple well known metrics, including length of identifiers, size and number of modules (functions/procedures), and numbers of indented, commented and blank lines. A tool was implemented to automatically assess the style of programs submitted by students. For evaluating the tool and approach used to construct groups, some experiments were conducted involving information systems students enrolled in a course on algorithms and data structures. The experiments showed that collaborative learning was very effective for improving the programming style of students, particularly for students that worked in heterogeneous groups (formed by students with different levels of knowledge of programming style)",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4116907,no
One-Way Delay Measurement: State of Art,2006,"Accurate OWD (one-way delay) measurement has a relevant role in network performance estimation and consequently in application design. In order to estimate OWD accurately it is essential to consider some parameters which influence the measure, such as operating system (OS) loaded on the PC, the treads, and the PC synchronization in the network. Due to the large research interest towards synchronization aspects, the authors have chosen to consider this parameter first. For this reason, three synchronization methods based on NTP (network time protocol), on GPS (global positioning system), and on IEEE 1588 standard are described and compared showing the advantages and disadvantages of the analyzed methods",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4124312,no
A Proposal and Empirical Validation of Metrics to Evaluate the Maintainability of Software Process Models,2006,"Software measurement is essential for understanding, defining, managing and controlling the software development and maintenance processes and it is not possible to characterize the various aspects of development in a quantitative way without having a deep understanding of software development activities and their relationships The current competitive marketplace calls for the continuous improvement of processes and as consequence companies have to change their processes in order to adapt to these new emerging needs. It implies the continuous change of their software process models and therefore, it is fundamental to facilitate the evolution of these models by evaluating its easiness of maintenance (maintainability). In this paper we introduce a set of metrics for software process models and discuss how these can be used as maintainability indicators. In particular, we report the results of a family of experiments that assess relationships between the structural properties, as measured by the metrics, of the process models and their maintainability. As a result a set of useful metrics to evaluate the software process models maintainability, have been obtained",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4124506,no
Parametric Models for Speech Quality Estimation in GSM Networks,2006,"Speech quality assessment is an emergent QoS aspect in cellular network monitoring. This contribution deals with the estimation of perceived speech quality in GSM networks, by means of parametric models. Our goal is to provide a model for MOS estimation, based on parameters available in the operation and maintenance centre (OMC) and in the measurement reports from mobile stations. Specifically, based on MOS values from a large database obtained performing intrusive tests in a real GSM network, and on the correspondent measured transmission parameters, an extensive data analysis has been conducted. The correlation coefficients of GSM parameters with the objective speech quality have been then maximized, and a simple linear model has been proposed. Moreover, handover events have been considered and their impact on perceived speech quality has been proved",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4129902,no
A Hybrid Fault-Tolerant Algorithm for MPLS Networks,2006,In this paper we present a novel fault-tolerant algorithm for use in MPLS based networks. The algorithm is employing both protection switching and path rerouting techniques and satisfies four selected performance criteria,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4129935,no
Fault Tolerant Job Scheduling in Computational Grid,2006,"In large-scale grids, the probability of a failure is much greater than in traditional parallel systems [I]. Therefore, fault tolerance has become a crucial area in grid computing. In this paper, we address the problem of fault tolerance in term of resource failure. We devise a strategy for fault tolerant job scheduling in computational grid. Proposed strategy maintains history of the fault occurrence of resource in grid information service (GIS). Whenever a resource broker has job to schedule it uses the resource fault occurrence history information from GIS and depending on this information use different intensity of check pointing and replication while scheduling the job on resources which have different tendency towards fault. Using check pointing proposed scheme can make grid scheduling more reliable and efficient. Further, it increases the percentage of jobs executed within specified deadline and allotted budget, hence helping in making grid trustworthy. Through simulation we have evaluated the performance of the proposed strategy. The experimental results demonstrate that proposed strategy effectively schedule the grid jobs in fault tolerant way in spite of highly dynamic nature of grid",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4136898,no
"Grid Performance Prediction: Requirements, Framework, and Models",2006,"Grid performance prediction (GPP) is critical for grid middleware component services and grid (middleware) users to make optimized grid resource usage decisions to meet QoS requirements committed in SLAs. Other sophisticated concepts in grid like multi-criteria scheduling, grid capacity planning, and grid advance reservation are also dependant on GPP. GPP spreads over all levels of grid resources in multiple dimensions. We explore it at different coarse grain levels to discover the most wanted grid performance prediction needs of different human and service clients of grid and/or grid services, and present a comprehensive grid performance prediction framework to support at fine grain levels. We also provide a broad architecture of ALP (application level prediction) and NLP (network level prediction) for our grid middleware",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4136905,no
"Fault Tolerance using ""Parallel Shadow Image Servers (PSIS)"" in Grid Based Computing Environment",2006,"This paper presents a critical review of the existing fault tolerance mechanism in grid computing and the overhead involved in terms of reprocessing or rescheduling of jobs, if in case a fault arisen. For this purpose we suggested the parallel shadow image server (PSIS) copying techniques in parallel to the resource manager for having the check points for rescheduling of jobs from the nearest flag, if in case the fault is detected. The job process is to be scheduled from the resource manager node to the worker nodes and then its' submitted back by the worker nodes in serialized form to the parallel shadow image servers from the worker nodes after the pre-specified amount of time, which we call the recent spawn or the flag check point for rescheduling or reprocessing of job. If the fault is arisen then the rescheduling is done from the recent check point and submitted to the worker node from where the job was terminated. This will not only save time but will improve the performance up to major extent",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4136949,no
Building Scalable Failure-proneness Models Using Complexity Metrics for Large Scale Software Systems,2006,"Building statistical models for estimating failure-proneness of systems can help software organizations make early decisions on the quality of their systems. Such early estimates can be used to help inform decisions on testing, refactoring, code inspections, design rework etc. This paper demonstrates the efficacy of building scalable failure-proneness models based on code complexity metrics across the Microsoft Windows operating system code base. We show the ability of such models to estimate failure-proneness and provide feedback on the complexity metrics to help guide refactoring and the design rework effort.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4137438,no
An Approach of a Technique for Effort Estimation of Iterations in Software Projects,2006,"The estimation of effort and cost is still one of the hardest tasks in software project management. At the moment, there are many techniques to accomplish this task, such as function points, use case points and COCOMO, but there is not much information available about how to use those techniques in non-waterfall software lifecycles such as iterative or spiral lifecycles projects. This paper shows the results obtained when applying a technique to estimate the effort of each construction iteration in software development projects that use iterative-incremental lifecycles. The results were obtained from software projects of a fourth-year course in Informatics. The technique proposes the use of Function Points and COCOMO II.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4137439,no
Software-based Packet Classification in Network Intrusion Detection System using Network Processor,2006,"As computer networking grows more important in daily usage, its security is also paramount. Network intrusion detection system (NIDS) observes network traffic for identifying malicious packets. Core to NIDS function is packet classification component. It is in charge of scanning network packet header. An improved packet classification component bears direct result for NIDS performance. This paper discusses technique to improve packet classification through the use of Bloom filter and hash table lookup. Because packet classification is an important function to other networking infrastructure, for instance firewall, quality of service, multimedia communication, an improved packet classification scheme could benefit application in related areas",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4142533,no
Considering Both Failure Detection and Fault Correction Activities in Software Reliability Modeling,2006,"Software reliability is widely recognized as one of the most significant aspects of software quality and is often determined by the number of software uncorrected faults in the system. In practice, it is essential for fault correction prediction, because this correction process consumes a heavy amount of time and resources to predict whether reliability goals have been achieved. Therefore, in this paper we discuss a general framework of the modeling of the failure detection and fault correction process. Under this general framework, we not only verify the existing non-homogeneous poisson process (NHPP) models but also derive several new NHPP models. In addition, we show that these approaches cover a number of well-known models under different conditions. Finally, numerical examples are shown to illustrate the results of the integration of the detection and correction processes",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4142608,no
Efficient Mutant Generation for Mutation Testing of Pointcuts in Aspect-Oriented Programs,2006,"Fault-based testing is an approach where the designed test data is used to demonstrate the absence of a set of prespecified faults, typically being frequently occurring faults. Mutation testing is a fault-based testing technique used to inject faults into an existing program, i.e., a variation of the original program and see if the test suite is sensitive enough to detect common faults. Aspect-oriented programming (AOP) provides new modularization of software systems by encapsulating crosscutting concerns. AspectJ, a language designed to support AOP uses abstractions like pointcuts, advice, and aspects to achieve AOP's primary functionality. Developers tend to write pointcut expressions with incorrect strength, thereby selecting additional events than intended to or leaving out necessary events. This incorrect strength causes aspects, the set of crosscutting concerns, to fail. Hence there is a need to test the pointcuts for their strength. Mutation testing of pointcuts includes two steps: creating effective mutants (variations) of a pointcut expression and testing these mutants using the designed test data. The number of mutants for a pointcut expression is usually large due to the usage of wildcards. It is tedious to manually identify effective mutants that are of appropriate strength and resemble closely the original pointcut expression. Our framework automatically generates mutants for a pointcut expression and identifies mutants that resemble closely the original expression. Then the developers could use the test data for the woven classes against these mutants to perform mutation testing.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4144722,no
A High-Performance VLSI Architecture for Intra Prediction and Mode Decision in H.264/AVC Video Encoding,2006,"The authors propose a high-performance hardware accelerator for intra prediction and mode decision in H.264/AVC video encoding. They use two intra prediction units to increase the performance. Taking advantage of function similarity and data reuse, the authors successfully reduce the hardware cost of the intra prediction units. Based on a modified mode decision algorithm, the design can deliver almost the same video quality as the reference software. The authors implemented the proposed architecture in Verilog and synthesized it targeting towards a TSMC 0.13mum CMOS cell library. Running at 75MHz, the 36K-gate circuit is capable of realtime encoding 720p HD (1280times720) video sequences at 30 frames per second (fps)",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4145455,no
Error and Rate Joint Control for Wireless Video Streaming,2006,"In this paper, a precise error-tracking scheme for robust transmission of real-time video streaming over wireless IP network is presented. By utilizing negative acknowledgements from feedback channel, the encoder can precisely calculate and track the propagated errors by examining the backward motion dependency. With this precise tracking, the error-propagation effects can be terminated completely by INTRA refreshing the affected macroblocks. In addition, due to lots of INTRA macroblocks refresh will entail a large increase of the output bit rate of a video encoder, several bit rate reduction techniques are proposed. They can be jointly used with INTRA refresh scheme to obtain uniform video quality performance instead of only changing the quantization scale. The simulations show that both control strategies yield significant video quality improvements in error-prone environments",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4149487,no
Design of Media Gateway Controller in 3G Softswitch,2006,"MGCP is a very important protocol in SG, and the quality of design of MGC has a critical impact on the performance of 3G core network, this paper studies the mechanism of MGCP, and proposes a design scheme of MGC, according to our analysis, we propose an implementation prototype of MGC",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4149570,no
QRP01-6: Resource Optimization Subject to a Percentile Response Time SLA for Enterprise Computing,2006,"We consider a set of computer resources used by a service provider to host enterprise applications subject to service level agreements. We present an approach for resource optimization in such an environment that minimizes the total cost of computer resources used by a service provider for an enterprise application while satisfying the QoS metric that the response time for executing service requests is statistically bounded. That is, gamma% of the time the response time is less than a pre-defined value. This QoS metric is more realistic than the mean response time typically used in the literature. Numerical results show the applicability of the approach and validate its accuracy.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4151053,no
QRP02-2: Design and Development of MPLS-based Recovery Strategies in NS2,2006,"In a multi-service IP network, it is a key challenge to provide QoS to end-user applications while effectively using network resources. Traffic Engineering support in MPLS networks allows operators to carry traffic with strict QoS requirements by means of advanced network capabilities, such as resource reservation, fault-tolerance and optimization of transmission resources. To deliver a reliable service, MPLS exploits a set of procedures that protect the traffic flows carried on different paths. Therefore, Label Switching Routers have to support mechanisms for fault detection, notification and recovery, whereas MPLS has to enable the configuration of different recovery techniques. In this paper, we present the design and the development of some software modules for the simulation of different MPLS-based recovery strategies in the discrete event simulator NS2. Moreover, the paper discusses some simulation results that show a performance comparison between path protection and path restoration strategies in a simulation scenario matching the Geant network.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4151055,no
SAT02-2: Modulation Classification of MPSK for Space Applications,2006,"Space missions are being developed with increasing levels of autonomy, as a means to increase their science return, fault-tolerance, and longevity. In this paper, we present NASA's low-complexity techniques to autonomously identify the modulation order of an M-ary phase-shift-keying signal in the presence of additive white Gaussian noise. This is part of a larger program to develop an autonomous radio that receives a signal without a priori knowledge of its modulation type, modulation index, data rate, and pulse shape. For classification between binary and quaternary phase-shift keying, these techniques represent a lopsided trade-off in which a factor of 100 reduction in complexity results in less than a 0.2 dB loss in performance relative to the maximum likelihood modulation classifier.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4151107,no
SPCp1-01: Voice Activity Detection for VoIP-An Information Theoretic Approach,2006,"Voice enabled applications over the Internet are rapidly gaining popularity. Reducing the total bandwidth requirement can make a non-trivial difference for the subscribers having low speed connectivity. Voice activity detection algorithms for VoIP applications can save bandwidth by filtering the frames that do not contain speech. In this paper we introduce a novel technique to identify the voice and silent regions of a speech stream very much suitable for VoIP calls. We use an information theoretic measure, called spectral entropy, for differentiating the silence from the speech zones. Specifically we developed a heuristic approach that uses an adaptive threshold to minimize the miss detection in the presence of noise. The performance of our approach is compared with the relatively new 3GPP TS 26.194 (AMR-WB) standard, along with the listeners' intelligibility rating. Our algorithm yields comparatively better saving in bandwidth, yet maintaining good quality of the speech streams.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4151233,no
WLC12-5: A TDMA-Based MAC Protocol for Industrial Wireless Sensor Network Applications using Link State Dependent Scheduling,2006,"Existing TDMA-based MAC protocols for wireless sensor networks are not specifically built to consider the harsh conditions of industrial environments where the communication channel is prone to signal fading. We propose a TDMA-based MAC protocol for wireless sensor networks built for industrial applications that uses link state dependent scheduling. In our approach, nodes gather samples of the channel quality and generate prediction sets from the sample sets in independent slots. Using the prediction sets, nodes only wake up to transmit/receive during scheduled slots that are predicted to be clear and sleep during scheduled slots that may potentially cause a transmitted signal to fade. We simulate our proposed protocol and compare its performance with a general non-link state dependent TDMA protocol and a CSMA protocol. We found that our protocol significantly improves packet throughput as compared to both the general non-link state dependent TDMA protocol and CSMA protocol. We also found that in conditions which are not perfect under our assumptions, the performance of our protocol degrades gracefully.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4151314,no
WLC19-4: Effective AP Selection and Load Balancing in IEEE 802.11 Wireless LANs,2006,"IEEE 802.11 wireless LANs have been widely deployed. How to efficiently balance the traffic loads among access points (APs), which will lead to improved network utilization and higher quality of user experience, has become an important issue. A key challenge is how to select an AP to use during the handoff process in a way that will achieve overall load balance in the network. The conventional approaches typically use signal-to-noise ratio (SNR) as the criterion without considering the load of each AP. In this paper, we propose two effective new AP selection algorithms. These algorithms estimate the AP traffic loads by observing and estimating the IEEE 802.11 frame delays and use the results to determine which AP to use. The algorithms can be implemented in software on mobile stations alone. We will show that better performance can be achieved when the APs provide assistance in delay measurements; however, the improvement is not significant. There is no need to exchange information among APs. The proposed algorithms are fully compatible with the existing standards and systems and they are easy to implement. We will present extensive simulation results to show that the proposed algorithms can significantly increase overall system throughput and reduce average frame delay.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4151354,no
P2D-3 Objective Performance Testing and Quality Assurance of Medical Ultrasound Equipment,2006,"The goal of this study was to develop a test protocol that contains the minimum set of performance measurements for predicting the clinical performance of ultrasound equipment and that is based on objective assessments by computerized image analysis. The post-processing look-up-table (LUT) is measured and linearized. The elevational focus (slice thickness) of the transducer is estimated and the in plane transmit focus is positioned at the same depth. The developed tests are: echo level dynamic range (dB), contrast resolution (i.e., ""gamma"" of display, #gray levels/dB) and -sensitivity, overall system sensitivity, lateral sensitivity profile, dead zone, spatial resolution, and geometric conformity of display. The concept of a computational observer is used to define the lesion signal-to-noise ratio, SNR<sub>L</sub> (or Mahalanobis distance), as a measure for contrast sensitivity. The whole performance measurement protocol has been implemented in software. Reports are generated that contain all the information about the measurements and results, such as graphs, images and numbers. The software package may be viewed and a run-time version downloaded at the website: http://www.qa4us.eu",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4152267,no
An integrated wired-wireless testbed for distance learning on networking,2006,"This paper addresses a remote testbed for distance learning designed to allow the investigation of various issues related to QoS management in wired/wireless networks used to support real-time applications. Several aspects, such as traffic handling in routers, congestion control and node mobility management, can be experimentally assessed. The testbed comprises various operating modes that the user can select to configure the traffic flows and modify the operational conditions of the network. A peculiarity of the testbed is node mobility support, which allows problems related to handoff and distance to be tackled. The testbed provides for both on-line measurements, through software modules which allow the user to monitor the network while it is operating, and off-line analysis of network behavior through log file inspection",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4152771,no
Measurement Error Analysis of Autonomous Decentralized Load Tracking System,2006,"Measurement error analysis is presented for autonomous decentralized load tracking system. We propose a load tracking mechanism based on incomplete management of load information from subsystems. The system employs autonomous decentralized architecture, which enables online expansion and fault tolerance. Each subsystem asynchronously broadcasts load information to the data field shared by the subsystems. Load information in the data field is measured for a limited period of time to make the tracking ability efficient. The number of measurements and the estimation of total load from the measurements are stochastic variables. This paper shows the statistical model of measurement noise disturbing the tracking mechanism",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4153035,no
Development of Automated Fault Location Software for Distribution Network,2006,"This paper presents on an ongoing development of automated fault location software for distribution network. The proposed method of the software is able to identify the most possible faulty section based on voltage sags feature i.e. magnitude and phase shift. The actual voltage sags caused by fault is compares with simulated voltage sags stored in a database. The matching will give all possible faulty sections that have been stored in the database. The software is developed by integrating various software modules. The module will be developed into a component type using component based development (CBD) approach. By developing the software from component, replacement or modification can be done to any of the module without affecting the whole software. The test results of the proposed method show satisfactory. Future works in order to improve the software and method are also discussed in this paper.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4154463,no
Hiddenness Control of Hidden Markov Models and Application to Objective Speech Quality and Isolated-Word Speech Recognition,2006,"Markov models are a special case of hidden Markov models (HMM). In Markov models the state sequence is visible, whereas in a hidden Markov model the underlying state sequence is hidden and the sequence of observations is visible. Previous research on objective techniques for output-based speech quality (OBQ) showed that the state transition probability matrix A of a Markov model is capable of capturing speech quality information. On the other hand similar experiments using HMMs showed that the observation symbol probability matrix B is more effective at capturing the speech quality information. This shows that the speech quality information in A matrix of a Markov model shifts to the B matrix of an HMM. An HMM can have varying degrees of hiddenness, which can be intuitively guessed from the entries of its observation probability matrix B for the discrete models. In this paper, we propose a visibility measure to assess the hiddenness of a given HMM, and also a method to control the hiddenness of a discrete HMM. We test the advantage of implementing hiddenness control in output-based objective speech quality (OBQ) and isolated-word speech recognition. Our test results suggest that hiddenness control improves the performance of HMM-based OBQ and might be useful for speech-recognition as well.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4176728,no
Low Complexity Scalable Video Coding,2006,"In this paper, we consider scalable video coding (SVC) which has higher complexity than H.264/AVC since it has spatial, temporal and quality scalability in addition to H.264/AVC functionality. Furthermore, inter-layer prediction and layered coding for spatial scalability make motion estimation and mode decision more complex. Therefore, we propose low complexity SVC schemes by using current developing SVC standard. It is archived by prediction method such as skip, direct, inter-layer MV prediction with fast mode and motion vector (MV) estimation at enhancement layer. In order to increase the performance of inter-layer MV prediction, combined MV interpolation is applied with adjustment of prediction direction. Additionally, fast mode and MV estimation are proposed from structural properties of motion-compensated temporal filtering (MCTF) to elaborate predicted macro block (MB) mode and MV. From the experimental results, proposed method has comparable performance to reference software model with significant lower complexity.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4176730,no
Specific nearby electronic scheme for multiplexed excitation and detection of piezoelectric silicon-based micromembranes resonant frequencies using FPGA technology,2006,"A new concept for a precise compensation of the static capacitance of piezoelectric silicon-based micromembranes is proposed. Combining analogue and digital (FPGA) hardware elements with specific software treatment, this system enables the parallel excitation and detection of the resonant frequencies (and the quality factors) of piezoelectric silicon-based micromembranes integrated on the same chip. The frequency measurement stability is less than 1 ppm (1-2 Hz) with a switching capability of 4 membranes/sec and a measurement bandwidth of 1.5 Mhz.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4178552,no
The RPC system for the CMS experiment,2006,"The CMS experiment at the CERN Large Hadron Collider (LHC) is equipped with a redundant muon trigger system based on drift tubes chambers (DT) and cathode strip chamber (CSC), for the precise position measurement, and resistive plate chambers (RPC), for the bunch crossing identification and a preliminary fast measurement of the muon transverse momentum. The CMS RPC system is constituted by 480 chambers in the barrel and 756 chambers in the forward corresponding to a total surface of about 3500 m<sup>2</sup>. The design and construction has involved many laboratories scattered all over the word. An accurate quality control procedure has been established at different levels of the production to achieve final detectors with operation parameters well inside the CMS specifications. In the summer 2006 a preliminary slice test involving a fraction of the CMS detector was performed. The performance of the RPC system with cosmic rays was studied versus the magnetic field using final running hardware and software protocols. Results on the RPC main working parameters are reported.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4179131,no
Probabilistic ISOCS Uncertainty Estimator: Application to the Segmented Gamma Scanner,2006,"Traditionally, high resolution gamma-ray spectroscopy (HRGS) has been used as a very powerful tool to determine the radioactivity of various items, such as samples in the laboratory, waste assay containers, or large items in-situ. However, in order to properly interpret the quality of the result, an uncertainty estimate must be made. This uncertainty estimate should include the uncertainty in the efficiency calibration of the instrument, as well as many other operational and geometrical parameters. Efficiency calibrations have traditionally been made using traceable radioactive sources. More recently, mathematical calibration techniques have become increasingly accurate and more convenient in terms of time and effort, especially for complex or unusual configurations. Whether mathematical or source-based calibrations are used, any deviations between the as-calibrated geometry and the as-measured geometry contribute to the total measurement uncertainty (TMU). Monte Carlo approaches require source, detector, and surrounding geometry inputs. For non-trivial setups, the Monte Carlo approach is time consuming both in terms of geometry input and CPU processing. Canberra Industries has developed a tool known as In-Situ Object Calibration Software (ISOCS) that utilizes templates for most common real life setups. With over 1000 detectors in use with this product, the ISOCS software has been well validated and proven to be much faster and acceptably accurate for many applications. A segmented gamma scanner (SGS) template is available within ISOCS and we use it here to model this assay instrument for the drummed radioactive waste. Recently, a technique has been developed which uses automated ISOCS mathematical calibrations to evaluate variations between reasonably expected calibration conditions and those that might exist during the actual measurement and to propagate them into an overall uncertainty on the final efficiency. This includes variations in container wall thickness - and diameter, sample height and density, sample non-uniformity, sample-detector geometry, and many other variables, which can be specified according to certain probability distributions. The software has a sensitivity analysis mode which varies one parameter at a time and allows the user to identify those variables that have the largest contribution to the uncertainty. There is an uncertainty mode which uses probabilistic techniques to combine all the variables and compute the average efficiency and the uncertainty in that efficiency, and then to propagate those values with the gamma spectroscopic analysis into the final result. In the areas of waste handling and environmental protection, nondestructive assay by gamma ray scanning can provide a fast, convenient, and reliable way of measuring many radionuclides in closed items. The SGS is designed to perform accurate quantitative assays on gamma emitting nuclides such as fission products, activation products, and transuranic nuclides. For the SGS, this technique has been applied to understand impacts of the geometry variations during calibration on the efficiency and to estimate the TMU.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4179189,no
On-line Monitoring of Mechanical Characteristics for Vacuum Circuit Breaker,2006,"A novel scheme combined the intelligent monitoring and fault diagnosis of mechanical characteristics for vacuum circuit breakers (VCBs) is proposed. The main principle of monitoring and diagnostic is stated and the design of hardware and software has been realized. By applying the developed on-line monitoring and diagnostic system, the mechanical characteristics of VCBs can be obtained and the detectable faults have been clearly identified, located, displayed and stored. The less maintenance and lower cost can be realized. Furthermore, the availability and reliability of the power system can be improved",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4194840,no
Comparative Study of Various Artificial Intelligence Techniques to Predict Software Quality,2006,"Software quality prediction models are used to identify software modules that may cause potential quality problems. These models are based on various metrics available during the early stages of software development life cycle like product size, software complexity, coupling and cohesion. In this survey paper, we have compared and discussed some software quality prediction approaches based on Bayesian belief network, neural networks, fuzzy logic, support vector machine, expectation maximum likelihood algorithm and case-based reasoning. This study gives better comparative insight about these approaches, and helps to select an approach based on available resources and desired level of quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4196400,no
Analytical Hierarchy Process Approach to Rank Measures for Structural Complexity of Conceptual Models,2006,"This paper presents the result of a controlled experiment conducted to determine the relative importance of some measures, identified in research, for the structural complexity of entity-relationship (ER) models. The relative importance amongst these measures is calculated by applying the analytical hierarchy process approach. The results reveal that the number of relations in an ER diagram are of the highest importance in measuring the structural complexity in terms of understandability, analyzability and modifiability; whereas, the number of attributes do not play an important role. The study presented here can lead to developing quantitative metrics for comparing the quality of alternative conceptual models of the same problem",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4196416,no
Challenges in System on Chip Verification,2006,"The challenges of system on a chip (SoC) verification is becoming increasingly complex as submicron process technology shrinks die size, enabling system architects to include more functionality in a single chip solution. A functional defect refers to the feature sets, protocols or performance parameters not conforming to the specifications of the SoC. Some of the functional defects can be solved by software workarounds but some require revisions of silicon. The revision of silicon not only costs millions of dollars but also impacts time to market, quality, customer commitments. Working silicon for the first revision of the SoC requires a robust module, chip and system verification strategy to uncover the logical and timing defects before tapeout. Different techniques are needed at each level (module, chip and system) to complete verification. In addition verification should quantify with a metric at every hierarchy to assess functional holes and address it. Verification metric can be a combination of code coverage, functional coverage, assertion coverage, protocol coverage, interface coverage and system coverage. A successful verification strategy also requires the test bench to be scalable, configurable, support reuse of functional tests, integration with tools and finally linkage to validation. The scope of this paper will discuss the verification strategy and pitfalls used in verification strategy and finally make recommendations for successful strategy.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4197222,no
Debug Support for Scalable System-on-Chip,2006,"On-chip debug is an important technique to detect and locate the faults in the practical software applications. Scalability and reusability are the essential features of system-on-chip (SoC). Therefore, the debug architecture should meet the requirement of those features. Furthermore, it is necessary for applications developers to communication with the SoC chip on-line. In this paper, we present the novel debug architecture to solve above problems. The debug architecture has been implemented in a typical SoC chip. The results of performance analysis show that the debug architecture has high performance at the cost of few resources and area.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4197226,no
Transient Error Detection in Embedded Systems Using Reconfigurable Components,2006,"In this paper, a hardware control flow checking technique is presented and evaluated. This technique uses re configurable of the shelf FPGA in order to concurrently check the execution flow of the target micro processor. The technique assigns signatures to the main program in the compile time and verifies the signatures using a FPGA as a watchdog processor to detect possible violation caused by the transient faults. The main characteristic of this technique is its ability to be applied to any kind of processor architecture and platforms. The low imposed hardware and performance overhead by this technique makes it suitable for those applications in which cost is a major concern, such as industrial applications. The proposed technique is experimentally evaluated on an 8051 microcontroller using software implemented fault injection (SWIFI). The results show that this technique detects about 90% of the injected control flow errors. The watchdog processor occupied 26% of an Altera Max-7000 FPGA chip logic cells. The performance overhead varies between 42% and 82% depending on the workload used.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4197508,no
Micropatterned non-invasive dry electrodes for Brain-Computer Interface,2006,"Partially or completely paralyzed patients can benefit from advanced neuro-prostheses in which a continuous recording of electroencephalogram (EEG) is required, operating some processing and classification to control a computer (BCI, brain-computer interfaces). Patients are so allowed to control external devices or to communicate simple messages through the computer, just concentrating their attention on codified movements or on a letter or icon on a digital keyboard. Conventional electrodes usually require skin preparation and application of electrolytic gel for high quality low amplitude biopotentials recordings and are not suitable for being easily used by patient or caregivers at home in BCI or equivalent systems. In this report we describe the fabrication and characterization of dry (gel not required), non-invasive, user-friendly biopotential electrodes. The electrodes consist of a bidimensional array of micro-needles designed to pierce the first dielectric skin layer (stratum corneum) and establishing a direct contact with the living and electrical conducting cells in the epidermis (no blood vessels and nerve terminations). The easy and immediate application of the spiked electrodes makes them also attractive for every surface long-term biosignal measurements, even at patient's home (EEG, electrocardiogram, etc).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4201268,no
Improving Software Quality Classification with Random Projection,2006,"Improving the quality of software products is one of the principal objectives of software engineering. Software metrics are the key tool in software quality management. In this paper we propose to use naive Bayes and RIPPER for software quality classification and use random projection to improve the performance of classifiers. Feature extraction via random projection has attracted considerable attention in recent years. The approach has interesting theoretical underpinnings and offers computational advantages. Results on benchmark dataset MIS, using Accuracy and Recall as performance measures, indicate that random projection can improve the classification performance of all four learners we investigate: Naive Bayes, RIPPER, MLP and IB1. With the help of random projection, naive Bayes and RIPPER are better than MLP and IB1 in finding fault-high software modules, which can be sought as potentially highly faulty modules where most of our testing and maintenance effort should be focused",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4216405,no
The Application of the System Parameter Fusion Principle to Assessing University Electronic Library Performance,2006,"Modern technology provides a great amount of information. But for computer monitoring systems or computer control systems, in order to have the situation in hand, we need to reduce the number of variables to one or two parameters, which express the quality and/or security of the whole system. In this paper, the authors introduce the system parameter fusion principle put forward by the third author and present how to apply it to assessing university electronic library performance combining with the Delphi technique and AHP",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4216535,no
Predicting Qualitative Assessments Using Fuzzy Aggregation,2006,"Given the complexity and sophistication of many contemporary software systems, it is often difficult to gauge the effectiveness, maintainability, extensibility, and efficiency of their underlying software components. A strategy to evaluate the qualitative attributes of a system's components is to use software metrics as quantitative predictors. We present a fusion strategy that combines the predicted qualitative assessments from multiple classifiers with the anticipated outcome that the aggregated predictions are superior to any individual classifier prediction. Multiple linear classifiers are presented with different, randomly selected, subsets of software metrics. In this study, the software components are from a sophisticated biomedical data analysis system, while the external reference test is a thorough assessment of both complexity and maintainability, by a software architect, of each system component. The fuzzy integration results are compared against the best individual classifier operating on a software metric subset",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4216813,no
Neural Model Predictive Controller for Multivariable Process,2006,"In this paper, control of a non-minimal quadruple tank process, which is non linear and multivariable is reported. A nonlinear Model Predictive Controller (NMPC) is developed using a Recurrent Neural Network (RNN) as a predictor. The process data is obtained from the laboratory scale experimental setup, which is used in training the RNN. The network trained is used in controlling the quadruple tank, solving the least square optimization problem with a quadratic performance objective. The control system is implemented in real-time on a laboratory scale plant using dSPACE interface card and Matlab software. The quality of controller using NMPC is compared with dynamic matrix control (DMC) for reference tracking and external disturbance rejection.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4237980,no
Optimal Design of Distributed Databases,2006,"The physical expansion of enterprises with the current trends in globalization has had a positive influence on the technologies of distributed systems. At the forefront of this technological revolution are distributed database systems. For a distributed database to be at optimal performance and thus provide an efficient service it needs to be designed appropriately. The significance of the perfect design is only emphasized by the multiple dimensions required in generating a design. The purpose of this paper is to suggest an approach to generate optimal designs for such distributed database systems and to develop a prototype to demonstrate the said approach. The approach emphasizes on the accuracy of inputs as it largely determines the quality of the final solution. Hence the extraction of network information, a key dimension, is automated to ensure precision. The global schema is fragmented considering data requirements as well as connectivity of each site. Allocation of fragments is treated as a combinatorial optimization problem and assigned to a memetic algorithm. An estimation of distribution algorithm complements the search effort of this memetic algorithm. Site options for replication server environments are investigated based on a shortest path algorithm. Usability of the system in an object oriented development environment, through conditional object-relational mapping, is also explored. The prototype was developed using an evolutionary prototyping approach. It was evaluated by several experts in the relevant fields of application. The results of which, confirmed the practicality of the suggested approach.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4250232,no
Fuzzyness And Imprecision In Software Engineering,2006,"In this paper, we study how fuzziness in software engineering emerges and how to reflect this fuzziness in measuring software qualities. Principal means in these processes are software metrics with values in categorical data represented by software metrics with values in fuzzy sets and linguistic variables. This study is aimed to support the development of high quality software. The process of program design as a transition from a problem to a program is studied. A classification of software metrics is developed with the aim of better structuring and optimization of the software fuzzy metric design. Processes of constructing new measures from existing ones often use aggregation operations. Here we study aggregation operations for fuzzy set based software metrics.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4259937,no
Intra- and Inter-Processors Memory Size Estimation for Multithreaded MPSoC Modeled in Simulink,2006,"Target architectures for Simulink-based flows have been generally fixed and partitioned manually. The quality of the results or the final performance of the MPSoC depends heavily on the adaptation of the architecture to the needs in terms of processing performance as well as the efficiency of the communication protocols. The processing horse power determination is not limited to the selection of appropriate processors, but also to the amount of memory needed to run the software tasks. Moreover, inter-processors buffer memories and their sizes are as critical and require attention and focus to avoid communication loss or bottlenecks.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4263481,no
TDR Single Ended and Differential Measurement Methodology,2006,"As the frequency of the high speed interfaces in computer platforms and systems continue to ramp due to architectural advancement and strong market demands, electronic packages have been stretched to its limit to support these interfaces. Furthermore, due to increase in the complexities in the package, signal integrity, manufacturing concern and package size reduced, designers are facing a lot of signal quality issue caused by impedance mismatch. To ensure signal integrity, it is necessary to understand and control impedance in the transmission environment through which the signals travel. Mismatches and variations can cause reflections that decrease signal quality as a whole. TDR (Time Domain Reflectometry) is the most common tools for verification and analysis of the transmission properties of high-speed systems and components. It is able to locate signal path discontinuities that cause reflections, verify traces characteristic impedance and estimate traces length. Hence, this paper is to discuss about TDR set up and type of TDR measurement procedure and methodology for single ended and differential pair interface in the package. There are several key elements that determine the precision of TDR measurement such as the edge speed of the stimulus pulse produced by the TDR, bandwidth of the channel used to received the pulse from the DUT, calibration and deskew process. Correlation between measurement data vs simulation data is performed in order to ensure the accuracy of the data collected. The simulation method is done by using Ansoft 3D and 2D simulation tools to characterized package interconnect such as traces, wirebond, VIA/PTH and ball structures. The output file (Spice File, .sp) will be important to Ansoft Designer software for TDR simulations.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4266647,no
Hardware assisted pre-emptive control flow checking for embedded processors to improve reliability,2006,"Reliability in embedded processors can be improved by control flow checking and such checking can be conducted using software or hardware. Proposed software-only approaches suffer from significant code size penalties, resulting in poor performance. Proposed hardware-assisted approaches are not scalable and therefore cannot be implemented in real embedded systems. This paper presents a scalable, cost effective and novel fault detection technique, to ensure proper control flow of a program. This technique includes architectural changes to the processor and software modifications. While architectural refinement incorporates additional instructions, the software transformation utilizes these instructions into the program flow. Applications from an embedded systems benchmark suite are used for testing and evaluation. The overheads are compared with the state of the art approach that performs the same error coverage using software-only techniques. Our method has greatly reduced overheads compared to the state of the art. Our approach increased code size by between 3.85-11.2% and reduced performance by just 0.24-1.47% for eight different industry standard applications. The additional hardware (gates) overhead in this approach was just 3.59%. In contrast, the state of the art software- only approach required 50-150% additional code, and reduced performance by 53.5-99.5% when error detection was inserted.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4278498,no
Multi-processor system design with ESPAM,2006,"For modern embedded systems, the complexity of embedded applications has reached a point where the performance requirements of these applications can no longer be supported by embedded system architectures based on a single processor. Thus, the emerging embedded System-on-Chip platforms are increasingly becoming multiprocessor architectures. As a consequence, two major problems emerge, i.e., how to design and how to program such multiprocessor platforms in a systematic and automated way in order to reduce the design time and to satisfy the performance needs of applications executed on these platforms. Unfortunately, most of the current design methodologies and tools are based on Register Transfer Level (RTL) descriptions, mostly created by hand. Such methodologies are inadequate, because creating RTL descriptions of complex multiprocessor systems is error-prone and time consuming. As an efficient solution to these two problems, in this paper we propose a methodology and techniques implemented in a tool called ESPAM for automated multiprocessor system design and implementation. ESPAM moves the design specification from RTL to a higher, so called system level of abstraction. We explain how starting from system level platform, application, and mapping specifications, a multiprocessor platform is synthesized and programmed in a systematic and automated way. Furthermore, we present some results obtained by applying our methodology and ESPAM tool to automatically generate multiprocessor systems that execute a real-life application, namely a Motion-JPEG encoder.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4278517,no
INS/GPS Integrated Navigation Uncertain System State Estimation Based on Minimal Variance Robust Filtering,2006,"INS/GPS integrated navigation systems are used for positioning and attitude determination in a wide range of applications. Combining INS and GPS organically, we can get an integrated navigating system which can overcome the respective defects of the two systems and develop their advantages to form a complementary structure at the same time. GPS measurements can be used correct the INS and sensor errors to provide high accuracy real-time navigation (Farrell, 1998). The integration of GPS and INS measurements is usually achieved using a Kalman filter. But, usually uncertainties exist in INS/GPS integrated real systems, which may cause filtering to divergence for classical Kalman filter. In order to solve this problem, a robust filter with minimal variance is addressed by this paper",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4281729,no
Evolving GA Classifiler for Audio Steganalysis based on Audio Quality Metrics,2006,"Differentiating anomalous audio document (Stego audio) from pure audio document (cover audio) is difficult and tedious. Steganalytic techniques strive to detect whether an audio contains a hidden message or not. This paper presents a genetic algorithm (GA) based approach to audio steganalysis, and the software implementation of the approach. The basic idea is that, the various audio quality metrics calculated on cover audio signals and on stego-audio signals vis-a-vis their denoised versions, are statistically different. GA is employed to derive a set of classification rules from audio data using these audio quality metrics, and fitness function is used to judge the quality of each rule. The generated rules are then used to detect or classify the audio documents in a real-time environment. Unlike most existing GA-based approaches, because of the simple representation of rules and the effective fitness function, the proposed method is easier to implement while providing the flexibility to generally detect any new steganography technique. The implementation of the GA based audio steganalyzer relies on the choice of these audio quality metrics and the construction of a two-class classifier, which will discriminate between the adulterated and the untouched audio samples. Experimental results show that the proposed technique provides promising detection rates.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4286072,no
A Fuzzy-Inference System Based Approach for the Prediction of Quality of Reusable Software Components,2006,"The requirement to improve software productivity has promoted the research on software metric technology. There are metrics for identifying the quality of reusable components. These metrics if identified in the design phase or even in the coding phase can help us to reduce the rework by improving quality of reuse of the component and hence improve the productivity due to probabilistic increase in the reuse level. A suit of metrics can be used to obtain the reusability in the modules. And the reusability can be obtained with the help of Fuzzy Logic. An algorithm has been proposed in which the inputs can be given to fuzzy system in form of Cyclometric Complexity, Volume, Regularity, Reuse-Frequency & Coupling, and output can be obtained in terms of reusability.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4289914,no
Polarisation of Electromagnetic Waves Analysis for Application in Mobile Communication Systems,2006,"The polarisation property of electromagnetic waves for applications in mobile communication systems has not been examined into detail yet. Due to multi-path effects and changes in the underlying coordinate systems of the involved antennas, the polarisation state of an electromagnetic wave at the receiving antenna of a mobile station is very likely to change. The presented project involves a data measurement and detailed visual investigation of these polarisation properties in the field of GSM mobile communication at the frequency band 1800 MHz. The measuring system mounted on a special car moves along a predefined trajectory around a GSM telecommunication cell. The collected data, which are received from a dual polarisation antenna, together with the information of the location, orientation and velocity of the system, which is provided by other sensors, is subsequently processed by the standard signal analysis hardware and the digital signal processing software, so that the polarisation behaviour of the received wave can be identified and visualized. The visualisation system is based on a digitized map into which the data received from GPS is inserted specifying the actual position of the measuring system. A particular polarisation state is simultaneously assigned to the position data in the digital map. The data analysis and signal filtering is performed to estimate multi-path propagation effects and to reduce the influence of them on the quality of the link. The results of this project could be for instance used in future generations of wireless mobile communication systems, where the polarisation dependent effects may contribute to increase reliability and capacity of the transmission service. This research was performed within the AMPER project, which is supported by the EU Commission.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4345298,no
Ensuring Numerical Quality in Grid Computing,2006,"Certain numerically intensive applications executed within a grid computing environment crucially depend on the properties of floating-point arithmetic implemented on the respective platform. Differences in these properties may have drastic effects. This paper identifies the central problems related to this situation. We propose an approach which gives the user valuable information on the various platforms available in a grid computing environment in order to assess the numerical quality of an algorithm run on each of these platforms. In this manner, the user will at least have very strong hints whether a program will perform reliably in a grid before actually executing it. Our approach extends the existing IeeeCC754 test suite by two ""grid-enabled"" modes: The first mode calculates a ""numerical checksum"" on a specific grid host and executes the job only if the checksum is identical to a locally generated one. The second mode provides the user with information on the reliability and IEEE 754-conformity of the underlying floating-point implementation of various platforms. Furthermore, it can help to find a set of compiler options to optimize the application's performance while retaining numerical stability.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402410,no
Pathological Voice Assessment,2006,"While there are number of guidelines and methods used in practice, there is no standard universally agreed upon system for assessment of pathological voices. Pathological voices are primarily labeled based on the perceptual judgments of specialists, a process that may result in different label(s) being assigned to a given voice sample. This paper focuses on the recognition of five specific pathologies. The main goal is to compare two different classification methods. The first method considers single label classification by assigning a new label (single label) to the ensembles to which they most likely belong. The second method employs all labels originally assigned to the voice samples. Our results show that the pathological voice assessment performance in the second method is improved with respect to the first method",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4462091,no
Clinical Evaluation of Watermarked Medical Images,2006,"Digital watermarking medical images provides security to the images. The purpose of this study was to see whether digitally watermarked images changed clinical diagnoses when assessed by radiologists. We embedded 256 bits watermark to various medical images in the region of non-interest (RONI) and 480K bits in both region of interest (ROI) and RONI. Our results showed that watermarking medical images did not alter clinical diagnoses. In addition, there was no difference in image quality when visually assessed by the medical radiologists. We therefore concluded that digital watermarking medical images were safe in terms of preserving image quality for clinical purposes",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4463040,no
The preliminary study on the clinical application of the WHAM (Wearable Heart Activity Monitor),2006,"In this paper, we investigated the validity of the WHAM (wearable heart activity monitor) in the clinical applications, which has been implemented as a wearable ambulatory device for continuously and long-term monitoring user's cardiac conditions. To this end, using the WHAM and the conventional Holter monitor the ECG signals over 24 hours were recorded during daily activities. The signal from the WHAM was compared with that from the conventional Holter monitor in terms of the readability of the signal, the quality of the signal, and the accuracy of arrhythmia detection. The performance of the WHAM was a little lower as compared with the conventional Holter monitor, although showing no significant difference (the readability of the signal: 97.2% vs 99.3%; the quality of the signal: 0.97 vs 0.98; the accuracy of arrhythmia detection: 96.2% vs 98.1%). From these results, it is likely that the WHAM shows the performance enough to be used in the clinical application as a wearable ambulatory monitoring device",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4463183,no
Fragility Issues of Medical Video Streaming over 802.11e-WLAN m-health Environments,2006,"This paper presents some of the fragility issues of a medical video streaming over 802.11e-WLAN in m-health applications. In particular, we present a medical channel-adaptive fair allocation (MCAFA) scheme for enhanced QoS support for IEEE 802.11 (WLAN), as a modification for the standard 802.11e enhanced distributed coordination function (EDCF) is proposed for enhanced medical data performance. The medical channel-adaptive fair allocation (MCAFA) proposed extends the EDCF, by halving the contention window (CW) after zeta consecutive successful transmissions to reduce the collision probability when channel is busy. Simulation results show that MCAFA outperforms EDCF in-terms of overall performance relevant to the requirements of high throughput of medical data and video streaming traffic in 3G/WLAN wireless environments",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4463254,no
A Systematic Review of Technical Evaluation in Telemedicine Systems,2006,"We conducted a systematic review of the literature to critically analyse the evaluation and assessment frameworks that have been applied to telemedicine systems. Subjective methods were predominantly used for technical evaluation (59 %), e.g. Likert scale. Those including objective measurements (41%) were restricted to simple metrics such as network time delays. Only three papers included a rigorous standards based objective approach. Our investigation has been unable to determine a definitive standards-based telemedicine evaluation framework that exists in the literature that may be applied systematically to assess and compare telemedicine systems. We conclude that work needs to be done to address this deficiency. We have therefore developed a framework that has been used to evaluate videoconferencing systems telemedicine applications. Our method seeks to be simple to allow relatively inexperienced users to make measurements, is objective and repeatable, is standards based, is inexpensive and requires little specialist equipment. We use the EIA 1956 broadcast test card to assess resolution, grey scale and for astigmatism. Colour discrimination is assessed with the TE 106 and Ishihara 24 colour scale chart. Network protocol analysis software is used to assess network performance (throughput, delay, jitter, packet loss)",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4463255,no
Improved automated quantification of left ventricular size and function from cardiac magnetic resonance images,2006,"Assessment of left ventricular (LV) size and function from cardiac magnetic resonance (CMR) images requires manual tracing of LV borders on multiple 2D slices, which is subjective, experience dependent, tedious and time-consuming. We tested a new method for automated dynamic segmentation of CMR images based on a modified region-based model, in which a level set function minimizes a functional containing information regarding the probability density distribution of the gray levels. Images (GE 1.5T FIESTA) obtained in 9 patients were analyzed to automatically detect LV endocardial boundaries and calculate LV volumes and ejection fraction (EF). These measurements were validated against manual tracing. The automated calculation ofLV volumes and EF was completed in each patient in <3 min and resulted in high level of agreement with no significant bias and narrow limits of agreement with the reference technique. The proposed technique allows fast automated detection of endocardial boundaries as a basis for accurate quantification of LV size and function from CMR images.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4511786,no
An Ontology-Based Approach for Domain Requirements Elicitation and Analysis,2006,"Domain requirements are fundamental for software reuse and are the product of domain analysis. This paper presents an ontology based approach to elicit and analyze domain requirements. An ontology definition is given out. Problem domain is decomposed into several sub problem domains by using subjective decomposition method. The top-down refinement method is used to refine each sub problem domain into primitive requirements. Abstract stakeholders are used instead of real ones when decomposing problem domain and domain primitive requirements are represented by ontology. Not only domain commonality, variability and qualities are presented, but also reasoning logic is used to detect and handle incompleteness and inconsistency of domain requirements. In addition, a case of 'spot and futures transaction' domain is used to illustrate the approach",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4673732,no
Grid Connection to Stand Alone Transitions of Slip Ring Induction Generator During Grid Faults,2006,"A grid connected power generation systems based on the superior controllers of an active and reactive power are useless during a grid failures like grid short-circuit or line braking. Therefore the change of operation mode from grid connection to stand alone allows for uninterruptible supply of a selected part of grid connected load. However, in the stand alone operation mode the superior controllers should provide fixed amplitude and frequency of the generated voltage in spite of the load nature. Moreover, a soft transition from grid connection mode to stand alone operation requires that, the mains outage detection method must be applied. A grid voltage recovery requires change of the generator operational mode from stand alone to grid connection. However, the protection of a load from rapid change of the supply voltage phase is necessary. This may be achieved by synchronization of the generated and grid voltages and controllable soft connection of the generator to the grid. The paper presents the transients of controllable soft connection and disconnection to the grid of the variable speed doubly fed induction generator (DFIG) power system. A description of the mains outage detection methods for the DFIG is based on the grid voltage amplitude and frequency measurement and comparison with a standard values. Also an angle controller, between generated and grid voltages, for synchronization process is described. The short description of the sensorless direct voltage control of the autonomous doubly fed induction generator (ADFIG) is presented. All the presented methods are proved based on PSIM simulation software and in a laboratorial conditions and the oscillograms with a test results are presented in the paper. A 2.2 kW slip-ring induction machine was applied as a generator and 3.5 kW DC motor was used as a primary mover to speed adjusting. A switching and sampling frequencies are equal to 8 kHz. For filtering the switching frequency distortions in the output volta- - ge external capacitances equal to 21 muF per phase are connected to the stator. The control algorithm is implemented in a DSP controller build on a floating point ADSP-21061 with an Altera/FPGA support",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4778033,no
Operational Fault Detection in cellular wireless base-stations,2006,"The goal of this work is to improve availability of operational base-stations in a wireless mobile network through non-intrusive fault detection methods. Since revenue is generated only when actual customer calls are processed, we develop a scheme to minimize revenue loss by monitoring real-time mobile user call processing activity. The mobile user call load profile experienced by a base-station displays a highly non-stationary temporal behavior with time-of-day, day-of-the-week and time-of-year variations. In addition, the geographic location also impacts the traffic profile, making each base-station have its own unique traffic patterns. A hierarchical base-station fault monitoring and detection scheme has been implemented in an IS-95 CDMA Cellular network that can detect faults at - base station level, sector level, carrier level, and channel level. A statistical hypothesis test framework, based on a combination of parametric, semi-parametric and non-parametric test statistics are defined for determining faults. The fault or alarm thresholds are determined by learning expected deviations during a training phase. Additionally, fault thresholds have to adapt to spatial and temporal mobile traffic patterns that slowly changes with seasonal traffic drifts over time and increasing penetration of mobile user density. Feedback mechanisms are provided for threshold adaptation and self-management, which includes automatic recovery actions and software reconfiguration. We call this method, Operational Fault Detection (OFD). We describe the operation of a few select features from a large family of OFD features in Base Stations; summarize the algorithms, their performance and comment on future work.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4798311,no
On effective use of reliability models and defect data in software development,2006,"In software technology today, several development methodologies such as extreme programming and open source development increasingly use feedback from customer testing. This makes the customer defect data become more readily available. This paper proposes an effective use of reliability models and defect data to help managers make software release decisions by applying a strategy for selecting a suitable reliability model, which best fits the customer defect data as testing progresses. We validate the proposed approach in an empirical study using a dataset of defect reports obtained from testing of three releases of a large medical system. The paper describes detailed results of our experiments and concludes with suggested guidelines on the usage of reliability models and defect data.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5507460,no
Empirical Validation of Three Software Metrics Suites to Predict Fault-Proneness of Object-Oriented Classes Developed Using Highly Iterative or Agile Software Development Processes,2007,"Empirical validation of software metrics suites to predict fault proneness in object-oriented (OO) components is essential to ensure their practical use in industrial settings. In this paper, we empirically validate three OO metrics suites for their ability to predict software quality in terms of fault-proneness: the Chidamber and Kemerer (CK) metrics, Abreu's Metrics for Object-Oriented Design (MOOD), and Bansiya and Davis' Quality Metrics for Object-Oriented Design (QMOOD). Some CK class metrics have previously been shown to be good predictors of initial OO software quality. However, the other two suites have not been heavily validated except by their original proposers. Here, we explore the ability of these three metrics suites to predict fault-prone classes using defect data for six versions of Rhino, an open-source implementation of JavaScript written in Java. We conclude that the CK and QMOOD suites contain similar components and produce statistical models that are effective in detecting error-prone classes. We also conclude that the class components in the MOOD metrics suite are not good class fault-proneness predictors. Analyzing multivariate binary logistic regression models across six Rhino versions indicates these models may be useful in assessing quality in OO classes produced using modern highly iterative or agile software development processes.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4181709,yes
Predicting Defects for Eclipse,2007,"We have mapped defects from the bug database of eclipse (one of the largest open-source projects) to source code locations. The resulting data set lists the number of pre- and post-release defects for every package and file in the eclipse releases 2.0, 2.1, and 3.0. We additionally annotated the data with common complexity metrics. All data is publicly available and can serve as a benchmark for defect prediction models.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273265,yes
Empirical Analysis of Software Fault Content and Fault Proneness Using Bayesian Methods,2007,"We present a methodology for Bayesian analysis of software quality. We cast our research in the broader context of constructing a causal framework that can include process, product, and other diverse sources of information regarding fault introduction during the software development process. In this paper, we discuss the aspect of relating internal product metrics to external quality metrics. Specifically, we build a Bayesian network (BN) model to relate object-oriented software metrics to software fault content and fault proneness. Assuming that the relationship can be described as a generalized linear model, we derive parametric functional forms for the target node conditional distributions in the BN. These functional forms are shown to be able to represent linear, Poisson, and binomial logistic regression. The models are empirically evaluated using a public domain data set from a software subsystem. The results show that our approach produces statistically significant estimations and that our overall modeling method performs no worse than existing techniques.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4302779,yes
Using Software Dependencies and Churn Metrics to Predict Field Failures: An Empirical Case Study,2007,"Commercial software development is a complex task that requires a thorough understanding of the architecture of the software system. We analyze the Windows Server 2003 operating system in order to assess the relationship between its software dependencies, churn measures and post-release failures. Our analysis indicates the ability of software dependencies and churn measures to be efficient predictors of post-release failures. Further, we investigate the relationship between the software dependencies and churn measures and their ability to assess failure-proneness probabilities at statistically significant levels.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343764,yes
Fault Prediction using Early Lifecycle Data,2007,"The prediction of fault-prone modules in a software project has been the topic of many studies. In this paper, we investigate whether metrics available early in the development lifecycle can be used to identify fault-prone software modules. More precisely, we build predictive models using the metrics that characterize textual requirements. We compare the performance of requirements-based models against the performance of code-based models and models that combine requirement and code metrics. Using a range of modeling techniques and the data from three NASA projects, our study indicates that the early lifecycle metrics can play an important role in project management, either by pointing to the need for increased quality monitoring during the development or by using the models to assign verification and validation activities.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402215,yes
Data Mining Static Code Attributes to Learn Defect Predictors,2007,"The value of using static code attributes to learn defect predictors has been widely debated. Prior work has explored issues like the merits of ""McCabes versus Halstead versus lines of code counts"" for generating defect predictors. We show here that such debates are irrelevant since how the attributes are used to build predictors is much more important than which particular attributes are used. Also, contrary to prior pessimism, we show that such defect predictors are demonstrably useful and, on the data studied here, yield predictors with a mean probability of detection of 71 percent and mean false alarms rates of 25 percent. These predictors would be useful for prioritizing a resource-bound exploration of code that has yet to be inspected",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4027145,yes
Inverse wave field extrapolation: a different NDI approach to imaging defects,2007,"Nondestructive inspection (NDI) based on ultrasound is widely used. A relatively recent development for industrial applications is the use of ultrasonic array technology. Here, ultrasonic beams generated by array transducers are controlled by a computer. This makes the use of arrays more flexible than conventional single-element transducers. However, the inspection techniques have principally remained unchanged. As a consequence, the properties of these techniques, as far as characterization and sizing are concerned, have not improved. For further improvement, in this paper we apply imaging theory developed for seismic exploration of oil and gas fields on the NDI application. Synthetic data obtained from finite difference simulations is used to illustrate the principle of imaging. Measured data is obtained with a 64-element linear array (4 MHz) on a 20-mm thick steel block with a bore hole to illustrate the imaging approach. Furthermore, three examples of real data are presented, representing a lack of fusion defect, a surface breaking crack, and porosity",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4037306,no
Integration of Ground-Penetrating Radar and Laser Position Sensors for Real-Time 3-D Data Fusion,2007,"Full-resolution 3-D ground-penetrating radar (GPR) imaging of the near surface should be simple and efficient. Geoscientists, archeologists, and engineers need a tool capable of generating interpretable subsurface views at centimeter-to-meter resolution of field sites ranging from smooth parking lots to rugged terrain. The authors have integrated novel rotary laser positioning technology with GPR into such a 3-D imaging system. The laser positioning enables acquisition of centimeter accurate x, y, and z coordinates from multiple small detectors attached to moving GPR antennas. Positions streaming with 20 updates/s from each detector are fused in real time with the GPR data. The authors developed software for automated data acquisition and real time 3-D GPR data quality control on slices at selected depths. Industry standard (SEGY) format data cubes and animations are generated within an hour after the last trace has been acquired. Such instant 3-D GPR can be used as an on-site imaging tool supporting field work, hypothesis testing, and optimized excavation and sample collection in the exploration of the static and dynamic nature of the shallow subsurface",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4039620,no
Reliable Effects Screening: A Distributed Continuous Quality Assurance Process for Monitoring Performance Degradation in Evolving Software Systems,2007,"Developers of highly configurable performance-intensive software systems often use in-house performance-oriented ""regression testing"" to ensure that their modifications do not adversely affect their software's performance across its large configuration space. Unfortunately, time and resource constraints can limit in-house testing to a relatively small number of possible configurations, followed by unreliable extrapolation from these results to the entire configuration space. As a result, many performance bottlenecks escape detection until systems are fielded. In our earlier work, we improved the situation outlined above by developing an initial quality assurance process called ""main effects screening"". This process 1) executes formally designed experiments to identify an appropriate subset of configurations on which to base the performance-oriented regression testing, 2) executes benchmarks on this subset whenever the software changes, and 3) provides tool support for executing these actions on in-the-field and in-house computing resources. Our initial process had several limitations, however, since it was manually configured (which was tedious and error-prone) and relied on strong and untested assumptions for its accuracy (which made its use unacceptably risky in practice). This paper presents a new quality assurance process called ""reliable effects screening"" that provides three significant improvements to our earlier work. First, it allows developers to economically verify key assumptions during process execution. Second, it integrates several model-driven engineering tools to make process configuration and execution much easier and less error prone. Third, we evaluate this process via several feasibility studies of three large, widely used performance-intensive software frameworks. Our results indicate that reliable effects screening can detect performance degradation in large-scale systems more reliably and with significantly less resources than conventional - echniques",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4052587,no
An Effective PSO-Based Memetic Algorithm for Flow Shop Scheduling,2007,"This paper proposes an effective particle swarm optimization (PSO)-based memetic algorithm (MA) for the permutation flow shop scheduling problem (PFSSP) with the objective to minimize the maximum completion time, which is a typical non-deterministic polynomial-time (NP) hard combinatorial optimization problem. In the proposed PSO-based MA (PSOMA), both PSO-based searching operators and some special local searching operators are designed to balance the exploration and exploitation abilities. In particular, the PSOMA applies the evolutionary searching mechanism of PSO, which is characterized by individual improvement, population cooperation, and competition to effectively perform exploration. On the other hand, the PSOMA utilizes several adaptive local searches to perform exploitation. First, to make PSO suitable for solving PFSSP, a ranked-order value rule based on random key representation is presented to convert the continuous position values of particles to job permutations. Second, to generate an initial swarm with certain quality and diversity, the famous Nawaz-Enscore-Ham (NEH) heuristic is incorporated into the initialization of population. Third, to balance the exploration and exploitation abilities, after the standard PSO-based searching operation, a new local search technique named NEH_1 insertion is probabilistically applied to some good particles selected by using a roulette wheel mechanism with a specified probability. Fourth, to enrich the searching behaviors and to avoid premature convergence, a simulated annealing (SA)-based local search with multiple different neighborhoods is designed and incorporated into the PSOMA. Meanwhile, an effective adaptive meta-Lamarckian learning strategy is employed to decide which neighborhood to be used in SA-based local search. Finally, to further enhance the exploitation ability, a pairwise-based local search is applied after the SA-based search. Simulation results based on benchmarks demonstrate the effectiveness of- - the PSOMA. Additionally, the effects of some parameters on optimization performances are also discussed",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4067067,no
Wrapperâ€“Filter Feature Selection Algorithm Using a Memetic Framework,2007,"This correspondence presents a novel hybrid wrapper and filter feature selection algorithm for a classification problem using a memetic framework. It incorporates a filter ranking method in the traditional genetic algorithm to improve classification performance and accelerate the search in identifying the core feature subsets. Particularly, the method adds or deletes a feature from a candidate feature subset based on the univariate feature ranking information. This empirical study on commonly used data sets from the University of California, Irvine repository and microarray data sets shows that the proposed method outperforms existing methods in terms of classification accuracy, number of selected features, and computational efficiency. Furthermore, we investigate several major issues of memetic algorithm (MA) to identify a good balance between local search and genetic search so as to maximize search quality and efficiency in the hybrid filter and wrapper MA",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4067093,no
A Multipurpose Code Coverage Tool for Java,2007,"Most test coverage analyzers help in evaluating the effectiveness of testing by providing data on statement and branch coverage achieved during testing. If made available, the coverage information can be very useful for many other related activities, like, regression testing, test case prioritization, test-suite augmentation, test-suite minimization, etc. In this paper, we present a Java-based tool JavaCodeCoverage for test coverage reporting. It supports testing and related activities by recording the test coverage for various code-elements and updating the coverage information when the code being tested is modified. The tool maintains the test coverage information for a set of test cases on individual as well as test suite basis and provides effective visualization for the same. Open source database support of the tool makes it very useful for software testing research",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4076910,no
Inside Architecture Evaluation: Analysis and Representation of Optimization Potential,2007,"The share of software in embedded systems has been growing permanently in the recent years. Thus, software architecture as well as its evaluation has become an important part of embedded systems design to define, assess, and assure architecture and system quality. Furthermore, design space exploration can be based on architecture evaluation. To achieve an efficient exploration process, architectural decisions need to be well considered. In this paper, analysis of architecture evaluation is performed to uncover dependencies of the quality attributes which are the first class citizens of architecture evaluation. With an explicit representation of such dependencies, valuable changes of an architecture can be calculated. Next to the exploration support, the analysis results help to document architecture knowledge and make architectural decisions explicit and traceable. The development process can now be based on dependable and well documented architectural decisions. Effects of changes become more predictable. Time and costs can be saved by avoiding suboptimal changes.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4077020,no
Constructing a Reading Guide for Software Product Audits,2007,"Architectural knowledge is reflected in various artifacts of a software product. In the case of a software product audit this architectural knowledge needs to be uncovered and its effects assessed, in order to evaluate the quality of the software product. A particular problem is to find and comprehend the architectural knowledge that resides in the software product documentation. The amount of documents, and the differences in for instance target audience and level of abstraction, make it a difficult job for the auditors to find their way through the documentation. This paper discusses how the use of a technique called latent semantic analysis can guide the auditors through the documentation to the architectural knowledge they need. Using latent semantic analysis, we effectively construct a reading guide for software product audits.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4077028,no
Special Issue on Critical Reliability Challenges and Practices [Guest Editorial],2007,"The six papers in this special issue address various research challenges in the reliability-related areas including optimization, software reliability and measurements, network reliability, software quality, semisupervised learning, Bayesian approach, statistical anomaly detection, flooding attacks, genetic algorithms, multistate systems, system redundancy, service reliability, multiparadigm modeling, heuristic algorithms, and grid and distributed computing. The selected papers are briefly summarized.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4100775,no
Generalized Discrete Software Reliability Modeling With Effect of Program Size,2007,"Generalized methods for software reliability growth modeling have been proposed so far. But, most of them are on continuous-time software reliability growth modeling. Many discrete software reliability growth models (SRGM) have been proposed to describe a software reliability growth process depending on discrete testing time such as the number of days (or weeks); the number of executed test cases. In this paper, we discuss generalized discrete software reliability growth modeling in which the software failure-occurrence times follow a discrete probability distribution. Our generalized discrete SRGMs enable us to assess software reliability in consideration of the effect of the program size, which is one of the influential factors related to the software reliability growth process. Specifically, we develop discrete SRGMs in which the software failure-occurrence times follow geometric and discrete Rayleigh distributions, respectively. Moreover, we derive software reliability assessment measures based on a unified framework for discrete software reliability growth modeling. Additionally, we also discuss optimal software release problems based on our generalized discrete software reliability growth modeling. Finally, we show numerical examples of software reliability assessment by using actual fault-counting data",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4100777,no
Software Quality Analysis of Unlabeled Program Modules With Semisupervised Clustering,2007,"Software quality assurance is a vital component of software project development. A software quality estimation model is trained using software measurement and defect (software quality) data of a previously developed release or similar project. Such an approach assumes that the development organization has experience with systems similar to the current project and that defect data are available for all modules in the training data. In software engineering practice, however, various practical issues limit the availability of defect data for modules in the training data. In addition, the organization may not have experience developing a similar system. In such cases, the task of software quality estimation or labeling modules as fault prone or not fault prone falls on the expert. We propose a semisupervised clustering scheme for software quality analysis of program modules with no defect data or quality-based class labels. It is a constraint-based semisupervised clustering scheme that uses k-means as the underlying clustering algorithm. Software measurement data sets obtained from multiple National Aeronautics and Space Administration software projects are used in our empirical investigation. The proposed technique is shown to aid the expert in making better estimations as compared to predictions made when the expert labels the clusters formed by an unsupervised learning algorithm. In addition, the software quality knowledge learnt during the semisupervised process provided good generalization performance for multiple test data sets. An analysis of program modules that remain unlabeled subsequent to our semisupervised clustering scheme provided useful insight into the characteristics of their software attributes",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4100780,no
Estimating Error Rate in Defective Logic Using Signature Analysis,2007,"As feature size approaches molecular dimensions and the number of devices per chip reaches astronomical values, VLSI manufacturing yield significantly decreases. This motivates interests in new computing models. One such model is called error tolerance. Classically, during the postmanufacturing test process, chips are classified as being bad (defective) or good. The main premise in error-tolerant computing is that some bad chips that fail classical go/no-go tests and do indeed occasionally produce erroneous results actually provide acceptable performance in some applications. Thus, new test techniques are needed to classify bad chips according to categories based upon their degree, of acceptability with respect to predetermined applications. One classification criterion is error rate. In this paper, we first describe a simple test structure that is a minor extension to current scan-test and built-in self-test structures and that can be used to estimate the error rate of a circuit. We then address three theoretical issues. First, we develop an elegant mathematical model that describes the key parameters associated with this test process and incorporates bounds on the error in estimating error rate and the level of confidence in this estimate. Next, we present an efficient testing procedure for estimating the error rate of a circuit under test. Finally, we address the problem of assigning bad chips to bins based on their error rate. We show that this can be done in an efficient, hence cost-effective, way and discuss the quality of our results in terms of such concepts as increase effective yield, yield loss, and test escape",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4118682,no
Search Algorithms for Regression Test Case Prioritization,2007,"Regression testing is an expensive, but important, process. Unfortunately, there may be insufficient resources to allow for the reexecution of all test cases during regression testing. In this situation, test case prioritization techniques aim to improve the effectiveness of regression testing by ordering the test cases so that the most beneficial are executed first. Previous work on regression test case prioritization has focused on greedy algorithms. However, it is known that these algorithms may produce suboptimal results because they may construct results that denote only local minima within the search space. By contrast, metaheuristic and evolutionary search algorithms aim to avoid such problems. This paper presents results from an empirical study of the application of several greedy, metaheuristic, and evolutionary search algorithms to six programs, ranging from 374 to 11,148 lines of code for three choices of fitness metric. The paper addresses the problems of choice of fitness metric, characterization of landscape modality, and determination of the most suitable search technique to apply. The empirical results replicate previous results concerning greedy algorithms. They shed light on the nature of the regression testing search space, indicating that it is multimodal. The results also show that genetic algorithms perform well, although greedy approaches are surprisingly effective, given the multimodal nature of the landscape",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4123325,no
Early Software Reliability Prediction Using Cause-effect Graphing Analysis,2007,"Early prediction of software reliability can help organizations make informed decisions about corrective actions. To provide such early prediction, we propose practical methods to: 1) systematically identify defects in a software requirements specification document using a technique derived from cause-effect graphing analysis (CEGA); 2) assess the impact of these defects on software reliability using a recursive algorithm based on binary decision diagram (BDD) technique. Using a numerical example, we show how predicting software reliability at the requirement analysis stage could be greatly facilitated by the use of the method presented in this paper. The acronyms used throughout this paper are alphabetically listed as follows: ACEG-actually implemented cause effect graph; BCEG-benchmark cause effect graph; BDD-binary decision diagram; CEGA-cause effect graphing analysis; PACS-personal access control system; SRS-software requirements specification document",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4126345,no
An Efficient Algorithm To Analyze New Imperfect Fault Coverage Models,2007,"Fault tolerance has been an essential architectural attribute for achieving high reliability in many critical applications of digital systems. Automatic recovery and reconfiguration mechanisms play a crucial role in implementing fault tolerance because an uncovered fault may lead to a system or subsystem failure even when adequate redundancy exists. In addition, an excessive level of redundancy may even reduce the system reliability. Therefore, an accurate analysis must account for not only the system structure but also the system fault and error handling behavior. The models that capture the fault and error handling behavior are called coverage models. The appropriate coverage modeling approach depends on the type of fault tolerant techniques used. Recent research emphasizes the importance of two new categories of coverage models: Fault Level Coverage (FLC) models and one-on-one level coverage (OLC) models. However, the methods for solving FLC and OLC models are much more limited, primarily because of the complex nature of the dependency introduced by the reconfiguration mechanisms. In this paper, we propose an efficient algorithm for solving FLC and OLC models.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4126388,no
Failure Time Based Reliability Growth in Product Development and Manufacturing,2007,"The failure-in-time (FIT) rate is widely used to quantify the reliability of a electronic component. It fails to indicate the portion of the failures due to either environmental or electrical stresses or issues that are related to process/handing, manufacturing and applications. To meet this end, FIT-based corrective action driven metrics are proposed to link the failure mode (FM) with components and non-component faults. First the conventional failure mode pareto is reviewed and its deficiency is discussed. Then a new index called the failure mode rate (FMR) is introduced to monitor the FM trend and evaluate the effectiveness of corrective actions (C/As). Based on the FMR, the FIT rate is extended to non-component failure mode and further to individual failure mode in predicting the reliability of electronic products. The extended FIT rate enables product designers to narrow down the root-cause of the failure, identify the C/A ownership, and estimate the MTBF improvement. The new metrics provide a guideline for prioritizing resources to attack the critical failures with the minimum cost.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4126400,no
High Computational Performance in Code Exited Linear Prediction Speech Model Using Faster Codebook Search Techniques,2007,"The most commonly used coder for producing good quality speech at rates below 10 kbits/s is code excited linear prediction (CELP). In this time domain analysis-by-synthesis coder, the excitation signal is chosen by attempting to match the reconstructed speech waveform as closely as possible to the original speech waveform by searching through a large vector quantizer codebook. The closest matching excitation signal (taken from code book) along with pitch, linear prediction coefficients (LPC) and pitch lag are typically transmitted from the encoder side in order to reconstruct the input speech signal. Codebook search is identified as the computationally most complex module and several methods to implement the same are described along with complexity reduction due to each method",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4127412,no
"Saying, ""I Am Testing,"" is Enough to Improve the Product: An Empirical Study",2007,"We report on an empirical study performed in an industrial environment in which the impact of modifying test practices is measured. First, the efficiency of the existing test practices is measured, by analyzing the software defects using a modified version of Orthogonal Defect Classification (ODC). The study was conducted in two phases lasting seven months each. In the first phase (Phase A), defects were recorded using existing implicit testing practices. Developers were then required to make those testing activities explicit based on a simple scheme. After a few months of employing this explicit testing practice, a new defect recording phase (Phase B), which also lasted seven months, was conducted. The impacts of explicit test practices are presented in terms of the ratios of defects found by the users and the nature of the testing activities performed. Results show that even a small improvement, such as requiring testing activities to be explicit, can have a measurable positive impact on the quality of the product.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4137060,no
RPB in Software Testing,2007,"Software testing is one of the crucial activities in the system development life cycle. The purpose of testing can be quality assurance, verification and validation, or reliability estimation. In Motorola<sup>reg</sup>, Automation Testing is a methodology TM uses by GSG-iSGT (Global Software Group - iDENtrade Subcriber Group Testing) to increase the testing volume, productivity and reduce test cycle-time in cell phone software testing. In addition, automated stress testing can make the products become more robust before release to the market. In this paper, the author discuss one of the automation stress testing tools that iSGT use, which is RPB (Record and PlayBack). RPB is a platform specific desktop rapid test-case creation tool. This tool is able to capture all the information on the phone in order to have highest accurately in testing. Furthermore, RPB allows user to do testing at anytime, anywhere provided there is an Internet connection. The authors also discuss the value that automation has bought to iDENtrade phone testing together with the metrics. We will also look into the advantages of the proposed system and some discussion of the future work.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4137063,no
Thread-Sensitive Pointer Analysis for Inter-Thread Dataflow Detection,2007,"Inter-thread dataflows are of great importance in analyzing concurrent programs. Suffering from the complexity of currency, it is still difficult to identify them from Java programs precisely and efficiently. A fundamental problem in inter-thread dataflow analysis is pointer analysis. Currently, pointer analyses can be classified into context-sensitive ones and context-insensitive ones. They are either too time consuming or too imprecise and hence not suitable for scalable and high precision inter-thread dataflow analysis. To change the situation, this paper proposes a thread-sensitive pointer analysis algorithm which only uses sequences of thread starts as calling contexts. As in inter-thread dataflow analysis, the major objective is analyzing interthread dataflows not intra-thread ones, the coarse-grained context-sensitivity can not only effectively speed up the analysis, but also preserve the analyze precision as much as possible",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4144626,no
XCML: A Runtime Representation for the Context Modelling Language,2007,"The context modelling language (CML), derived from object role modeling (ORM), is a powerful approach for capturing the pertinent object types and relationships between those types in context-aware applications. Its support for data quality metrics, context histories and fact type classifications make it an ideal design tool for context-aware systems. However, CML currently lacks a suitable representation for exchanging context models and instances in distributed systems. A runtime representation can be used by context-aware applications and supporting infrastructure to exchange context information and models between distributed components, and it can be used as the storage representation when relational database facilities are not present. This paper shows the benefits of using CML for modelling context as compared to commonly used RDF/OWL-based context models, shows that translations of CML to RDF or OWL are lossy, discusses existing techniques for serialising ORM models, and presents an alternative XML-based representation for CML called XCML",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4144787,no
A Probabilistic Approach to Predict Changes in Object-Oriented Software Systems,2007,"Predicting the changes in the next release of a software system has become a quest during its maintenance phase. Such a prediction can help managers to allocate resources more appropriately which results in reducing costs associated with software maintenance activities. A measure of change-proneness of a software system also provides a good understanding of its architectural stability. This research work proposes a novel approach to predict changes in an object oriented software system. The rationale behind this approach is that in a well-designed software system, feature enhancement or corrective maintenance should affect a limited amount of existing code. The goal is to quantify this aspect of quality by assessing the probability that each class will change in a future generation. Our proposed probabilistic approach uses the dependencies obtained from the UML diagrams, as well as other data extracted from source code of several releases of a software system using reverse engineering techniques. The proposed systematic approach has been evaluated on a multi-version medium size open source project namely JFlex, the fast scanner generator for Java. The obtained results indicate the simplicity and accuracy of our approach in the comparison with existing methods in the literature",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4145022,no
NAES: A Natural Adaptive Exponential Smoothing for Channel Prediction in WLANs,2007,"The advent of wireless networks has increased the demand for research greatly. Context-aware applications must adapt to the environment in which they are inserted, and, for this, information on both device's hardware and the characteristics of the environment is crucial. In this work, we propose a method - called natural adaptive exponential smoothing (NAES) - to describe and forecast, in real time, the channel behavior of IEEE 802.11 WLAN networks. The NAES method uses a variation of the exponential smoothing technique to compute the channel quality indicators, namely the received signal strength (RSS) and the link quality. A comparison with the results obtained by the Trigg and Leach (1967)(TL) method shows that NAES outperforms TL method.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4147109,no
System Level Performance Assessment of SOC Processors with SystemC,2007,"This paper presents a system level methodology for modeling, and analyzing the performance of system-on-chip (SOC) processors. The solution adopted focuses on minimizing assessment time by modeling processors behavior only in terms of the performance metrics of interest. Formally, the desired behavior is captured through a C/C++ executable model, which uses finite state machines (FSM) as the underlying model of computation (MOC). To illustrate and validate our methodology we applied it to the design of a 16-bit reduced instruction set (RISC) processor. The performance metrics used to assess the quality of the design considered are power consumption and execution time. However, the methodology can be extended to any performance metric. The results obtained demonstrate the robustness of the proposed method both in terms of assessment time and accuracy",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4148970,no
Evaluating the Quality of Models Extracted from Embedded Real-Time Software,2007,"Due to the high cost of modeling, model-based techniques are yet to make their impact in the embedded systems industry, which still persist on maintaining code-oriented legacy systems. Re-engineering existing code-oriented systems to fit model-based development is a risky endeavor due to the cost and efforts required to maintain correspondence between the code and model. We aim to reduce the cost of modeling and model maintenance by automating the process, thus facilitating model-based techniques. We have previously proposed the use of automatic model extraction from recordings of existing embedded real-time systems. To estimate the quality of the extracted models of timing behavior, we need a framework for objective evaluation. In this paper, we present such a framework to empirically test and compare extracted models, and hence obtain an implicit evaluation of methods for automatic model extraction. We present a set of synthetic benchmarks to be used as test cases for emulating timing behaviors of diverse systems with varying architectural styles, and extract automatic models out of them. We discuss the difficulties in comparing response time distributions, and present an intuitive and novel approach along with associated algorithms for performing such a comparison. Using our empirical framework, and the comparison algorithms, one could objectively determine the correspondence between the model and the system being modeled",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4148976,no
Design of a Window Comparator with Adaptive Error Threshold for Online Testing Applications,2007,This paper presents a novel window comparator circuit whose error threshold can be adaptively adjusted according to its input signal levels. It is ideal for analog online testing applications. Advantages of adaptive comparator error thresholds over constant or relative error thresholds in analog testing applications are discussed. Analytical equations for guiding the design of proposed comparator circuitry are derived. The proposed comparator circuit has been designed and fabricated using a CMOS 0.18mu technology. Measurement results of the fabricated chip are presented,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4149084,no
Optimization of Behavioral Modeling for Codesign of Embedded System,2007,"Behavioral modeling for codesign system is transformed into internal model known as control/data flow graph and produces a register-transfer-level (RTL) model of the hardware implementation for a given schedule. The internal model for codesign system is partitioned after scheduling and its communication cost is evaluated. The communication between components is through the buffered channels, the size of the buffer is estimated by its edge cut-set and system delay for different models are achieved to measure the quality of partitioning as opposed to general partitioning approaches that use number of nodes in each partition as constraint. In this paper scheduling and allocation algorithm (SAA) discusses helpful optimization method for resource-constrained and time constrained system in high level synthesis tool. The approach is based on data path for CDFG model that capture the design information from the source file specified by VHDL language from its equivalent separate Control flow graph and data flow graph. The proposed algorithm is also compared with other algorithm through estimation of schedules with a benchmark example. The buffer size is calculated with different objectives in partitioning and optimum partitioning is proposed",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4156655,no
Web Service Recommendation Based on Client-Side Performance Estimation,2007,"Quality of service (QoS) is one of the important factors to be considered when choosing a Web service (WS). This paper focuses on performance which is one of the most important QoS attributes. Currently WS recommendation and selection are based on the advertised server-side performance. Since WS clients reside in a heterogeneous environment, performance experienced by clients for the same WS can vary widely. Therefore, WS recommendation based on server-side performance may not be very effective. We propose a WS analysis and recommendation framework that takes into account environment heterogeneity. The analysis framework helps establish WS profiles and client profiles, which are used in the recommendation process to estimate the client-side performance and to determine the most suitable WS provider. By carrying out experiments in the real world (Internet) environment, we demonstrate the fundamental concepts related to client-side performance estimation. Some recommendation scenarios which utilize these estimation techniques are also demonstrated.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4159661,no
Obstacles to Comprehension in Usage Based Reading,2007,"Usage based reading (UBR) is a recent approach to object oriented software inspections. Like other scenario based reading (SBR) techniques it proposes a prescriptive reading procedure. However, the impact of such procedures upon comprehension is not well known, and consideration has not been given to established software cognition theories. This paper describes a study examining software comprehension in UBR inspections. Participants traced the events of a UML sequence diagram through Java source code while thinking aloud. An electronic interface collected real-time data, allowing the identification of ""points of interest"", which were categorised according to issues affecting participants' performance. Together with indicators of participants' cognitive processes, this suggests that adherence to UBR scenarios is non-trivial. While UBR can detect more critical defects, we argue that a re-think of its prescriptive nature, including the use of cognition support, is required before it can become a practical reading technique.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4159676,no
Enhancing Adaptive Random Testing through Partitioning by Edge and Centre,2007,"Random testing (RT) is a simple but widely used software testing method. Recently, an approach namely adaptive random testing (ART) was proposed to enhance the fault-detection effectiveness of RT. The basic principle of ART is to enforce random test cases as evenly spread over the input domain as possible. A variety of ART methods have been proposed, and some research has been conducted to compare them. It was found that some ART methods have a preference of selecting test cases from edges of the input domain over from the centre. As a result, these methods may not perform very well under some situations. In this paper, we propose an approach to alleviating the edge preference. We also conducted some simulations and the results confirm that our new approach can improve the effectiveness of these ART methods.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4159679,no
Measuring the Strength of Indirect Coupling,2007,"It is widely accepted that coupling plays an important role in software quality, particularly in the areas of software maintenance, so effort should be made to keep coupling levels to a minimum in order to reduce the complexity of the system. We have previously introduced the concept of ""indirect"" coupling - coupling formed by relationships/dependencies that are not directly evident - with the belief that high levels of indirect coupling can constitute greater costs to maintenance as it is harder to detect. In this paper we extend our previous studies by proposing metrics that can advance our understanding of the exact relationship between indirect coupling and maintainability. In particulars the metrics focus on the reflection of ""strength"" as it is a fundamental component of coupling. We present our observations on the results of applying the metrics to existing Java applications.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4159684,no
Coupling Metrics for Predicting Maintainability in Service-Oriented Designs,2007,"Service-oriented computing (SOC) is emerging as a promising paradigm for developing distributed enterprise applications. Although some initial concepts of SOC have been investigated in the research literature, and related technologies are in the process of adoption by an increasing number of enterprises, the ability to measure the structural attributes of service-oriented designs thus predicting the quality of the final software product does not currently exist. Therefore, this paper proposes a set of metrics for quantifying the structural coupling of design artefacts in service-oriented systems. The metrics, which are validated against previously established properties of coupling, are intended to predict the quality characteristic of maintainability of service-oriented software. This is expected to benefit both research and industrial communities as existing object-oriented and procedural metrics are not readily applicable to the implementation of service-oriented systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4159685,no
Revisiting Hot Passive Replication,2007,"Passive replication has been extensively studied in the literature. However, there is no comprehensive study yet with regard to its degree of communication synchrony. Therefore, we propose a new, detailed classification of hot passive replication protocols, including a survey of the fault tolerance and performance of each class",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4159792,no
Automating the Pluto Experience: An Examination of the New Horizons Autonomous Operations Subsystem,2007,"New Horizons is a NASA sponsored mission to explore Pluto and its largest moon Charon. The New Horizons spacecraft, designed, built and operated by the Johns Hopkins University Applied Physics Laboratory (APL), was successfully launched in January 2006. New Horizon's closest encounter with Pluto will occur in the summer of 2015. Upon completion of its primary science objectives at the Pluto encounter, the spacecraft is expected to visit one or more Kuiper Belt objects in the outermost region of the solar system. This long duration mission requires high reliability and imposes some demanding fault management requirements upon the spacecraft. The spacecraft is highly redundant with onboard software that provides a rule based expert system for performing autonomous fault detection and recovery. Generally referred to as Autonomy, this software's design was largely driven by two factors, the concept of operations for the mission and the level of redundancy in the spacecraft hardware. This paper examines the unique mission requirements that drove the Autonomy design. It discusses how the Autonomy system supports all of the various phases of the mission and provides examples of how unique mission requirements of the New Horizons mission were implemented.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4161523,no
Fault-Tolerant 2D Fourier Transform with Checksum Encoding,2007,"Space-based applications increasingly require more computational power to process large volumes of data and alleviate the downlink bottleneck. In addressing these demands, commercial-off-the-shelf (COTS) systems can serve a vital role in achieving performance requirements. However, these technologies are susceptible to radiation effects in the harsh environment of space. In order to effectively exploit high-performance COTS systems in future spacecraft, proper care must be taken with hardware and software architectures and algorithms that avoid or overcome the data errors that can lead to erroneous results. One of the more common kernels in space-based applications is the 2D fast Fourier transform (FFT). Many papers have investigated fault-tolerant FFT, but no algorithm has been devised that would allow for error correction without re-computation from original data. In this paper, we present a new method of applying algorithm-based fault tolerance (ABFT) concepts to the 2D-FFT that will not only allow for error detection but also error correction within memory-constrained systems as well as ensure coherence of the data after the computation. To further improve reliability of this ABFT approach, we propose use of a checksum encoding scheme that addresses issues related to numerical precision and overflow. The performance of the fault-tolerant 2D-FFT will be presented and featured as part of a dependable range Doppler processor, which is a subcomponent of synthetic-aperture radar algorithms. This work is supported by the Dependable Multiprocessor project at Honeywell and the University of Florida, one of the experiments in the Space Technology 8 (ST-8) mission of NASA's New Millennium Program.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4161528,no
"Using Parallel Processing Tools to Predict Rotorcraft Performance, Stability, and Control",2007,"This paper discusses the development of the High Performance Computing (HPC) Collaborative Simulation and Test (CST) portfolio CST-03 program, one of the projects in the Common HPC Software Support Initiative (CHSSI) portfolio. The objective of this development was to provide computationally scalable tools to predict rotorcraft performance, stability, and control. The ability to efficiently predict and optimize vehicle performance, stability, and control from high fidelity computer models would greatly enhance the design and testing process and improve the quality of systems acquisition. Through this CHSSI development, the US Navy Test Pilot School performance, stability, and control test procedures were fully implemented in a high performance parallel computing environment. These Navy flight test support options were parallelized, implemented, and validated in the FLIGHTLAB comprehensive, multidisciplinary modeling environment. These tools were designed to interface with other CST compatible models and a standalone version of the tools (FLIGHTLAB-ASPECT) was delivered for use independent of the FLIGHTLAB development system. Tests on the MAUI Linux cluster indicated that there was over 25 times speedup using 32 CPUs. The tests also met the accuracy criteria as defined for the Beta trial.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4161583,no
Rate Control for H.264 Video With Enhanced Rate and Distortion Models,2007,"A new rate control scheme for H.264 video encoding with enhanced rate and distortion models is proposed in this work. Compared with existing H.264 rate control schemes, our scheme has offered several new features. First, the inter-dependency between rate-distortion optimization (RDO) and rate control in H.264 is resolved via quantization parameter estimation and update. Second, since the bits of the header information may occupy a larger portion of the total bit budget, which is especially true when being coded at low bit rates, a rate model for the header information is developed to estimate header bits more accurately. The number of header bits is modeled as a function of the number of nonzero motion vector (MV) elements and the number of MVs. Third, a new source rate model and a distortion model are proposed. For this purpose, coded 4 times 4 blocks are identified and the number of source bits and distortion are modeled as functions of the quantization stepsize and the complexity of coded 4 times 4 blocks. Finally, a R-D optimized bit allocation scheme among macroblocks (MBs) is proposed to improve picture quality. Built upon the above ideas, a rate control algorithm is developed for the H.264 baseline-profile encoder under the constant bit rate constraint. It is shown by experimental results that the new algorithm can control bit rates accurately with the R-D performance significantly better than that of the rate control algorithm implemented in the H.264 software encoder JM8.1a",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4162551,no
ESTIMATING LEUKOCYTE VELOCITIES FROM HIGH-SPEED 1D LINE SCANS ORIENTED ORTHOGONAL TO BLOOD FLOW,2007,"The fast and direct measurement of leukocyte behaviour in microvessels in situ is possible by labelling them with fluorescent compounds and subsequently imaging them by confocal laser scanning microscopy. Using a high-speed single line scanning technique a time dependent trace of leukocytes in microvessels can be recorded (x-t scan). These x-t traces, which are orientated orthogonal to the blood flow, are characterized by distinct, but often noise-affected elliptical streaks, which carry information on speed and frequency of leukocytes crossing the scan line. Here we describe the combination of a confocal laser scanning system and an image analysis software package that allows rapid and reproducible estimation of leukocyte velocities in microvessels",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4193301,no
Design Opportunity Tree for Requirement Management and Software Process Improvement,2007,"There are many risk items of the requirement phase that cause the defect occurring in the latter phase and the quality problems during process management and project progress. This paper designs the opportunity tree that requirement manage the defects and their problems solution as well. For the similar projects, we can estimate defects and prepare to solve them by using domain expert knowledge and the Opportunity Tree, which can greatly improve the software process. And this paper identifies risk items to produce reliable software and analyzes in the requirements phase. Also, this paper is intended to develop the relationship between defects and their causes to introduce. Therefore, using defect causes, we understand the associated relationship between defect triggers and design Opportunity Tree to manage the defects.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4197305,no
Scientific programming with Java classes supported with a scripting interpreter,2007,"jLab environment provides a Matlab/Scilab like scripting language that is executed by an interpreter, implemented in the Java language. This language supports all the basic programming constructs and an extensive set of built in mathematical routines that cover all the basic numerical analysis tasks. Moreover, the toolboxes of jLab can be easily implemented in Java and the corresponding classes can be dynamically integrated to the system. The efficiency of the Java compiled code can be directly utilised for any computationally intensive operations. Since jLab is coded in pure Java, the build from source process is much cleaner, faster, platform independent and less error prone than the similar C/C++/Fortran-based open source environments (e.g. Scilab and Octave). Neuro-Fuzzy algorithms can require enormous computation resources and at the same time an expressive programming environment. The potentiality of jLab is demonstrated by describing the implementation of a Support Vector Machine toolkit and by comparing its performance with a C/C++ and a Matlab version and across different computing platforms (i.e. Linux, Sun/Solaris and Windows XP)",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4197558,no
Composite Event Detection in Wireless Sensor Networks,2007,"Sensor networks can be used for event alarming applications. To date, in most of the proposed schemes, the raw or aggregated sensed data is periodically sent to a data consuming center. However, with this scheme, the occurrence of an emergency event such as a fire is hardly reported in a timely manner which is a strict requirement for event alarming applications. In sensor networks, it is also highly desired to conserve energy so that the network lifetime can be maximized. Furthermore, to ensure the quality of surveillance, some applications require that if an event occurs, it needs to be detected by at least k sensors where k is a user-defined parameter. In this work, we examine the timely energy-efficient k-watching event detection problem (TEKWEO). A topology-and-routing-supported algorithm is proposed which constructs a set of detection sets that satisfy the short notification time, energy conservation, and tunable quality of surveillance requirements for event alarming applications. Simulation results are shown to validate the proposed algorithm.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4197939,no
Accurate Software-Related Average Current Drain Measurements in Embedded Systems,2007,"Performing accurate average current drain measurements of digital programmable components (e.g., microcontrollers, digital signal processors, System-on-Chip, or wireless modules) is a critical and error-prone measurement problem for embedded system manufacturers due to the impulsive time-varying behavior of the current waveforms drawn from a battery in real operating conditions. In this paper, the uncertainty contributions affecting the average current measurements when using a simple and inexpensive digital multimeter are analyzed in depth. Also, a criterion to keep the standard measurement uncertainty below a given threshold is provided. The theoretical analysis is validated by means of meaningful experimental results",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4201005,no
Automatic Instruction-Level Software-Only Recovery,2007,"Software-only reliability techniques protect against transient faults without the overhead of hardware techniques. Although existing low-level software-only fault-tolerance techniques detect faults, they offer no recovery assistance. This article describes three automatic, instruction-level, software-only recovery techniques representing different trade-offs between reliability and performance",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4205122,no
QoS Management of Real-Time Data Stream Queries in Distributed Environments,2007,"Many emerging applications operate on continuous unbounded data streams and need real-time data services. Providing deadline guarantees for queries over dynamic data streams is a challenging problem due to bursty stream rates and time-varying contents. This paper presents a prediction-based QoS management scheme for real-time data stream query processing in distributed environments. The prediction-based QoS management scheme features query workload estimators, which predict the query workload using execution time profiling and input data sampling. In this paper, we apply the prediction-based technique to select the proper propagation schemes for data streams and intermediate query results in distributed environments. The performance study demonstrates that the proposed solution tolerates dramatic workload fluctuations and saves significant amounts of CPU time and network bandwidth with little overhead",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4208850,no
Independent Model-Driven Software Performance Assessments of UML Designs,2007,"In many software development projects, performance requirements are not addressed until after the application is developed or deployed, resulting in costly changes to the software or the acquisition of expensive high-performance hardware. To remedy this, researchers have developed model-driven performance analysis techniques for assessing how well performance requirements are being satisfied early in the software lifecycle. In some cases, companies may not have the expertise to perform such analysis on their software; therefore they have an independent assessor perform the analysis. This paper describes an approach for conducting independent model-driven software performance assessments of UML 2.0 designs and illustrates this approach using a real-time signal generator as a case study",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4208857,no
An Analysis of Performance Interference Effects in Virtual Environments,2007,"Virtualization is an essential technology in modern datacenters. Despite advantages such as security isolation, fault isolation, and environment isolation, current virtualization techniques do not provide effective performance isolation between virtual machines (VMs). Specifically, hidden contention for physical resources impacts performance differently in different workload configurations, causing significant variance in observed system throughput. To this end, characterizing workloads that generate performance interference is important in order to maximize overall utility. In this paper, we study the effects of performance interference by looking at system-level workload characteristics. In a physical host, we allocate two VMs, each of which runs a sample application chosen from a wide range of benchmark and real-world workloads. For each combination, we collect performance metrics and runtime characteristics using an instrumented Ken hypervisor. Through subsequent analysis of collected data, we identify clusters of applications that generate certain types of performance interference. Furthermore, we develop mathematical models to predict the performance of a new application from its workload characteristics. Our evaluation shows our techniques were able to predict performance with average error of approximately 5%",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4211036,no
Automatic Application Specific Floating-point Unit Generation,2007,"This paper describes the creation of custom floating point units (FPUs) for application specific instruction set processors (ASIPs). ASIPs allow the customization of processors for use in embedded systems by extending the instruction set, which enhances the performance of an application or a class of applications. These extended instructions are manifested as separate hardware blocks, making the creation of any necessary floating point instructions quite unwieldy. On the other hand, using a predefined FPU includes a large monolithic hardware block with considerable number of unused instructions. A customized FPU will overcome these drawbacks, yet the manual creation of one is a time consuming, error prone process. This paper presents a methodology for automatically generating floating-point units (FPUs) that are customized for specific applications at the instruction level. Generated FPUs comply with the IEEE754 standard, which is an advantage over FP format customization. Custom FPUs were generated for several Mediabench applications. Area savings over a fully-featured FPU without resource sharing of 26%-80% without resource sharing and 33%-87% with resource sharing, were obtained. Clock period increased in some cases by up to 9.5% due to resource sharing",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4211840,no
Design Fault Directed Test Generation for Microprocessor Validation,2007,"Functional validation of modern microprocessors is an important and complex problem. One of the problems in functional validation is the generation of test cases that has higher potential to find faults in the design. We propose a model based test generation framework that generates tests for design fault classes inspired from software validation. There are two main contributions in this paper. Firstly, we propose a microprocessor modeling and test generation framework that generates test suites to satisfy modified condition decision coverage (MCDC), a structural coverage metric that detects most of the classified design faults as well as the remaining faults not covered by MCDC. Secondly, we show that there exists good correlation between types of design faults proposed by software validation and the errors/bugs reported in case studies on microprocessor validation. We demonstrate the framework by modeling and generating tests for the microarchitecture of VESPA, a 32-bit microprocessor. In the results section, we show that the tests generated using our framework's coverage directed approach detects the fault classes with 100% coverage, when compared to model-random test generation",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4211892,no
Microarchitectural Support for Program Code Integrity Monitoring in Application-specific Instruction Set Processors,2007,"Program code in a computer system can be altered either by malicious security attacks or by various faults in microprocessors. At the instruction level, all code modifications are manifested as bit flips. In this work, we present a generalized methodology for monitoring code integrity at run-time in application-specific instruction set processors (ASIPs), where both the instruction set architecture (ISA) and the underlying micro architecture can be customized for a particular application domain. We embed monitoring microoperations in machine instructions, thus the processor is augmented with a hardware monitor automatically. The monitor observes the processor's execution trace of basic blocks at run-time, checks whether the execution trace aligns with the expected program behavior, and signals any mismatches. Since microoperations are at a lower software architecture level than processor instructions, the microarchitectural support for program code integrity monitoring is transparent to upper software levels and no recompilation or modification is needed for the program. Experimental results show that our microarchitectural support can detect program code integrity compromises with small area overhead and little performance degradation",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4211901,no
Algorithms for BER-Constrained Variable-Length Equalizers driven by Channel Response Knowledge over Frequency-Selective Radio Channel,2007,"In mobile radio systems, transmission conditions actually encounter large variations depending on the effective environmental configurations. Training sequences are periodically inserted into the transmitted messages so that the receiver can estimate the channel response. Based on this knowledge, we study the problem of adapting the equalizer filter length to a given channel impulse response under bit error rate constraint. In this paper, both linear equalizer (LE) and decision feedback equalizer (DFE) are studied. Accurate control of the equalizer length can improve the system's performance while reducing the handset power consumption or reducing software load in a software radio context. Besides, in a multi-service system, it allows to manage variable quality of service requirements. Optimal and suboptimal criteria for determining the equalizer length are discussed in order to optimize the overall complexity of the receiver including training phase and decoding phases.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4212794,no
Reliability Analysis of Self-Healing Network using Discrete-Event Simulation,2007,"The number of processors embedded on high performance computing platforms is continuously increasing to accommodate user desire to solve larger and more complex problems. However, as the number of components increases, so does the probability of failure. Thus, both scalable and fault-tolerance of software are important issues in this field. To ensure reliability of the software especially under the failure circumstance, the reliability analysis is needed. The discrete-event simulation technique offers an attractive a ternative to traditional Markovian-based analytical models, which often have an intractably large state space. In this paper, we analyze reliability of a self-healing network developed for parallel runtime environments using discrete-event simulation. The network is designed to support transmission of messages across multiple nodes and at the same time, to protect against node and process failures. Results demonstrate the flexibility of a discrete-event simulation approach for studying the network behavior under failure conditions and various protocol parameters, message types, and routing algorithms.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4215409,no
A Comprehensive Empirical Study of Count Models for Software Fault Prediction,2007,"Count models, such as the Poisson regression model, and the negative binomial regression model, can be used to obtain software fault predictions. With the aid of such predictions, the development team can improve the quality of operational software. The zero-inflated, and hurdle count models may be more appropriate when, for a given software system, the number of modules with faults are very few. Related literature lacks quantitative guidance regarding the application of count models for software quality prediction. This study presents a comprehensive empirical investigation of eight count models in the context of software fault prediction. It includes comparative hypothesis testing, model selection, and performance evaluation for the count models with respect to different criteria. The case study presented is that of a full-scale industrial software system. It is observed that the information obtained from hypothesis testing, and model selection techniques was not consistent with the predictive performances of the count models. Moreover, the comparative analysis based on one criterion did not match that of another criterion. However, with respect to a given criterion, the performance of a count model is consistent for both the fit, and test data sets. This ensures that, if a fitted model is considered good based on a given criterion, then the model will yield a good prediction based on the same criterion. The relative performances of the eight models are evaluated based on a one-way anova model, and Tukey's multiple comparison technique. The comparative study is useful in selecting the best count model for estimating the quality of a given software system",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4220784,no
An Assessment of Testing-Effort Dependent Software Reliability Growth Models,2007,"Over the last several decades, many Software Reliability Growth Models (SRGM) have been developed to greatly facilitate engineers and managers in tracking and measuring the growth of reliability as software is being improved. However, some research work indicates that the delayed S-shaped model may not fit the software failure data well when the testing-effort spent on fault detection is not a constant. Thus, in this paper, we first review the logistic testing-effort function that can be used to describe the amount of testing-effort spent on software testing. We describe how to incorporate the logistic testing-effort function into both exponential-type, and S-shaped software reliability models. The proposed models are also discussed under both ideal, and imperfect debugging conditions. Results from applying the proposed models to two real data sets are discussed, and compared with other traditional SRGM to show that the proposed models can give better predictions, and that the logistic testing-effort function is suitable for incorporating directly into both exponential-type, and S-shaped software reliability models",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4220785,no
Count Models for Software Quality Estimation,2007,"Identifying which software modules, during the software development process, are likely to be faulty is an effective technique for improving software quality. Such an approach allows a more focused software quality & reliability enhancement endeavor. The development team may also like to know the number of faults that are likely to exist in a given program module, i.e., a quantitative quality prediction. However, classification techniques such as the logistic regression model (lrm) cannot be used to predict the number of faults. In contrast, count models such as the Poisson regression model (prm), and the zero-inflated Poisson (zip) regression model can be used to obtain both a qualitative classification, and a quantitative prediction for software quality. In the case of the classification models, a classification rule based on our previously developed generalized classification rule is used. In the context of count models, this study is the first to propose a generalized classification rule. Case studies of two industrial software systems are examined, and for each we developed two count models, (prm, and zip), and a classification model (lrm). Evaluating the predictive capabilities of the models, we concluded that the prm, and the zip models have similar classification accuracies as the lrm. The count models are also used to predict the number of faults for the two case studies. The zip model yielded better fault prediction accuracy than the prm. As compared to other quantitative prediction models for software quality, such as multiple linear regression (mlr), the prm, and zip models have a unique property of yielding the probability that a given number of faults will occur in any module",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4220787,no
A Multi-Objective Software Quality Classification Model Using Genetic Programming,2007,"A key factor in the success of a software project is achieving the best-possible software reliability within the allotted time & budget. Classification models which provide a risk-based software quality prediction, such as fault-prone & not fault-prone, are effective in providing a focused software quality assurance endeavor. However, their usefulness largely depends on whether all the predicted fault-prone modules can be inspected or improved by the allocated software quality-improvement resources, and on the project-specific costs of misclassifications. Therefore, a practical goal of calibrating classification models is to lower the expected cost of misclassification while providing a cost-effective use of the available software quality-improvement resources. This paper presents a genetic programming-based decision tree model which facilitates a multi-objective optimization in the context of the software quality classification problem. The first objective is to minimize the ""Modified Expected Cost of Misclassification"", which is our recently proposed goal-oriented measure for selecting & evaluating classification models. The second objective is to optimize the number of predicted fault-prone modules such that it is equal to the number of modules which can be inspected by the allocated resources. Some commonly used classification techniques, such as logistic regression, decision trees, and analogy-based reasoning, are not suited for directly optimizing multi-objective criteria. In contrast, genetic programming is particularly suited for the multi-objective optimization problem. An empirical case study of a real-world industrial software system demonstrates the promising results, and the usefulness of the proposed model",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4220788,no
Efficient Analysis of Systems with Multiple States,2007,A multistate system is a system in which both the system and its components may exhibit multiple performance levels (or states) varying from perfect operation to complete failure. Examples abound in real applications such as communication networks and computer systems. Analyzing the probability of the system being in each state is essential to the design and tuning of dependable multistate systems. The difficulty in analysis arises from the non-binary state property of the system and its components as well as dependence among those multiple states. This paper proposes a new model called multistate multivalued decision diagrams (MMDD) for the analysis of multistate systems with multistate components. The computational complexity of the MMDD-based approach is low due to the nature of the decision diagrams. An example is analyzed to illustrate the application and advantages of the approach.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4220955,no
Sentient Networks: A New Dimension in Network Capability,2007,"As computer networks evolve to the next generation, they offer flexible infrastructures for communication as well as high vulnerability. In order to sustain the reliability, to prevent misuse, and to provide radically new services, these networks need new capabilities in non-traditional domains of operation such as perception and consciousness. Therefore, we propose Sentient Networks, as an early approach towards realizing them. There are several applications for such networking capability. For one, sentient networks can enable packet detection in a blackbox manner, detect encryption of data transferred, detecting and classifying data types such as voice, video, and data. Some of the applications of such capabilities, for example, include the use of emotion content of the voice packets through a network and classifying these packets into normal, moderately panicked, and extremely panicked. Such classifications can be used to provide better QoS schemes for panicked callers. We obtained a packet detection accuracy of about 65-70% for data packets and about 98% accuracy for detecting encrypted TCP packets. The emotion-aware QoS provision mechanism provided approximately 60% improvement in delay performance for voice packets generated by panicked sources.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4221027,no
Detecting VLIW Hard Errors Cost-Effectively through a Software-Based Approach,2007,"Research indicates that as technology scales, hard errors such as wear-out errors are increasingly becoming a critical challenge for microprocessor design. While hard errors in memory structures can be efficiently detected by error correction code, detecting hard errors for functional units cost-effectively is a challenging problem. In this paper, we propose to exploit the idle cycles of the under-utilized VLIW functional units to run test instructions for detecting wear-out errors without increasing the hardware cost or significantly impacting performance. We also explore the design space of this software-based approach to balance the error detection latency and the performance for VLIW architectures. Our experimental results indicate that such a software-based approach can effectively detect hard errors with minimum impact on performance for VLIW processors, which is particularly useful for reliable embedded applications with cost constraints.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4221157,no
Developing Metrics for Evolving a Musically Satisfying Result from a Population of Computer Generated Music Compositions,2007,"Creating a computational metric of the quality and musicality of a computer generated music composition is a challenging problem. This paper describes a possible way to develop numerically based metrics for standardizing the fitness evaluation of a population of computer generated music compositions. These metrics are based on assertions by music theorists about the dynamic forces within music that provides the music with a sense of motion and phrasing. The actions of several musical parameters are ranked according to the sense of intensity they produce. This process is applied to an acoustic music composition, and the values are then plotted on a graph. The composite curve from these values give the evaluator a sense of the patterns of lower and higher degrees of intensity, thus providing a sense of how well the music provides a sense of motion and phrasing",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4221413,no
On the Automatic Generation of Test Programs for Path-Delay Faults in Microprocessor Cores,2007,"Delay testing is mandatory for guaranteeing the correct behavior of today's high-performance microprocessors. Several methodologies have been proposed to tackle this issue resorting to additional hardware or to software self test techniques. Software techniques are particularly promising as they resort to Assembly programs in normal mode of operation, without requiring circuit modifications; however, the problem of generating effective and efficient test programs for path- delay fault detection is still open. This paper presents an innovative approach for the generation of path-delay self-test programs for microprocessors, based on an evolutionary algorithm and on ad-hoc software simulation/hardware emulation heuristic techniques. Experimental results show how the proposed methodology allows generating suitable test programs in reasonable times.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4221592,no
The Future of Software Performance Engineering,2007,"Performance is a pervasive quality of software systems; everything affects it, from the software itself to all underlying layers, such as operating system, middleware, hardware, communication networks, etc. Software Performance Engineering encompasses efforts to describe and improve performance, with two distinct approaches: an early-cycle predictive model-based approach, and a late-cycle measurement-based approach. Current progress and future trends within these two approaches are described, with a tendency (and a need) for them to converge, in order to cover the entire development cycle.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4221619,no
Assessment of Package Cohesion and Coupling Principles for Predicting the Quality of Object Oriented Design,2007,"In determining the quality of design two factors are important, namely coupling and cohesion. This paper highlights the principles of package architecture from cohesion and coupling point of view and discusses the method for extracting metric associated with them. The method is supported with the help of case study. The results arrived at from the case study are discussed further for utilizing them for predicting the quality of software.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4221858,no
Company-Wide Implementation of Metrics for Early Software Fault Detection,2007,"To shorten time-to-market and improve customer satisfaction, software development companies commonly want to use metrics for assessing and improving the performance of their development projects. This paper describes a measurement concept for assessing how good an organization is at finding faults when most cost-effective, i. e. in most cases early. The paper provides results and lessons learned from applying the measurement concept widely at a large software development company. A major finding was that on average, 64 percent of all faults found would have been more cost effective to find during unit tests. An in-depth study of a few projects at a development unit also demonstrated how to use the measurement concept for identifying which parts in the fault detection process that needs to be improved to become more efficient (e.g. reduce the amount of time spent on rework).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4222617,no
On Sufficiency of Mutants,2007,"Mutation is the practice of automatically generating possibly faulty variants of a program, for the purpose of assessing the adequacy of a test suite or comparing testing techniques. The cost of mutation often makes its application infeasible. The cost of mutation is usually assessed in terms of the number of mutants, and consequently the number of ""mutation operators"" that produce them. We address this problem by finding a smaller subset of mutation operators, called ""sufficient"", that can model the behaviour of the full set. To do this, we provide an experimental procedure and adapt statistical techniques proposed for variable reduction, model selection and nonlinear regression. Our preliminary results reveal interesting information about mutation operators.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4222686,no
Interaction Analysis of Heterogeneous Monitoring Data for Autonomic Problem Determination,2007,"Autonomic systems require continuous self-monitoring to ensure correct operation. Available monitoring data exists in a variety of formats, including log files, performance counters, traces, and state and configuration parameters. Such heterogeneity, together with the extremely large volume of data that could be collected, makes analysis very complex. To allow for more-effective problem determination, there is a need for a comprehensive integration of management data. In addition, monitoring should be adaptive to the current perceived operation of the system. In this paper we present an architecture to meet the above goals. We leverage an open-source XML-based format for data integration and describe an approach to automatically adjust monitoring for diagnosis when anomalies are detected. We have implemented a partial prototype using an Eclipse-based open-source platform. We show the effectiveness of our prototype based on fault-injection experiments. We also study issues of disparity of data formats, information overload, scalability, and automated problem determination.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4224159,no
A Multi-Agent Multi-Tiered Approach To Information Fusion,2007,"The amount of information available to decision makers today is astounding. To manage this, decision-aid software is needed that can organize and clarify large amounts of data. In this paper, we propose a multi-level, multi-agent architecture that accomplishes data fusion through layers encompassing initial data input and classification, secondary classification and grouping, and finally human-agent interaction. Small, limited-scope agents cooperate to complete the analysis/fusion process. This cooperation is mediated by corresponding opinions in the mode of belief calculus and subjective logic utilizing our evidential reasoning network (ERN). Thus, the user gains the ability to control and filter fusion activities based on the quality and pedigree of the underlying data. We present an initial set of agents along with a preliminary testing scenario and discuss the results of running the system through this scenario",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4227551,no
Fast Failure Detection in a Process Group,2007,"Failure detectors represent a very important building block in distributed applications. The speed and the accuracy of the failure detectors is critical to the performance of the applications built on them. In a common implementation of failure detector based on heartbeats, there is a tradeoff between speed and accuracy so it is difficult to be both fast and accurate. Based on the observation that in many distributed applications, one process takes a special role as the leader, we propose a fast failure detection (FFD) algorithm that detects the failure of the leader both fast and accurately. Taking advantage of spatial multiple timeouts, FFD detects the failure of the leader within a time period of just a little more than one heartbeat interval, making it almost the fastest detection algorithm possible based on heartbeat messages. FFD could be used stand alone in a static configuration where the leader process is fixed at one site. In a dynamic setting, where the role of leader has to be assumed by another site if the current leader fails, FFD could be used in collaboration with a leader election algorithm to speed up the process of electing a new leader.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4228024,no
Annotation Integration and Trade-off Analysis for Multimedia Applications,2007,"Multimedia applications for mobile devices, such as video/audio streaming, process streams of incoming data in a regular, predictable way. Content-aware optimizations through annotations allow us to highly improve the power savings at the various levels of abstraction: hardware/OS, network, application. However, in a typical system there is a continuous interaction between the components of the system at all levels, which requires a careful analysis of the combined effect of the aforementioned techniques. We investigate such an interaction and we describe metrics for estimating the effect various trade-off have on power and quality. By applying our metrics at the various abstraction levels we show how better energy savings can be achieved with lower quality degradations, through power-quality trade-offs and cross-layer interaction.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4228259,no
Management of Virtual Machines on Globus Grids Using GridWay,2007,"Virtual machines are a promising technology to overcome some of the problems found in current grid infrastructures, like heterogeneity, performance partitioning or application isolation. In this work, we present straightforward deployment of virtual machines in globus grids. This solution is based on standard services and does not require additional middleware to be installed. Also, we assess the suitability of this deployment in the execution of a high throughput scientific application, the XMM-Newton scientific analysis system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4228276,no
Incorporating Latency in Heterogeneous Graph Partitioning,2007,"Parallel applications based on irregular meshes make use of mesh partitioners for efficient execution. Some mesh partitioners can map a mesh to a heterogeneous computational platform, where processor and network performance may vary. Such partitioners generally model the computational platform as a weighted graph, where the weight of a vertex gives relative processor performance, and the weight of a link indicates the relative transmission rate of the link between two processors. However, the performance of a network link is typically characterized by two parameters, bandwidth and latency, which cannot be captured in a single weight. We show that taking into account the network heterogeneity of a computational resource can significantly improve the quality of a domain decomposition obtained using graph partitioning. Furthermore, we show that taking into account bandwidth and latency of the network links is significantly better than just considering the former. This work is presented as an extension to the PaGridpartitioner, and includes a model for estimated execution time, which is used as a cost function by the partitioner but could also be used for performance prediction by application-oriented schedulers.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4228305,no
A Probabilistic Approach to Measuring Robustness in Computing Systems,2007,"System builders are becoming increasingly interested in robust design. We believe that a methodology for generating robustness metrics helps the robust design research efforts and, in general, is an important step in the efforts to create robust computing systems. The purpose of the research in this paper is to quantify the robustness of a resource allocation, with the eventual objective of setting a standard that could easily be instantiated for a particular computing system to generate robustness metric. We present our theoretical foundation for robustness metric and give its instantiation for a particular system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4228308,no
Implementing Adaptive Performance Management in Server Applications,2007,"Performance and scalability are critical quality attributes for server applications in Internet-facing business systems. These applications operate in dynamic environments with rapidly fluctuating user loads and resource levels, and unpredictable system faults- Adaptive (autonomic) systems research aims to augment such server applications with intelligent control logic that can detect and react to sudden environmental changes. However, developing this adaptive logic is complex in itself. In addition, executing the adaptive logic consumes processing resources, and hence may (paradoxically) adversely effect application performance. In this paper we describe an approach for developing high-performance adaptive server applications and the supporting technology. The Adaptive Server Framework (ASF) is built on standard middleware services, and can be used to augment legacy systems with adaptive behavior without needing to change the application business logic. Crucially, ASF provides built-in control loop components to optimize the overall application performance, which comprises both the business and adaptive logic. The control loop is based on performance models and allows systems designers to tune the performance levels simply by modifying high level declarative policies. We demonstrate the use of ASF in a case study.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4228612,no
An Evolutionary Approach to Software Modularity Analysis,2007,"Modularity determines software quality in terms of evolvability, changeability, maintainability, etc. and a module could be a vertical slicing through source code directory structure or class boundary. Given a modularized design, we need to determine whether its implementation realizes the designed modularity. Manually comparing source code modular structure with abstracted design modular structure is tedious and error-prone. In this paper, we present an automated approach to check the conformance of source code modularity to the designed modularity. Our approach uses design structure matrices (DSMs) as a uniform representation; it uses existing tools to automatically derive DSMs from the source code and design, and uses a genetic algorithm to automatically cluster DSMs and check the conformance. We applied our approach to a small canonical software system as a proof of concept experiment. The results supported our hypothesis that it is possible to check the conformance between source code structure and design structure automatically, and this approach has the potential to be scaled for use in large software systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4228628,no
Assessing Module Reusability,2007,"We propose a conceptual framework for assessing the reusability of modules. To do so, we define reusability of a module as the product of its functionality and its applicability. We then generalize the framework to the assessment of modularization techniques.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4228629,no
Protein Attributes Microtuning System (PAMS): an effective tool to increase protein structure prediction by data purification,2007,"Given the expense of more direct determinations, using machine-learning schemes to predict a protein secondary structure from the sequence alone remains an important methodology. To achieve significant improvements in prediction accuracy, the authors have developed an automated tool to prepare very large biological datasets, to be used by the learning network. By focusing on improvements in data quality and validation, our experiments yielded a highest prediction accuracy of protein secondary structure of 90.97%. An important additional aspect of this achievement is that the predictions are based on a template-free statistical modeling mechanism. The performance of each different classifier is also evaluated and discussed. In this paper a protein set of 232 protein chains are proposed to be used in the prediction. Our goal is to make the tools discussed available as services in part of a digital ecosystem that supports knowledge sharing amongst the protein structure prediction community.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4233773,no
The Reduction of Simulation Software Execution Time for Models of Integrated Electric Propulsion Systems through Partitioning and Distribution,2007,"Software time-domain simulation models are useful to the naval engineering community both for the system design of future vessels and for the in-service support of existing vessels. For future platforms, the existence of a model of the vessel's electrical power system provides a means of assessing the performance of the system against defined requirements. This could be at the stage of requirements definition, bid assessment or any subsequent stage in the design process. For in-service support of existing platforms, the existence of a model of the vessel's electrical power system provides a means of assessing the possible cause and effect of operational defects reported by ship's staff, or of assessing the possible future implications of some change in the equipment line-up or operating conditions for the vessel. Detailed high fidelity time-domain simulation of systems, however, can be problematic due to extended execution time. This arises from the model's mathematically stiff nature: models of Integrated Electric Propulsion systems can also require significant computational resource. A conventional time-domain software simulation model is only able to utilize a single computer processor at any one time. The duration of time required to obtain results from a software model could be significantly reduced if more computer processors were utilized simultaneously. This paper details the development of a distributed simulation environment. This environment provides a mechanism for partitioning a time-domain software simulation model and running it on a cluster of computer processors. The number of processors utilized in the cluster ranges between four and sixteen nodes. The benefit of this approach is that reductions in simulation duration are achievable by an appropriate choice of model partitioning. From an engineering perspective, any net timing reduction translates to an increase in the availability of data, from which more efficient analysis and design follows.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4233799,no
"On the Role of Numerical Preciseness for Generalization, Classification, Type-1, and Type-2 Fuzziness",2007,"When performing data analysis on a computing device no mathematically idealized real number set IR is available. A basic resolution is given, so that a fuzzy model is in fact always a discrete model and not a continuous one. Due to the limited preciseness the computing device offers only a limited number of decimals in a limited discrete number space OR. This contribution considers effects on the generalization and fuzziness of data when replacing IR by IIR The effects are studied for data, that are numerically rounded or when intervals are considered. Often part of the data is missing or is of limited quality, so that it is of practical interest to consider the exact underlying space IIR and not the hypothetical space IR. We calculate precisely type-1 and type-2 fuzzy membership functions under preciseness assumptions of the elements in IIR.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4233908,no
Software Reliability Models,2007,"The reliability is one very important parameter of electronic devices, hardware, and applications software. We review in this paper basic reliability terms, software reliability models -predictive models, assessment models. We try to explain basic principles of software reliability models, their practical using in technical activities. We show also key assumptions on which software reliability models are based, specific assumptions related to each specific software reliability model and some examples of concrete software reliability models. In conclusions there are summary of knowledge about software reliability models.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4234177,no
A Divergence-measure Based Classification Method for Detecting Anomalies in Network Traffic,2007,"We present 'D-CAD,' a novel divergence-measure based classification method for anomaly detection in network traffic. The D-CAD method identifies anomalies by performing classification on features drawn from software sensors that monitor network traffic. We compare the performance of the D-CAD method with two classifier based anomaly detection methods implemented using supervised Bayesian estimation and supervised maximum-likelihood estimation. Results show that the area under receiver operating characteristic curve (AUC) of the D-CAD method is as high as 0.9524, compared to an AUC value of 0.9102 of the supervised maximum-likelihood estimation based anomaly detection method and to an AUC value of 0.8887 of the supervised Bayesian estimation based anomaly detection method.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4239021,no
Fault Tolerant Control in NCS Medium Access Constraints,2007,"This paper deals with the problem of fault-tolerant control of a Network Control System (NCS) for the case in which the sensors, actuators and controller are inter-connected via various Medium Access Control protocols which define the access scheduling and collision arbitration policies in the network and employing the so-called periodic communication sequence. A new procedure for controlling a system over a network using the concept of an NCS-Information-Packet is described which comprises an augmented vector consisting of control moves and fault flags. The size of this packet is used to define a <i>Completely Fault Tolerant NCS.</i> The fault-tolerant behaviour and control performance of this scheme is illustrated through the use of a process model and controller. The plant is controlled over a network using Model-based Predictive Control and implemented via MATLABcopy and LABVIEWcopy software.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4239028,no
Intelligent Monitoring System Based on the Embedded Technology: A Case Study,2007,"Intelligent systems for condition monitoring, alarm diagnosis and fault detection have been widely used in modern industrial manufacturing, as they are capable of providing more efficient decision making support. This paper proposes a distributed intelligent monitoring system based on the embedded technology. Using advanced RISC machines microprocessor embedded with a real-time multi-task micro kernel, the developed system possesses the functions such as data acquisition, signal preprocessing, mass storage, realtime monitoring, and remote intelligent control. In virtue of a wireless CDMA communication network, it is especially appropriate to be applied to the circumstance where building a wired network is not feasible. Design and implementation about the hardware and software of the system are presented in detail. The high performance, reliability and security of the system have been validated in the case of remote intelligent monitoring system in industrial oil exploration and production. Due to its flexible structures of both software and hardware, the developed system is easy to be adapted to other applications.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4239112,no
An Improved CFCSS Control Flow Checking Algorithm,2007,"Satellite-borne embedded systems require the properties of low-powered and reliability in the spatial radiation environment. The control flow checking is an effective way for the running systems to prevent the broken-down caused by single event upsets. Control flow checking by software signatures (CFCSS) is a representative of pure software method that checks the control flow of a program using assigned signatures. Because of the existence of multiple-branch-in (MBI) nodes, the fault detection coverage may decrease in this algorithm. To overcome this shortcoming, an improved algorithm called improved control flow checking by software signatures (ICFCSS) that eliminates the MBI node by modifying the control flow graph (CFG) is presented in this paper. Fault injection experiments show that ICFCSS incurs higher fault detection coverage than CFCSS techniques, without significant performance decreasing.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4244831,no
Fault Tolerant Signal Processing for Masking Transient Errors in VLSI Signal Processors,2007,"This paper proposes fault tolerant signal processing strategies for achieving reliable performance in VLSI signal processors that are prone to transient errors due to increasingly smaller feature dimensions and supply voltages. The proposed methods are based on residue number system (RNS) coding, involving either hardware redundancy or multiple execution redundancy (MER) strategies designed to identify and overcome transient errors. RNS techniques provide powerful low-redundancy fault tolerance properties that must be introduced at VLSI design levels, whereas MER strategies generally require higher degrees of redundancy that can be introduced at software programming levels.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4253202,no
A Comparative Analysis of the Influence of Methods for Outliers Detection on the Performance of Data Driven Models,2007,"In this paper we describe, test, and compare the performance of a number of techniques used for outlier detection to improve modeling capabilities of soft sensors on the basis of the quality of available data. We analyze methods based on standard deviation of population, on residuals of a linear input-output regression, on the structure correlation of the data, on principal components and partial least squares (both linear and nonlinear) in multi dimensional space (2D, 3D, 4D), on Q and T2 statistics, on the distance of each observation from the mean of the data, and on the Mahalanobis distance. We apply techniques for outlier detection both on a fictitious model data and on real data acquired from a sulfur recovery unit of a refinery. We show that outlier removal almost always improves modeling capabilities of considered techniques.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4258091,no
Ground Penetrating Radar: A Smart Sensor for the Evaluation of the Railway Trackbed,2007,"Ground Penetrating Radar (GPR) has become an increasingly attractive method for the engineering community, in particular for shallow high-resolution applications such as railway trackbed evaluation. It is a non-destructive smart sensing technique, which can be applied dynamically to achieve a continuous profile of the trackbed structure. Due to recent hardware and software improvements, real time cursory analysis can be performed in the field. Based on collected field data, the present paper investigates the applicability of the GPR smart sensor system in terms of the railways trackbed assessment and concludes on the capability of the GPR sensing technique to assess adequately the ballast quality and the trackbed formation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4258127,no
A new FPGA-based edge detection system for the gridding of DNA microarray images,2007,"A deoxyribonucleic acid (DNA) microarray is a collection of microscopic DNA spots attached to a solid surface, such as glass, plastic or silicon chip forming an array. DNA microarray technologies are an essential part of modern biomedical research. The analysis of DNA microarray images allows the identification of gene expressions in order to draw biologically meaningful conclusions for applications that ranges from the genetic profiling to the diagnosis of oncology diseases. Unfortunately, DNA microarray technology has a high variation of data quality. Therefore, in order to obtain reliable results, complex and extensive image analysis algorithms should be applied before actual DNA microarray information can be used for biomedical purpose. In this paper, we present a novel hardware acceleration architecture specifically designed to process and measure DNA microarray images. The proposed architecture uses several units working in a single instruction-multiple data fashion managed by a microprocessor core. An FPGA-based prototypal implementation of the developed architecture is presented. Experimental results on several realistic DNA microarray images show a reduction of the computation time of one order of magnitude if compared with previously developed software-based approach.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4258145,no
Open Architecture Software Design for Online Spindle Health Monitoring,2007,"This paper presents an open systems architecture-based software design for an online spindle health monitoring system. The software is implemented using the graphical programming language of LabVIEW, and presents the spindle health status in two types of windows: simplified spindle condition display and warning window for standard machine operators (operator window) and advanced diagnosis window for machine experts (expert window). The capability of effective and efficient spindle defect detection and localization has been realized using the analytic wavelet-based envelope spectrum algorithm. The software provides a user-friendly human-machine interface and contributes directly to the development of a new generation of smart machine tools.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4258268,no
A digital signal processing approach for modulation quality assessment in WiMAX systems,2007,"Modulation quality in WiMAX systems that rely on OFDM modulation is dealt with. A digital signal processing approach is, in particular, proposed, aimed at assessing the performance of transmitters in terms of standard parameters. At present, WiMAX technology deployment is at the very beginning. Also, measurement instrumentation mandated to assist WiMAX apparatuses production and installation is not completely mature, and entitled to be significantly improved in terms of functionality and performance. In particular, no dedicated instrument is already present on the market, but the available solutions are arranged complementing existing hardware, such as real-time spectrum analyzers and vector signal analyzers, with a proper analysis software. Differently from the aforementioned solutions, the proposed approach is independent of the specific hardware platform mandated to the demodulation of the incoming WiMAX signal. It can operate, in fact, with any hardware capable of achieving and delivering the baseband I and Q components of the signal under analysis. Moreover, being open-source it can be improved or upgraded according to future needs.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4258506,no
Toward Globally Optimal Event Monitoring & Aggregation For Large-scale Overlay Networks,2007,"Overlay networks have emerged as a powerful and flexible platform for developing new disruptive network applications. The performance and reliability of overlay applications depend on the capability of overlay networks to dynamically adapt to various factors such as link/node failures, overlay link quality, and overlay node characteristics. In order to achieve this, the overlay applications require scalable and open overlay monitoring services to monitor, aggregate globally distributed events and take appropriate control actions. In this paper, we propose the techniques and algorithms to create an optimal event monitoring and aggregation infrastructure (called MOON) that minimizes the monitoring latency (i.e., event retrival/detection time) and event aggregation cost (i.e., intrusiveness) considering the large-scale geographical and network distribution of overlay nodes. The proposed monitoring infrastructure, MOON, clusters and organizes overlay nodes efficiently such that overlay applications can globally monitor and query correlated events in an overlay network with minimum latency and monitoring cost. Our simulations and experimental studies show the evaluation of MOON under many various topological structures, network sizes, and event aggregation volumes.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4258540,no
Reducing Complexity of Software Deployment with Delta Configuration,2007,"Deploying a modern software service usually involves installing several software components, and configuring these components properly to realize the complex interdependencies between them. This process, which accounts for a significant portion of information technology (IT) cost, is complex and error-prone. In this paper, we propose delta configuration - an approach that reduces the cost of software deployment by eliminating a large number of choices on parameter values that administrators have to make during deployment. In delta configuration, the complex software stack of a distributed service is first installed and tested in a test environment. The resulting software images are then captured and used for deployment in production environments. To deploy a software service, we only need to copy these pre-configured software images into a production environment and modify them to account for the difference between the test environment and a production environment. We have implemented a prototype system that achieves software deployment using delta configuration of the configuration state captured inside virtual machines. We perform a case study to demonstrate that our scheme leads to substantial reduction in complexity for the customer, over the traditional software deployment method.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4258590,no
Accelerated 65nm Yield Ramp through Optimization of Inspection on Process-Design Sensitive Test Chips,2007,"This paper describes an integrated methodology that combines short-flow test chips useful for exploring process-design systematic as well as random failure modes and an advanced inspection tool platform to characterize and monitor key Defects-of-Interest for accelerated defect-based yield learning at the 65 nm technology node. Utilization of a unique fast electrical testing scheme, rapid analysis software along with optimized inspection facilitated shorter learning cycles for accelerated process development. Knowledge derived from the CVreg- based inspection setup in a leading 300 mm fab was successfully transferred to manufacturing to facilitate inspection optimization for key Defects-of-Interest on product wafers.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4259249,no
Definition of Metric Dependencies for Monitoring the Impact of Quality of Services on Quality of Processes,2007,"Service providers have to monitor the quality of offered services and to ensure the compliance of service levels provider and requester agreed on. Thereby, a service provider should notify a service requester about violations of service level agreements (SLAs). Furthermore, the provider should point to impacts on affected processes in which services are invoked. For that purpose, a model is needed to define dependencies between quality of processes and quality of invoked services. In order to measure quality of services and to estimate impacts on the quality of processes, we focus on measurable metrics related to functional elements of processes, services as well as components implementing services. Based on functional dependencies between processes and services of a service-oriented architecture (SOA), we define metric dependencies for monitoring the impact of quality of invoked services on quality of affected processes. In this paper we discuss how to derive metric dependency definitions from functional dependencies by applying dependency patterns, and how to map metric and metric dependency definitions to an appropriate monitoring architecture.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4261095,no
Probabilistic Field Coverage using a Hybrid Network of Static and Mobile Sensors,2007,"Providing field coverage is a key issue in many sensor network applications. For a field with unevenly distributed static sensors, a quality coverage with acceptable network lifetime is often difficult to achieve. We propose a hybrid network that consists of both static and mobile sensors, and we suggest that it can be a cost-effective solution for held coverage. The main challenges of designing such a hybrid network are, first, determining necessary coverage contributions from each type of sensors; and second, scheduling the sensors to achieve the desired coverage contributions, which includes activation scheduling for static sensors and movement scheduling for mobile sensors. In this paper, we offer an analytical study on the above problems, and the results also lead to a practical system design. Specifically, we present an optimal algorithm for calculating the contributions from different types of sensors, which fully exploits the potentials of the mobile sensors and maximizes the network lifetime. We then present a random walk model for the mobile sensors. The model is distributed with very low control overhead. Its parameters can be fine-tuned to match the moving capability of different mobile sensors and the demands from a broad spectrum of applications. A node collaboration scheme is then introduced to further enhance the system performance. We demonstrate through analysis and simulation that, in our hybrid design, a small set of mobile sensors can effectively address the uneven distribution of the static sensors and significantly improve the coverage quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4262452,no
Performance metrics and configuration strategies for group network communication,2007,"There is an increasing number of group-based multimedia applications over the Internet, for example, voice conference or multi-player games. For these applications, it is often necessary to select a strategy to distribute the multimedia streams or mixing the multimedia stream data so as to provide better quality of service (QoS) guarantees. However, there is no appropriate metrics to evaluate the QoS of a group multimedia session, despite abundant literature on how to evaluate the QoS for two-party communication (e.g. MOS, E-Model). In this paper, we propose a new measure which is called the group mean opinion score (GMOS). To leverage on existing work, our definition of GMOS is based on two-party MOS, hence, it can be estimated via measurement of network parameters and fitting these data into the E-Model. We conduct large scale experiments using the latest SKYPE conference software. We first calibrate the GMOS based on the subjective scores of our experiments, then for individual conference sessions, we check whether our approach can pick a server configuration strategy to achieve the best GMOS. The study shows our proposed methodology is very promising and the potential of applying to other group-based applications.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4262468,no
Image Quality and Performance Based on Henry Classification and Finger Location,2007,"This paper presents an analysis of the quality of the image, minutiae count, and overall performance of fingerprint images based on the Henry system of fingerprint classification and the finger's relative location on the presenting hand. To do this, 50 users submitted 3 images from four fingers (index, middle, ring, and little). The National Institute of Standards and Technology (NIST) Fingerprint Image Software, release 2 (NFIS2) was used to analyze image quality and minutiae count. Neurotechnologija Ltd.'s VeriFinger was used to produce receiver operating characteristics (ROCs) in order to analyze performance. Our results show differences not only in image quality and minutiae count, but also matching performance based on Henry system classification and finger location.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4263213,no
Novel Distortion Estimation Technique for Hardware-Based JPEG2000 Encoder System,2007,"Optimal rate-control is an important feature of JPEG2000 which allows simple truncation of compressed bit stream to achieve best image quality at a given target bit rate. Accurate distortion estimation (DE) with respect to the allowed bit stream truncation points, is essential for the rate-control performance. In this paper, we address the issues involved in accurate DE for the hardware oriented implementation of JPEG2000 encoding systems. We propose a novel hardware efficient DE technique. Rate control based on the proposed technique results in a average peak signal-to-noise ratio degradation of only 0.02 dB with respect to the optimal DE technique used in the software implementations of JPEG2000. This is the best performance reported in comparison to existing techniques. The proposed technique requires only an additional 4096 memory bits per block coder which is 80% less than the memory requirements of optimal technique.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4265628,no
Distance Relay With Out-of-Step Blocking Function Using Wavelet Transform,2007,Out-of-step blocking function in distance relays is required to distinguish between a power swing and a fault. Speedy and reliable detection of symmetrical faults during power swings presents a challenge. This paper introduces wavelet transform to reliably and quickly detect power swings as well as detect any fault during a power swing. The total number of dyadic wavelet levels of voltage/current waveforms and the choice of particular levels for such detection are carefully studied. A logic block based on the wavelet transform is developed. The output of this block is combined with the output of the conventional digital distance relay to achieve desired performance during power swings. This integrated relay is extensively tested on a simulated system using PSCAD/ EMTDC<sup>reg</sup> software.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4265653,no
Prony-Based Optimal Bayes Fault Classification of Overcurrent Protection,2007,"The development of deregulation and demand for high-quality electrical energy has lead to a new requirement in different fields of power systems. In the protection field, this means that high sensitivity and fast operation during the fault are required while maltripping of relay protection is not acceptable. One case that may lead to a maltrip of the high-sensitive overcurrent relay is the starting current of the induction motor or inrush current of the transformer. This transient current has the potential to affect the correct operation of protection relays close to the component being switched. In the case of switching events, such transients must not lead to overcurrent relay operation; therefore, a reliable and secure relay response becomes a critical matter. Meanwhile, proper techniques must be used to prevent maltripping of such relays, due to transient currents in the network. In this paper, the optimal Bayes classifier is utilized to develop a method for discriminating the fault from nonfault events. The proposed method has been designed based on extracting the modal parameters of the current waveform using the Prony method. By feeding the fundamental frequency damping and ratio of the 2nd harmonic amplitude over the fundamental harmonic amplitude to the classifier, the fault case is discriminated from the switching case. The suitable performance of this algorithm is demonstrated by simulation of different faults and switching conditions on a power system using PSCAD/EMTDC software.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4265676,no
Classification of Electrical Disturbances in Real Time Using Neural Networks,2007,"Power-quality (PQ) monitoring is an essential service that many utilities perform for their industrial and larger commercial customers. Detecting and classifying the different electrical disturbances which can cause PQ problems is a difficult task that requires a high level of engineering knowledge. This paper presents a novel system based on neural networks for the classification of electrical disturbances in real time. In addition, an electrical pattern generator has been developed in order to generate common disturbances which can be found in the electrical grid. The classifier obtained excellent results (for both test patterns and field tests) thanks in part to the use of this generator as a training tool for the neural networks. The neural system is integrated on a software tool for a PC with hardware connected for signal acquisition. The tool makes it possible to monitor the acquired signal and the disturbances detected by the system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4265702,no
Expert System for Power Quality Disturbance Classifier,2007,"Identification and classification of voltage and current disturbances in power systems are important tasks in the monitoring and protection of power system. Most power quality disturbances are non-stationary and transitory and the detection and classification have proved to be very demanding. The concept of discrete wavelet transform for feature extraction of power disturbance signal combined with artificial neural network and fuzzy logic incorporated as a powerful tool for detecting and classifying power quality problems. This paper employes a different type of univariate randomly optimized neural network combined with discrete wavelet transform and fuzzy logic to have a better power quality disturbance classification accuracy. The disturbances of interest include sag, swell, transient, fluctuation, and interruption. The system is modeled using VHSIC hardware description language (VHDL), a hardware description language, followed by extensive testing and simulation to verify the functionality of the system that allows efficient hardware implementation of the same. This proposed method classifies, and achieves 98.19% classification accuracy for the application of this system on software-generated signals and utility sampled disturbance events.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4265719,no
An Efficient Network Anomaly Detection Scheme Based on TCM-KNN Algorithm and Data Reduction Mechanism,2007,"Network anomaly detection plays a vital role in securing network security and infrastructures. Current research focuses concentrate on how to effective reduce high false alarm rate and usually ignore the fact that the poor quality data for the modeling of normal patterns as well as the high computational cost make the current anomaly detection methods not act as well as we expect. Based on these, we first propose a novel data mining scheme for network anomaly detection in this paper. Moreover, we adopt data reduction mechanisms (including genetic algorithm (GA) based instance selection and filter based feature selection methods) to boost the detection performance, meanwhile reduce the computational cost of TCM-KNN. Experimental results on the well-known KDD Cup 1999 dataset demonstrate the proposed method can effectively detect anomalies with high detection rates, low false positives as well as with high confidence than the state-of-the-art anomaly detection methods. Furthermore, the data reduction mechanisms would greatly improve the performance of TCM-KNN and make it be a good candidate for anomaly detection in practice.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4267564,no
Quality-Based Fusion of Multiple Video Sensors for Video Surveillance,2007,"In this correspondence, we address the problem of fusing data for object tracking for video surveillance. The fusion process is dynamically regulated to take into account the performance of the sensors in detecting and tracking the targets. This is performed through a function that adjusts the measurement error covariance associated with the position information of each target according to the quality of its segmentation. In this manner, localization errors due to incorrect segmentation of the blobs are reduced thus improving tracking accuracy. Experimental results on video sequences of outdoor environments show the effectiveness of the proposed approach.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4267881,no
A Middleware Architecture for Replica Voting on Fuzzy Data in Dependable Real-time Systems,2007,"Majority voting among replicated data collection devices enhances the trust-worthiness of data flowing from a hostile external environment. It allows a correct data fusion and dissemination by the end-users, in the presence of content corruptions and/or timing failures that may possibly occur during data collection. In addition, a device may operate on fuzzy inputs, thereby generating a data that occasionally deviates from the reference datum in physical world. In this paper, we provide a QoS-oriented approach to manage the data flow through various system elements. The application-level QoS parameters we consider are timeliness and accuracy of data. The underlying protocol-level parameters that influence data delivery performance are the data sizes, network bandwidth, device asynchrony, and data fuzziness. A replica voting protocol takes into account the interplay between these parameters as the faulty behavior of malicious devices unfolds in various forms during data collection. Our QoS-oriented approach casts the well-known fault-tolerance techniques, namely, 2-phase voting, with control mechanisms that adapt the data delivery to meet the end-to-end constraints - such as latency, data integrity, and resource cost. The paper describes a middleware architecture to realize our QoS-oriented approach to the management of replicated data flows.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4267988,no
Differentiation of Wireless and Congestion Losses in TCP,2007,"TCP is the most commonly used data transfer protocol. It assumes every packet loss to be congestion loss and reduces the sending rate. This will decrease the sender's throughput when there is an appreciable rate of packet loss due to link error and not due to congestion. This issue is significant for wireless links. We present an extension of TCP-Casablanca, which improves TCP performance over wireless links. A new discriminator is proposed that not only differentiates congestion and wireless losses, but also identifies the congestion level in the network, i.e., whether the network is lightly congested or heavily congested and throttles the sender's rate according to the congestion level in the network.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4268134,no
Use of a Genetic Algorithm to Identify Source Code Metrics Which Improves Cognitive Complexity Predictive Models,2007,"In empirical software engineering predictive models can be used to classify components as overly complex. Such modules could lead to faults, and as such, may be in need of mitigating actions such as refactoring or more exhaustive testing. Source code metrics can be used as input features for a classifier, however, there exist a large number of measures that capture different aspects of coupling, cohesion, inheritance, complexity and size. In a large dimensional feature space some of the metrics may be irrelevant or redundant. Feature selection is the process of identifying a subset of the attributes that improves a classifier's discriminatory performance. This paper presents initial results of a genetic algorithm as a feature subset selection method that enhances a classifier's ability to discover cognitively complex classes that degrade program understanding.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4268267,no
Robust Metric Reconstruction from Challenging Video Sequences,2007,"Although camera self-calibration and metric reconstruction have been extensively studied during the past decades, automatic metric reconstruction from long video sequences with varying focal length is still very challenging. Several critical issues in practical implementations are not adequately addressed. For example, how to select the initial frames for initializing the projective reconstruction? What criteria should be used? How to handle the large zooming problem? How to choose an appropriate moment for upgrading the projective reconstruction to a metric one? This paper gives a careful investigation of all these issues. Practical and effective approaches are proposed. In particular, we show that existing image-based distance is not an adequate measurement for selecting the initial frames. We propose a novel measurement to take into account the zoom degree, the self-calibration quality, as well as image-based distance. We then introduce a new strategy to decide when to upgrade the projective reconstruction to a metric one. Finally, to alleviate the heavy computational cost in the bundle adjustment, a local on-demand approach is proposed. Our method is also extensively compared with the state-of-the-art commercial software to evidence its robustness and stability.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4270143,no
Real-Time Model-Based Fault Detection and Diagnosis for Alternators and Induction Motors,2007,"This paper describes a real-time model-based fault detection and diagnosis software. The electric machines diagnosis system (EMDS) covers field winding shorted-turns fault in alternators and stator windings shorted-turns fault in induction motors. The EMDS has a modular architecture. The modules include: acquisition and data treatment; well-known parameters estimation algorithms, such as recursive least squares (RLS) and extended Kalman filter (EKF); dynamic models for faults simulation; faults detection and identification tools, such as M.L.P. and S.O.M. neural networks and fuzzy C-means (FCM) technique. The modules working together detect possible faulty conditions of various machines working in parallel through routing. A fast, safe and efficient data manipulation requires a great DataBase managing system (DBMS) performance. In our experiment, the EMDS real-time operation demonstrated that the proposed system could efficiently and effectively detect abnormal conditions resulting in lower-cost maintenance for the company.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4270639,no
Applying Novel Resampling Strategies To Software Defect Prediction,2007,"Due to the tremendous complexity and sophistication of software, improving software reliability is an enormously difficult task. We study the software defect prediction problem, which focuses on predicting which modules will experience a failure during operation. Numerous studies have applied machine learning to software defect prediction; however, skewness in defect-prediction datasets usually undermines the learning algorithms. The resulting classifiers will often never predict the faulty minority class. This problem is well known in machine learning and is often referred to as learning from unbalanced datasets. We examine stratification, a widely used technique for learning unbalanced data that has received little attention in software defect prediction. Our experiments are focused on the SMOTE technique, which is a method of over-sampling minority-class examples. Our goal is to determine if SMOTE can improve recognition of defect-prone modules, and at what cost. Our experiments demonstrate that after SMOTE resampling, we have a more balanced classification. We found an improvement of at least 23% in the average geometric mean classification accuracy on four benchmark datasets.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4271036,no
Iterative Cross Section Sequence Graph for Handwritten Character Segmentation,2007,"The iterative cross section sequence graph (ICSSG) is an algorithm for handwritten character segmentation. It expands the cross section sequence graph concept by applying it iteratively at equally spaced thresholds. The iterative thresholding reduces the effect of information loss associated with image binarization. ICSSG preserves the characters' skeletal structure by preventing the interference of pixels that causes flooding of adjacent characters' segments. Improving the structural quality of the characters' skeleton facilitates better feature extraction and classification, which improves the overall performance of optical character recognition (OCR). Experimental results showed significant improvements in OCR recognition rates compared to other well-established segmentation algorithms.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4271521,no
Statistical QoS Guarantee and Energy-Efficiency in Web Server Clusters,2007,"In this paper we study the soft real-time web cluster architecture needed to support e-commerce and related applications. Our testbed is based on an industry standard, which defines a set of Web interactions and database transactions with their deadlines, for generating real workload and bench-marking e-commerce applications. In these soft real-time systems, the quality of service (QoS) is usually defined as the fraction of requests that meet the deadlines. When this QoS is measured directly, regardless of whether the request missed the deadline by an epsilon amount of time or by a large difference, the result is always the same. For this reason, only counting the number of missed requests in a period avoids the observation of the real state of the system. Our contributions are theoretical propositions of how to control the QoS, not measuring the QoS directly, but based on the probability distribution of the tardiness in the completion time of the requests. We call this new QoS metric tardiness quantile metric (TQM). The proposed method provides fine-grained control over the QoS so that we can make a closer examination of the relation between QoS and energy efficiency. We validate the theoretical results showing experiments in a multi-tiered e-commerce web cluster implemented using only open-source software solutions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4271683,no
WCET-Directed Dynamic Scratchpad Memory Allocation of Data,2007,"Many embedded systems feature processors coupled with a small and fast scratchpad memory. To the difference with caches, allocation of data to scratchpad memory must be handled by software. The major gain is to enhance the predictability of memory accesses latencies. A compile-time dynamic allocation approach enables eviction and placement of data to the scratchpad memory at runtime. Previous dynamic scratchpad memory allocation approaches aimed to reduce average-case program execution time or the energy consumption due to memory accesses. For real-time systems, worst-case execution time is the main metric to optimize. In this paper, we propose a WCET-directed algorithm to dynamically allocate static data and stack data of a program to scratchpad memory. The granularity of placement of memory transfers (e.g. on function, basic block boundaries) is discussed from the perspective of its computation complexity and the quality of allocation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4271692,no
Toward the Use of Automated Static Analysis Alerts for Early Identification of Vulnerability- and Attack-prone Components,2007,"Extensive research has shown that software metrics can be used to identify fault- and failure-prone components. These metrics can also give early indications of overall software quality. We seek to parallel the identification and prediction of fault- and failure-prone components in the reliability context with vulnerability- and attack-prone components in the security context. Our research will correlate the quantity and severity of alerts generated by source code static analyzers to vulnerabilities discovered by manual analyses and testing. A strong correlation may indicate that automated static analyzers (ASA), a potentially early technique for vulnerability identification in the development phase, can identify high risk areas in the software system. Based on the alerts, we may be able to predict the presence of more complex and abstract vulnerabilities involved with the design and operation of the software system. An early knowledge of vulnerability can allow software engineers to make informed risk management decisions and prioritize redesign, inspection, and testing efforts. This paper presents our research objective and methodology.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4271764,no
Efficient Probing Techniques for Fault Diagnosis,2007,"Increase in the network usage and the widespread application of networks for more and more performance critical applications has caused a demand for tools that can monitor network health with minimum management traffic. Adaptive probing holds a potential to provide effective tools for end-to-end monitoring and fault diagnosis over a network. In this paper we present adaptive probing tools that meet the requirements to provide an effective and efficient solution for fault diagnosis. In this paper, we propose adaptive probing based algorithms to perform fault localization by adapting the probe set to localize the faults in the network. We compare the performance and efficiency of the proposed algorithms through simulation results.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4271766,no
Evaluating the Combined Effect of Vulnerabilities and Faults on Large Distributed Systems,2007,"On large and complex distributed systems hardware and software faults, as well as vulnerabilities, exhibit significant dependencies and interrelationships. Being able to assess their actual impact on the overall system dependability is especially important. The goal of this paper is to propose a unifying way of describing a complex hardware and software system, in order to assess the impact of both vulnerabilities and faults by means of the same underlying reasoning mechanism, built on a standard Prolog inference engine. Some preliminary experimental results show that a prototype tool based on these techniques is both feasible and able to achieve encouraging performance levels on several synthetic test cases.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4272886,no
Evaluation of MDA/PSM database model quality in the context of selected non-functional requirements,2007,"Conceptual, logical, and physical database models can be regarded as PIM, PSM<sub>1</sub>, and PSM<sub>2</sub> data models within MDA architecture, respectively. Many different logical database models can be derived from a given conceptual database model by applying a set of transformations rules. To choose a logical database model for further transformation (to physical database model at PSM<sub>2</sub> level) some selection criteria based on quality demands, e.g. database efficiency, easy maintainability or portability, should be established. To evaluate quality of the database models some metrics should be provided. We present metrics for measuring two selected, conflicted quality characteristics (efficiency and maintainability), next we analyse correlation between the metrics, and finally propose how to assess quality of logical database models.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4272887,no
An Artificial Immune System Approach for Fault Prediction in Object-Oriented Software,2007,"The features of real-time dependable systems are availability, reliability, safety and security. In the near future, real-time systems will be able to adapt themselves according to the specific requirements and real-time dependability assessment technique will be able to classify modules as faulty or fault-free. Software fault prediction models help us in order to develop dependable software and they are commonly applied prior to system testing. In this study, we examine Chidamber-Kemerer (CK) metrics and some method-level metrics for our model which is based on artificial immune recognition system (AIRS) algorithm. The dataset is a part of NASA Metrics Data Program and class-level metrics are from PROMISE repository. Instead of validating individual metrics, our mission is to improve the prediction performance of our model. The experiments indicate that the combination of CK and the lines of code metrics provide the best prediction results for our fault prediction model. The consequence of this study suggests that class-level data should be used rather than method-level data to construct relatively better fault prediction models. Furthermore, this model can constitute a part of real-time dependability assessment technique for the future.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4272915,no
Failure Resilience for Device Drivers,2007,"Studies have shown that device drivers and extensions contain 3-7 times more bugs than other operating system code and thus are more likely to fail. Therefore, we present a failure-resilient operating system design that can recover from dead drivers and other critical components - primarily through monitoring and replacing malfunctioning components on the fly - transparent to applications and without user intervention. This paper focuses on the post-mortem recovery procedure. We explain the working of our defect detection mechanism, the policy-driven recovery procedure, and post-restart reintegration of the components. Furthermore, we discuss the concrete steps taken to recover from network, block device, and character device driver failures. Finally, we evaluate our design using performance measurements, software fault-injection experiments, and an analysis of the reengineering effort.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4272954,no
Using Process-Level Redundancy to Exploit Multiple Cores for Transient Fault Tolerance,2007,"Transient faults are emerging as a critical concern in the reliability of general-purpose microprocessors. As architectural trends point towards multi-threaded multi-core designs, there is substantial interest in adapting such parallel hardware resources for transient fault tolerance. This paper proposes a software-based multi-core alternative for transient fault tolerance using process-level redundancy (PLR). PLR creates a set of redundant processes per application process and systematically compares the processes to guarantee correct execution. Redundancy at the process level allows the operating system to freely schedule the processes across all available hardware resources. PLR's software-centric approach to transient fault tolerance shifts the focus from ensuring correct hardware execution to ensuring correct software execution. As a result, PLR ignores many benign faults that do not propagate to affect program correctness. A real PLR prototype for running single-threaded applications is presented and evaluated for fault coverage and performance. On a 4-way SMP machine, PLR provides improved performance over existing software transient fault tolerance techniques with 16.9% overhead for fault detection on a set of optimized SPEC2000 binaries.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4272981,no
Experimental Risk Assessment and Comparison Using Software Fault Injection,2007,"One important question in component-based software development is how to estimate the risk of using COTS components, as the components may have hidden faults and no source code available. This question is particularly relevant in scenarios where it is necessary to choose the most reliable COTS when several alternative components of equivalent functionality are available. This paper proposes a practical approach to assess the risk of using a given software component (COTS or non-COTS). Although we focus on comparing components, the methodology can be useful to assess the risk in individual modules. The proposed approach uses the injection of realistic software faults to assess the impact of possible component failures and uses software complexity metrics to estimate the probability of residual defects in software components. The proposed approach is demonstrated and evaluated in a comparison scenario using two real off-the-shelf components (the RTEMS and the RTLinux real-time operating system) in a realistic application of a satellite data handling application used by the European Space Agency.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273002,no
On the Quality of Service of Crash-Recovery Failure Detectors,2007,"In this paper, we study and model a crash-recovery target and its failure detector's probabilistic behavior. We extend quality of service (QoS) metrics to measure the recovery detection speed and the proportion of the detected failures of a crash-recovery failure detector. Then the impact of the dependability of the crash-recovery target on the QoS bounds for such a crash-recovery failure detector is analysed by adopting general dependability metrics such as MTTF and MTTR. In addition, we analyse how to estimate the failure detector's parameters to achieve the QoS from a requirement based on Chen's NFD-S algorithm. We also demonstrate how to execute the configuration procedure of this crash-recovery failure detector. The simulations are based on the revised NFD-S algorithm with various MTTF and MTTR. The simulation results show that the dependability of a recoverable monitored target could have significant impact on the QoS of such a failure detector and match our analysis results.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273025,no
Performability Models for Multi-Server Systems with High-Variance Repair Durations,2007,"We consider cluster systems with multiple nodes where each server is prone to run tasks at a degraded level of service due to some software or hardware fault. The cluster serves tasks generated by remote clients, which are potentially queued at a dispatcher. We present an analytic queueing model of such systems, represented as an M/MMPP/1 queue, and derive and analyze exact numerical solutions for the mean and tail-probabilities of the queue-length distribution. The analysis shows that the distribution of the repair time is critical for these performability metrics. Additionally, in the case of high-variance repair times, the model reveals so-called blow-up points, at which the performance characteristics change dramatically. Since this blowup behavior is sensitive to a change in model parameters, it is critical for system designers to be aware of the conditions under which it occurs. Finally, we present simulation results that demonstrate the robustness of this qualitative blow-up behavior towards several model variations.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273028,no
Using Economics as Basis for Modelling and Evaluating Software Quality,2007,"The economics and cost of software quality have been discussed in software engineering for decades now. There is clearly a relationship and a need to manage cost and quality in combination. Moreover, economics should be the basis of any quality analysis. However, this implies several issues that have not been addressed to an extent so that managing the economics of software quality is common practice. This paper discusses these issues, possible solutions, and research directions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273048,no
Fault-Adaptive Control for Robust Performance Management of Computing Systems,2007,"This paper introduces a fault-adaptive control approach for the robust and reliable performance management of computing systems. Fault adaptation involves the detection and isolation of faults, and then taking appropriate control actions to mitigate the fault effects and maintain control.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273115,no
Adequate and Precise Evaluation of Quality Models in Software Engineering Studies,2007,"Many statistical techniques have been proposed and introduced to predict fault-proneness of program modules in software engineering. Choosing the ""best"" candidate among many available models involves performance assessment and detailed comparison. But these comparisons are not simple due to varying performance measures and the related verification and validation cost implications. Therefore, a methodology for precise definition and evaluation of the predictive models is still needed. We believe the procedure we outline here, if followed, has a potential to enhance the statistical validity of future experiments.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273257,no
Global Sensitivity Analysis of Predictor Models in Software Engineering,2007,"Predictor models are an important tool in software projects for quality and cost control as well as management. There are various models available that can help the software engineer in decision-making. However, such models are often difficult to apply in practice because of the amount of data needed. Sensitivity analysis offers provides means to rank the input factors w.r.t. their importance and thereby reduce and optimise the measurement effort necessary. This paper presents an example application of global sensitivity analysis on a software reliability model used in practice. It describes the approach and the possibilities offered.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273259,no
Make the Most of Your Time: How Should the Analyst Work with Automated Traceability Tools?,2007,"Several recent studies employed traditional information retrieval (IR) methods to assist in the mapping of elements of software engineering artifacts to each other. This activity is referred to as candidate link generation because the final say in determining the final mapping belongs to the human analyst. Feedback techniques that utilize information from the analyst (on whether the candidate links are correct or not) have been shown to improve the quality of the mappings. Yet the analyst is making an investment of time in providing the feedback. This leads to the question of whether or not guidance can be provided to the analyst on how to best utilize that time. This paper simulates a number of approaches an analyst might take to evaluating the same candidate link list, and discovers that more structured and organized approaches appear to save time/effort of the analyst.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273260,no
Modeling the Effect of Size on Defect Proneness for Open-Source Software,2007,"Quality is becoming increasingly important with the continuous adoption of open-source software. Previous research has found that there is generally a positive relationship between module size and defect proneness. Therefore, in open-source software development, it is important to monitor module size and understand its impact on defect proneness. However, traditional approaches to quality modeling, which measure specific system snapshots and obtain future defect counts, are not well suited because open-source modules usually evolve and their size changes over time. In this study, we used Cox proportional hazards modeling with recurrent events to study the effect of class size on defect-proneness in the Mozilla product. We found that the effect of size was significant, and we quantified this effect on defect proneness.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273266,no
Complexity Measures for Secure Service-Oriented Software Architectures,2007,"As software attacks become widespread, the ability for a software system to resist malicious attacks has become a key concern in software quality engineering. Software attack ability is a concept proposed recently in the research literature to measure the extent to which a software system or service could be the target of successful attacks. Like most external attributes, attack ability is to some extent disconnected from the internal of software products. To mitigate software attack ability, we need to identify and manipulate related internal software attributes. Our goal in this paper is to study software complexity as one such internal attribute. We apply the User System Interaction Effect (USIE) model, a security measurement abstraction paradigm proposed in previous research, to define and validate a sample metric for service complexity. We thereby establish the usefulness of our sample metric through empirical investigation using open source software system as target application.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273267,no
Authoring Tool for Business Performance Monitoring and Control,2007,"To monitor and warrant the performance of concerned business operations, formal representation for business performance models are needed so business analysts can describe what they expect from business service providers. This paper proposes a metamodel and an authoring tool for describing business performance for business services in the context of SOA. A business performance model explicitly captures the intentions of the consumers of the target business services. The authoring tool simplifies the modeling of the performance models. With our proposed mechanism, computational models for business performance management are generated. The separation of concerns in terms of business performance models and computational models allows for seamless transitions from business level performance models to runtime.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273416,no
Integrated Management of Company Processes and Standard Processes: A Platform to Prepare and Perform Quality Management Appraisals,2007,"Business processes have been introduced in many companies during the last years. But it was not clear how to measure the quality of these processes. ISO/IEC 15504 and CMMI have filled this gap and provide measurement frameworks to assess the maturity of processes. However, introducing and adapting processes to comply with these standards is difficult and error-prone. Especially the integration of the requirements of the standards into the company processes is hard to accomplish. In this paper we propose an integrated process modeling approach that is able to bridge the gap between business processes and requirements of the standards. Building on this integrated model we are able to produce reports that systematically uncover and display weaknesses in the process.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273472,no
Evaluating the Post-Delivery Fault Reporting and Correction Process in Closed-Source and Open-Source Software,2007,"Post-delivery fault reporting and correction are important activities in the software maintenance process. It is worthwhile to study these activities in order to understand the difference between open-source and closed-source software products from the maintenance perspective. This paper proposes three metrics to evaluate the post-delivery fault reporting and correction process, the average fault hidden time, the average fault pending time, and the average fault correction time. An empirical study is further performed to compare the fault correction processes of NASA Ames (closed-source) projects and three open-source projects: Apache Tomcat, Apache Ant, and Gnome Panel.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273475,no
Refactoring--Does It Improve Software Quality?,2007,"Software systems undergo modifications, improvements and enhancements to cope with evolving requirements. This maintenance can cause their quality to decrease. Various metrics can be used to evaluate the way the quality is affected. Refactoring is one of the most important and commonly used techniques of transforming a piece of software in order to improve its quality. However, although it would be expected that the increase in quality achieved via refactoring is reflected in the various metrics, measurements on real life systems indicate the opposite. We analyzed source code version control system logs of popular open source software systems to detect changes marked as refactorings and examine how the software metrics are affected by this process, in order to evaluate whether refactoring is effectively used as a means to improve software quality within the open source community.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273477,no
A Rapid Fault Injection Approach for Measuring SEU Sensitivity in Complex Processors,2007,"Processors are very common components in current digital systems and to assess their reliability is an essential task during the design process. In this paper a new fault injection solution to measure SEU sensitivity in processors is presented. It consists in a hardware-implemented module that performs fault injection through the available JTAG-based On-Chip Debugger (OCD). It can be widely applicable to different processors since JTAG standard is an extended interface and OCDs are usually available in current processors. The hardware implementation avoids the communication between the target system and the software debugging tool. The method has been applied to a complex processor, the ARM7TDMI. Results illustrate the approach is a fast, efficient and cost-effective solution.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4274827,no
Online Applications of Wavelet Transforms to Power System Relaying - Part II,2007,"Recent wavelet developments in power engineering applications, include detection, localization, classification, identification, storage, compression, and network/system analysis of the power quality disturbance signals, to very recently, power system relaying [1,2,3,4]. This paper assesses the online use of wavelet analysis to power system relaying. The paper presents a novel technique for transmission-line fault detection and classification using the DWT for which an optimal selection of mother wavelet and data window size based on the minimum entropy criterion has been performed. The paper starts with the review of recent work within the field of wavelet analysis and its applications to power systems engineering. Then, the theoretical background of the technique is presented and the proposed method is described in detail. Finally, the effect of different parameters on the algorithm are examined in order to highlight its performance. Typical fault conditions on a practical 220 kV power system as generated by ATP/EMTP is analyzed with Daubechies wavelets. The performance of the fault classifier is tested using MATLAB software. The feasibility of using wavelet analysis to detect and classify faults is investigated. Finally it discusses the results, limitations and possible improvement. It is found that the use of wavelet transforms together with an effective classification procedure is considered to be straightforward, fast, computationally efficient and allow for real-time accurate applications in monitoring and classifying techniques in power engineering.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4275213,no
Estimation of Fault Location on Distribution Feeders using PQ Monitoring Data,2007,"This paper investigates the challenges in the extraction of fault data (fault current magnitude and type) for fault location application based on actual field data and proposes a procedure for practical implementation. The proposed scheme is implemented as a stand-alone software program, and is tested using actual field data collected at distribution substations and the results are compared with results of the state-of-the-art software package currently used in a utility.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4275244,no
Implementation and Applications of Wide-area monitoring systems,2007,"This paper discusses the design and applications of wide area monitoring and control systems, which can complement classical protection systems and SCADA/EMS applications. System wide installed phasor measurement units send their measured data to a central computer, where snapshots of the dynamic system behavior are made available online. This new quality of system information opens up a wide range of new applications to assess and actively maintain system's stability in case of voltage, angle or frequency instability, thermal overload and oscillations. Recent developed algorithms and their design for these application areas are introduced. With practical examples the benefits in terms of system security are shown..",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4275326,no
"Advancements in Distributed Generation Issues Interconnection, Modeling, and Tariffs",2007,"The California Energy Commission is cost-sharing research with the Department of Energy through the National Renewable Energy Laboratory to address distributed energy resources (DER) topics. These efforts include developing interconnection and power management technologies, modeling the impacts of interconnecting DER with an area electric power system, and evaluating possible modifications to rate policies and tariffs. As a result, a DER interconnection device has been developed and tested. A workshop reviewed the status and issues of advanced power electronic devices. Software simulations used validated models of distribution circuits that incorporated DER, and tests and measurements of actual circuits with and without DER systems are being conducted to validate these models. Current policies affecting DER were reviewed and rate making policies to support deployment of DER through public utility rates and policies were identified. These advancements are expected to support the continued and expanded use of DER systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4275532,no
A Web-Based Fuel Management Software System for a Typical Indian Coal based Power Plant,2007,"The fuel management system forms an integral part of the management process in a power plant and hence is one of the most critical areas. It deals with the management of commercial, operational and administrative functions pertaining to estimating fuel requirements, selection of fuel suppliers, fuel quality check, transportation and fuel handling, payment for fuel received, consumption and calculation of fuel efficiency. The results are then used for cost benefit analysis to suggest further plant improvement. At various levels, management information reports need to be extracted to communicate the required information across various levels of management. The core processes of fuel management involve a huge amount of paper work and manual labour, which makes it tedious, time-consuming and prone to human errors. Moreover, the time taken at each stage as well as the transparency of the relevant information has a direct bearing on the economics and efficient operation of the power plant. Both system performance and information transparency can be enhanced by the introduction of Information Technology in managing this area. This paper reports on the development of Web-based Fuel Management System Software, based on 3-tiered J2EE architecture, which aims at systematic functioning of the Core Business Processes of Fuel Management of a typical coal-fired thermal power plant in the Indian power scenario.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4275713,no
Modeling Distribution Overcurrent Protective Devices for Time-Domain Simulations,2007,"Poor overcurrent protective device coordination can cause prolonged and unnecessary voltage variation problems. The coordination of many protective devices can be a difficult task and unfortunately, device performance and accuracy are not evaluated once the device settings are chosen and deployed. Ideally, an automated system would interrogate system data at the substation and estimate voltage variations and assess protective device performance. To test the accuracy of the automated system, a simulation model is developed to generate test data. The time-domain overcurrent protective device models can be used to estimate the duration of voltage sag during utility fault clearing operation as well. This paper presents the modeling of overcurrent protective device models created in a time-domain power system simulator. The radial distribution simulation, also made in the same time-domain software, allows testing of different overcurrent protection device settings and placement.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4275922,no
Bad-Smell Metrics for Aspect-Oriented Software,2007,"Aspect-oriented programming (AOP) is a new programming paradigm that improves separation of concerns by decomposing the crosscutting concerns in aspect modules. Bad smells are metaphors to describe software patterns that are generally associated with bad design and bad programming of object-oriented programming (OOP). New notions and different ways of thinking for developing aspect-oriented (AO) software inevitably introduce bad smells which are specific bad design and bad programming in AO software called AO bad smells. Software metrics have been used to measure software artifact for a better understanding of its attributes and to assess its quality. Bad-smell metrics should be used as indicators for determining whether a particular fraction of AO code contains bad smells or not. Therefore, this paper proposes definition of metrics corresponding to the characteristic of each AO bad smell as a means to detecting them. The proposed bad-smell metrics are validated and the results show that the proposed bad- smell metrics can preliminarily indicate bad smells hidden in AO software.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4276524,no
Resource Allocation Based On Workflow For Enhancing the Performance of Composite Service,2007,"Under SOA, multiple services can be aggregated to create a new composite service based on some predefined workflow. The QoS of this composite service is determined by the cooperation of all these Web services. With workflow pipelining, it is unreasonable to improve the overall service performance by only considering individual services without considering the relationship among them. In this paper, we propose to allocate resources by tracing and predicting workloads dynamically with the pipelining of service requests in workflow graph. At any moment, there are a number of service requests being handled by different services. Firstly, we predict future workloads for any requests as soon as they arrive at any service in the workflow. Secondly, we allocate resources for the predicted workloads to enhance the performance by replicating more services to resources. Our target is to maximize the number of successful requests with the constraints of limited resources. Experiment shows that our dynamic resource allocation mechanism is more efficient for enhancing the global performance of composite service than static resource allocation mechanism in general.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4278703,no
Web Service decision-making model based on uncertain-but-bounded attributes,2007,"Web services have become one of the most popular technologies of Web application. But the quality of Web services is not as stable as traditional software components' because of the uncertainty of network. Based on non-probability-set, the convex method is used to judge the range of performance affected by uncertain-but-bounded attributes. This method only requires the highest and lowest value of the uncertain attribute values and need not to know their probability distribution. This paper proposes the metric algorithm of the change of web service quality, and proposes the Web service decisionmaking algorithm based on the theory of multiple attributes decision by TOPSIS (technique for order preference by similarity to idea solution) in operations research.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4278781,no
Continuous SPA: Continuous Assessing and Monitoring Software Process,2007,"In the past ten years many assessment approaches have been proposed to help manage software process quality. However, few of them are configurable and real-time in practice. Hence, it is advantageous to find ways to monitor the current status of software processes and detect the improvement opportunities. In this paper, we introduce a web- based prototype system (Continuous SPA) on continuous assessing and monitoring software process, and perform a practical study in one process area: project management. Our study results are positive and show that features such as global management, well-defined responsibility and visualization can be integrated in process assessment to help improve the software process management.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4278791,no
An Architecture of a Workflow System for Integrated Asset Management in the Smart Oil Field Domain,2007,"Integrated asset management (IAM) is the vision of IT-enabled transformation of oilfield operations where information integration from a variety of tools for reservoir modeling, simulation, and performance prediction will lead to rapid decision making for continuous optimization of oil production. In this paper, we discuss the similarities and differences of IAM applications and typical e-Science applications. We then propose an architecture for a workflow system for IAM based on the four key requirements: support for creation, orchestration and management of workflows including those involving legacy tools, support for audit trails and data quality indicators for data objects, usability and extensibility. Our architecture builds upon current research in the scientific workflow area and applies many of its learnings to address the requirements of our system. We propose some implementation strategies and technologies and identify some of the key research challenges in realizing our architectural vision.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4278796,no
On the Contributions of an End-to-End AOSD Testbed,2007,"Aspect-Oriented Software Development (AOSD) techniques are gaining increased attention from both academic and industrial organisations. In order to promote a smooth adoption of such techniques it is of paramount importance to perform empirical analysis of AOSD to gather a better understanding of its benefits and limitations. In addition, the effects of aspect-oriented (AO) mechanisms on the entire development process need to be better assessed rather than just analysing each development phase in isolation. As such, this paper outlines our initial effort on the design of a testbed that will provide end-to-end systematic comparison of AOSD techniques with other mainstream modularisation techniques. This will allow the proponents of AO and non- AO techniques to compare their approaches in a consistent manner. The testbed is currently composed of: (i) a benchmark application, (ii) an initial set of metrics suite to assess certain internal and external software attributes, and (in) a ""repository"" of artifacts derived from AOSD approaches that are assessed based on the application of (i) and (ii). This paper mainly documents a selection of techniques that will be initially applied to the benchmark. We also discuss the expected initial outcomes such a testbed will feed back to the compared techniques. The applications of these techniques are contributions from different research groups working on AOSD.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4279204,no
Automated Functional Conformance Test Generation for Semantic Web Services,2007,"We present an automated approach to generate functional conformance tests for semantic Web services. The semantics of the Web services are defined using the inputs, outputs, preconditions, effects (IOPEs) paradigm. For each Web service, our approach produces testing goals which are refinements of the Web service preconditions using a set of fault models. A novel planner component accepts these testing goals, along with an initial state of the world and the Web service definitions to generate a sequence of Web service invocations as a test case. Another salient feature of our approach is generation of verification sequences to ensure that the changes to the world produced by an effect are implemented correctly. Lastly, a given application incorporating a set of semantic Web services may be accessible through several interfaces such as 1) direct invocation of the Web services, or 2) a graphical user interface (GUI). Our technique allows generation of executable test cases which can be applied through both interfaces. We describe the techniques used in our test generation approach. We also present results which compare two approaches: an existing manual approach without the formal IOPEs information and the IOPEs-based approach reported in this paper. These results indicate that the approach described here leads to substantial savings in effort with comparable results for requirements coverage and fault detection effectiveness.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4279589,no
Probabilistic QoS and soft contracts for transaction based Web services,2007,"Web services orchestrations and choreographies require establishing quality of service (QoS) contracts with the user. This is achieved by performing QoS composition, based on contracts established between the orchestration and the called Web services. These contracts are typically stated in the form of hard guarantees (e.g., response time always less than 5 msec). In this paper we propose using soft contracts instead. Soft contracts are characterized by means of probability distributions for QoS parameters. We show how to compose such contracts, to yield a global contract (probabilistic) for the orchestration. Our approach is implemented by the TOrQuE tool. Experiments on TOrQuE show that overly pessimistic contracts can be avoided and significant room for safe overbooking exists.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4279591,no
Utility-based QoS Brokering in Service Oriented Architectures,2007,"Quality of service (QoS) is an important consideration in the dynamic service selection in the context of service oriented architectures. This paper extends previous work on QoS brokering for SOAs by designing, implementing, and experimentally evaluating a service selection QoS broker that maximizes a utility function for service consumers. Utility functions allow stakeholders to ascribe a value to the usefulness of a system as a function of several attributes such as response time, throughput, and availability. This work assumes that consumers of services provide to a QoS broker their utility functions and their cost constraints on the requested services. Service providers register with the broker by providing service demands for each of the resources used by the services provided and cost functions for each of the services. Consumers request services from the QoS broker, which selects a service provider that maximizes the consumer's utility function subject to its cost constraint. The QoS broker uses analytic queuing models to predict the QoS values of the various services that could be selected under varying workload conditions. The broker and services were implemented using a J2EE/Weblogic platform and experiments were conducted to evaluate the broker's efficacy. Results showed that the broker adequately adapts its selection of service providers according to cost constraints.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4279627,no
Software Metric Estimation: An Empirical Study Using An Integrated Data Analysis Approach,2007,"Automatic software effort estimation is important for quality management in the software development industry, but it still remains a challenging issue. In this paper we present an empirical study on the software effort estimation problem using a benchmark dataset. A number of machine learning techniques are employed to construct an integrated data analysis approach that extracts useful information from visualisation, feature selection, model selection and validation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4280207,no
Yarn hairiness determination the algorithms of computer measurement methods,2007,The article describes the algorithm of the edge detection in the computer application of the assessment of yarn quality. The proposed algorithm and the measurement method allows the real setting of the length and number of the protruding fibres. That solution introduces a new quality to the measurement of yarn hairiness.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4283454,no
Reusable Component Analysis for Component-Based Embedded Real-Time Systems,2007,"Component-Based Software Engineering (CBSE) promises an improved ability to reuse software which would potentially decrease the development time while also improving the quality of the system, since the components are (re-)used by many. However, CBSE has not been as successful in the embedded systems domain as in the desktop domain, partly because requirements on embedded systems are stricter (e.g. requirements on safety, real-time and minimizing hardware resources). Moreover these requirements differ between industrial domains. Paradoxically, components should be context-unaware to be reusable at the same time as they should be context sensitive in order to be predictable and resource efficient. This seems to be a fundamental problem to overcome before the CBSE paradigm will be successful also in the embedded systems domain. Another problem is that some of the stricter requirements for embedded systems require certain analyses to be made, which may be very complicated and time-consuming for the system developer. This paper describes how one particular kind of analysis, of worst-case execution time, would fit into the CBSE development processes so that the component developer performs some analyses and presents the results in a form that is easily used for component and system verification during system development. This process model is not restricted to worst-case execution time analysis, but we believe other types of analyses could be performed in a similar way.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4283842,no
A Formal Model for Quality of Service Measurement in e-Government,2007,"Quality is emerging as a promising approach to promote the development of services in e-Government. A proper Quality of Services is mandatory in order to satisfy citizens and firms' needs and to accept the use of ICT in our life. This paper describes our ongoing research on QoS run-time measurement and preliminary ideas on the appliance of run-time monitoring to guarantee assurance of service applications. For this purpose, we also define and describe a formal model based on a set of quality parameters for e-Government services. These parameters can be useful both as a basis for understanding and assessing competing services, and as a way to determine what improvements are needed to assure citizens and firms satisfaction.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4283865,no
"Air Quality Derivation utilizing Landsat TM image over Penang, Malaysia",2007,"This paper outlines recent developments in optical remote sensing of Landsat TM data for air quality monitoring for atmospheric particulate matter having a diameter less than 10-micro meter (PM10). The objective of this study is to evaluate the performance of the developed algorithm and suitability of remote sensing data for PM10 mapping. We used a DustTrak Aerosol Monitor 8520 to collect in situ data. The PM10 data were collected simultaneously during the satellite Landsat overpass the study area. An algorithm has been developed based on the optical aerosol characteristic in the atmosphere to estimate PM10 over Penang Island, Malaysia. The digital numbers were determined corresponding to the ground-truth locations for each band and then converted to radiance and reflectance values. The atmospheric reflectance values was extracted from the satellite observed reflectance values subtracters by the amount given by the surface reflectance. The surface refleatance values were retrieved using ATCOR2 in the PCI Geomatica 9.1 image processing software. The atmospheric reflectance values were later used for PM10 mapping using the calibrated algorithm. Results from this research have indicated that PM10 data have positive correlation with atmospheric reflectance in two visible bands (Red and Blue band). Finally, the map of pollution concentration generated from the satellite image using the proposed algorithm to illustrate spatial distribution pattern of air pollution for the study area. The proposed algorithm produced high correlation coefficient, R, and low root-mean-square, RMS, values. The concentrations of PM10 are high in the industrial zones and urban areas of Penang, Malaysia.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4283990,no
Predictive Early Object Shedding in Media Processing Workflows,2007,"Media-rich ubiquitous distributed media processing workflow systems continuously sense users' needs, status, and the context, filter and fuse a multitude of real-time media data, and react by adapting the environment to the user. One challenge facing these systems is that they need to process real-time data arriving continuously from the sensors and data rates and qualities may vary dramatically. Thus, reducing the amount of unqualified data objects that need to be processed within the underlying media processing workflows can enhance the performance significantly. In this paper, we first focus on the prediction of the output qualities at the actuator, especially in the presence of fusion operators in the workflow. We then present quality-aware early object elimination schemes to enable informed resource savings in continuous real-time media processing workflow middleware.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4284869,no
VLSI Oriented Fast Multiple Reference Frame Motion Estimation Algorithm for H.264/AVC,2007,"In H.264/AVC standard, motion estimation can be processed on multiple reference frames (MRF) to improve the video coding performance. For the VLSI real-time encoder, the heavy computation of fractional motion estimation (FME) makes the integer motion estimation (IME) and FME must be scheduled in two macro block (MB) pipeline stages, which makes many fast MRF algorithms inefficient for the computation reduction. In this paper, two algorithms are provided to reduce the computation of FME and IME. First, through analyzing the block's Hadamard transform coefficients, all-zero case after quantization can be accurately detected. The FME processing in the remaining frames for the block, detected as all-zero one, can be eliminated. Second, because the fast motion object blurs its edges in image, the effect of MRF to aliasing is weakened. The first reference frame is enough for fast motion MBs and MRF is just processed on those slow motion MBs with a small search range. The computation of IME is also highly reduced with this algorithm. Experimental results show that 61.4%-76.7% computation can be saved with the similar coding quality as the reference software. Moreover, the provided fast algorithms can be combined with fast block matching algorithms to further improve the performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4285047,no
A Queueing-Theory-Based Fault Detection Mechanism for SOA-Based Applications,2007,"SOA has become more and more popular, but fault tolerance is not supported in most SOA-based applications yet. Although fault tolerance is a grand challenge for enterprise computing, we can partially resolve this problem by focusing on its some aspect. This paper focuses on fault detection and puts forward a queueing-theory-based fault detection mechanism to detect the services that fail to satisfy performance requirements. This paper also gives a reference service model and reference architecture of fault-tolerance control center of Enterprise Services Bus for SOA- based applications.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4285211,no
A Queueing-Theory-Based Fault Detection Mechanism for SOA-Based Applications,2007,"SOA has become more and more popular, but fault tolerance is not supported in most SOA-based applications yet. Although fault tolerance is a grand challenge for enterprise computing, we can partially resolve this problem by focusing on its some aspect. This paper focuses on fault detection and puts forward a queueing-theory-based fault detection mechanism to detect the services that fail to satisfy performance requirements. This paper also gives a reference service model and reference architecture of fault-tolerance control center of Enterprise Services Bus for SOAbased applications.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4285223,no
Deployment of Accountability Monitoring Agents in Service-Oriented Architectures,2007,"Service-oriented architecture (SOA) provides a flexible paradigm to compose dynamic service processes using individual services. However, service processes can be vastly complex, involving many service partners, thereby giving rise to difficulties in terms of pinpointing the service(s) responsible for problematic outcomes. In this research, we study efficient and effective mechanisms to deploy agents to monitor and detect undesirable services in a service process. We model the agent deployment problem as the classic weighted set covering (WSC) problem and present agent selection solutions at different stages of service process deployment. We propose the MASS (merit based agent and service selection) algorithm that considers agent cost during QoS-based service composition by using a meritagent_cost heuristic metric. We also propose the IGA (incremental greedy algorithm) to achieve fast agent selection when a service process is reconfigured after service failures. The performance study shows that our proposed algorithms are effective on saving agent cost and efficient on execution time.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4285228,no
Challenges and Opportunities in Information Quality,2007,"Summary form only given. Each year companies are spending hundreds of thousands of dollars in data cleansing and other activities to improve the quality of information they use to conduct business. The hidden cost of bad data - lost opportunities, low productivity, waste, and myriads of other consequences - is believed to be much higher than these direct costs. One study estimates this combined cost due to bad data to be over U$30 billion in year 2006 alone. As business operations rely more and more on computerized systems, this cost is bound to increase at an alarming rate. Information quality (or data quality) has been an integral part of various enterprise systems such as master data management, customer data integration, and ETL (extraction, transform, and load). We are witnessing trends of renewed awareness and efforts, both in research and practice, to address information quality collectively as an independent value in enterprise computing. International organizations such as EPC Global and the International Standardization Organization (ISO) have launched working groups to study and possibly introduce standards that can be used to define, assess, and enhance information quality throughout the supply chain. Issues in information quality range over multiple disciplines including software engineering, databases, statistics, organizational operations, and accounting. The scope and goal of information quality management would depend on the organization's objectives and business models. Assessing the impact of data quality is a complex task involving key business performance indexes such as sales, profitability, and customer satisfaction. Methods of assuring data quality must address operational processes as well as supporting technologies. This panel, with input from experts from both academia and industry, explores the challenges and opportunities in information quality in the dynamic environment of today's enterprise computing.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4285254,no
The Weight-Watcher Service and its Lightweight Implementation,2007,"This paper presents the weight-watcher service. This service aims at providing resource consumption measurements and estimations for software executing on resource-constrained devices. By using the weight-watcher, software components can continuously adapt and optimize their quality of service with respect to resource availability. The interface of the service is composed of a profiler and a predictor. We present an implementation that is lightweight in terms of CPU and memory. We also performed various experiments that convey (a) the tradeoff between the memory consumption of the service and the accuracy of the prediction, as well as (b) a maximum overhead of 10% on the execution speed of the VM for the profiler to provide accurate measurements.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4285742,no
Research of Real-time Forecast Technology in Fault of Missile Guidance System Based on Grey Model and Data Fusion,2007,"To solve fault forecast in missile guidance system, a new fault forecast method was presented, in which the grey system and multiple-sensor data fusion were used. Grey Model (GM) forecast is invalid when the data sequence is zero-mean random process, to overcome the drawback, present an improved GM method. The simulation results show the fault forecast method has better performance in missile guidance systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4287471,no
Voice Conversion Adopting SOLAFS,2007,"An improved method of voice conversion is proposed to make the speech of a source speaker sound like uttered by a target speaker. Speaker individuality transformation is achieved by altering the spectral envelope and prosodic information. The main advantage of this method is to firstly apply the synchronized overlap-add fixed synthesis (SOLAFS) to modify the source speaker's speaking rate to match that of the target speaker, which enhances the performance of the whole conversion system compared with conventional systems without such a procedure. Besides, a precise estimation for the target excitation is advanced only with the information of the matched source's excitation and the average pitch period of the target speaker. The proposed scheme is evaluated using both subjective and objective measures. The experimental results show that the system is capable of effectively transforming speaker identity whilst the converted speech maintains high quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4287567,no
Recognizing Humans Based on Gait Moment Image,2007,"This paper utilizes the periodicity of swing distances to estimate gait period. It shows good adaptability to low quality silhouette images. Gait moment image (GMI) is implemented based on the estimated gait period. GMI is the gait probability image at each key moment in gait period. It reduces the noise of the silhouettes extracted from low quality videos by gait probability distribution at each key moment. Moment deviation image (MDI) is generated by using silhouette images and GMIs. As a good complement of gait energy image (GEI), MDI provides more motion features than the basic GEI. MDI is utilized together with GEI to represent a subject. The nearest neighbor classifier is adopted to recognize subjects. The proposed algorithm is evaluated on the USF gait database, and the performance is compared with the baseline algorithm and two other algorithms. Experimental results show that this algorithm achieves a higher total recognition rate than the other algorithms.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4287755,no
A New Inner Congestion Control Mechanism in Terabit Routers,2007,"Direct interconnection network (DIN) has gained more and more attention when designing the switching fabric in the terabit routers. Various congestion control mechanisms are proposed for DIN when it is employed in the parallel computer systems. However, they all have own shortcomings and are not suitable for fabrics in the terabit routers. Based on the requirements of terabit class routers we propose a new congestion control mechanism in this paper. It uses a new detection criterion, i.e. the length of the source buffer, which is more accurate. Besides, the presence of the real time traffic is considered in particular. No throttling is imposed on such traffic to provide QoS. The input traffic will go into different source queue according to their traffic class. The input queue for the real time traffic will no longer be stopped to provide QoS. Finally, simulations are carried out by OPNET software. The results show that our congestion control mechanism eliminates performance degradation for loads beyond saturation, keeping adequate levels of throughput at high loads.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4287845,no
A Resource Allocation Model with Cost-Performance Ratio in Data Grid,2007,"Resource allocation for data transfer is a fundamental issue for achieving high performance in data grid environments. In this paper, we survey the existing researches on allocation problems in grid environment and propose a resource allocation model based on the cost-performance ratio for commerce environments. This ratio takes both price and quality of data resource into consideration at the same time. According to the optimization objective we define two types of ratios: Price-aware ratio and quality-aware ratio. They are suitable for the environments where allocations put more emphasis on quality or price of resource respectively. Based on the maximal cost-efficiency, we formulize the allocation optimization problem and present two theorems and a deduction about its solutions. The corresponding proofs are also given in the paper. Finally, we present two algorithms to allocate and re-allocate data resources. The analysis shows that the algorithms can satisfy our requirements.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4287880,no
A Traceability Link Model for the Unified Process,2007,"Traceability links are widely accepted as efficient means to support an evolutionary software development. However, their usage in analysis and design is effort consuming and error prone due to lacking or missing methods and tools for their creation, update and verification. In this paper we analyse and classify Unified Process artefacts to establish a traceability link model for this process. This model defines all required links between the artefacts. Furthermore, it provides a basis for the (semi)-automatic establishment and the verification of links in Unified Process development projects. We also define a first set of rules as step towards an efficient management of the links. In the ongoing project the rule set is extended to establish a whole framework of methods and rules.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4287940,no
Clustering Ensemble based on the Fuzzy KNN Algorithm,2007,"Compared with the single clustering algorithm, Clustering Ensembles are deemed to be more robust and accurate, with combining multiple partitions of the given data into a single clustering solution of better quality. In this paper, we proposed a new Clustering Ensemble algorithm based on Fuzzy K Nearest Neighbor (FKNNCE) to generate the similarity matrix of data to summarize the ensemble and then use hierarchical clustering algorithm to get the final partition, without specified number of clusters in advance. After discussing some related topics, the paper adopts real data and conducts an Intrusion Detection Model to evaluate the performance of the Clustering Ensemble algorithm, furthermore compare it with other algorithms. Experimental results demonstrate the effectiveness of the proposed algorithm.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4287995,no
On the Customization of Components: A Rule-Based Approach,2007,"Realizing the quality-of-service (QoS) requirements for a software system continues to be an important and challenging issue in software engineering. A software system may need to be updated or reconfigured to provide modified QoS capabilities. These changes can occur at development time or at runtime. In component-based software engineering, software systems are built by composing components. When the QoS requirements change, there is a need to reconfigure the components. Unfortunately, many components are not designed to be reconfigurable, especially in terms of QoS capabilities. It is often labor-intensive and error-prone work to reconfigure the components, as developers need to manually check and modify the source code. Furthermore, the work requires experienced senior developers, which makes it costly. The limitations motivate the development of a new rule-based semiautomated component parameterization technique that performs code analysis to identify and adapt parameters and changes components into reconfigurable ones. Compared with a number of alternative QoS adaptation approaches, the proposed rule-based technique has advantages in terms of flexibility, extensibility, and efficiency. The adapted components support the reconfiguration of potential QoS trade-offs among time, space, quality, and so forth. The proposed rule-based technique has been successfully applied to two substantial libraries of components. The F-measure or balanced F-score results for the validation are excellent, that is, 94 percent. Index Terms-Performance measures, rule-based processing, representations.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4288145,no
"Comments on ""Data Mining Static Code Attributes to Learn Defect Predictors""",2007,"In this correspondence, we point out a discrepancy in a recent paper, ""data mining static code attributes to learn defect predictors,"" that was published in this journal. Because of the small percentage of defective modules, using probability of detection (pd) and probability of false alarm (pf) as accuracy measures may lead to impractical prediction models.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4288196,no
"Problems with Precision: A Response to ""Comments on 'Data Mining Static Code Attributes to Learn Defect Predictors'""",2007,"Zhang and Zhang argue that predictors are useless unless they have high precison&recall. We have a different view, for two reasons. First, for SE data sets with large neg/pos ratios, it is often required to lower precision to achieve higher recall. Second, there are many domains where low precision detectors are useful.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4288197,no
2007 IEEE International Conference on Communications,2007,"The following topics are dealt with: communications QoS, reliability and performance modelling; wireless mesh, ad hoc and cellular networks; network routing; network congestion control; QoS control in mobile networks; wireless local area networks; network recovery and protection; network resource management; network traffic engineering; QoS techniques for video and voice; LDPC codes; cooperative networks; MIMO communications; OFDM modulation; turbo coding; iterative decoding; ad hoc and peer-to-peer network security; authentication and biometric security; distributed security and denial-of-service attacks; firewalls; intrusion detection and avoidance; secure communication protocols for ad hoc networks; Web security; multimedia communications; home services; peer-to-peer streaming; overlay networks; wireless multimedia; error control and recovery; voice over IP; Internet architecture; service-aware networks; context and revenue management; passive optical networks; optical switching; WDM networks; signal processing for wireless systems; synchronization; channel estimation and equalization; cross-layer design; medium access Control; CDMA; space-time coding; satellite communications; transport protocols; software defined radio; cognitive radio and smart antenna technologies.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4288672,no
A Measurement Based Dynamic Policy for Switched Processing Systems,2007,"Switched processing systems (SPS) represent a canonical model for many areas of applications of communication, computer and manufacturing systems. They are characterized by flexible, interdependent service capabilities and multiple classes of job traffic flows. Recently, increased attention has been paid to the issue of improving quality of service (QoS) performance in terms of delays and backlogs of the associated scheduling policies, rather than simply maximizing the system's throughput. In this study, we investigate a measurement based dynamic service allocation policy that significantly improves performance with respect to delay metrics. The proposed policy solves a linear program at selected points in time that are in turn determined by a monitoring strategy that detects 'significant' changes in the intensities of the input processes. The proposed strategy is illustrated on a small SPS subject to different types of input traffic.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4288728,no
Flow Management for SIP Application Servers,2007,"In this paper, we study how to build a front-end flow management system for SIP application servers. This is challenging because of some special characteristics of SIP and SIP applications. (1) SIP flows are well organized into sessions. The session structure should be respected when managing SIP flows. (2) SIP has been adopted by telecom industry, whose applications have more critical QoS requirements than WEB ones. (3) SIP message retransmissions exacerbate the overload situation in case of load bursts; moreover, they may trigger persistent retransmission phenomenon, which retains large response times even after the original burst disappears. To address the combination of these challenges, we propose a novel front-end SIP flow management system FEFM. FEFM integrates concurrency limiting, message scheduling and admission control to achieve overload protection and performance management. It also devises some techniques such as response time prediction, twin-queue scheduling, and retransmission removal to accomplish SLA-oriented improvement, reduce the call rejection rate and banish the persistent retransmission phenomenon. Intensive experiments show that FEFM achieves overload protection in burst period, improves performance significantly, and has the ability to compromise different tradeoffs between throughput and SLA satisfaction.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4288782,no
The Study of Noise-rejection by Using Pulse Time-delay Identification Method and the Analysis of PD Data Obtained in the Field,2007,"Based on the mechanism of partial discharges occurring in the stator winding insulations of generators, a set of on-line PD measurement with double sensors is developed in this paper. After comparing the different types of measurement, the noise suppressing method is focused on the technique of ""pulse time-delay identification"" which is based on installing double sensors for every phase. The principle of the method is introduced in this paper. The software program designed by Labview provides the power frequency graph, N-Q and N- Ã‚Â¿ two dimension diagram, three dimension diagram and maximum quantity of PD and NQN tendency diagram to make a further analysis of the PD data. In addition, the dada obtained in the field is analyzed here, including an insulation fault detected in a plant successfully.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4290885,no
Defining and Detecting Bad Smells of Aspect-Oriented Software,2007,"Bad smells are software patterns that are generally associated with bad design and bad programming. They can be removed by using the refactoring technique which improves the quality of software. Aspect-oriented (AO) software development, which involves new notions and the different ways of thinking for developing software and solving the crosscutting problem, possibly introduces different kinds of design flaws. Defining bad smells hidden in AO software in order to point out bad design and bad programming is then necessary. This paper proposes the definition of new AO bad smells. Moreover, appropriate existing AO refactoring methods for eliminating each bad smell are presented. The proposed bad smells are validated. The results show that after removing the bad smells by using appropriate refactoring methods, the software quality is increased.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4290985,no
Analysis of Conflicts among Non-Functional Requirements Using Integrated Analysis of Functional and Non-Functional Requirements,2007,"Conflicts among non-functional requirements are often identified subjectively and there is a lack of conflict analysis in practice. Current approaches fail to capture the nature of conflicts among non-functional requirements, which makes the task of conflict resolution difficult. In this paper, a framework has been provided for the analysis of conflicts among non-functional requirements using the integrated analysis of functional and non-functional requirements. The framework identifies and analyzes conflicts based on relationships among quality attributes, functionalities and constraints. Since, poorly structured requirement statements usually result in confusing specifications; we also developed canonical forms for representing non-functional requirements. . The output of our framework is a conflict hierarchy that refines conflicts among non-functional requirements level by level. Finally, a case study is provided in which the proposed framework was applied to analyze and detect conflicts among the non-functional requirements of a search engine.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291007,no
Measuring and Assessing Software Reliability Growth through Simulation-Based Approaches,2007,"In the past decade, several rate-based simulation approaches were proposed to predict software failure process. But most of them did not take the number of available debuggers into consideration and this may not be reasonable. In practice, the number of debuggers is always limited and controlled. If all debuggers or developers are busy, the new detected faults should be willing to wait (for a long time to be corrected and removed). Besides, practical experiences also show that the fault removal time is non-negligible and the number of removed faults generally lags behind the total number of detected faults. Based on these facts, in this paper, we will apply queueing theory to describe and explain the possible debugging behavior during software development. Two simulation procedures are developed based on G/G/ infin and G/G/m queueing models. The proposed methods will be illustrated with real software failure data. Experimental results will be analyzed and discussed in detail. The results we obtained will greatly help to understand the influence of size of debugger teams on the software failure correction activities and other related reliability assessments.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291036,no
Test Case Prioritization for Black Box Testing,2007,"Test case prioritization is an effective and practical technique that helps to increase the rate of regression fault detection when software evolves. Numerous techniques have been reported in the literature on prioritizing test cases for regression testing. However, existing prioritization techniques implicitly assume that source or binary code is available when regression testing is performed, and therefore cannot be implemented when there is no program source or binary code to be analyzed. In this paper, we presented a new technique for black box regression testing, and we performed an experiment to measure our technique. Our results show that the new technique is helpful to improve the effectiveness of fault detection when performing regression test in black box environment.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291039,no
Bivariate Software Fault-Detection Models,2007,"In this paper, we develop bivariate software fault-detection models with two time measures: calendar time (day) and test-execution time (CPU time) and incorporate both of them to assess the quantitative software reliability with higher accuracy. The resulting stochastic models are characterized by a simple binomial process and the bivariate order statistics of software fault-detection times with different time scales.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291048,no
A Control-Theoretic Approach to QoS Adaptation in Data Stream Management Systems Design,2007,"In data stream management systems (DSMSs), request handling has to meet various QoS requirements. Traditional open-loop DSMS ignores system status information and run-time history in decision making and is unable to achieve stable QoS performance over run-time as the workload or environment changes. This paper discusses the design of a close-loop DSMS that applies adaptive control in control theory to adapt itself to the changing environment and workloads in order to optimize the QoS performance over run-time. The proposed adaptive DSMS is built on system identification and successive approximation knowledge in control theory and uses a least square approach for parameter estimation. Experimental data shows that the proposed adaptive DSMS outperforms the traditional open-loop DSMS in overall QoS among various network conditions and workload settings.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291131,no
"Quality Metrics for Internet Applications: Developing ""New"" from ""Old""",2007,"This discussion concerns 'metrics'. More specifically, we discuss quantitative metrics for evaluating Internet applications: what should we quantify, monitor and analyse in order to characterise, evaluate and develop Internet applications based on reusing existing Internet applications,which is widely available in the Internet. Due to the distinctive evolution nature of Internet applications, assessing quality of software will provide ease and higher accuracy for Web developers. However, there is a great gap between the rapid development of Internet applications and the slow speed of developing corresponding metric measures. To tackle this issue, we look into measuring the quality of Internet applications and enable Web developers to enhance the quality of their programs and identify reusable components from Internet-based resources.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291159,no
Redefining Performance Evaluation Tools for Real-Time QRS Complex Classification Systems,2007,"In a heartbeat classification procedure, the detection of QRS complex waveforms is necessary. In many studies, this heartbeat extraction function is not considered: the inputs of the classifier are assumed to be correctly identified. This communication aims to redefine classical performance evaluation tools in entire QRS complex classification systems and to evaluate the effects induced by QRS detection errors on the performance of heartbeat classification processing (normal versus abnormal). Performance statistics are given and discussed considering the MIT/BIH database records that are replayed on a real-time classification system composed of the classical detector proposed by Hamilton and Tompkins, followed by a neural-network classifier. This study shows that a classification accuracy of 96.72% falls to 94.90% when a drop of 1.78% error rate is introduced in the detector quality. This corresponds to an increase of about 50% bad classifications.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291669,no
Failure and Coverage Factors Based Markoff Models: A New Approach for Improving the Dependability Estimation in Complex Fault Tolerant Systems Exposed to SEUs,2007,"Dependability estimation of a fault tolerant computer system (FTCS) perturbed by single event upsets (SEUs) requires obtaining first the probability distribution functions for the time to recovery (TTR) and the time to failure (TTF) random variables. The application cross section (sigmaAP) approach does not give directly all the required information. This problem can be solved by means of the construction of suitable Markoff models. In this paper, a new method for constructing such models based on the system's failure and coverage factors is presented. Analytical dependability estimation is consistent with fault injection experiments performed in a fault tolerant operating system developed for a complex, real time data processing system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291722,no
Automatic Synthesis of Fault Detection Modules for Mobile Robots,2007,"In this paper, we present a new approach for automatic synthesis of fault detection modules for autonomous mobile robots. The method relies on the fact that hardware faults typically change the flow of sensory perceptions received by the robot and the subsequent behavior of the control program. We collect data from three experiments with real robots. In each experiment, we record all sensory inputs from the robots while they are operating normally and after software-simulated faults have been injected. We use back- propagation neural networks to synthesize task-dependent fault detection modules. The performance of the modules is evaluated in terms of false positives and latency.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291986,no
How Business Goals Drive Architectural Design,2007,"This paper illustrates how business goals can significantly impact a software management system's architecture without necessarily affecting its functionality. These goals include 1) supporting hardware devices from different manufacturers, 2) considering language, culture, and regulations of different markets, 3) assessing tradeoffs and risks to determine how the product should support these goals, 4) refining goals such as scaling back on intended markets, depending on the company's comfort level with the tradeoffs and risks. More importantly, these business goals correspond to quality attributes the end system must exhibit. The system must be modifiable to support a multitude of hardware devices and consider different languages and cultures. Supporting different regulations in different geographic markets requires the system to respond to life-threatening events in a timely manner performance requirement.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4292022,no
Scrum and CMMI Level 5: The Magic Potion for Code Warriors,2007,Projects combining agile methods with CMMI<sup>1</sup> are more successful in producing higher quality software that more effectively meets customer needs at a faster pace. Systematic Software Engineering works at CMMI level 5 and uses lean Software Development as a driver for optimizing software processes. Early pilot projects at Systematic showed productivity on Scrum teams almost twice that of traditional teams. Other projects demonstrated a story based test driven approach to software development reduced defects found during final test by 40%. We assert that Scrum and CMMI together bring a more powerful combination of adaptability and predictability than either one alone and suggest how other companies can combine them.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4293608,no
Detecting and Reducing Partition Nodes in Limited-routing-hop Overlay Networks,2007,"Many Internet applications use overlay networks as their basic facilities, like resource sharing, collaborative computing, and so on. Considering the communication cost, most overlay networks set limited hops for routing messages, so as to restrain routing within a certain scope. In this paper we describe partition nodes in such limited- routing-hop overlay networks, whose failure may potentially lead the overlay topology to be partitioned so that seriously affect its performance. We propose a proactive, distributed method to detect partition nodes and then reduce them by changing them into normal nodes. The results of simulations on both real-trace and generated topologies, scaling from 500 to 10000 nodes, show that our method can effectively detect and reduce partition nodes and improve the connectivity and fault tolerance of overlay networks.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4293756,no
Quality of Service of Grid Computing: Resource Sharing,2007,"Rapid advancement of communication technology has changed the landscape of computing. New models of computing, such as business-on-demand, Web services, peer-to-peer networks, and grid computing have emerged to harness distributed computing and network resources to provide powerful services. The non-deterministic characteristic of the resource availability in these new computing platforms raises an outstanding challenge: how to support quality of service (QoS) to meet a user's demand? In this study, we conduct a thorough study of QoS of distributed computing, especially on grid computing where the requirement of distributed sharing and coordination goes to the extreme. We start at QoS policies, and then focus on technical issues of the enforcement of the policies and performance optimization under each policy. This study provides a classification of existing software system based on their underlying policies, a systematic understanding of QoS, and a framework for QoS of grid computing.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4293807,no
Train Management Platform for advanced maintenance of Passenger Information Systems,2007,"A Passenger Information System provides information about the train's running to the passengers on-board. Many nodes are present in this system, located all along the train. To guarantee that the information provided is correct and to perform predictive maintenance on this system in order to reduce costs, an intelligent distributed software application is presented in this paper. This proposed application eases the tasks of fault detection and correlation and can lead to an improvement in performance by aiding in the task of preventive maintenance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4295921,no
Building a Novel GP-Based Software Quality Classifier Using Multiple Validation Datasets,2007,"One problem associated with software quality classification (SQC) modeling is that the historical metric dataset obtained from a single software project are often not adequate to build robust and accurate models. To address this issue, multiple datasets obtained from different software projects are used for SQC modeling in recent research works. Our previous study has demonstrated that using multiple datasets for validation can achieve robust genetic programming (GP)-based SQC models. This paper further investigates the effectiveness of using multiple validation datasets. Moreover, a novel GP-based classifier consisting of training, multiple-dataset validation, and voting phases, is proposed. The experiments are carried out on seven NASA software projects. The results are compared with the results achieved by seventeen other data mining techniques. The comparisons demonstrate that the performance of our approach is significantly better by using multiple datasets from different software projects with similar reliability goals.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4296693,no
An Empirical Study of the Classification Performance of Learners on Imbalanced and Noisy Software Quality Data,2007,"In the domain of software quality classification, data mining techniques are used to construct models (learners) for identifying software modules that are most likely to be fault-prone. The performance of these models, however, can be negatively affected by class imbalance and noise. Data sampling techniques have been proposed to alleviate the problem of class imbalance, but the impact of data quality on these techniques has not been adequately addressed. We examine the combined effects of noise and imbalance on classification performance when seven commonly-used sampling techniques are applied to software quality measurement data. Our results show that some sampling techniques are more robust in the presence of noise than others. Further, sampling techniques are affected by noise differently given different levels of imbalance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4296694,no
A practical method for the software fault-prediction,2007,"In the paper, a novel machine learning method, SimBoost, is proposed to handle the software fault-prediction problem when highly skewed datasets are used. Although the method, proved by empirical results, can make the datasets much more balanced, the accuracy of the prediction is still not satisfactory. Therefore, a fuzzy-based representation of the software module fault state has been presented instead of the original faulty/non-faulty one. Several experiments were conducted using datasets from NASA Metrics Data Program. The discussion of the results of experiments is provided.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4296695,no
Software Defects Prediction using Operating Characteristic Curves,2007,We present a software defect prediction model using operating characteristic curves. The main idea behind our proposed technique is to use geometric insight in helping construct an efficient and fast prediction method to accurately predict the. cumulative number of failures at any given stage during the software development process. Our predictive approach uses the number of detected faults instead of the software failure-occurrence time in the testing phase. Experimental results illustrate the effectiveness and the much improved performance of the proposed method in comparison with the Bayesian prediction approaches.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4296704,no
An Improved Approach to Passive Testing of FSM-based Systems,2007,"Fault detection is a fundamental part of passive testing which determines whether a system under test (SUT) is faulty by observing the input/output behavior of the SUT without interfering its normal operations. In this paper, we propose a new approach to finite state machine (FSM)-based passive fault detection which improves the performance of the approach in [4] and gathers more information during testing compared with the approach in [4]. The results of theoretical and experimental evaluations are reported.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4296717,no
Deriving Queuing Network Model for UML for Software Performance Prediction,2007,"It is an important issue for software architects to estimate the performance of software in the early stage of development process due to the needs to verify QoS. Queueing network model is a very useful tool to analyze the performance of a system from abstract model. In this paper, we propose a transformation technique from UML into queueing network model. This approach avoids the need for a prototype implementation since we can determine the overall form of performance equation from the architectural design description. We prove the accuracy of derived queueing network model, which is summarized at 85 percent, through a ubiquitous commerce system which extends mobile commerce system developed by our prior work.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4296927,no
Anomaly-based Fault Detection System in Distributed System,2007,"One of the important design criteria for distributed systems and their applications is their reliability and robustness to hardware and software failures. The increase in complexity, inter connectedness, dependency and the asynchronous interactions between the components that include hardware resources (computers, servers, network devices), and software (application services, middleware, web services, etc.) makes the fault detection and tolerance a challenging research problem. In this paper, we present an innovative approach based on statistical and data mining techniques to detect faults (hardware or software) and also identify the source of the fault. In our approach, we monitor and analyze in realtime all the interactions between all the components of a distributed system. We used data mining and supervised learning techniques to obtain the rules that can accurately model the normal interactions among these components. Our anomaly analysis engine will immediately produce an alert whenever one or more of the interaction rules that capture normal operations is violated due to a software or hardware failure. We evaluate the effectiveness of our approach and its performance to detect software faults that we inject asynchronously, and compare the results for different noise level.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4297016,no
Exploring Genetic Programming and Boosting Techniques to Model Software Reliability,2007,"Software reliability models are used to estimate the probability that a software fails at a given time. They are fundamental to plan test activities, and to ensure the quality of the software being developed. Each project has a different reliability growth behavior, and although several different models have been proposed to estimate the reliability growth, none has proven to perform well considering different project characteristics. Because of this, some authors have introduced the use of Machine Learning techniques, such as neural networks, to obtain software reliability models. Neural network-based models, however, are not easily interpreted, and other techniques could be explored. In this paper, we explore an approach based on genetic programming, and also propose the use of boosting techniques to improve performance. We conduct experiments with reliability models based on time, and on test coverage. The obtained results show some advantages of the introduced approach. The models adapt better to the reliability curve, and can be used in projects with different characteristics.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4298229,no
Detecting Primary Transmitters via Cooperation and Memory in Cognitive Radio,2007,"Effective detection of the activity of primary transmitters is known to be one of the major challenges to the implementation of cognitive radio systems. In this paper, we investigate the use of cooperation and memory (using tools from change detection) as means to enhance primary detection. We focus on the simple case of two secondary users and one primary source. Numerical results show the relevant performance benefits of both cooperation and memory.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4298330,no
On the Evaluation of Header Compression for VoIP Traffic over DVB-RCS,2007,"The transition towards the 'All IP' environment was long ago foreseen, but the actual shift towards this goal did not happen until recently. This inevitable convergence is naturally coupled with a number of significant advantages, but also introduces some problems that were not present before. One of these, which is the focus of this paper, is the efficient transport of voice traffic over IP (VoIP). Since voice streams consist of numerous but small packets, the overhead that is caused by the RTP/UDP/IP headers is comparable - if not higher than the capacity required for the actual payload. This handicap becomes even more severe in the case of radio communications, where the scarcity of bandwidth demands an as efficient as possible utilization. The above facts make the introduction of header compression algorithms necessary, in order to mitigate the problem of overwhelming overhead. This paper describes the design and implementation of a software platform that aims to evaluate the performance of a well known header compression scheme within the context of DVB-RCS. More specifically, the focus of this work is to assess quantitatively the gains in capacity and the degradation of quality of service, when the Compressed RTP header compression scheme is employed in this satellite environment. The presented testbed models the impairments of the satellite channel and applies the header compression mechanism on real VoIP traffic.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4299089,no
Identification Of Software Performance Bottleneck Components In Reuse based Software Products With The Application Of Acquaintanceship Graphs,2007,Component-based software engineering provides an opportunity for better quality and increased productivity in software development by using reusable software components [9]. Also performance is a make-or-break quality for software. The systematic application of software performance engineering techniques throughout the development process can help to identify design alternatives that preserve desirable qualities such as extensibility and reusability while meeting performance objectives [1]. Implementing the effective performance-based management of software intensive projects has proven to be challenging task now a days. This paper aims at identifying the major reasons of software performance failures in terms of the component communication path. My work focused on one of the applications of discrete mathematics namely graph theory. This study makes an attempt to predict the most used components to the least used with the help of acquaintanceship graphs and also the shortest communication path between any two components with the help of adjacency matrix. Experiments are conducted with four components and the result shows a promising approach towards component utilization and bottleneck determination that describe the major areas in the component communication to concentrate in achieving success in cost-effective development of highly performance software.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4299916,no
Redundant Coupling Detection Using Dynamic Dependence Analysis,2007,"Most of the software engineers realize the importance of avoiding coupling between programs modules in order to achieve the advantages of modules reusability. However, most of them practice modules' coupling in many cases without necessities. Many techniques have been proposed to detect module's coupling in computer programs. However, developers desire to automatically identify unnecessary module's coupling that can be eliminated with minimum effort, which we refer to as redundant coupling. Redundant coupling is the module coupling that does not contribute to the output of the program. In this paper we introduce an automated approach that uses the dynamic dependence analysis to detect the redundant coupling between program's modules. Such technique guides the developers to avoid redundant coupling when testing their programs.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4299924,no
Biosensor for the Detection of Specific DNA Sequences by Impedance Spectroscopy,2007,"In the agro-food industry there is a great need for rapid, sensitive and label-free biosensors for food quality control, for example the detection of pathogenic agents such as listeria or salmonella in dairy food samples. We are currently working on the development of a real-time monitoring system based on an interdigitated electrode DNA biochip for the detection of specific DNA target sequences. The biosensor contains four active areas on which DNA probes of various kinds can be immobilized, and is integrated into a microfluidic system. Impedimetric measurements of matching and non-matching synthetic target DNA sequences have been performed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4300289,no
Quality-of-Service Management in ASEMA System for Unicast MPEG4 Video Transmissions,2007,"The Active Service Environment Management system (ASEMA) provides best possible multimedia service experience to end user. One of the main aspects of the ASEMA is to provide a variable live streaming service to the end users of the ASEMA. The ASEMA implements this through dynamic change in the properties of the video stream delivered to the end user's end device. The live video stream used in the ASEMA is delivered to the end users is in MPEG 4 video format. The video itself is streamed on top of the real time protocol (RTP) and the parameters are negotiated with the real time streaming protocol (RTSP) before the streaming commences. The research problem of this is to provide easy solution for QoS measurements in networks that are closed nature. The research is based on the constructive method of the related publications and the results are deducted from the constructed quality of service management of the ASEMA system. The management is founded on the measuring of the receiving and sending bit rate values in both ends, in the sending end and in the receiving end. If a fluctuation in the values are detected the video stream's properties are changed dynamically.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4301080,no
Early Software Product Improvement with Sequential Inspection Sessions: An Empirical Investigation of Inspector Capability and Learning Effects,2007,"Software inspection facilitates product improvement in early phases of software development by detecting defects in various types of documents, e.g., requirements and design specifications. Empirical study reports show that usage-based reading (UBR) techniques can focus inspectors on most important use cases. However, the impact of inspector qualification and learning effects in the context of inspecting a set of documents in several sessions is still not well understood. This paper contributes a model for investigating the impact of inspector capability and learning effects on inspection effectiveness and efficiency in a large-scale empirical study in an academic context. Main findings of the study are (a) the inspection technique UBR better supported the performance inspectors with lower experience in sequential inspection cycles (learning effect) and (b) when inspecting objects of similar complexity significant improvements of defect detection performance could be measured.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4301086,no
A Two-Step Model for Defect Density Estimation,2007,"Identifying and locating defects in software projects is a difficult task. Further, estimating the density of defects is more difficult. Measuring software in a continuous and disciplined manner brings many advantages such as accurate estimation of project costs and schedules, and improving product and process qualities. Detailed analysis of software metric data gives significant clues about the locations and magnitude of possible defects in a program. The aim of this research is to establish an improved method for predicting software quality via identifying the defect density of fault prone modules using machine-learning techniques. We constructed a two-step model that predicts defect density by taking module metric data into consideration. Our proposed model utilizes classification and regression type learning methods consecutively. The results of the experiments on public data sets show that the two-step model enhances the overall performance measures as compared to applying only regression methods.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4301095,no
Distance Estimation of Switched Capacitor Banks in Utility Distribution Feeders,2007,"This paper proposes an efficient and accurate technique for estimating the location of an energized capacitor bank downline from a monitoring equipment. The proposed technique is based on fundamental circuit theory and works with existing capacitor switching transient data of radial power distribution systems. Once the direction of a switched capacitor bank is found to be downstream from a power quality monitoring point, the feeder line reactance is estimated using the initial voltage change. The efficacy of the proposed technique is demonstrated with system data from IEEE test feeders modeled in a commercial time-domain modeling software package.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4302528,no
Enlarging Instruction Streams,2007,"Web applications are widely adopted and their correct functioning is mission critical for many businesses. At the same time, Web applications tend to be error prone and implementation vulnerabilities are readily and commonly exploited by attackers. The design of countermeasures that detect or prevent such vulnerabilities or protect against their exploitation is an important research challenge for the fields of software engineering and security engineering. In this paper, we focus on one specific type of implementation vulnerability, namely, broken dependencies on session data. This vulnerability can lead to a variety of erroneous behavior at runtime and can easily be triggered by a malicious user by applying attack techniques such as forceful browsing. This paper shows how to guarantee the absence of runtime errors due to broken dependencies on session data in Web applications. The proposed solution combines development-time program annotation, static verification, and runtime checking to provably protect against broken data dependencies. We have developed a prototype implementation of our approach, building on the JML annotation language and the existing static verification tool ESC/Java2, and we successfully applied our approach to a representative J2EE-based e-commerce application. We show that the annotation overhead is very small, that the performance of the fully automatic static verification is acceptable, and that the performance overhead of the runtime checking is limited.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4302707,no
Failure Detection in Large-Scale Internet Services by Principal Subspace Mapping,2007,"Fast and accurate failure detection is becoming essential in managing large-scale Internet services. This paper proposes a novel detection approach based on the subspace mapping between the system inputs and internal measurements. By exploring these contextual dependencies, our detector can initiate repair actions accurately, increasing the availability of the system. Although a classical statistical method, the canonical correlation analysis (CCA), is presented in the paper to achieve subspace mapping, we also propose a more advanced technique, the principal canonical correlation analysis (PCCA), to improve the performance of the CCA-based detector. PCCA extracts a principal subspace from internal measurements that is not only highly correlated with the inputs but also a significant representative of the original measurements. Experimental results on a Java 2 platform, enterprise edition (J2EE)- based Web application demonstrate that such property of PCCA is especially beneficial to failure detection tasks.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4302740,no
Synchronization Techniques for Power Quality Instruments,2007,"The measurement of voltage characteristics in power systems requires the accurate estimation of the power supply frequency and signal synchronization, even in the presence of disturbances. The authors developed and tested two innovative techniques for instrument synchronization. The first is based on signal spectral analysis techniques performed by means of the Chirp- transform analysis. The second is a phase-locked loop (PLL) software based on a time-domain coordinate transformation and an innovative phase-detection technique. To evaluate how synchronization techniques are adversely affected by the application of a disturbing influence, experimental tests were carried out, taking into account the requirements of the standards. The proposed techniques were compared with a standard hardware solution. In this paper, the proposed techniques are described, the experimental results are presented, and the accuracy specifications are discussed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4303373,no
Fault Tolerance of Multiprocessor-Structured Control System by Hardware and Software Reconfiguration,2007,"Since the traditional redundancy for fault tolerance of a control system is complex in structure and expensive, a novel method for fault tolerance of multiprocessor-structured control system by hardware and software reconfiguration is presented. Based on the fact that the control system is composed of several processors, this method performs fault detection by self-diagnosis implemented in each processor and validation of exchanged information between the processors, tolerates fault by hardware and software reconfiguration carried out by monitoring and configuring device. Security strategy and operation mode were presented. The principle of the monitoring and configuring device was discussed in detail. The method was proved by a control system of dc motor and got a satisfied result.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4304170,no
Node-Replacement Policies to Maintain Threshold-Coverage in Wireless Sensor Networks,2007,"With the rapid deployment of wireless sensor networks, there are several new sensing applications with specific requirements. Specifically, target tracking applications are fundamentally concerned with the area of coverage across a sensing site in order to accurately track the target. We consider the problem of maintaining a minimum threshold-coverage in a wireless sensor network, while maximizing network lifetime and minimizing additional resources. We assume that the network has failed when the sensing coverage falls below the minimum threshold-coverage. We develop three node-replacement policies to maintain threshold-coverage in wireless sensor networks. These policies assess the candidature of each failed sensor node for replacement. Based on different performance criteria, every time a sensor node fails in the network, our replacement policies either replace with a new sensor or ignore the failure event. The node-replacement policies replace a failed node according to a node weight. The node weight is assigned based on one of the following parameters: cumulative reduction of sensing coverage, amount of energy increase per node, and local reduction of sensing coverage. We also implement a first-fail-first-replace policy and a no-replacement policy to compare the performance results. We evaluate the different node-replacement polices through extensive simulations. Our results show that given a fixed number of replacement sensor nodes, the node-replacement policies significantly increase the network lifetime and the quality of coverage, while keeping the sensing-coverage about a pre-set threshold.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4317909,no
Improving the Diagnosis of Ischemic CVA's through CT Scan with Neural Networks,2007,"Technological and computing evolution promoted new opportunities to improve the quality of life, in particular, the quality of diagnostic evaluations. Computerized tomography is one of the imaging equipments of diagnosis which has most benefited from technological improvements. Because of that, and due to the quality of the diagnosis produced, it is one of the most employed equipments in clinical applications. The ischaemic cerebral vascular accident (ICVA) is the pathology that confirms the frequent use of the computerized tomography. The interest for this pathology, and in general for the encephalon image analysis as a preventive diagnosis, is mainly due to the frequent occurrence of ICV As in development countries and its social-economic impact. In this sense, we propose to evaluate the ability of artificial neural networks (ANN) for automatic identification of ICVA by means of tissue density images obtained by computerised tomography. This work employed cranioencephalon computerised tomography exams and their respective medical reports, to train ANNs classifiers. Features extracted from the images were used as inputs to the classifiers. Once the ANNs were trained, the neural classifiers were tested with data never seen by the network. At this stage we may conclude that the ANNs may significantly contribute as an ICV As computerised tomography diagnostic aid, since among the test cases the automatic identification of ischaemic lesions has been performed with no false negatives e very few false positives.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4318302,no
An Applied Study of Destructive Measurement System Analysis,2007,"Measurement system analysis (MSA) is used to assess the ability of a measurement system to detect meaningful differences in process variables. For destructive measurement system, there are two methods to design and analyze it based on the homogenous batch size, including nested MSA and crossed MSA. At the same time, the P/T ratio (""tolerance method"") is modified to measure the suitability of destructive measurement system to make pass/fail decisions to a specification. Finally, rip off force testing system of chargers is assessed by crossed MSA and modified P/T ratio. Some suggestions for destructive MSA are also presented.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4318364,no
Application of Multi-function Measure Control Analyzing Device in Electric Distribution,2007,"With the rapid development of computer technology, many kinds of monitor and analysis devices such as power quality monitor, fault record, fault detection and other device can be integrated into one device by a computer for electric distribution. And it will look very expedient and can complete all of the work in electric distribution. In this paper, a kind of method to design such multifunctional device called multi-function measure control analyzing device employed in electric distribution will be introduced. And a new algorithm using the fractal number of phase voltages and phase currents is presented for power grid monitor detection and fault line selection. The apparatus we have developed operates very well in electric distribution. And the application results show that this system is very applicable and can be used with great validity in dynamic monitoring fault diagnosis and trouble removal.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4318819,no
Countermeasures Against Branch Target Buffer Attacks,2007,"Branch Prediction Analysis has been recently proposed as an attack method to extract the key from software implementations of the RSA public key cryptographic algorithm. In this paper, we describe several solutions to protect against such an attack and analyze their impact on the execution time of the cryptographic algorithm. We show that the code transformations required for protection against branch target buffer attacks can be automated and impose only a negligible performance penalty.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4318987,no
A Practical Model for Measuring Maintainability,2007,"The amount of effort needed to maintain a software system is related to the technical quality of the source code of that system. The ISO 9126 model for software product quality recognizes maintainability as one of the 6 main characteristics of software product quality, with adaptability, changeability, stability, and testability as subcharacteristics of maintainability. Remarkably, ISO 9126 does not provide a consensual set of measures for estimating maintainability on the basis of a system's source code. On the other hand, the maintainability index has been proposed to calculate a single number that expresses the maintainability of a system. In this paper, we discuss several problems with the MI, and we identify a number of requirements to be fulfilled by a maintainability model to be usable in practice. We sketch a new maintainability model that alleviates most of these problems, and we discuss our experiences with using such as system for IT management consultancy activities.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4335232,no
Paceline: Improving Single-Thread Performance in Nanoscale CMPs through Core Overclocking,2007,"Under current worst-case design practices, manufacturers specify conservative values for processor frequencies in order to guarantee correctness. To recover some of the lost performance and improve single-thread performance, this paper presents the Paceline leader-checker microarchitecture. In Paceline, a leader core runs the thread at higher-than-rated frequency, while passing execution hints and prefetches to a safely-clocked checker core in the same chip multiprocessor. The checker redundantly executes the thread faster than without the leader, while checking the results to guarantee correctness. Leader and checker cores periodically swap functionality. The result is that the thread improves performance substantially without significantly increasing the power density or the hardware design complexity of the chip. By overclocking the leader by 30%, we estimate that Paceline improves SPECint and SPECfp performance by a geometric mean of 21% and 9%, respectively. Moreover, Paceline also provides tolerance to transient faults such as soft errors.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4336213,no
CacheScouts: Fine-Grain Monitoring of Shared Caches in CMP Platforms,2007,"As multi-core architectures flourish in the marketplace, multi-application workload scenarios (such as server consolidation) are growing rapidly. When running multiple applications simultaneously on a platform, it has been shown that contention for shared platform resources such as last-level cache can severely degrade performance and quality of service (QoS). But today's platforms do not have the capability to monitor shared cache usage accurately and disambiguate its effects on the performance behavior of each individual application. In this paper, we investigate low-overhead mechanisms for fine-grain monitoring of the use of shared cache resources along three vectors: (a) occupancy - how much space is being used and by whom, (b) interference - how much contention is present and who is being affected and (c) sharing - how are threads cooperating. We propose the CacheScouts monitoring architecture consisting of novel tagging (software-guided monitoring IDs), and sampling mechanisms (set sampling) to achieve shared cache monitoring on per application basis at low overhead (<0.1%) and with very little loss of accuracy (<5%). We also present case studies to show how CacheScouts can be used by operating systems (OS) and virtual machine monitors (VMMs) for (a) characterizing execution profiles, (b) optimizing scheduling for performance management, (c) providing QoS and (d) metering for chargeback.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4336224,no
JudoSTM: A Dynamic Binary-Rewriting Approach to Software Transactional Memory,2007,"With the advent of chip-multiprocessors, we are faced with the challenge of parallelizing performance-critical software. Transactional memory (TM) has emerged as a promising programming model allowing programmers to focus on parallelism rather than maintaining correctness and avoiding deadlock. Many implementations of hardware, software, and hybrid support for TM have been proposed; of these, software-only implementations (STMs) are especially compelling since they can be used with current commodity hardware. However, in addition to higher overheads, many existing STM systems are limited to either managed languages or intrusive APIs. Furthermore, transactions in STMs cannot normally contain calls to unobservable code such as shared libraries or system calls. In this paper we present JudoSTM, a novel dynamic binary-rewriting approach to implementing STM that supports C and C++ code. Furthermore, by using value-based conflict detection, JudoSTM additionally supports the transactional execution of both (i) irreversible system calls and (ii) library functions that may contain locks. We significantly lower overhead through several novel optimizations that improve the quality of rewritten code and reduce the cost of conflict detection and buffering. We show that our approach performs comparably to Rochester's RSTM library-based implementation- demonstrating that a dynamic binary-rewriting approach to implementing STM is an interesting alternative.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4336226,no
A Study of a Transactional Parallel Routing Algorithm,2007,"Transactional memory proposes an alternative synchronization primitive to traditional locks. Its promise is to simplify the software development of multi-threaded applications while at the same time delivering the performance of parallel applications using (complex and error prone) fine grain locking. This study reports our experience implementing a realistic application using transactional memory (TM). The application is Lee's routing algorithm and was selected for its abundance of parallelism but difficulty of expressing it with locks. Each route between a source and a destination point in a grid can be considered a unit of parallelism. Starting from this simple approach, we evaluate the exploitable parallelism of a transactional parallel implementation and explore how it can be adapted to deliver better performance. The adaptations do not introduce locks nor alter the essence of the implemented algorithm, but deliver up to 20 times more parallelism. The adaptations are derived from understanding the application itself and TM. The evaluation simulates an abstracted TM system and, thus, the results are independent of specific software or hardware TM implemented, and describe properties of the application.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4336228,no
Subjective Evaluation of Techniques for Proper Name Pronunciation,2007,"Automatic pronunciation of unknown words of English is a hard problem of great importance in speech technology. Proper names constitute an especially difficult class of words to pronounce because of their variable origin and uncertain degree of assimilation of foreign names to the conventions of the local speech community. In this paper, we compare four different methods of proper name pronunciation for English text-to-speech (TTS) synthesis. The first (intended to be used as the primary strategy in a practical TTS system) uses a set of manually supplied pronunciations, referred to as the ldquodictionaryrdquo pronunciations. The remainder are pronunciations obtained from three different data-driven approaches (intended as candidates for the back-up strategy in a real system) which use the dictionary of ldquoknownrdquo proper names to infer pronunciations for unknown names. These are: pronunciation by analogy (PbA), a decision tree method (CART), and a table look-up method (TLU). To assess the acceptability of the pronunciations to potential users of a TTS system, subjective evaluation was carried out, in which 24 listeners rated 1200 synthesized pronunciations of 600 names by the four methods using a five-point (opinion score) scale. From over 50 000 proper names and their pronunciations, 150 so-called one-of-a-kind pronunciations were selected for each of the four methods (600 in total). A one-of-a-kind pronunciation is one for which one of the four methods disagrees with the other three methods, which agree among themselves. Listener opinions on one-of-a-kind pronunciations are argued to be a good measure of the overall quality of a particular method. For each one-of-a-kind pronunciation, there is a corresponding so-called rest pronunciation (another 600 in total), on which the remaining three competitor methods agree, for which listener opinions are taken to be indicative of the general quality of the competition. Nonparametric tests of significance of mean opin- on scores show that the dictionary pronunciations are rated superior to the automatically inferred pronunciations with little difference between the data-driven methods for the one-of-a-kind pronunciations, but for the rest pronunciations there is suggestive evidence that PbA is superior to both CART and TLU, which perform at approximately the same level.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4337635,no
Software-Based Failure Detection and Recovery in Programmable Network Interfaces,2007,"Emerging network technologies have complex network interfaces that have renewed concerns about network reliability. In this paper, we present an effective low-overhead fault tolerance technique to recover from network interface failures. Failure detection is based on a software watchdog timer that detects network processor hangs and a self-testing scheme that detects interface failures other than processor hangs. The proposed self-testing scheme achieves failure detection by periodically directing the control flow to go through only active software modules in order to detect errors that affect instructions in the local memory of the network interface. Our failure recovery is achieved by restoring the state of the network interface using a small backup copy containing just the right amount of information required for complete recovery. The paper shows how this technique can be made to minimize the performance impact to the host system and be completely transparent to the user.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4339198,no
Optimization of Variability in Software Product Lines,2007,"The widespread use of the product line approach allows companies to realize significant improvements in time-to- market, cost, productivity, and quality. However, a fundamental problem in software product line engineering is that a product line of industrial size can easily incorporate several thousand variable features. The complexity caused by this amount of variability makes variability management and product derivation tasks extremely difficult. To address this problem, we present a new method to optimize the variability provided in a software product line. Our method constructs a visualization that provides a classification of the usage of variable features in real products derived from the product line. We show how this classification can be used to derive restructuring strategies for simplifying the variability. The effectiveness of our work is demonstrated by presenting a case study of optimizing the variability in a large industrial software product line.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4339264,no
Prediction of Self-Similar Traffic and its Application in Network Bandwidth Allocation,2007,"In this paper, traffic prediction models based on chaos theory are studied and compared with FARIMA (fractional autoregressive integrated moving average) predictors by means of the adopted measurements of predictability. The traffic prediction results are applied in the bandwidth allocation of a mesh network, and the OPNET simulation platform is developed in order to compare their effects. The adopted predictability measurements are inadequate because although the chaotic predictor based on the Lyapunov exponent with worse values of the measurements can timely predict the burstiness of self- similar traffic, the FARIMA predictor forecasts the burstiness with a time-delay. The DAMA (dynamic assignment multiaccess) bandwidth allocation strategy combined with the chaotic predictor can provide better QoS performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4340270,no
Adaptive Dual-Cross-Diamond-Hexagon Search Algorithm for Fast Block Motion Estimation,2007,"Motion estimation (ME) is the cardinal technique of video coding standard, since it could significantly affect the output quality of a coding system. Based on the analyses of the limitation on the SAD matching and the allocation for encoding bits, this paper proposes the adaptive dual-cross-diamond-hexagon algorithm according to three main factors affecting the efficiency of ME, namely, the prediction of initial search center, matching criteria and searching strategy. Experimental results show that the proposed algorithm performs faster than MVFAST and PMVFAST, whereas similar prediction quality is still maintained.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4340475,no
Usability Evaluation of B2C Web Site,2007,"Web site usability is a critical metric for assessing the quality of the B2C Web site. A measure of usability must not only provide a rating for a specific Web site, but also should it illuminate the specific strengths and weaknesses about site design. In this paper, the usability and usability evaluation of B2C Web site are described. A comprehensive set of usability guidelines developed by Microsoft (MUG) is revised and utilized. The index and sub index comprising these guidelines are present firstly. The weights of each indexes and sub indexes are decided by AHP(Analytical Hierarchy Process). Base on the investigation data, a mathematic arithmetic is proposed to calculate the grade of each B2C Web site. The illustrated example shows that the evaluation approach of this paper is very effective.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4340724,no
Functional Test-Case Generation by a Control Transaction Graph for TLM Verification,2007,"Transaction level modeling allows exploring several SoC design architectures leading to better performance and easier verification of the final product. Test cases play an important role in determining the quality of a design. Inadequate test-cases may cause bugs to remain after verification. Although TLM expedites the verification of a hardware design, the problem of having high coverage test cases remains unsettled at this level of abstraction. In this paper, first, in order to generate test-cases for a TL model we present a Control-Transaction Graph (CTG) describing the behavior of a TL Model. A Control Graph is a control flow graph of a module in the design and Transactions represent the interactions such as synchronization between the modules. Second, we define dependent paths (DePaths) on the CTG as test-cases for a transaction level model. The generated DePaths can find some communication errors in simulation and detect unreachable statements concerning interactions. We also give coverage metrics for a TL model to measure the quality of the generated test-cases. Finally, we apply our method on the SystemC model of AMBA-AHB bus as a case study and generate test-cases based on the CTG of this model.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4341464,no
Functional Verification of RTL Designs driven by Mutation Testing metrics,2007,"The level of confidence in a VHDL description directly depends on the quality of its verification. This quality can be evaluated by mutation-based test, but the improvement of this quality requires tremendous efforts. In this paper, we propose a new approach that both qualifies and improves the functional verification process. First, we qualify test cases thanks to the mutation testing metrics: faults are injected in the design under verification (DUV) (making DUV's mutants) to check the capacity of test cases to detect theses mutants. Then, a heuristic is used to automatically improve IPs validation data. Experimental results obtained on RTL descriptions from ITC'99 benchmark show how efficient is our approach.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4341472,no
An Object Oriented Complexity Metric Based on Cognitive Weights,2007,"Complexity in general is defined as ""the degree to which a system or component has a design or implementation that is difficult to understand and verify "". Complexity metrics are used to predict critical information about reliability and maintainability of software systems. Object oriented software development requires a different approach to software metrics. In this paper, an attempt has been made to propose a metric for an object oriented code, which calculates the complexity of a class at method level. The proposed measure considers the internal architecture of the class, subclass, and member functions, while other proposed metrics for object oriented programming do not. An attempt has also been made to evaluate and validate the proposed measure in terms of Weyuker's properties and against the principles of measurement theory. It has been found that seven of nine Weyuker's properties have been satisfied by the proposed measure. It also satisfies most of the parameters required by the measurement theory perspective, hence establishes as a well-structured one.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4341883,no
First International Symposium on Empirical Software Engineering and Measurement-Title,2007,The following topics are dealt with: empirical software engineering; software measurement; experience management; software testing; software validation; software effort estimation; software quality; software defect prediction; software metrics; software data mining; software architecture.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343711,no
A Critical Analysis of Empirical Research in Software Testing,2007,"In the foreseeable future, software testing will remain one of the best tools we have at our disposal to ensure software dependability. Empirical studies are crucial to software testing research in order to compare and improve software testing techniques and practices. In fact, there is no other way to assess the cost-effectiveness of testing techniques, since all of them are, to various extents, based on heuristics and simplifying assumptions. However, when empirically studying the cost and fault- detection rates of a testing technique, a number of validity issues arise. Further, there are many ways in which empirical studies can be performed, ranging from simulations to controlled experiments with human subjects. What are the strengths and drawbacks of the various approaches? What is the best option under which circumstances? This paper presents a critical analysis of empirical research in software testing and will attempt to highlight and clarify the issues above in a structured and practical manner.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343726,no
"Assessing, Comparing, and Combining Statechart- based testing and Structural testing: An Experiment",2007,"Although models have been proven to be helpful in a number of software engineering activities there is still significant resistance to model-driven development. This paper investigates one specific aspect of this larger problem. It addresses the impact of using statecharts for testing class clusters that exhibit a state-dependent behavior. More precisely, it reports on a controlled experiment that investigates their impact on testing fault-detection effectiveness. Code-based, structural testing is compared to statechart-based testing and their combination is investigated to determine whether they are complementary. Results show that there is no significant difference between the fault detection effectiveness of the two test strategies but that they are significantly more effective when combined. This implies that a cost-effective strategy would specify statechart-based test cases early on, execute them once the source code is available, and then complete them with test cases based on code coverage analysis.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343731,no
Test Inspected Unit or Inspect Unit Tested Code?,2007,"Code inspection and unit testing are two popular fault- detecting techniques at unit level. Organizations where inspections are done generally supplement it with unit testing, as both are complementary. A natural question is the order in which the two techniques should be exercised as this may impact the overall effectiveness and efficiency of the verification process. In this paper, we present a controlled experiment comparing the two execution-orders, namely, code inspection followed by unit testing (CI-UT) and unit testing followed by code inspection (UT-CI), performed by a group of fresh software engineers in a company. The subjects inspected program-units by traversing a set of usage scenarios and applied unit testing by writing JUnit tests for the same. Our results showed that unit testing can be more effective, as well as more efficient, if applied after code inspection whereas the later is unaffected of the execution- order. Overall results suggest that sequence CI-UT performs better than UT-CI in time-constrained situations.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343732,no
Defect Detection Efficiency: Test Case Based vs. Exploratory Testing,2007,"This paper presents a controlled experiment comparing the defect detection efficiency of exploratory testing (ET) and test case based testing (TCT). While traditional testing literature emphasizes test cases, ET stresses the individual tester's skills during test execution and does not rely upon predesigned test cases. In the experiment, 79 advanced software engineering students performed manual functional testing on an open-source application with actual and seeded defects. Each student participated in two 90-minute controlled sessions, using ET in one and TCT in the other. We found no significant differences in defect detection efficiency between TCT and ET. The distributions of detected defects did not differ significantly regarding technical type, detection difficulty, or severity. However, TCT produced significantly more false defect reports than ET. Surprisingly, our results show no benefit of using predesigned test cases in terms of defect detection efficiency, emphasizing the need for further studies of manual testing.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343733,no
Comparing Model Generated with Expert Generated IV&V Activity Plans,2007,"An IV&V activity plan describes what assurance activities to perform, where to do them, when, and to what extent. Meaningful justification for an IV&V budget and evidence that activities performed actually provide high assurance has been difficult to provide from plans created (generally ad hoc) by experts. JAXA now uses the ""strategic IV&V planning and cost model"" to addresses these issues and complement expert planning activities. This research presents a grounded empirical study that compares plans generated by the strategic model to those created by experts on several past IV&V projects. Through this research, we found that the model generated plan typically is a superset of the experts ' plan. We found that experts tended to follow the most cost-effective route but had a bias in their particular activity selections. Ultimately we found increased confidence in both expert and model based planning and now have new tools for assessing and improving them.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343734,no
"Filtering, Robust Filtering, Polishing: Techniques for Addressing Quality in Software Data",2007,"Data quality is an important aspect of empirical analysis. This paper compares three noise handling methods to assess the benefit of identifying and either filtering or editing problematic instances. We compare a 'do nothing' strategy with (i) filtering, (ii) robust filtering and (Hi) filtering followed by polishing. A problem is that it is not possible to determine whether an instance contains noise unless it has implausible values. Since we cannot determine the true overall noise level we use implausible val.ues as a proxy measure. In addition to the ability to identify implausible values, we use another proxy measure, the ability to fit a classification tree to the data. The interpretation is low misclassification rates imply low noise levels. We found that all three of our data quality techniques improve upon the 'do nothing' strategy, also that the filtering and polishing was the most effective technique for dealing with noise since we eliminated the fewest data and had the lowest misclassification rates. Unfortunately the polishing process introduces new implausible values. We believe consideration of data quality is an important aspect of empirical software engineering. We have shown that for one large and complex real world data set automated techniques can help isolate noisy instances and potentially polish the values to produce better quality data for the analyst. However this work is at a preliminary stage and it assumes that the proxy measures of lity are appropriate.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343737,no
An Estimation Model for Test Execution Effort,2007,"Testing is an important activity to ensure software quality. Big organizations can have several development teams with their products being tested by overloaded test teams. In such situations, test team managers must be able to properly plan their schedules and resources. Also, estimates for the required test execution effort can be an additional criterion for test selection, since effort may be restrictive in practice. Nevertheless, this information is usually not available for test cases never executed before. This paper proposes an estimation model for test execution effort based on the test specifications. For that, we define and validate a measure of size and execution complexity of test cases. This measure is obtained from test specifications written in a controlled natural language. We evaluated the model through an empirical study on the mobile application domain, which results suggested an accuracy improvement when compared with estimations based only on historical test productivity.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343738,no
Usability Evaluation Based on Web Design Perspectives,2007,"Given the growth in the number and size of Web Applications worldwide, Web quality assurance, and more specifically Web usability have become key success factors. Therefore, this work proposes a usability evaluation technique based on the combination of Web design perspectives adapted from existing literature, and heuristics. This new technique is assessed using a controlled experiment aimed at measuring the efficiency and effectiveness of our technique, in comparison to Nielsen's heuristic evaluation. Results indicated that our technique was significantly more effective than and as efficient as Nielsen's heuristic evaluation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343742,no
Toward Reducing Fault Fix Time: Understanding Developer Behavior for the Design of Automated Fault Detection Tools,2007,"The longer a fault remains in the code from the time it was injected, the more time it will take to fix the fault. Increasingly, automated fault detection (AFD) tools are providing developers with prompt feedback on recently-introduced faults to reduce fault fix time. If however, the frequency and content of this feedback does not match the developer's goals and/or workflow, the developer may ignore the information. We conducted a controlled study with 18 developers to explore what factors are used by developers to decide whether or not to address a fault when notified of the error. The findings of our study lead to several conjectures about the design of AFD tools to effectively notify developers of faults in the coding phase. The AFD tools should present fault information that is relevant to the primary programming task with accurate and precise descriptions. The fault severity and the specific timing of fault notification should be customizable. Finally, the AFD tool must be accurate and reliable to build trust with the developer.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343745,no
Evaluating the Impact of Adaptive Maintenance Process on Open Source Software Quality,2007,"The paper focuses on measuring and assessing the relation of adaptive maintenance process and quality of open source software (OSS). A framework for assessing adaptive maintenance process is proposed and applied. The framework consists of six sub- processes. Five OSSs with considerable number of releases have been studied empirically. Their main evolutionary and quality characteristics have been measured. The main results of the study are the following:. 1) Software maintainability is affected mostly by the activities of the 'analysis' maintenance sub-process. 2) Software testability is affected by the activities of all maintenance sub-processes. 3) Software reliability is affected mostly by the activities of the 'design' and 'delivery' maintenance sub- processes. 4) Software complexity is affected mostly by the activities of the 'problem identification', design', 'implementation' and 'test' sub-processes. 5) Software flexibility is affected mostly by the activities of the 'delivery' sub-process.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343746,no
The Effects of Over and Under Sampling on Fault-prone Module Detection,2007,"The goal of this paper is to improve the prediction performance of fault-prone module prediction models (fault-proneness models) by employing over/under sampling methods, which are preprocessing procedures for a fit dataset. The sampling methods are expected to improve prediction performance when the fit dataset is unbalanced, i.e. there exists a large difference between the number of fault-prone modules and not-fault-prone modules. So far, there has been no research reporting the effects of applying sampling methods to fault-proneness models. In this paper, we experimentally evaluated the effects of four sampling methods (random over sampling, synthetic minority over sampling, random under sampling and one-sided selection) applied to four fault-proneness models (linear discriminant analysis, logistic regression analysis, neural network and classification tree) by using two module sets of industry legacy software. All four sampling methods improved the prediction performance of the linear and logistic models, while neural network and classification tree models did not benefit from the sampling methods. The improvements of Fl-values in linear and logistic models were 0.078 at minimum, 0.224 at maximum and 0.121 at the mean.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343747,no
Generalizing fault contents from a few classes,2007,"The challenges in fault prediction today are to get a prediction as early as possible, at as low a cost as possible, needing as little data as possible and preferably in such a language that your average developer can understand where it came from. This paper presents a fault sampling method where a summary of a few, easily available metrics is used together with the results of a few sampled classes to generalize the fault content to an entire system. The method is tested on a large software system written in Java, that currently consists of around 2000 classes and 300,000 lines of code. The evaluation shows that the fault generalization method is good at predicting fault-prone clusters and that it is possible to generalize the values of a few representative classes.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343748,no
Fine-Grained Software Metrics in Practice,2007,"Modularity is one of the key features of the Object- Oriented (00) paradigm. Low coupling and high cohesion help to achieve good modularity. Inheritance is one of the core concepts of the 00 paradigm which facilitates modularity. Previous research has shown that the use of the friend construct as a coupling mechanism in C+ + software is extensive. However, measures of the friend construct are scarse in comparison with measures of inheritance. In addition, these existing measures are coarse-grained, in spite of the widespread use of the friend mechanism. In this paper, a set of software metrics are proposed that measure the actual use of the friend construct, inheritance and other forms of coupling. These metrics are based on the interactions for which each coupling mechanism is necessary and sufficient. Previous work only considered the declaration of a relationship between classes. The software metrics introduced are empirically assessed using the LEDA software system. Our results indicate that the friend mechanism is used to a very limited extent to access hidden methods in classes. However, access to hidden attributes is more common.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343757,no
Evaluating Software Project Control Centers in Industrial Environments,2007,"Many software development organizations still lack support for detecting and reacting to critical project states in order to achieve planned goals. One means to institutionalize project control, systematic quality assurance, and management support on the basis of measurement and explicit models is the establishment of so-called software project control centers. However, there is only little experience reported in the literature with respect to setting up and applying such control centers in industrial environments. One possible reason is the lack of appropriate evaluation instruments (such as validated questionnaires and appropriate analysis procedures). Therefore, we developed an initial measurement instrument to systematically collect experience with respect to the deployment and use of control centers. Our main research goal was to develop and evaluate the measurement instrument. The instrument is based on the technology acceptance model (TAM) and customized to project controlling. This article illustrates the application and evaluation of this measurement instrument in the context of industrial case studies and provides lessons learned for further improvement. In addition, related work and conclusions for future work are given.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343759,no
Fault-Prone Filtering: Detection of Fault-Prone Modules Using Spam Filtering Technique,2007,"The fault-prone module detection in source code is of importance for assurance of software quality. Most of previous conventional fault-prone detection approaches have been based on using software metrics. Such approaches, however, have difficulties in collecting the metrics and constructing mathematical models based on the metrics. In order to mitigate such difficulties, we propose a novel approach for detecting fault-prone modules using a spam filtering technique. Because of the increase of needs for spam e-mail detection, the spam filtering technique has been progressed as a convenient and effective technique for text mining. In our approach, fault-prone modules are detected in a way that the source code modules are considered as text files and are applied to the spam filter directly. In order to show the usefulness of our approach, we conducted an experiment using source code repository of a Java based open source development. The result of experiment shows that our approach can classify more than 70% of software modules correctly.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343765,no
Characterizing Software Architecture Changes: An Initial Study,2007,"With today's ever increasing demands on software, developers must produce software that can be changed without the risk of degrading the software architecture. Degraded software architecture is problematic because it makes the system more prone to defects and increases the cost of making future changes. The effects of making changes to software can be difficult to measure. One way to address software changes is to characterize their causes and effects. This paper introduces an initial architecture change characterization scheme created to assist developers in measuring the impact of a change on the architecture of the system. It also presents an initial study conducted to gain insight into the validity of the scheme. The results of this study indicated a favorable view of the viability of the scheme by the subjects, and the scheme increased the ability of novice developers to assess and adequately estimate change effort.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343769,no
An Approach to Global Sensitivity Analysis: FAST on COCOMO,2007,"There are various models in software engineering that are used to predict quality-related aspects of the process or artefacts. The use of these models involves elaborate data collection in order to estimate the input parameters. Hence, an interesting question is which of these input factors are most important. More specifically, which factors need to be estimated best and which might be removed from the model? This paper describes an approach based on global sensitivity analysis to answer these questions and shows its applicability in a case study on the COCOMO application at NASA.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343772,no
An Approach to Outlier Detection of Software Measurement Data using the K-means Clustering Method,2007,"The quality of software measurement data affects the accuracy of project manager's decision making using estimation or prediction models and the understanding of real project status. During the software measurement implementation, the outlier which reduces the data quality is collected, however its detection is not easy. To cope with this problem, we propose an approach to outlier detection of software measurement data using the k-means clustering method in this work.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343773,no
A cost effectiveness indicator for software development,2007,"Product quality, development productivity, and staffing needs are main cost drivers in software development. The paper proposes a cost-effectiveness indicator that combines these drivers using an economic criterion.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343774,no
Estimating the Quality of Widely Used Software Products Using Software Reliability Growth Modeling: Case Study of an IBM Federated Database Project,2007,"Software producers can better manage the quality of their deployed software products using estimates of quality. Current best practices for making estimates are to use software reliability growth modeling (SRGM), which assumes that testing environments approximate deployment environments. This important assumption does not hold for widely used software products, which are operated in a wide variety of configurations under many different usage scenarios. However, the literature contains little empirical data on the impact of this violation of assumptions on the accuracy and the usefulness of predictions. In this paper, we report results and experiences using SRGM on an IBM federated database project. We examine defect data from 3 releases spanning approximately 9 years. We find SRGM to be of limited use to the project: absolute relative errors are at least 34%, and predictions are, at times, implausible. We discuss alternative approaches for estimating quality of widely used software products.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343776,no
Investigating Test Teams' Defect Detection in Function test,2007,"In a case study, the defect detection for functional test teams is investigated. In the study it is shown that the test teams not only discover defects in the features under test that they are responsible for, but also defects in interacting components, belonging to other test teams' features. The paper presents the metrics collected and the results as such from the study, which gives insights into a complex development environment and highlights the need for coordination between test teams in function test.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343778,no
Comparison of Outlier Detection Methods in Fault-proneness Models,2007,"In this paper, we experimentally evaluated the effect of outlier detection methods to improve the prediction performance of fault-proneness models. Detected outliers were removed from a fit dataset before building a model. In the experiment, we compared three outlier detection methods (Mahalanobis outlier analysis (MOA), local outlier factor method (LOFM) and rule based modeling (RBM)) each applied to three well-known fault-proneness models (linear discriminant analysis (LDA), logistic regression analysis (LRA) and classification tree (CT)). As a result, MOA and RBM improved Fl-values of all models (0.04 at minimum, 0.17 at maximum and 0.10 at mean) while improvements by LOFM were relatively small (-0.01 at minimum, 0.04 at maximum and 0.01 at mean).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343779,no
Assessing the Quality Impact of Design Inspections,2007,"Inspections are widely used and studies have found them to be effective in uncovering defects. However, there is less data available regarding the impact of inspections on different defect types and almost no data quantifying the link between inspections and desired end product qualities. This paper addresses this issue by investigating whether design inspection checklists can be tailored so as to effectively target certain defect types without impairing the overall defect detection rate. The results show that the design inspection approach used here does uncover useful design quality issues and that the checklists can be effectively tailored for some types of defects.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343782,no
Fault-Driven Re-Scheduling For Improving System-level Fault Resilience,2007,"The productivity of HPC system is determined not only by their performance, but also by their reliability. The conventional method to limit the impact of failures is checkpointing. However, existing research shows that such a reactive fault tolerance approach can only improve system productivity marginally. Leveraging the recent progress made in the field of failure prediction, we propose fault-driven rescheduling (FARS) to improve system resilience to failures, and investigate the feasibility and effectiveness of utilizing failure prediction to dynamically adjust the placement of active jobs (e.g. running jobs) in response to failure prediction. In particular, a rescheduling algorithm is designed to enable effective job adjustment by evaluating performance impact of potential failures and rescheduling on user jobs. The proposed FARS complements existing research on fault-aware scheduling by allowing user jobs to avoid imminent failures at runtime. We evaluate FARS by using actual workloads and failure events collected from production HPC systems. Our preliminary results show the potential of FARS on improving system resilience to failures.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343846,no
Testing conformance on Stochastic Stream X-Machines,2007,"Stream X-machines have been used to specify real systems requiring to represent complex data structures. One of the advantages of using stream X-machines to specify a system is that it is possible to produce a test set that, under certain conditions, detects all the faults of an implementation. In this paper we present a formal framework to test temporal behaviors in systems where temporal aspects are critical. Temporal requirements are expressed by means of random variables and affect the duration of actions. Implementation relations are presented as well as a method to determine the conformance of an implementation with respect to a specification by applying a test set.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343939,no
Hardness for Explicit State Software Model Checking Benchmarks,2007,"Directed model checking algorithms focus computation resources in the error-prone areas of concurrent systems. The algorithms depend on some empirical analysis to report their performance gains. Recent work characterizes the hardness of models used in the analysis as an estimated number of paths in the model that contain an error. This hardness metric is computed using a stateless random walk. We show that this is not a good hardness metric because models labeled hard with a stateless random walk metric have easily discoverable errors with a stateful randomized search. We present an analysis which shows that a hardness metric based on a stateful randomized search is a tighter bound for hardness in models used to benchmark explicit state directed model checking techniques. Furthermore, we convert easy models into hard models as measured by our new metric by pushing the errors deeper in the system and manipulating the number of threads that actually manifest an error.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343941,no
State-based Testing is Functional Testing,2007,"Empirical studies report unsatisfactory fault detection of state-based methods in class testing and advocate the use of functional methods to complement state-based testing. In this paper, we take the view that the modest fault detection of state-based class testing reported in the literature is actually due to the inappropriate state diagram used. We show that functional testing of a class can be reduced to state-based testing, provided that the right state model is produced. We present a strategy for constructing a state diagram for a class method, based on a domain partition derived through functional techniques. We also describe a method for deriving test sequences from the resulting state diagrams, essentially a variant of the W-method. The paper also reports results from an experimental evaluation of the proposed approach, based on mutants generated by Mu- Java.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4344099,no
On the Accuracy of Spectrum-based Fault Localization,2007,"Spectrum-based fault localization shortens the test- diagnose-repair cycle by reducing the debugging effort. As a light-weight automated diagnosis technique it can easily be integrated with existing testing schemes. However, as no model of the system is taken into account, its diagnostic accuracy is inherently limited. Using the Siemens Set benchmark, we investigate this diagnostic accuracy as a function of several parameters (such as quality and quantity of the program spectra collected during the execution of the system), some of which directly relate to test design. Our results indicate that the superior performance of a particular similarity coefficient, used to analyze the program spectra, is largely independent of test design. Furthermore, near- optimal diagnostic accuracy (exonerating about 80% of the blocks of code on average) is already obtained for low-quality error observations and limited numbers of test cases. The influence of the number of test cases is of primary importance for continuous (embedded) processing applications, where only limited observation horizons can be maintained.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4344104,no
Software Fault Prediction using Language Processing,2007,"Accurate prediction of faulty modules reduces the cost of software development and evolution. Two case studies with a language-processing based fault prediction measure are presented. The measure, refereed to as a QALP score, makes use of techniques from information retrieval to judge software quality. The QALP score has been shown to correlate with human judgements of software quality. The two case studies consider the measure's application to fault prediction using two programs (one open source, one proprietary). Linear mixed-effects regression models are used to identify relationships between defects and QALP score. Results, while complex, show that little correlation exists in the first case study, while statistically significant correlations exists in the second. In this second study the QALP score is helpful in predicting faults in modules (files) with its usefulness growing as module size increases.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4344105,no
Design of Multi-Function Monitoring Integrated Device in Power System,2007,"With high development of electronics and computer technology, a new real-time system of multi-function monitoring integrated device based on industrial control computer automatic monitoring system with multi-sensor is designed in this paper. This system presents a scheme for power quality monitoring, electric parameters and data record, fault line diagnosis and selection with the concerns of real-time and complex algorithms application. The design of hardware system and the software design are introduced in detail. In a sense, we use a kind of a new technology spanning several disciplines, information blending, into the design in this paper. Further, a new algorithm using the calculation of fractal number to judge the failure line is presented for power grid monitor detection and fault line selection. By debugging and practical running, the design is proven to be accurate and practical. Comparing with the former designs, this design has better real-time performance, smaller volume and lower cost.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4348032,no
A Study on Performance Measurement of a Plastic Packaging Organization's Manufacturing System by AHP Modeling,2007,"By the effect of globalization, products, services, capital, technology, and people began to circulate more freely in the world. As a conclusion, in order to achieve and gain an advantage against competitors, manufacturing firms had to adopt themselves to changing conditions and evaluate their critical performance criteria. In this study, the aim is to determine general performance criteria and their characteristics and classifications from previous studies and evaluate performance criteria for a plastic packaging organization by utilizing analytic hierarchy process (AHP) modeling. A specific manufacturing organization, operating in the Turkish plastic packaging sector has been selected and the manufacturing performance criteria have been determined for that specific organization. Finally, the selected criteria have been assessed according to their relative importance by utilizing AHP approach and expert choice (EC) software program. As a result of this study, operating managers chose cost, quality, customer satisfaction and time factors as criteria for this organization. As the findings of the study indicate, the manufacturing organization operating in the plastic packaging sector, overviews its operations and measures its manufacturing performance basically on those four criteria and their sub criteria. Finally, relative importance of those main measures and their sub criteria are determined in consideration to plastic packaging sector.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4349449,no
Study on Software of VXIbus Boundary Scan Test Generation,2007,"The goal of this paper is to develop a set of software of boundary-scan test (BST) generation using some test generation algorithms and test data. In order to get the test data quickly and effectively, a new innovative method of establishing test project description (TPD) file is presented. During the testing of two different boundary-scan circuit boards, all faults can be detected, indicating that the expected design objective is achieved.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4350620,no
Predict Malfunction-Prone Modules for Embedded System Using Software Metrics,2007,"High software dependability is significant for many software systems, especially in embedded system. Dependability is usually measured from the user's viewpoint in terms of time between failures, according to an operational profile. A software malfunction is defined as a defect in an executable software product that may cause a failure. Thus, malfunctions are attributed to the software modules that cause failures. Developers tend to focus on malfunctions, because they are closely related to the amount of rework necessary to prevent future failures. This paper defined a software module malfunction-prone by class cohesion metrics when there is a high risk that malfunctions will be discovered during operations. Also proposed a novel cohesion measure method for derived classes in embedded system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4350736,no
Numerical Simulation of the Temperature Distribution in SiC Sublimation Growth System,2007,"Although serious attempts have been developed silicon carbide bulk crystal growth technology to an industrial process during the last years, the quality of crystal remains deficient. One of the major problems is that the thermal field of SiC growth systems is not fully understood. Numerical simulation is considered as an important tool for the investigation of the thermal field distribution inside the growth crucible system involved with SiC bulk growth. We employ the finite-element software package ANSYS to provide additional information on the thermal field distribution. A two-dimensional model has been developed to simulate the axisymmetric growth system consist of a cylindrical susceptor (graphite crucible), a graphite felt insulation, and a copper inductive coil. The modeling field is coupled electromagnetic heating and thermal transfer. The induced magnetic field is used to predict heat generation due to magnetic induction. Conduction, convection and radiation in various components of the system are accounted for the heat transfer ways. The thermal field in SiC sublimation growth system was provided.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4350761,no
Design and Implementation of Testing Network for Power Line Fault Detection Based on nRF905,2007,"The design scheme of automatic power line fault detection system based on wireless sensor network is proposed. The hardware architecture and software design based on nRF905 are also described in detail. The experimental results show that the system has the advantages of simple installation, high stability and accuracy.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4350969,no
Research and Application of ATML for Aircraft Electric Power Diagnosis and Prognostic System,2007,"The technology of integrated health management (IHVM) is an important approach to improve the security, reliability and maintainability of airplanes and spacecrafts, and to reduce their cost significantly. Taken aircraft electric power system into consideration, it is one of the important equipment that provides electric energy for all electro-equipments in airplane. Its working state is a crucial to ensure airplane normal and secure flight, so the research of condition detection, fault diagnosis and prognosis is a great concern in the field of IHVM system. Aiming at interoperability and complexity of data exchanging for aircraft electric power diagnosis and prognostic system, a method of using ATML (automatic test markup language) based on XML (extensible markup language) is proposed in order to describe the information of devices and conditions for fault diagnosis and prognostic in the system. First, based on reference frame of integrated health management system, a distributed multi-level fault diagnosis and prognosis system structure that exchange data through ATML is designed. Secondly, condition monitor is a foundation of fault diagnosis and prognosis, and a schema of test information for aircraft electric power parameters is designed by using ATML after data of crucial fault is analyzed. The result shows that the method can implement seamless interaction among test, diagnosis and prognosis. At same time, sharing and transplant of test information and diagnostic knowledge is improved and the process of fault diagnosis is accelerated.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4350993,no
Performance Analysis of CORBA Replication Models,2007,"Active and passive replication models constitute an effective way to achieve the availability objectives of distributed real-time (DRE) systems. These two models have different impacts on the application performance. Although these models have been commonly used in practical systems, a systematic quantitative evaluation of their influence on the application performance has not been conducted. In this paper we describe a methodology to analyze the application performance in the presence of active and passive replication models. For each one of these models, we obtain an analytical expression for the application response time in terms of the model parameters. Based on these analytical expressions, we derive the conditions under which one replication model has better performance over the other. Our results indicate that the superiority of one replication model over the other is governed not only by the model parameters but also by the application characteristics. We illustrate the value of the analytical expressions to assess the influence of the parameters of each model on the application response time and for a comparative analysis of the two models.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4351394,no
A Web based System to Calculate Quality Metrics for Digital Passport Photographs,2007,"Quality measurement of digital passport photographs is a complex problem. The first problem to solve is to establish the quality metrics, quality scales and the level of acceptance for a certain quality. The second problem to solve is to offer an integrated system which covers the principles of software engineering plus a user-friendly interface. This paper introduces the solution for both problems. The method to calculate different and novel quality metrics based on a conventional estimation and a quality metric based on nonconformance of quality requirements for digital passport photographs. Conformance clauses of quality requirements were obtained from the specifications of the machine readable-travel documents (MRTD) document of the International-Civil Aviation Organization (ICAO) ICAO/MRTD and from the document of the International Standardization Organization (ISO). Series of algorithms were implemented to measure the quality of passport photographs in two ways: as an image and as a biometric sample.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4351432,no
A Modeling of Software Architecture Reliability,2007,"This paper introduces software architecture reliability estimation and some typical software reliability model based architecture. We modify model of software reliability estimation so that to improve precision of estimating software architecture reliability. At the same time, this paper proposed the simplified method of calculating software architecture reliability based on the state transition matrix. The improved model is validated by an application system in the paper, and the result show that the precision is effectively increased.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4351614,no
Silicon Debug for Timing Errors,2007,"Due to various sources of noise and process variations, assuring a circuit to operate correctly at its desired operational frequency has become a major challenge. In this paper, we propose a timing-reasoning-based algorithm and an adaptive test-generation algorithm for diagnosing timing errors in the silicon-debug phase. We first derive three metrics that are strongly correlated to the probability of a candidate's being an actual error source. We analyze the problem of circuit timing uncertainties caused by delay variations and test sampling. Then, we propose a candidate-ranking heuristic, which is robust with respect to such sources of timing uncertainty. Based on the initial ranking result and the timing information, we further propose an adaptive path-selection and test-generation algorithm to generate additional diagnostic patterns for further improvement of the first-hit-rate. The experimental results demonstrate that combining the ranking heuristic and the adaptive test-generation method would result in a very high resolution for timing diagnosis.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4352014,no
Exploration of Quantitative Scoring Metrics to Compare Systems Biology Modeling Approaches,2007,"In this paper, we report a focused case study to assess whether quantitative metrics are useful to evaluate molecular-level system biology models on cellular metabolism. Ideally, the bio-modeling community shall be able assess systems biology models based on objective and quantitative metrics. This is because metric-based model design not only can accelerate the validation process, but also can improve the efficacy of model design. In addition, the metric will enable researchers to select models with any desired quality standards to study biological pathway. In this case study, we compare popular systems biology modeling approaches such as Michaelis-Menten kinetics and generalized mass action and flux balance analysis to examine the difficulties in developing quantitative metrics for bio-model assessment. We created a set of guidelines in evaluating the efficacy of various bio-modeling approaches and system analysis in several "";bio-systems of interest"";. We found that quantitative scoring metrics are essential aids for (i) model adopters and users to determine fundamental distinctions among bio-models, and (ii) model developers to improve key areas in bio-modeling. Eventually, we want to extend this evaluation practice to broad systems biology modeling.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4352493,no
Determination of simple thresholds for accelerometry-based parameters for fall detection,2007,"The increasing population of elderly people is mainly living in a home-dwelling environment and needs applications to support their independency and safety. Falls are one of the major health risks that affect the quality of life among older adults. Body attached accelerometers have been used to detect falls. The placement of the accelerometric sensor as well as the fall detection algorithms are still under investigation. The aim of the present pilot study was to determine acceleration thresholds for fall detection, using triaxial accelerometric measurements at the waist, wrist, and head. Intentional falls (forward, backward, and lateral) and activities of daily living (ADL) were performed by two voluntary subjects. The results showed that measurements from the waist and head have potential to distinguish between falls and ADL. Especially, when the simple threshold-based detection was combined with posture detection after the fall, the sensitivity and specificity of fall detection were up to 100 %. On the contrary, the wrist did not appear to be an optimal site for fall detection.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4352552,no
Experience at Italian National Institute of Health in the quality control in telemedicine: tools for gathering data information and quality assessing,2007,"The authors proposed a set of tools and procedures to perform a Telemedicine Quality Control process (TM-QC) to be submitted to the telemedicine (TM) manufacturers. The proposed tools were: the Informative Questionnaire (InQu), the Classification Form (ClFo), the Technical File (TF), the Quality Assessment Checklist (QACL). The InQu served to acquire the information about the examined TM product/service; the ClFo allowed to classify a TM product/service as belonging to one application area of TM. The TF was intended as a technical dossier of product and forced the TM supplier to furnish the only requested documentation of its product, so to avoid redundant information. The QACL was a checklist of requirements, regarding all the essential aspects of the telemedical applications, that each TM products/services must be met. The final assessment of the TM product/service was carried out via the QACL, by computing the number of agreed requirements: on the basis of this computation, a Quality Level (QL) was assigned to the telemedical application. Seven levels were considered, ranging from the Basic Quality Level (QL1- B) to the Excellent Quality Level (QL7-E). The TM-QC process resulted a powerful tool to perform the quality control of the telemedical applications and should be a guidance to all the TM practitioners, from the manufacturers to the expert evaluators. The quality control process procedures proposed thus could be adopted in future as routine procedures and could be useful in the assessing the TM delivering into the National Health Service versus the traditional face to face healthcare services.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4352911,no
High-available grid services through the use of virtualized clustering,2007,"Grid applications comprise several components and web-services that make them highly prone to the occurrence of transient software failures and aging problems. This type of failures often incur in undesired performance levels and unexpected partial crashes. In this paper we present a technique that offers high-availability for Grid services based on concepts like virtualization, clustering and software rejuvenation. To show the effectiveness of our approach, we have conducted some experiments with OGSA-DAI middleware. One of the implementations of OGSA-DAI makes use of use of Apache Axis V1.2.1, a SOAP implementation that suffers from severe memory leaks. Without changing any bit of the middleware layer we have been able to anticipate most of the problems caused by those leaks and to increase the overall availability of the OGSA-DAI Application Server. Although these results are tightly related with this middleware it should be noted that our technique is neutral and can be applied to any other Grid service that is supposed to be high-available.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4354113,no
On-Line Periodic Self-Testing of High-Speed Floating-Point Units in Microprocessors,2007,"On-line periodic testing of microprocessors is a viable low-cost alternative for a wide variety of embedded systems which cannot afford hardware or software redundancy techniques but necessitate the detection of intermittent or permanent faults. Low-cost, on-line periodic testing has been previously applied to the integer datapaths of microprocessors but not to their high-performance real number processing counterparts consisting of sophisticated high-speed floating-point (FP) units. In this paper, we present, an effective on-line periodic self-testing methodology for high-speed FP units and demonstrate it on high-speed FP adders/subtracters of both single and double precision. The proposed self-test code development methodology leads to compact self-test routines that exploit the integer part of the processors instruction set architecture to apply test sets to the FP subsystem periodically. The periodic self-test routines exhibit very low memory storage requirements along with a very small number of memory references which are both fundamental requirements for on-line periodic testing. A comprehensive set of experiments on both single and double precision FP units including pipelined versions, and on a RISC processor with a complete FP unit demonstrate the efficacy of the methodology in terms of very high fault coverage and low memory footprint thus rendering the proposed methodology highly appropriate for on-line periodic testing.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4358407,no
On The Detection of Test Smells: A Metrics-Based Approach for General Fixture and Eager Test,2007,"As a fine-grained defect detection technique, unit testing introduces a strong dependency on the structure of the code. Accordingly, test coevolution forms an additional burden on the software developer which can be tempered by writing tests in a manner that makes them easier to change. Fortunately, we are able to concretely express what a good test is by exploiting the specific principles underlying unit testing. Analogous to the concept of code smells, violations of these principles are termed test smells. In this paper, we clarify the structural deficiencies encapsulated in test smells by formalizing core test concepts and their characteristics. To support the detection of two such test smells, General Fixture and Eager Test, we propose a set of metrics defined in terms of unit test concepts. We compare their detection effectiveness using manual inspection and through a comparison with human reviewing. Although the latter is the traditional means for test quality assurance, our results indicate it is not a reliable means for test smell detection. This work thus stresses the need for a more reliable detection mechanism and provides an initial contribution through the validation of test smell metrics.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4359471,no
An Observation-Based Approach to Performance Characterization of Distributed n-Tier Applications,2007,"The characterization of distributed n-tier application performance is an important and challenging problem due to their complex structure and the significant variations in their workload. Theoretical models have difficulties with such wide range of environmental and workload settings. Experimental approaches using manual scripts are error-prone, time consuming, and expensive. We use code generation techniques and tools to create and run the scripts for large-scale experimental observation of n-tier benchmarking application performance measurements over a wide range of parameter settings and software/hardware combinations. Our experiments show the feasibility of experimental observations as a sound basis for performance characterization, by studying in detail the performance achieved by (up to 3) database servers and (up to 12) application servers in the RUBiS benchmark with a workload of up to 2700 concurrent users.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362192,no
Historical Risk Mitigation in Commercial Aircraft Avionics as an Indicator for Intelligent Vehicle Systems,2007,"How safety is perceived in conjunction with consumer products has much to do with its presentation to the buying public and the company reputation for performance and safety. As the automobile industry implements integrated vehicle safety and driver aid systems, the question of public perception of the true safety benefits would seem to parallel the highly automated systems of commercial aircraft, a market in which perceived benefits of flying certainly outweigh concerns of safety. It is suggested that the history of critical aircraft systems provides a model for the wide-based implementation of automated systems in automobiles. The requirement for safety in aircraft systems as an engineering design parameter takes on several forms such as wear-out, probability of catastrophic failure and mean time between replacement or repair (MTBR). For automobile systems as in aircraft, it is a multidimensional topic encompassing a variety of hardware and software functions, fail-safe or fail-operational capability and operator and control interaction. As with critical flight systems, the adherence to specific federal safety requirements is also a cost item to which all manufacturers must adhere, but that also provides a common baseline to which all companies must design. Long a requirement for the design of systems for military and commercial aircraft control, specific safety standards have produced methodologies for analysis and system mechanization that would suggest the operational safety design methods needed for automobiles. Ultimately, tradeoffs must be completed to attain an acceptable level of safety when compared to the cost for developing and selling the system. As seen with commercial aircraft, acceptance of product safety by the public is not based on understanding strict technical requirements but is primarily the result of witnessing many hours of fault free operation, and seeking opinions of those they feel are knowledgeable. This brief study will use data from p- reliminary concept studies for the Automated Highway System and developments by human factors analysts and sociologists concerning perceptions of risk to present an evaluation of the technological methods historically used to mitigate risk in critical aircraft systems and how they might apply to automation in automobiles.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362216,no
Estimating Uncertainty of a Measurement Process,2007,"Estimating a measurement of software quality is a challenge where uncertainty and variation have the greatest impact. Especially, at times when there is not enough information. Here, EUMP (estimating uncertainty of a measurement process) is introduced. EUMP is a recursive process which is using both multi regression and Monte Carlo simulation. It can be systematically obtained through EUMP for all distribution functions of both dependent and independent variables. Moreover, dependent variable can be estimated. Finally, a predictable system will be described, where the error of an estimated dependent variable will be proven it goes to zero when time (t) goes to infinity.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362529,no
Image Quality Assessment: an Overview and some Metrological Considerations,2007,"This paper presents the state of the art in the field of Image Quality Assessment (IQA), providing a classification of some of the most important objective and subjective IQA methods. Furthermore, some aspects of the field are analysed from a metrological point of view, also through comparison with the software quality measurement area. In particular, a statistical approach to the evaluation of the uncertainty for IQA objective methods is presented and an example is provided. The topic of measurement modelling for subjective IQA methods is also analysed. Finally, a case study of images corrupted by impulse noise is discussed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362569,no
Mining the Lexicon Used by Programmers during Sofware Evolution,2007,"Identifiers represent an important source of information for programmers understanding and maintaining a system. Self-documenting identifiers reduce the time and effort necessary to obtain the level of understanding appropriate for the task at hand. While the role of the lexicon in program comprehension has long been recognized, only a few works have studied the quality and enhancement of the identifiers and no works have studied the evolution of the lexicon. In this paper, we characterize the evolution of program identifiers in terms of stability metrics and occurrences of renaming. We assess whether an evolution process similar to the one occurring for the program structure exists for identifiers. We report data and results about the evolution of three large systems, for which several releases are available. We have found evidence that the evolution of the lexicon is more limited and constrained than the evolution of the structure. We argue that the different evolution results from several factors including the lack of advanced tool support for lexicon construction, documentation, and evolution.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362614,no
Re-computing Coverage Information to Assist Regression Testing,2007,"This paper presents a technique that leverages an existing regression test-selection algorithm to compute accurate, updated coverage data on a version of the software, P<sub>i+1</sub>, without rerunning any test cases that do not execute the changes from the previous version of the software, Pi, to P<sub>i+1</sub>-Users of our technique can avoid the expense of rerunning the entire test suite on P<sub>i+1</sub> or the inaccuracy produced by previous approaches that estimate coverage data for P<sub>i+1</sub> or reuse outdated coverage data from Pi. This paper also presents a tool, RECOVER, that implements our technique, along with a set of empirical studies. The studies show the inaccuracies that can exist when an application-regression-test selection-uses estimated and outdated coverage data. The studies also show that the overhead incurred by our technique is negligible.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362629,no
Combinatorial Interaction Regression Testing: A Study of Test Case Generation and Prioritization,2007,"Regression testing is an expensive part of the software maintenance process. Effective regression testing techniques select and order (or prioritize) test cases between successive releases of a program. However, selection and prioritization are dependent on the quality of the initial test suite. An effective and cost efficient test generation technique is combinatorial interaction testing, CIT, which systematically samples all t-way combinations of input parameters. Research on CIT, to date, has focused on single version software systems. There has been little work that empirically assesses the use of CIT test generation as the basis for selection or prioritization. In this paper we examine the effectiveness of CIT across multiple versions of two software subjects. Our results show that CIT performs well in finding seeded faults when compared with an exhaustive test set. We examine several CIT prioritization techniques and compare them with a re-generation/prioritization technique. We find that prioritized and re-generated/prioritized CIT test suites may find faults earlier than unordered CIT test suites, although the re-generated/prioritized test suites sometimes exhibit decreased fault detection.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362638,no
Fault Detection Probability Analysis for Coverage-Based Test Suite Reduction,2007,"Test suite reduction seeks to reduce the number of test cases in a test suite while retaining a high percentage of the original suite's fault detection effectiveness. Most approaches to this problem are based on eliminating test cases that are redundant relative to some coverage criterion. The effectiveness of applying various coverage criteria in test suite reduction is traditionally based on empirical comparison of two metrics derived from the full and reduced test suites and information about a set of known faults: (1) percentage size reduction and (2) percentage fault detection reduction, neither of which quantitatively takes test coverage data into account. Consequently, no existing measure expresses the likelihood of various coverage criteria to force coverage-based reduction to retain test cases that expose specific faults. In this paper, we develop and empirically evaluate, using a number of different coverage criteria, a new metric based on the ""average expected probability of finding a fault"" in a reduced test suite. Our results indicate that the average probability of detecting each fault shows promise for identifying coverage criteria that work well for test suite reduction.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362646,no
Maintaining Multi-Tier Web Applications,2007,"Large-scale multi-tier web applications are inherently dynamic, complex, heterogeneous and constantly evolving. Maintaining such applications is important yet inevitably expensive. First, the size of the test suite of an evolving system will be continuously growing. Second, to ensure that the changes will not affect the quality of the systems, regression testing is frequently performed. To effectively and efficiently maintain web applications after each change, obsolete test cases must be removed and regression testing should selectively re-test. To this end there is a need for an inter-tier change impact analysis, which requires a coherent model rendering inter-tier dependence information. We present a technique that makes use of an integrated inter-connection dependence model to analyze cross-tier change impacts. These are then used to select affected test cases for regression testing and to identify repairable test cases for reuse, or to discard obsolete non-repairable test cases. Our empirical study shows that with this technique, the maintenance cost of the target system can be significantly reduced.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362648,no
The Economics of Open Source Software: An Empirical Analysis of Maintenance Costs,2007,"A quality degradation effect of proprietary code has been observed as a consequence of maintenance. This quality degradation effect, called entropy, is a cause for higher maintenance costs. In the Open Source context, the quality of code is a fundamental tenet of open software developers. As a consequence, the quality degradation principle measured by entropy cannot be assumed to be valid. The goal of the paper is to analyze the entropy of Open Source applications by measuring the evolution of maintenance costs over time. Analyses are based on cost data collected from a sample of 1251 Open Source application versions, compared with the costs estimated with a traditional model for proprietary software. Findings indicate that Open Source applications are less subject to entropy, have lower maintenance costs and also a lower need for maintenance interventions aimed at restoring quality. Finally, results show that a lower entropy is favored by greater functional simplicity.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362652,no
Improving Predictive Models of Software Quality Using an Evolutionary Computational Approach,2007,"Predictive models can be used to identify components as potentially problematic for future maintenance. Source code metrics can be used as input features to classifiers, however, there exist a large number of structural measures that capture different aspects of coupling, cohesion, inheritance, complexity and size. Feature selection is the process of identifying a subset of attributes that improves a classifier's performance. The focus of this study is to explore the efficacy of a genetic algorithm as a method of improving a classifier's ability to identify problematic components.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362671,no
SUDS: An Infrastructure for Creating Bug Detection Tools,2007,SUDS is a powerful infrastructure for creating dynamic bug detection tools. It contains phases for both static analysis and dynamic instrumentation allowing users to create tools that take advantage of both paradigms. The results of static analysis phases can be used to improve the quality of dynamic bug detection tools created with SUDS and could be expanded to find defects statically. The instrumentation engine is designed in a manner that allows users to create their own correctness models quickly but is flexible to support construction of a wide range of different tools. The effectiveness of SUDS is demonstrated by showing that it is capable of finding bugs and that performance is improved when static analysis is used to eliminated unnecessary instrumentation.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362904,no
Stateful Detection in High Throughput Distributed Systems,2007,"With the increasing speed of computers and the complexity of applications, many of today's distributed systems exchange data at a high rate. Significant work has been done in error detection achieved through external fault tolerance systems. However, the high data rate coupled with complex detection can cause the capacity of the fault tolerance system to be exhausted resulting in low detection accuracy. We present a new stateful detection mechanism which observes the exchanged application messages, deduces the application state, and matches against anomaly-based rules. We extend our previous framework (the monitor) to incorporate a sampling approach which adjusts the rate of verified messages. The sampling approach avoids the previously reported breakdown in the monitor capacity at high application message rates, reduces the overall detection cost and allows the monitor to provide accurate detection. We apply the approach to a reliable multicast protocol (TRAM) and demonstrate its performance by comparing it with our previous framework.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4365703,no
Impact of TSPi on Software Projects,2007,"This paper describes the effect of TSPi (introduction to the team software process) on key performance dimensions in software projects, including the ability to estimate, the quality of the software produced and the productivity achieved. The study examines the impact of the TSPi on the performance of 31 software teams. Finally an analysis comparing the results through two iterations is discussed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4367770,no
An Unsupervised Intrusion Detection Method Combined Clustering with Chaos Simulated Annealing,2007,"Keeping networks security has never been such an imperative task as today. Threats come from hardware failures, software flaws, tentative probing and malicious attacks. In this paper, a new detection method, Intrusion Detection based on Unsupervised Clustering and Chaos Simulated Annealing algorithm (IDCCSA), is proposed. As a novel optimization technique, chaos has gained much attention and some applications during the past decade. For a given energy or cost function, by following chaotic ergodic orbits, a chaotic dynamic system may eventually reach the global optimum or its good approximation with high probability. To enhance the performance of simulated annealing which is to find a near-optimal partitioning clustering, simulated annealing algorithm is proposed by incorporating chaos. Experiments with KDD cup 1999 show that the simulated annealing combined with chaos can effectively enhance the searching efficiency and greatly improve the detection quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4370702,no
A Fault Detection Mechanism for Fault-Tolerant SOA-Based Applications,2007,"Fault tolerance is an important capability for SOA-based applications, since it ensures the dynamic composition of services and improves the dependability of SOA-based applications. Fault detection is the first step of fault detection, so this paper focuses on fault detection, and puts forward a fault detection mechanism, which is based on the theories of artificial neural network and probability change point analysis rather than static service description, to detect the services that fail to satisfy performance requirements at runtime. This paper also gives reference model of fault-tolerance control center of enterprise services bus.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4370804,no
Comparison of Artificial Neural Network and Regression Models in Software Effort Estimation,2007,"Good practices in software project management are basic requirements for companies to stay in the market, because the effective project management leads to improvements in product quality and cost reduction. Fundamental measurements are the prediction of size, effort, resources, cost and time spent in the software development process. In this paper, predictive Artificial Neural Network (ANN) and Regression based models are investigated, aiming at establishing simple estimation methods alternatives. The results presented in this paper compare the performance of both methods and show that artificial neural networks are effective in effort estimation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4371055,no
A Constructive RBF Neural Network for Estimating the Probability of Defects in Software Modules,2007,"Much of the current research in software defect prediction focuses on building classifiers to predict only whether a software module is fault-prone or not. Using these techniques, the effort to test the software is directed at modules that are labelled as fault-prone by the classifier. This paper introduces a novel algorithm based on constructive RBF neural networks aimed at predicting the probability of errors in fault-prone modules; it is called RBF-DDA with Probabilistic Outputs and is an extension of RBF-DDA neural networks. The advantage of our method is that we can inform the test team of the probability of defect in a module, instead of indicating only if the module is fault-prone or not. Experiments carried out with static code measures from well-known software defect datasets from NASA show the effectiveness of the proposed method. We also compared the performance of the proposed method in software defect prediction with kNN and two of its variants, the S-POC-NN and R-POC-NN. The experimental results showed that the proposed method outperforms both S-POC-NN and R-POC-NN and that it is equivalent to kNN in terms of performance with the advantage of producing less complex classifiers.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4371415,no
An Evaluation of Unstructured Text Mining Software,2007,"Five text mining software tools were evaluated by four undergraduate students inexperienced in the text mining field. The software was run on the Microsoft Windows XP operating system, and employed a variety of techniques to mine unstructured text for information. The considerations used to evaluate the software included cost, ease of learning, functionality, ease of use and effectiveness. Hands on mining of text files also led us to more informative conclusions of the software. Through our evaluation we found that two software products (SAS and SPSS) had qualities that made them more desirable than the others.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4373985,no
Vector signal analyzer implemented as a synthetic instrument,2007,"Synthetic Instruments use the substantial signal processing assets of a field programmable gate array (FPGA) to perform the multiple tasks of targeted digital signal processing (DSP) based instruments. The signal conditioning common to many instruments includes analog spectral translation, filtering, and gain control to align the bandwidth and dynamic range of the input signal to the bandwidth and dynamic range capabilities of the A-to-D converter (ADC) which moves the signal from the analog domain to the sampled data domain. Once in the sampled data domain, the signal processing performed by the FPGA includes digital spectral translation, filtering, and gain control to perform its chartered DSP tasks. A common DSP task is spectral analysis from which frequency dependent (i.e., spectral) amplitude and phase is extracted from an input time signal. Another high interest DSP task is vector signal analysis from which time dependent (i.e., temporal) amplitude and phase is extracted from the input time signal. With access to the time varying amplitude-phase profiles of the input signal, the vector signal analyzer can present many of the quality measures of a modulation process. These include estimates of undesired attributes such as modulator distortion, phase noise, clock-jitter, l-Q imbalance, inter-symbol interference, and others. Here, the boundary between synthetic instruments (SI) and software defined radios (SDR) becomes very thin indeed. Essentially this is where the SI is asked to become a smart SDR, performing all the tasks of a DSP radio receiver and reporting small variations between the observed modulated signal parameters and those of an ideal modulated signal. Various quality measures (e.g., the size of errors) have value in qualifying and probing performance boundaries of communication systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4374215,no
Building a Database to Support Intelligent Computational Quality Assurance of Resistance Spot Welding Joints,2007,"A database system for storing information on resistance spot welding processes is outlined. Data stored in the database can be used for computationally estimating the quality of spot welding joints and for adaptively setting up new welding processes in order to ensure consistent high quality. This is achieved by storing current and voltage signals in the database, extracting features out of those signals and using the features as training input for classifier algorithms. Together the database and the associated data mining modules form an adaptive system that improves its performance over time. An entity-relationship model of the application domain is presented and then converted into a concrete database design. Software interfaces for accessing the database are described and the utility of the database and the access interfaces as components of a welding quality assurance system is evaluated. A relational database with tables for storing test sets, welds, signals, features and metadata is found suitable for the purpose. The constructed database has served well as a repository for research data and is ready to be transferred to production use at a manufacturing site.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4374911,no
Are Two Heads Better than One? On the Effectiveness of Pair Programming,2007,"Pair programming is a collaborative approach that makes working in pairs rather than individually the primary work style for code development. Because PP is a radically different approach than many developers are used to, it can be hard to predict the effects when a team switches to PP. Because projects focus on different things, this article concentrates on understanding general aspects related to effectiveness, specifically project duration, effort, and quality. Not unexpectedly, our meta-analysis showed that the question of whether two heads are better than one isn't precise enough to be meaningful. Given the evidence, the best answer is ""it depends"" - on both the programmer's expertise and the complexity of the system and tasks to be solved. Two heads are better than one for achieving correctness on highly complex programming tasks. They might also have a time gain on simpler tasks. Additional studies would be useful. For example, further investigation is clearly needed into the interaction of complexity and programmer experience and how they affect the appropriateness of a PP approach; our current understanding of this phenomenon rests chiefly on a single (although large) study. Only by understanding what makes pairs work and what makes them less efficient can we take steps to provide beneficial work conditions, to avoid detrimental conditions, and to avoid pairing altogether when conditions are detrimental. With the right cooks and the right combination of ingredients, the broth has the potential to be very good indeed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4375233,no
Estimation of ADSL and ADSL2+ Service Quality,2007,"In this paper, estimation of ADSL and ADSL2+ service performances is presented. Since the transmission medium quality is one of the key requirements for performance capabilities of these services, an analytical method for modelling of primary per-unit length parameters of twisted pair is described. This model is implemented in the existing software extended to allow for estimation of data rates on telephone subscriber loops for xDSL service. For the example of particular twisted pair telephone cable, an experimental measurement of its parameters is done, which is followed by their modelling using described software. At the end, comparison of experimental and modelled results of capacity for specific cable is presented.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4375939,no
Mechatronic Software Testing,2007,"The paper describes mechatronic software testing techniques. Testing is different from common testing and includes special features of mechatronic systems. It may be put into effect with the aim of improving quality, assessing reliability, checking and conforming correctness. Various adapted techniques may be employed for the purpose, such as, for example, the white box technique or the black box technique when correctness testing, endurance and stress testing in relability testing or the usage of ready programs for performance testing.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376049,no
Automatic Document Logo Detection,2007,"Automatic logo detection and recognition continues to be of great interest to the document retrieval community as it enables effective identification of the source of a document. In this paper, we propose a new approach to logo detection and extraction in document images that robustly classifies and precisely localizes logos using a boosting strategy across multiple image scales. At a coarse scale, a trained Fisher classifier performs initial classification using features from document context and connected components. Each logo candidate region is further classified at successively finer scales by a cascade of simple classifiers, which allows false alarms to be discarded and the detected region to be refined. Our approach is segmentation free and lay-out independent. We define a meaningful evaluation metric to measure the quality of logo detection using labeled groundtruth. We demonstrate the effectiveness of our approach using a large collection of real-world documents.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4377038,no
OCR Accuracy Improvement through a PDE-Based Approach,2007,"This paper focuses on improving the optical character recognition (OCR) system 's accuracy by restoring damaged character through a PDE (Partial Differential Equation)-based approach. This approach, proposed by D. Tschumperle, is an anisotropic diffusion approach driven by local tensors fields. Actually, such approach has many useful properties that are relevant for use in character restoration. For instance, this approach is very appropriate for the processing of oriented patterns which are major characteristics of textual documents. It incorporates both edge enhancing diffusion that tends to preserve local structures during smoothing and coherence-enhancing diffusion that processes oriented structures by smoothing along the flow direction. Furthermore, this tensor diffusion-based approach compared to the existing sate of the art requires neither segmentation nor training steps. Some experiments, done on degraded document images, illustrate the performance of this PDE-based approach in improving both of the visual quality and the OCR accuracy rates for degraded document images.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4377079,no
A Best Practice Guide to Resource Forecasting for Computing Systems,2007,"Recently, measurement-based studies of software systems have proliferated, reflecting an increasingly empirical focus on system availability, reliability, aging, and fault tolerance. However, it is a nontrivial, error-prone, arduous, and time-consuming task even for experienced system administrators, and statistical analysts to know what a reasonable set of steps should include to model, and successfully predict performance variables, or system failures of a complex software system. Reported results are fragmented, and focus on applying statistical regression techniques to monitored numerical system data. In this paper, we propose a best practice guide for building empirical models based on our experience with forecasting Apache web server performance variables, and forecasting call availability of a real-world telecommunication system. To substantiate the presented guide, and to demonstrate our approach in a step by step manner, we model, and predict the response time, and the amount of free physical memory of an Apache web server system, as well as the call availability of an industrial telecommunication system. Additionally, we present concrete results for a) variable selection where we cross benchmark three procedures, b) empirical model building where we cross benchmark four techniques, and c) sensitivity analysis. This best practice guide intends to assist in configuring modeling approaches systematically for best estimation, and prediction results.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4378407,no
Harbor: Software-based Memory Protection For Sensor Nodes,2007,"Many sensor nodes contain resource constrained microcontrollers where user level applications, operating system components, and device drivers share a single address space with no form of hardware memory protection. Programming errors in one application can easily corrupt the state of the operating system or other applications. In this paper, we propose Harbor, a memory protection system that prevents many forms of memory corruption. We use software based fault isolation (""sandboxing"") to restrict application memory accesses and control flow to protection domains within the address space. A flexible and efficient memory map data structure records ownership and layout information for memory regions; writes are validated using the memory map. Control flow integrity is preserved by maintaining a safe stack that stores return addresses in a protected memory region. Run-time checks validate computed control flow instructions. Cross domain calls perform low-overhead control transfers between domains. Checks are introduced by rewriting an application's compiled binary. The sand- boxed result is verified on the sensor node before it is admitted for execution. Harbor's fault isolation properties depend only on the correctness of this verifier and the Harbor runtime. We have implemented and tested Harbor on the SOS operating system. Harbor detected and prevented memory corruption caused by programming errors in application modules that had been in use for several months. Harbor's overhead, though high, is less than that of application-specific virtual machines, and reasonable for typical sensor workloads.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4379694,no
Edge Weighted Spatio-Temporal Search for Error Concealment,2007,"In temporal error concealment (EC), the sum of absolute difference (SAD) is commonly used to identify the best replacement macroblock. Even though the use of SAD ensures spatial continuity and produces visually good results, it is insufficient to ensure edge alignment. Other distortion criteria based solely on structural alignment may also perform poorly in the absence of strong edges. In this paper, we propose a spatio-temporal EC search algorithm using an edge weighted SAD distortion criterion. This distortion criterion ensures both edge alignment and spatial continuity. We assume the loss of motion information and use zero motion vector as the starting search point. We show that the proposed algorithm outperforms the use of unweighted SAD in general. Most importantly, the perceptual quality of EC is improved due to edge alignment while ensuring spatial continuity.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4380001,no
Protection of Induction Motor Using PLC,2007,"The goal of this paper is to protect induction motors against possible failures by increasing the reliability, the efficiency, and the performance. The proposed approach is a sensor-based technique. For this purpose, currents, voltages, speed and temperature values of the induction motor were measured with sensors. When any fault condition is detected during operation of the motor, PLC controlled on-line operation system activates immediately. The performance of the protection system proposed is discussed by means of application results. The motor protection achieved in the study can be faster than the classical techniques and applied to larger motors easily after making small modifications on both software and hardware.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4380120,no
A WSAD-Based Fact Extractor for J2EE Web Projects,2007,"This paper describes our implementation of a fact extractor for J2EE Web applications. Fact extractors are part of each reverse engineering toolset; their output is used by reverse engineering analyzers and visualizers. Our fact extractor has been implemented on top of IBM's Websphere Application Developer (WSAD). The extractor's schema has been defined with the Eclipse Modeling Framework (EMF) using a graphical modeling approach. The extractor extensively reuses functionality provided by WSAD, EMF, and Eclipse, and is an example of component-based development. In this paper, we show how we used this development approach to accomplish the construction of our fact extractor, which, as a result, could be realized with significantly less code and in shorter time compared to a homegrown extractor implemented from scratch. We have assessed our extractor and the produced facts with a table- based and a graph-based visualizer. Both visualizers are integrated with Eclipse.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4380245,no
Improving Usability of Web Pages for Blinds,2007,"Warranting the access to Web contents to any citizen, even to people with physical disabilities, is a major concern of many government organizations. Although guidelines for Web developers have been proposed by international organisations (such as the W3C) to make Web site contents accessible, the wider part of today's Web sites are not completely usable by peoples with sight disabilities. In this paper, two different approaches for dynamically transforming Web pages into aural Web pages, i.e. pages that are optimised for blind peoples, will be presented. The approaches exploit heuristic techniques for summarising Web pages contents and providing them to blind users in order to improve the usability of Web sites. The techniques have been validated in an experiment where usability metrics have been used to assess the effectiveness of the Web page transformation techniques.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4380250,no
Digital Thermal Microscope for Biomedical Application,2007,"In order to analyze the biomedical, we proposed a novel digital thermal microscope based on the uncooled focal plane detector, aiming to achieve the long-wave infrared microscope image, especially for biomedical analysis. Both the mathematical mode of noise equivalent temperature difference (NETD) and the noise equivalent eradiation difference (NEED) were established for micro thermal imaging system. Based on the mathematical model, some measures were taken to increase the system temperature resolution. Furthermore the uncooled focal plane arrays has inherent non-uniformities, so we proposed an adaptive algorithm that can complete NUC by only one frame. Results of our thermal microscope have proved that NUC can weaken striping noise greatly and plateau histogram equalization can further enhance the image quality. The software for the thermal microscope is provided based on Visual C++ and the methods mentioned above. Results of real thermal image experiments have shown that the digital thermal microscope is designed successfully and achieves good performance. With the thermal microscope, minute sized thermal analysis can be achieved. Thus it will become an effective means for diagnosis and the detection of cancer, and it can also accelerate the development of methods for biomedical engineering. The system is very meaningful for academic analysis and is promising for practical applications.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4382067,no
A Self-Consistent Substrate Thermal Profile Estimation Technique for Nanoscale ICsâ€”Part II: Implementation and Implications for Power Estimation and Thermal Management,2007,"As transistors continue to evolve along Moore's Law and silicon devices take advantage of this evolution to offer increasing performance, there is a critical need to accurately estimate the silicon-substrate (junction or die) thermal gradients and temperature profile for the development and thermal management of future generations of all high-performance integrated circuits (ICs) including microprocessors. This paper presents an accurate chip-level leakage-aware method that self-consistently incorporates various electrothermal couplings between chip power, junction temperature, operating frequency, and supply voltage for substrate thermal profile estimation and also employs a realistic package thermal model that comprehends different packaging layers and noncubic structure of the package, which are not accounted for in traditional analyses. The evaluation using the proposed methodology is efficient and shows excellent agreements with an industrial-quality computational-fluid-dynamics (CFD) based commercial software. Furthermore, the methodology is shown to become increasingly effective with increase in leakage as technology scales. It is shown that considering electrothermal couplings and realistic package thermal model not only improves the accuracy of estimating the heat distribution across the chip but also has significant implications for precise power estimation and thermal management in nanometer-scale CMOS technologies.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4383040,no
Size and Frequency of Class Change from a Refactoring Perspective,2007,"A previous study by Bieman et al., investigated whether large, object-oriented classes were more susceptible to change than smaller classes. The measure of change used in the study was the frequency with which the features of a class had been changed over a specific period of time. From a refactoring perspective, the frequency of class change is of value But even for a relatively simple refactoring such as 'rename method', multiple classes may undergo minor modification without any net increase in class (and system) size. In this paper, we suggest that the combination of 'versions of a class and number of added lines of code ' in the bad code 'smell' detection process may give a better impression of which classes are most suitable candidates for refactoring; as such, effort in detecting bad code smells should apply to classes with a high growth rate as well as a high change frequency. To support our investigation, data relating to changes from 161 Java classes was collected. Results concluded that it is not necessarily the case that large classes are more change-prone than relatively smaller classes. Moreover, the bad code smell detection process is informed by using the combination of change frequency and class size as a heuristic.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4383093,no
A Requirement Level Modification Analysis Support Framework,2007,"Modification analysis is an essential phase of most software maintenance processes, requiring decision makers to perform and predict potential change impacts, feasibility and costs associated with a potential modification request. The majority of existing techniques and tools supporting modification analysis focusing on source code level analysis and require an understanding of the system and its implementation. In this research, we present a novel approach to support the identification of potential modification and re-testing efforts associated with a modification request, without the need for analyzing or understanding the system source code. We combine Use Case Maps with Formal Concept Analysis to provide a unique modification analysis framework that can assist decision makers during modification analysis at the requirements level. We demonstrate the applicability of our approach on a telephony system case study.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4383100,no
A Scalable Parallel Deduplication Algorithm,2007,"The identification of replicas in a database is fundamental to improve the quality of the information. Deduplication is the task of identifying replicas in a database that refer to the same real world entity. This process is not always trivial, because data may be corrupted during their gathering, storing or even manipulation. Problems such as misspelled names, data truncation, data input in a wrong format, lack of conventions (like how to abbreviate a name), missing data or even fraud may lead to the insertion of replicas in a database. The deduplication process may be very hard, if not impossible, to be performed manually, since actual databases may have hundreds of millions of records. In this paper, we present our parallel deduplication algorithm, called FER- APARDA. By using probabilistic record linkage, we were able to successfully detect replicas in synthetic datasets with more than 1 million records in about 7 minutes using a 20- computer cluster, achieving an almost linear speedup. We believe that our results do not have similar in the literature when it comes to the size of the data set and the processing time.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4384045,no
Assessing the Object-level behavioral complexity in Object Relational Databases,2007,"Object Relational Database Management Systems model set of interrelated objects using references and collection attributes. The static metrics capture the internal quality of the database schema at the class -level during design time. Complex databases like ORDB exhibit dynamism during runtime and hence require performance-level monitoring. This is achieved by measuring the access and invocations of the objects during runtime, thus assessing the behavior of the objects. Runtime coupling and cohesion metrics are deemed as attributes of measuring the Object-level behavioral complexity. In this work, we evaluate the runtime coupling and cohesion metrics and assess their influence in measuring the behavioral complexity of the objects in ORDB. Further, these internal measures of object behavior are externalized in measuring the performance of the database in entirety. Experiments on sample ORDB schemas are conducted using statistical analysis and correlation clustering techniques to assess the behavior of the objects in real time. The results indicate the significance of the object behavior in influencing the database performance. The scope of this work and the future works in extending this research form the concluding note.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4384084,no
Technique Integration for Requirements Assessment,2007,"In determining whether to permit a safety-critical software system to be certified and in performing independent verification and validation (IV&V) of safety- or mission-critical systems, the requirements traceability matrix (RTM) delivered by the developer must be assessed for accuracy. The current state of the practice is to perform this work manually, or with the help of general-purpose tools such as word processors and spreadsheets Such work is error-prone and person-power intensive. In this paper, we extend our prior work in application of Information Retrieval (IR) methods for candidate link generation to the problem of RTM accuracy assessment. We build voting committees from five IR methods, and use a variety of voting schemes to accept or reject links from given candidate RTMs. We report on the results of two experiments. In the first experiment, we used 25 candidate RTMs built by human analysts for a small tracing task involving a portion of a NASA scientific instrument specification. In the second experiment, we randomly seeded faults in the RTM for the entire specification. Results of the experiments are presented.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4384177,no
Statistical Assessment of Global and Local Cylinder Wear,2007,"Assessment of cylindricity has been traditionally performed on the basis of cylindrical crowns containing a set of points that are supposed to belong to a controlled cylinder. As such, all sampled points must lie within a crown. In contrast, the present paper analyzes the cylindricity for wear applications, in which a statistical trend is assessed, rather than to assure that all points fall within a given tolerance. Principal component analysis is used to identify the central axis of the sampled cylinder, allowing to find the actual (expected value of the) radius and axis of the cylinder. Application of k-cluster and transitive closure algorithms allow to identify particular areas of the cylinder which are specially deformed. For both, the local areas and the global cylinder, a quantile analysis allows to numerically grade the degree of deformation of the cylinder. The algorithms implemented are part of the CYLWEAR<sup>copy</sup> system and used to assess local and global wear cylinders.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4384788,no
Developing Intentional Systems with the PRACTIONIST Framework,2007,"Agent-based systems have become a very attractive approach for dealing with the complexity of modern software applications and have proved to be useful and successful in some industrial domains. However, engineering such systems is still a challenge due to the lack of effective tools and actual implementations of very interesting and fascinating theories and models. In this area the so-called intentional stance of systems can be very helpful to efficiently predict, explain, and define the behaviour of complex systems, without having to understand how they actually work, but explaining them in terms of some mental qualities or attitudes, rather than their physical or design stance. In this paper we present the PRACTIONIST framework, that supports the development of PRACTIcal reasONIng sySTems according to the BDI model of agency, which uses some mental attitudes such as beliefs, desires, and intentions to describe and specify the behaviour of system components. We adopt a goal-oriented approach and a clear separation between the deliberation phase and the means-ends reasoning, and consequently between the states of affairs to pursue and the way to do it. Moreover, PRACTIONIST allows developers to implement agents that are able to reason about their beliefs and the other agents' beliefs, expressed by modal logic formulas.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4384847,no
Discovering Web Services Using Semantic Keywords,2007,"With the increasing growth in popularity of Web services, the discovery of relevant services becomes a significant challenge. In order to enhance the service discovery is necessary that both the Web service description and the request for discovering a service explicitly declare their semantics. Some languages and frameworks have been developed to support rich semantic service descriptions and discover using ontology concepts. However, the manual creation of such concepts is tedious and error-prone and many users accustomed to automatic tools might not want to invert his time in obtaining this knowledge. In this paper we propose a system that assists to both service producers and service consumers in the discovery of semantic keywords which can be used to describe and discover Web services respectively. First, our system enhances semantically the list of keywords extracted from the elements that comprise the description of a Web service and the user keywords used for discover a service. Second, an ontology matching process is used to discovers matchings between the ontological terms of a service description and a request for service selection. Third, a subsumption reasoning algorithm tries to find service description(s) which match the user request.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4384863,no
The Design of a Multimedia Protocol Analysis Software Environment,2007,"We have developed a variant of Estelle, called Time-Estelle which is able to express multimedia quality of service (QoS) parameters, synchronisation scenarios, and time-dependent and probabilistic behaviours of multimedia protocols. We have developed an approach to verifying a multimedia protocol specified in Time-Estelle. To predict the performance of a multimedia system, we have also developed a method for the performance analysis of a multimedia protocol specified in Time-Estelle. However, without the support of a software environment to automate the processes, verification and performance analysis methods would be very time-consuming. This paper describes the design of such a software environment.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4385206,no
A Fault Detection Mechanism for Service-Oriented Architecture Based on Queueing Theory,2007,"SOA is an ideal solution to application building, since it reuses the existing services as many as possible. The fault tolerance is one important capability to ensure the SOA- based applications are high reliable and available. However, fault tolerance is such a complex issue for most SOA providers that they hardly provide this capability in their products. This paper provides a queuing-theory-based algorithm to fault detection, which can be used to detect the services whose performance becomes unsatisfactory at runtime according to the QoS descriptor. Based on this algorithm, this paper also gives the reference models of the extended service and the architecture of fault-tolerance control center of enterprise services bus for SOA-based applications.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4385227,no
Test Case Prioritization Based on Varying Testing Requirement Priorities and Test Case Costs,2007,"Test case prioritization is an effective and practical technique in regression testing. It schedules test cases in order of precedence that increases their ability to meet some performance goals, such as code coverage, rate of fault detection. In previous work, the test case prioritization techniques and metrics usually assumed that testing requirement priorities and test case costs are uniform. In this paper, basing on varying testing requirement priorities and test case costs, we present a new, general test case prioritization technique and an associated metric. The case study illustrates that the rate of ""units-oftesting-requirement-priority-satisfied- per-unit-test-case-cost"" can be increased, and then the testing quality and customer satisfaction can be improved.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4385476,no
Towards Automatic Measurement of Probabilistic Processes,2007,In this paper we propose a metric for finite processes in a probabilistic extension of CSP. The kernel of the metric corresponds to trace equivalence and most of the operators in the process algebra is shown to satisfy non-expansiveness property with respect to this metric. We also provide an algorithm to calculate the distance between two processes to a prescribed discount factor in polynomial time. The algorithm has been implemented in a tool that helps us to measure processes automatically.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4385480,no
A Reinforcement-Learning Approach to Failure-Detection Scheduling,2007,"A failure-detection scheduler for an online production system must strike a tradeoff between performance and reliability. If failure-detection processes are run too frequently, valuable system resources are spent checking and rechecking for failures. However, if failure-detection processes are run too rarely, a failure can remain undetected for a long time. In both cases, system performability suffers. We present a model-based learning approach that estimates the failure rate and then performs an optimization to find the tradeoff that maximizes system performability. We show that our approach is not only theoretically sound but practically effective, and we demonstrate its use in an implemented automated deadlock-detection system for Java.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4385492,no
Failure Analysis of Open Source J2EE Application Servers,2007,"Open source J2EE Application Servers (J2EE ASs) have already attracted industrial attentions in recent years, but the reliability is still an obstacle to their wide acceptance. The detail of software failures in J2EE AS was seldom discussed in the past, although such information is valuable to evaluate and improve its reliability. In this paper, we present a measurement- based failure classification and analysis. Presented results indicate open source J2EE AS's reliability needs improvement. Some notable results include: (1) the implementation of a widely used fault-tolerant mechanism, clustering, needs improvement; (2) only 15% of reported failures are removed within a week, and about 10% still open by the time we finish the study; (3) different J2EE ASs have different unreliable services so reliability improvement activities should be specific to both AS and applications.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4385496,no
A Multivariate Analysis of Static Code Attributes for Defect Prediction,2007,"Defect prediction is important in order to reduce test times by allocating valuable test resources effectively. In this work, we propose a model using multivariate approaches in conjunction with Bayesian methods for defect predictions. The motivation behind using a multivariate approach is to overcome the independence assumption of univariate approaches about software attributes. Using Bayesian methods gives practitioners an idea about the defectiveness of software modules in a probabilistic framework rather than the hard classification methods such as decision trees. Furthermore the software attributes used in this work are chosen among the static code attributes that can easily be extracted from source code, which prevents human errors or subjectivity. These attributes are preprocessed with feature selection techniques to select the most relevant attributes for prediction. Finally we compared our proposed model with the best results reported so far on public datasets and we conclude that using multivariate approaches can perform better.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4385500,no
Distribution Metric Driven Adaptive Random Testing,2007,"Adaptive random testing (ART) was developed to enhance the failure detection capability of random testing. The basic principle of ART is to enforce random test cases evenly spread inside the input domain. Various distribution metrics have been used to measure different aspects of the evenness of test case distribution. As expected, it has been observed that the failure detection capability of an ART algorithm is related to how evenly test cases are distributed. Motivated by such an observation, we propose a new family of ART algorithms, namely distribution metric driven ART, in which, distribution metrics are key drivers for evenly spreading test cases inside ART. Out study uncovers several interesting results and shows that the new algorithms can spread test cases more evenly, and also have better failure detection capabilities.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4385507,no
Cohesion Metrics for Predicting Maintainability of Service-Oriented Software,2007,"Although service-oriented computing (SOC) is a promising paradigm for developing enterprise software systems, existing research mostly assumes the existence of black box services with little attention given to the structural characteristics of the implementing software, potentially resulting in poor system maintainability. Whilst there has been some preliminary work examining coupling in a service-oriented context, there has to date been no such work on the structural property of cohesion. Consequently, this paper extends existing notions of cohesion in OO and procedural design in order to account for the unique characteristics of SOC, allowing the derivation of assumptions linking cohesion to the maintainability of service-oriented software. From these assumptions, a set of metrics are derived to quantify the degree of cohesion of service oriented design constructs. Such design level metrics are valuable because they allow the prediction of maintainability early in the SDLC.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4385516,no
Abstraction in Assertion-Based Test Oracles,2007,"Assertions can be used as test oracles. However, writing effective assertions of right abstraction levels is difficult because on the one hand, detailed assertions are preferred for thorough testing (i.e., to detect as many errors as possible), but on the other hand abstract assertions are preferred for readability, maintainability, and reusability. As assertions become a practical tool for testing and debugging programs, this is an important and practical problem to solve for the effective use of assertions. We advocate the use of model variables - specification-only variables of which abstract values are given as mappings from concrete program states - to write abstract assertions for test oracles. We performed a mutation testing experiment to evaluate the effectiveness of the use of model variables in assertion-based test oracles. According to our experiment, assertions written in terms of model variables are as effective as assertions written without using model variables in detecting (injected) faults, and the execution time overhead of model variables are negligible. Our findings are applicable to other use of runtime checkable assertions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4385528,no
A Phase-Locked Loop for the Synchronization of Power Quality Instruments in the Presence of Stationary and Transient Disturbances,2007,"Power quality instrumentation requires accurate fundamental frequency estimation and signal synchronization, even in the presence of both stationary and transient disturbances. In this paper, the authors present a synchronization technique for power quality instruments based on a single-phase software phase-locked loop (PLL), which is able to perform the synchronization, even in the presence of such disturbances. Moreover, PLL is able to detect the occurrence of a transient disturbance. To evaluate if and how the synchronization technique is adversely affected by the application of stationary and transient disturbing influences, appropriate testing conditions have been developed, taking into account the requirements of the in-force standards and the presence of the voltage transducer.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4389091,no
Rough Set Theory Measures to Knowledge Generation,2007,"The accelerated growth of the information volumes on processes, phenomena and reports brings about an increasing interest in the possibility of discovering knowledge from data sets. This is a challenging task because in many cases it deals with extremely large, inherently not structured and fuzzy data, plus the presence of uncertainty. Therefore it is required to know a priori the quality of future procedures without using any additional information. In this paper we propose new measures to evaluate the quality of training sets used by algorithms for learning of supervised classifiers. Our training set assessment relied on measures furnished by rough sets theory. Our experimental results involved three classifiers (k-NN, C-4.5 and MLP) from international data bases. New training sets are built taking into account the results of the measures and the accuracy obtained by the classifiers, with the aim of infer the accuracy that the classifiers would obtain using a new training set. This is possible using a rule generator (C4.5) and a function estimation algorithm (k-NN).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4389641,no
On Metrics-Driven Software Process,2007,"Metrics can drive software processing, because they found the base of its quantizing management. There are two fundamental requirements in engineering: formal modeling and quantitative modeling. We must emphasize that metrics for software engineering is insufficient in quantification now. In order to improve software and software processing, the factors that affect schedule, cost and quality of software development should be measured. Metrics produces adjustable and iterative motions in software processing. Based on Jaynes' maximum entropy principle, this paper establishes a model to quantify the factors and introduces distance to compare the metric indicators. The authors propose that the metric estimation tree can be used and the nodes that stand for the software attributes in the tree can be marked with their corresponding evaluation values. Dynamic feedback in the software processing will be combined with AHP (analytic hierarchy process), and the project and process of development will be learned and analyzed entirely, concentratedly and dynamically.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4392660,no
Analysis of Anomalies in IBRL Data from a Wireless Sensor Network Deployment,2007,"Detecting interesting events and anomalous behaviors in wireless sensor networks is an important challenge for tasks such as monitoring applications, fault diagnosis and intrusion detection. A key problem is to define and detect those anomalies with few false alarms while preserving the limited energy in the sensor network. In this paper, using concepts from statistics, we perform an analysis of a subset of the data gathered from a real sensor network deployment at the Intel Berkeley Research Laboratory (IBRL) in the USA, and provide a formal definition for anomalies in the IBRL data. By providing a formal definition for anomalies in this publicly available data set, we aim to provide a benchmark for evaluating anomaly detection techniques. We also discuss some open problems in detecting anomalies in energy constrained wireless sensor networks.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4394914,no
Quality Assessment Based on Attribute Series of Software Evolution,2007,"Defect density and defect prediction are essential for efficient resource allocation in software evolution. In an empirical study we applied data mining techniques for value series based on evolution attributes such as number of authors, commit messages, lines of code, bug fix count, etc. Daily data points of these evolution attributes were captured over a period of two months to predict the defects in the subsequent two months in a project. For that, we developed models utilizing genetic programming and linear regression to accurately predict software defects. In our study, we investigated the data of three independent projects, two open source and one commercial software system. The results show that by utilizing series of these attributes we obtain models with high correlation coefficients (between 0.716 and 0.946). Further, we argue that prediction models based on series of a single variable are sometimes superior to the model including all attributes: in contrast to other studies that resulted in size or complexity measures as predictors, we have identified the number of authors and the number of commit messages to versioning systems as excellent predictors of defect densities.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4400154,no
An Experimental Platform for Root Cause Diagnosis Research,2007,"To obtain a healthy integrated production system that achieves defined quality goals in service oriented architecture (SOA), such as availability and performance, the timely detection and resolution of failures is needed. The goal of this thesis identify the primary or root causes (faults) of a set of observed symptoms in an integrated production system that indicate degradation and failure in system components leading to abnormal system performance. Our hypothesis is, if we combine more diagnosis methods (such as more symptom repository, dynamic analysis capabilities, different artificial intelligent (AI) reasoning capabilities or other available technologies and tools) into an existing tool such as open source AspectJ based diagnostic tool Glassbox, we can find more amount of root causes and more precise - ""actual"" root causes. We are building an experiment platform to validate this hypothesis.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4400181,no
Enhancing the Assessment Environment within a Learning Management Systems,2007,There are many open source or proprietary assessment tools on the market. These tools are embedded in learning management systems (LMS) and have as goal evaluation of learners. A problem that currently appears is that assessment tools are not always fair or accurate in classifying students according with accumulated knowledge. We propose a software module called quality module (QM) that may run along with an assessment tool within an LMS. The QM performs offline analysis and obtains knowledge regarding the classification capability of the assessment tool. The QM also obtains knowledge for course managers regarding the course difficulty such that course materials and quizzes may be altered in order to obtain a more accurate assessment tool and thus better classification among learners.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4400345,no
Mining Software Data,2007,"Data mining techniques and machine learning methods are commonly used in several disciplines. It is possible that they could also provide a basis for quality assessment of software development processes and the final software product. Number of researches who employ such techniques and methods on software cost and effort estimation are increasing. This article provides a software quality perspective on data mining and machine learning applications, and explains the current research challenges that are related to incorporating these tools based on several metrics derived from the software development processes and the software itself.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4401084,no
Service-Oriented Business Process Modeling and Performance Evaluation based on AHP and Simulation,2007,"With the evolution of grid technologies and the application of service-oriented architecture (SOA), more and more enterprises are integrated and collaborated with each other in a loosely coupled environment. A business process in that environment, i.e., the service-oriented business process (SOBP), shows highly flexibility for its free selection and composition of different services. The performance of the business process usually has to be evaluated and predicted before its being implemented. And it has special features since it includes both business-level and IT-level attributes. However, the existing modeling and performance evaluation methods of business process are mainly concentrated on business-level performance. And the researches on service selection and composition are usually limited to the IT-level metrics. An extended activity-network-based SOBP Model, its three-level performance metrics, and the corresponding calculation algorithm are proposed to fulfill these requirements. The advantages of our method in SOBP modeling and performance evaluation are highlighted also.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402135,no
The 18th IEEE International Symposium on Software Reliability - Title page,2007,The following topics are dealt with: software system reliability; dependable software systems; validation; security testing and analysis; test automation; software security; metrics and measurements and software quality prediction.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402183,no
Software Reliability Modeling with Test Coverage: Experimentation and Measurement with A Fault-Tolerant Software Project,2007,"As the key factor in software quality, software reliability quantifies software failures. Traditional software reliability growth models use the execution time during testing for reliability estimation. Although testing time is an important factor in reliability, it is likely that the prediction accuracy of such models can be further improved by adding other parameters which affect the final software quality. Meanwhile, in software testing, test coverage has been regarded as an indicator for testing completeness and effectiveness in the literature. In this paper, we propose a novel method to integrate time and test coverage measurements together to predict the reliability. The key idea is that failure detection is not only related to the time that the software experiences under testing, but also to what fraction of the code has been executed by the testing. This is the first time that execution time and test coverage are incorporated together into one single mathematical form to estimate the reliability achieved. We further extend this method to predict the reliability of fault- tolerant software systems. The experimental results with multi-version software show that our reliability model achieves a substantial estimation improvement compared with existing reliability models.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402193,no
Requirement Error Abstraction and Classification: A Control Group Replicated Study,2007,"This paper is the second in a series of empirical studies about requirement error abstraction and classification as a quality improvement approach. The Requirement error abstraction and classification method supports the developers' effort in efficiently identifying the root cause of requirements faults. By uncovering the source of faults, the developers can locate and remove additional related faults that may have been overlooked, thereby improving the quality and reliability of the resulting system. This study is a replication of an earlier study that adds a control group to address a major validity threat. The approach studied includes a process for abstracting errors from faults and provides a requirement error taxonomy for organizing those errors. A unique aspect of this work is the use of research from human cognition to improve the process. The results of the replication are presented and compared with the results from the original study. Overall, the results from this study indicate that the error abstraction and classification approach improves the effectiveness and efficiency of inspectors. The requirement error taxonomy is viewed favorably and provides useful insights into the source of faults. In addition, human cognition research is shown to be an important factor that affects the performance of the inspectors. This study also provides additional evidence to motivate further research.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402198,no
Prioritization of Regression Tests using Singular Value Decomposition with Empirical Change Records,2007,"During development and testing, changes made to a system to repair a detected fault can often inject a new fault into the code base. These injected faults may not be in the same files that were just changed, since the effects of a change in the code base can have ramifications in other parts of the system. We propose a methodology for determining the effect of a change and then prioritizing regression test cases by gathering software change records and analyzing them through singular value decomposition. This methodology generates clusters of files that historically tend to change together. Combining these clusters with test case information yields a matrix that can be multiplied by a vector representing a new system modification to create a prioritized list of test cases. We performed a post hoc case study using this technique with three minor releases of a software product at IBM. We found that our methodology suggested additional regression tests in 50% of test runs and that the highest-priority suggested test found an additional fault 60% of the time.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402199,no
Correlations between Internal Software Metrics and Software Dependability in a Large Population of Small C/C++ Programs,2007,"Software metrics are often supposed to give valuable information for the development of software. In this paper we focus on several common internal metrics: Lines of Code, number of comments, Halstead Volume and McCabe's Cyclomatic Complexity. We try to find relations between these internal software metrics and metrics of software dependability: Probability of Failure on Demand and number of defects. The research is done using 59 specifications from a programming competition---The Online Judge--on the internet. Each specification provides us between 111 and 11,495programs for our analysis; the total number of programs used is 71,917. We excluded those programs that consist of a look-up table. The results for the Online Judge programs are: (1) there is a very strong correlation between Lines of Code and Hal- stead Volume; (2) there is an even stronger correlation between Lines of Code and McCabe's Cyclomatic Complexity; (3) none of the internal software metrics makes it possible to discern correct programs from incorrect ones; (4) given a specification, there is no correlation between any of the internal software metrics and the software dependability metrics.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402211,no
Using In-Process Testing Metrics to Estimate Post-Release Field Quality,2007,"In industrial practice, information on the software field quality of a product is available too late in the software lifecycle to guide affordable corrective action. An important step towards remediation of this problem lies in the ability to provide an early estimation of post-release field quality. This paper evaluates the Software Testing and Reliability Early Warning for Java (STREW-J) metric suite leveraging the software testing effort to predict post-release field quality early in the software development phases. The metric suite is applicable for software products implemented in Java for which an extensive suite of automated unit test cases are incrementally created as development proceeds. We validated the prediction model using the STREW-J metrics via a two-phase case study approach which involved 27 medium-sized open source projects, and five industrial projects. The error in estimation and the sensitivity of the predictions indicate the STREW-J metric suite can be used effectively to predict post-release software field quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402212,no
Data Mining Techniques for Building Fault-proneness Models in Telecom Java Software,2007,"This paper describes a study performed in an industrial setting that attempts to build predictive models to identify parts of a Java system with a high fault probability. The system under consideration is constantly evolving as several releases a year are shipped to customers. Developers usually have limited resources for their testing and inspections and would like to be able to devote extra resources to faulty system parts. The main research focus of this paper is two-fold: (1) use and compare many data mining and machine learning techniques to build fault-proneness models based mostly on source code measures and change/fault history data, and (2) demonstrate that the usual classification evaluation criteria based on confusion matrices may not be fully appropriate to compare and evaluate models.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402213,no
Predicting Subsystem Failures using Dependency Graph Complexities,2007,"In any software project, developers need to be aware of existing dependencies and how they affect their system. We investigated the architecture and dependencies of Windows Server 2003 to show how to use the complexity of a subsystem's dependency graph to predict the number of failures at statistically significant levels. Such estimations can help to allocate software quality resources to the parts of a product that need it most, and as early as possible.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402214,no
Development of an Online Real Time Web Accessible Low-Voltage Switchgear Arcing Fault Early Warning System,2007,"This paper discusses the design and development aspects associated with hardware and software of a Web accessible online real time low-voltage switchgear arcing fault early warning system based on NI C series I/O Modules and NI cRIO-9004 controller. Based on the higher-order harmonic differential approach, the switchgear arcing fault early warning system was implemented. The NI cRIO-9004 controller with IP addressable and remote access capabilities provides options for comprehensive fault recording and diagnostic capabilities. Dedicated test-bench was established in the laboratory environment to validate the algorithm and test the performance of this newly developed system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402280,no
A Structural Complexity Metric for Software Components,2007,"At present, the number of components increases largely, and component-based software development (CBSD) is becoming a new effective software development paradigm, how to measure their reliability, maintainability and complexity attracts more and more attentions. This paper presents a metric to assess the structural complexity of components. Moreover it proves that the metric satisfies some good properties.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402665,no
An AOP-based Performance Evaluation Framework for UML Models,2007,"Performance is a key aspect of non-functional software requirements. While performance cross-cuts much of the software functionality, it is frequently difficult to express in traditional software development representations. In this paper we propose a framework for evaluating software performance using aspect-oriented programming (AOP) and examine its strengths and limitations. The framework provides a mechanism for supporting software performance evaluation prior to final software release. AOP is a promising software engineering technique for expressing cross-cutting characteristics of software systems. We consider software performance as a cross-cutting concern since it is not confined only a few modules, but often spread over multiple functional and non-functional elements. A key strength of our framework is the use of a code instrumentation mechanism of AOP - AspectJ code for performance analysis is separated from Java code for functional requirements. Java code is executable regardless of Aspect J code and can be woven together with AspectJ code when performance is evaluated. Our performance evaluation modules, written in AspectJ are semi-automatically or automatically generated from the UML [1] models with annotated performance profiles. The AspectJ code generator facilitates performance evaluation by allowing performance requirements that have been specified in UML models to be analyzed. The UML diagrams can then be improved by reflecting the feedback from the results of the performance analysis.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402781,no
One in a baker's dozen: debugging debugging,2007,"In the work of Voas (1993), they outlined 13 major software engineering issues needing further research: (1) what is software quality? (2) what are the economic benefits behind existing software engineering techniques?, (3) does process improvement matter?, (4) can you trust software metrics and measurement?, (5) why are software engineering standards confusing and hard to comply with, (6) are standards interoperable, (7) how to decommission software?, (8) where are reasonable testing and debugging stoppage criteria?, (9) why are COTS components so difficult to compose?, (10) why are reliability measurement and operational profile elicitation viewed suspiciously, (11) can we design in the ""ilities"" both technically and economically, (12) how do we handle the liability issues surrounding certification, and (13) is intelligence and autonomic computing feasible? This paper focuses on a simple and easy to understand metric that addresses the eighth issue, a testing and debugging testing stoppage criteria based on expected probability of failure graphs.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4404729,no
"Scalable, Adaptive, Time-Bounded Node Failure Detection",2007,"This paper presents a scalable, adaptive and time-bounded general approach to assure reliable, real-time node-failure detection (NFD) for large-scale, high load networks comprised of commercial off-the-shelf (COTS) hardware and software. Nodes in the network are independent processors which may unpredictably fail either temporarily or permanently. We present a generalizable, multilayer, dynamically adaptive monitoring approach to NFD where a small, designated subset of the nodes are communicated information about node failures. This subset of nodes are notified of node failures in the network within an interval of time after the failures. Except under conditions of massive system failure, the NFD system has a zero false negative rate (failures are always detected with in a finite amount of time after failure) by design. The NFD system continually adjusts to decrease the false alarm rate as false alarms are detected. The NFD design utilizes nodes that transmit, within a given locality, ""heartbeat"" messages to indicate that the node is still alive. We intend for the NFD system to be deployed on nodes using commodity (i.e. not hard-real-time) operating systems that do not provide strict guarantees on the scheduling of the NFD processes. We show through experimental deployments of the design, the variations in the scheduling of heartbeat messages can cause large variations in the false-positive notification behavior of the NFD subsystem. We present a per-node adaptive enhancement of the NFD subsystem that dynamically adapts to provide run-time assurance of low false-alarm rates with respect to past observations of heartbeat scheduling variations while providing finite node-failure detection delays. We show through experimentation that this NFD subsystem is highly scalable and uses low resource overhead.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4404740,no
Behavioral Fault Modeling for Model-based Safety Analysis,2007,"Recent work in the area of model-based safety analysis has demonstrated key advantages of this methodology over traditional approaches, for example, the capability of automatic generation of safety artifacts. Since safety analysis requires knowledge of the component faults and failure modes, one also needs to formalize and incorporate the system fault behavior into the nominal system model. Fault behaviors typically tend to be quite varied and complex, and incorporating them directly into the nominal system model can clutter it severely. This manual process is error-prone and also makes model evolution difficult. These issues can be resolved by separating the fault behavior from the nominal system model in the form of a ""fault model"", and providing a mechanism for automatically combining the two for analysis. Towards implementing this approach we identify key requirements for a flexible behavioral fault modeling notation. We formalize it as a domain-specific language based on Lustre, a textual synchronous dataflow language. The fault modeling extensions are designed to be amenable for automatic composition into the nominal system model.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4404742,no
Combining Software Quality Analysis with Dynamic Event/Fault Trees for High Assurance Systems Engineering,2007,"We present a novel approach for probabilistic risk assessment (PRA) of systems which require high assurance that they will function as intended. Our approach uses a new model i.e., a dynamic event/fault tree (DEFT) as a graphical and logical method to reason about and identify dependencies between system components, software components, failure events and system outcome modes. The method also explicitly includes software in the analysis and quantifies the contribution of the software components to overall system risk/ reliability. The latter is performed via software quality analysis (SQA) where we use a Bayesian network (BN) model that includes diverse sources of evidence about fault introduction into software; specifically, information from the software development process and product metrics. We illustrate our approach by applying it to the propulsion system of the miniature autonomous extravehicular robotic camera (mini-AERCam). The software component considered for the analysis is the related guidance, navigation and control (GN&C) component. The results of SQA indicate a close correspondence between the BN model estimates and the developer estimates of software defect content. These results are then used in an existing theory of worst-case reliability to quantify the basic event probability of the software component in the DEFT.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4404747,no
Analytic Model for Web Anomalies Classification,2007,"In this paper, an analytic technique is proposed to improve the dynamic Web application quality and reliability. The technique integrates orthogonal defect classification (ODC), and Markov chain to classify as well as analyze the collective view of Web errors. The error collective view will be built from access logs and defect data. This classification technique will enable viewing the Web errors in page, path, and application contexts. This technique will help in developing reliable Web applications that benefit from the understanding of Web anomalies and past issues. The preliminary results of applying this approach to a case study from telecommunications industry are included to demonstrate its' viability.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4404772,no
Parsimonious classifiers for software quality assessment,2007,"Modeling to predict fault-proneness of software modules is an important area of research in software engineering. Most such models employ a large number of basic and derived metrics as predictors. This paper presents modeling results based on only two metrics, lines of code and cyclomatic complexity, using radial basis functions with Gaussian kernels as classifiers. Results from two NASA systems are presented and analyzed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4404779,no
Conquering Complexity,2007,"In safety-critical systems, the potential impact of each separate failure is normally studied in detail and remedied by adding backups. Failure combinations, though, are rarely studied exhaustively; there are just too many of them, and most have a low probability of occurrence. Defect detection in software development is usually understood to be a best effort at rigorous testing just before deployment. But defects can be introduced in all phases of software design, not just in the final coding phase. Defect detection therefore shouldn't be limited to the end of the process, but practiced from the very beginning. In a rigorous model-based engineering process, each phase is based on the construction of verifiable models that capture the main decisions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4404823,no
EEG Spike Detection Based on Qualitative Modeling of Visual Observation,2007,"An EEG spike wave detection method based on qualitative modeling of visual observation is proposed in this paper. The method, based on qualitative measurement of sharpness degree of waves at spike wave frequencies, constructs a qualitative description model of visual observation to discriminate spike waves from none spike waves. The application of this method is illustrated with an example. And the corresponding result shows that this method is effective and direct to a certain extent.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4406175,no
The effect on tracking loop performance when using the efficient search method for fast fine frequency acquisition,2007,"When implementing real-time software based GNSS receiver, the accurate and fast signal acquisition and tracking method is more necessary than the hardware based receiver owing to lots of computation time. Generally, in the software based GNSS receiver, parallel code phase signal acquisition method using FFT has been used. But it has the defect of the limited frequency resolution according to input data's length. Considering the carrier frequency resolution must be about tens of Hz in order to perform the tracking loop correctly, the fine frequency acquisition method to minimize the computation time should be necessary to overcome this limitation. Also the examination whether the method will enhance the tracking loop's performance and cause the reduction of computation time should be necessary. To evaluate these necessities, in this paper, the efficient search method for fine frequency acquisition will be suggested and tested. Furthermore, the effect on signal tracking loop, when the result of signal acquisition obtained by the suggested method is applied, will be analyzed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4406964,no
Robustness of a Spoken Dialogue Interface for a Personal Assistant,2007,"Although speech recognition systems have become more reliable in recent years, they are still highly error-prone. Other components of a spoken language dialogue system must then be robust enough to handle these errors effectively, to avoid recognition errors from adversely affecting the overall performance of the system. In this paper, we present the results of a study focusing on the robustness of our agent-based dialogue management approach. We found that while the speech recognition software produced serious errors, the dialogue manager was generally able to respond reasonably to users' utterances.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4407272,no
"Negotiation Dynamics: Analysis, Concession Tactics, and Outcomes",2007,"Given that a negotiation outcome is determined to a large extent by the successive offers exchanged by negotiating agents, it is useful to analyze dynamic patterns of the bidding, what Raiffa calls the ""negotiation dance"". Patterns in such exchanges may provide additional insight into the strategies used by the agents. The current practice of evaluating a negotiation strategy, however, is to primarily focus on fairness and quality aspects of the agreement. There is a lack of tools and methods that facilitate a precise analysis of the negotiation dynamics. To fill this gap, this paper introduces a method for analysis based on a classification of negotiation steps. The method provides the basic tools to perform a detailed and quantified analysis of a negotiation between two agents in terms of dynamic properties of the negotiation trace. The method can be applied to well-designed tournaments, but can also be used to analyze single 1-on-l negotiation. Example findings of applying the method to analyze the ABMP and Trade-Off strategies show that sensitivity to the preferences of the opponent is independent, respectively dependent, on a correct model of that opponent. Furthermore, the results illustrate that having domain knowledge is not always enough to avoid making unintentional steps.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4407323,no
"Software-Based Online Detection of Hardware Defects Mechanisms, Architectural Support, and Evaluation",2007,"As silicon process technology scales deeper into the nanometer regime, hardware defects are becoming more common. Such defects are bound to hinder the correct operation of future processor systems, unless new online techniques become available to detect and to tolerate them while preserving the integrity of software applications running on the system. This paper proposes a new, software-based, defect detection and diagnosis technique. We introduce a novel set of instructions, called access-control extension (ACE), that can access and control the microprocessor's internal state. Special firmware periodically suspends microprocessor execution and uses the ACE instructions to run directed tests on the hardware. When a hardware defect is present, these tests can diagnose and locate it, and then activate system repair through resource reconfiguration. The software nature of our framework makes it flexible: testing techniques can be modified/upgraded in the field to trade off performance with reliability without requiring any change to the hardware. We evaluated our technique on a commercial chip-multiprocessor based on Sun's Niagara and found that it can provide very high coverage, with 99.22% of all silicon defects detected. Moreover, our results show that the average performance overhead of software-based testing is only 5.5%. Based on a detailed RTL-level implementation of our technique, we find its area overhead to be quite modest, with only a 5.8% increase in total chip area.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4408248,no
Mutable Protection Domains: Towards a Component-Based System for Dependable and Predictable Computing,2007,"The increasing complexity of software poses significant challenges for real-time and embedded systems beyond those based purely on timeliness. With embedded systems and applications running on everything from mobile phones, PDAs, to automobiles, aircraft and beyond, an emerging challenge is to ensure both the functional and timing correctness of complex software. We argue that static analysis of software is insufficient to verify the safety of all possible control flow interactions. Likewise, a static system structure upon which software can be isolated in separate protection domains, thereby defining immutable boundaries between system and application-level code, is too inflexible to the challenges faced by real-time applications with explicit timing requirements. This paper, therefore, investigates a concept called ""mutable protection domains"" that supports the notion of hardware-adaptable isolation boundaries between software components. In this way, a system can be dynamically reconfigured to maximize software fault isolation, increasing dependability, while guaranteeing various tasks are executed according to specific time constraints. Using a series of simulations on multidimensional, multiple-choice knapsack problems, we show how various heuristics compare in their ability to rapidly reorganize the fault isolation boundaries of a component- based system, to ensure resource constraints while simultaneously maximizing isolation benefit. Our ssh oneshot algorithm offers a promising approach to address system dynamics, including changing component invocation patterns, changing execution times, and mispredictions in isolation costs due to factors such as caching.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4408320,no
A Discrete Differential Operator for Direction-based Surface Morphometry,2007,"This paper presents a novel directional morphometry method for surfaces using first order derivatives. Non-directional surface morphometry has been previously used to detect regions of cortical atrophy using brain MRI data. However, evaluating directional changes on surfaces requires computing gradients to obtain a full metric tensor. Non-directionality reduces the sensitivity of deformation-based morphometry to area-preserving deformations. By proposing a method to compute directional derivatives, this paper enables analysis of directional deformations on surfaces. Moreover, the proposed method exhibits improved numerical accuracy when evaluating mean curvature, compared to the so-called cotangent formula. The directional deformation of folding patterns was measured in two groups of surfaces and the proposed methodology allowed to defect morphological differences that were not detected using previous non-directional morphometry. The methodology uses a closed-form analytic formalism rather than numerical approximation and is readily generalizable to any application involving surface deformation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4408886,no
9D-6 Signal Analysis in Scanning Acoustic Microscopy for Non-Destructive Assessment of Connective Defects in Flip-Chip BGA Devices,2007,"Failure analysis in industrial applications often require methods working non-destructively for allowing a variety of tests at a single device. Scanning acoustic microscopy in the frequency range above 100 MHz provides high axial and lateral resolution, a moderate penetration depth and the required non-destructivity. The goal of this work was the development of a method for detecting and evaluating connective defects in densely integrated flip-chip ball grid array (BGA) devices. A major concern was the ability to automatically detect and differentiate the ball-connections from the surrounding underfill and the derivation of a binary classification between void and intact connection. Flip chip ball grid arrays with a 750 mum silicon layer on top of the BGA were investigated using time resolved scanning acoustic microscopy. The microscope used was an Evolution II (SAM TEC, Aalen, Germany) in combination with a 230 MHz transducer. Short acoustic pulses were emitted into the silicon through an 8 mm liquid layer. In receive mode reflected signals were recorded, digitized and stored at the SAM's internal hard drive. The off-line signal analysis was performed using custom-made MATLAB (The Mathworks, Natick, USA) software. The sequentially working analysis characterized echo signals by pulse separation to determine the positions of BGA connectors. Time signals originated at the connector interface were then investigated by wavelet- (WVA) and pulse separation analysis (PSA). Additionally the backscattered amplitude integral (BAI) was estimated. For verification purposes defects were evaluated by X-ray- and scanning electron microscopy (SEM). It was observed that ball connectors containing cracks seen in the SEM images show decreased values of wavelet coefficients (WVC). However, the relative distribution was broader compared to intact connectors. It was found that the separation of pulses originated at the entrance and exit of the ball array corresponded to the condition of- the connector. The success rate of the acoustic method in detecting voids was 96.8%, as verified by SEM images. Defects revealed by the acoustic analysis and confirmed by SEM could be detected by X-ray microscopy only in 64% of the analysed cases. The combined analyses enabled a reliable and non destructive detection of defect ball-grid array connectors. The performance of the automatically working acoustical method seemed superior to X-ray microscopy in detecting defect ball connectors.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4409782,no
P3B-2 Clutter From Multiple Scattering and Aberration in a Nonlinear Medium,2007,"Aberration, clutter, and reverberation degrade the quality of ultrasonic images. When an acoustic pulse propagates through tissue these effects occur simultaneously and it is difficult to obtain independent estimates for the precise source of the point spread function broadening. The purpose of this paper is to characterize the sources of clutter and reverberation with a simulation of ultrasonic propagation through the abdomen. A full-wave equation that describes nonlinear propagation in a heterogeneous attenuating medium is solved numerically with finite differences in the time domain (FDTD). Three dimensional solutions of the equation are verified with water tank measurements of a commercial diagnostic ultrasound transducer and are shown to be in excellent agreement in terms of the fundamental and harmonic acoustic fields, and the power spectrum at the focus. The linear and nonlinear components of the algorithm are also verified independently. In the linear non-attenuating regime solutions match results from Field II, a well established software package used in transducer modeling, to within 0.3 dB. In addition to thermoviscous attenuation we present a numerical solution of the relaxation attenuation laws that allows modeling of arbitrary frequency dependent attenuation, such as that observed in tissue. A perfectly matched layer (PML) is implemented at the boundaries with a novel numerical implementation that allows the PML to be used with high order discretizations. A -78 dB reduction in the reflected amplitude is demonstrated. The numerical algorithm is used to simulate a focused ultrasonic pulse propagating through a histologically determined representation of the human abdomen. An ultrasound image is created in silicon using the same physical and algorithmic process used in an ultrasound scanner: a series of pulses are transmitted through heterogeneous scattering tissue and the received echoes are used in a delay-and-sum beamforming algorithm to generate a ima- ges. The resulting harmonic image exhibits characteristic improvement in lesion boundary definition and contrast when compared to the fundamental image. We demonstrate a mechanism of harmonic image quality improvement by showing that the harmonic point spread function is less sensitive to reverberation clutter.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4410010,no
Evolving Conditional Value Sets of Cost Factors for Estimating Software Development Effort,2007,"The software cost estimation process is one of the most critical managerial activities related to project planning, resource allocation and control. As software development is a highly dynamic procedure, the difficulty of providing accurate cost estimations tends to increase with development complexity. The inherent problems of the estimation process stem from its dependence on several complex variables, whose values are often imprecise, unknown, or incomplete, and their interrelationships are not easy to comprehend. Current software cost estimation models do not inspire enough confidence and accuracy with their predictions. This is mainly due to the models' sensitivity to project data values, and this problem is amplified because of the vast variances found in historical project attribute data. This paper aspires to provide a framework for evolving value ranges for cost attributes and attaining mean effort values using the Al-oriented problem-solving approach of genetic algorithms, with a twofold aim. Firstly, to provide effort estimations by analogy to the projects classified in the evolved ranges and secondly, to identify any present correlations between effort and cost attributes.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4410279,no
Cooperative mixed strategy for service selection in service oriented architecture,2007,"In service oriented architecture (SOA), service brokers could find many service providers which offer same function with different quality of service (QoS). Under this condition, users may encounter difficulty to decide how to choose from the candidates to obtain optimal service quality. This paper tackles the service selection problem (SSP) of time-sensitive services using the theory of games creatively. Pure strategies proposed by current studies are proved to be improper to this problem because the decision conflicts among the users result in poor performance. A novel cooperative mixed strategy (CMS) with good computability is developed in this paper to solve such inconstant-sum non-cooperative n-person dynamic game. Unlike related researches, CMS offers users an optimized probability mass function instead of a deterministic decision to select a proper provider from the candidates. Therefore it is able to eliminate the fluctuation of queue length, and raise the overall performance of SOA significantly. Furthermore, the stability and equilibrium of CMS are proved by simulations.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4413993,no
A real-time implementation of chaotic contour tracing and filling of video objects on reconfigurable hardware,2007,"This paper proposes a real-time, robust, scalable and compact field programmable gate array (FPGA) based architecture and its implementation of contour tracing and filling of video objects. Achieving real-time performance on general purpose sequential processors is difficult due to the heavy computational complexity in contour tracing and filling, thus a hardware acceleration is inevitable. Our finding to the existing related work confirms that the proposed architecture is much more feasible, cost effective and features important algorithmic-specific qualities, including deleting dead contour branches and removing noisy contours, which are required in many video processing applications. Moreover, performance analysis shows that our hardware approach achieves an order of magnitude performance improvement over the existing pure software-based implementations. Our implementation attained an optimum processing clock of 156 MHz while utilizing minimal hardware resources and power. The proposed FPGA design was successfully simulated, synthesized and verified for its functionality, accuracy and performance on an actual hardware platform which consists of a frame grabber with a user programmable Xilinx Virtex-4 SX35 FPGA.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4414062,no
A Distributed Algorithm for Service Commitment in Allocating Services to Applications,2007,"In this paper, we extend our previous work on committing service instances to the applications in a service-oriented environment, and we propose a distributed heuristic algorithm which is able to estimate the number of future service instances needed by each application instance in a future time. Also, this algorithm does not assume any specific type of distribution function for services execution time and applications interarrival time. In this paper, after presenting the mathematical basis for the proposed distributed algorithm, we explain how this algorithm can be implemented in a distributed environment, and through the simulations and performance comparisons, we show that the proposed algorithm improves the performance of the system significantly, compared to a No commitment policy system and a full commitment policy system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4414493,no
Implementation and Performance Evaluation of an Adaptable Failure Detector for Distributed System,2007,"Unreliable failure detectors have been an important abstraction to build dependable distributed applications over asynchronous distributed systems subject to faults. Their implementations are commonly based on timeouts to ensure algorithm termination. However, for systems built on the Internet, it is hard to estimate this time value due to traffic variations. In order to increase the performance, self-tuned failure detectors dynamically adapt their timeouts to the communication delay behavior added of a safety margin. In this paper, we propose a new implementation of a failure detector. This implementation is a variant of the heartbeat failure detector which is adaptable and can support scalable applications. In this implementation we dissociate two aspects: a basic estimation of the expected arrival date to provide a short detection time, and an adaptation of the quality of service. The latter is based on two principles: an adaptation layer and a heuristic to adapt the sending period of ""I am alive"" messages.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4415345,no
Research of Software Reliability Based on Synthetic Architecture,2007,"In this paper we introduce some typical architecture-based software reliability model for software architecture reliability estimation, and modify model of software reliability estimation so that to improve precision of estimating software architecture reliability. Our approach is based on Markov chain properties and software architecture. At the same time, we propose the method of how to use the modified model to make a reliability estimation in a synthetic architecture so that to expand the application domain of this model. In addition, we conduct an experiment on to validate this method..",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4415453,no
UML-based safety analysis of distributed automation systems,2007,"HAZOP (hazard and operability) studies are carried out to analyse complex automated systems, especially large and distributed automated systems. The aim is to systematically assess the automated system regarding possibly negative effects of deviations from standard operation on safety and performance. Today, HAZOP studies require significant manual effort and tedious work of several costly experts. The authors of this paper propose a knowledge-based approach to support the HAZOP analysis and to reduce the required manual effort. The main ideas are (1) to incorporate knowledge about typical problems in automation systems, in combination with their causes and their effects, in a rule base, and (2) to apply this rule base by means of a rule engine on the description of the automated system under consideration. This yields a list of possible dangers regarding safety risks and performance reductions. These results can be used by the automation experts to improve the system's design. Within this paper, the general approach is presented, and an example application is dealt with where the system design is given in the form of a UML class diagram, and the HAZOP study is focused on hazards caused by faulty communication within the distributed system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4416901,no
Neural network controlled voltage disturbance detector and output voltage regulator for Dynamic Voltage Restorer,2007,"This paper describes the high power DVR (Dynamic Voltage Restorer) with the neural network controlled voltage disturbance detector and output voltage regulator. Two essential parts of DVR control are how to detect the voltage disturbance such as voltage sag and how to compensate it as fast as possible respectively. The new voltage disturbance detector was implemented by using the delta rule of the neural network control. Through the proposed method, we can instantaneously track the amplitude of each phase voltage under the severe unbalanced voltage conditions. Compared to the conventional synchronous reference frame method, the proposed one shows the minimum time delay to determine the instance of the voltage disturbance event. Also a modified d-q transformed voltage regulator for single phase inverter was adopted to obtain the fast dynamic response and the robustness, where three independent single phase inverters are controlled by using the amplitude of source voltage obtained by neural network controller. By using the proposed voltage regulator, the voltage disturbance such as voltage sag can be compensated quickly to the nominal voltage level. The proposed disturbance detector and the voltage regulator were applied to the high power DVR (1000 kV A@440 V) that was developed for the application of semiconductor manufacture plant. The performances of the proposed DVR control were verified through computer simulation and experimental results. Finally, conclusions are given.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4417587,no
Fast and reliable plagiarism detection system,2007,"Plagiarism and similarity detection software is well-known in universities for years. Despite the variety of methods and approaches used in plagiarism detection, the typical trade-off between the speed and the reliability of the algorithm still remains. We introduce a new two-step approach to plagiarism detection that combines high algorithmic performance and the quality of pairwise file comparison. Our system uses fast detection method to select suspicious files only, and then invokes precise (and slower) algorithms to get reliable results. We show that the proposed method does not noticeably reduce the quality of the pairwise comparison mechanism while providing better speed characteristics.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4417860,no
An Approach for Assessment of Reliability of the System Using Use Case Model,2007,"Existing approaches on reliability assessment of the system entirely depends on expertise, knowledge of system analysts and computation of usage probability of different user operations. Existing approach is therefore can not be taken as accurate, particularly to deal with new or unfamiliar systems. Further modern systems are very large and complex to manipulate without any automation. Addressing these issues, we propose an analytical technique in our work. In this paper, we propose a novel approach to assess reliability of the system using use case model. We consider a metric to assess the reliability of a system under development.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4418306,no
Assessing tram schedules using a library of simulation components,2007,"Assessing tram schedules is important to assure an efficient use of infrastructure and for the provision of a good quality service. Most existing infrastructure modeling tools provide support to assess an individual aspect of rail systems in isolation, and do not provide enough flexibility to assess many aspects that influence system performance at once. We propose a library of simulation components that enable rail designers to assess different system configurations. In this paper we show how we implemented some basic safety measures used in rail systems such as: reaction to control objects (e.g. traffic lights), priority rules, and block safety systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4419815,no
Discrete event simulation modeling of resource planning and service order execution for service businesses,2007,"In this paper, we present a framework for developing discrete-event simulation models for resource-intensive service businesses. The models simulate interactions of activities of demand planning of service engagements, supply planning of human resources, attrition of resources, termination of resources and execution of service orders to estimate business performance of resource-intensive service businesses. The models estimate serviceability, costs, revenue, profit and quality of service businesses. The models are also used in evaluating effectiveness of various resource management analytics and policies. The framework is aided by an information meta-model, which componentizes modeling objects of service businesses and allows effective integration of the components.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4419858,no
Transactional Memory Execution for Parallel Multithread Programming without Lock,2007,"With the increasing popularity of shared-memory programming model, especially at the advent of multicore processors, applications need to become more concurrent to take advantage of the increased computational power provided by chip level multiprocessing. Traditionally locks are used to enforce data dependence and timing constraints between the various threads. However locks are error- prone, and often leading to unwanted race conditions, priority inversion, or deadlock. Therefore, recent waves of research projects are exploring transaction memory systems as an alternative synchronization mechanism to locks. This paper presents a software transactional memory execution model for parallel multithread programming without lock.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4420173,no
GLIMS: Progress in mapping the worldâ€™s glaciers,2007,"The global land ice measurement from space (GLIMS) project is a cooperative effort of over sixty institutions world-wide with the goal of inventorying a majority of the world's estimated 160000 glaciers. Data sent from regional center analysts to the GLIMS team at the national snow and ice data center (NSIDC) in Boulder, Colorado are inserted into a geospatial database. The GLIMS Glacier Database now contains outlines of over 58 000 glaciers. As submissions to the database from all over the world increase, we find that we must accommodate a greater diversity in the character and quality of the data submitted than was originally anticipated. We present an overview of the current glacier outline inventory, and examine issues related to data coverage, and data quality. A significant achievement of the GLIMS project is that the database and interfaces to it, as well as the GLIMSView tool, provided to help in the production of GLIMS analyses, are all built from open source software. Issues we've dealt with to achieve a high-quality glacier database include: 1. data submitted without proper metadata; 2. data submitted with incorrect georegistration; 3. varying analyst interpretations of what exactly constitutes a glacier; 4. arbitrary termination of glacier boundaries at political boundaries; 5. arbitrary termination of glacier boundaries at edges of available satellite images.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4423723,no
Adaptive digital synchronization of measuring window in low-cost DSP power quality measurement systems,2007,"The problems with the power quality encountered in cases of low voltage hospital power networks are increasingly common. It is due to the fact that there are usually a few power networks, backup power sources and a wild range of appliances. That situation may cause disturbances in power network and decrease the quality of electric energy. The problem is significant because power supply networks in hospitals provide electricity for very sensitive and expensive measuring apparatus which guards patients' life. These arguments led to creating relatively cheap systems of evaluating energy quality and disturbance detection. It presents not only hardware but also software optimization of the method of measuring the quality of electric energy which meets the basic standards (EN 50160, EN 61000-4- 7, EN 1000-4-30). The article justifies the usage of modern floating-point digital signal processor TMS320C6713 as a computing unit. It is a low budget alternative which can be used in other similar cases. Presented hardware selection has other additional specific features suitable for applying in hospital power supply network. The other part of the paper presents the algorithm of software which was implemented on DSP-system. It includes adaptive digital synchronization of the measuring window with power frequency. Also the algorithm for digital phase locked loop was presented.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4424097,no
"Multiple signal processing techniques based power quality disturbance detection, classification, and diagnostic software",2007,"This work presents the development steps of the software PQMON, which targets power quality analysis applications. The software detects and classifies electric system disturbances. Furthermore, it also makes diagnostics about what is causing such disturbances and suggests line of actions to mitigate them. Among the disturbances that can be detected and analyzed by this software are: harmonics, sag, swell and transients. PQMON is based on multiple signal processing techniques. Wavelet transform is used to detect the occurrence of the disturbances. The techniques used to do such feature extraction are: fast Fourier transform, discrete Fourier transform, periodogram, and statistics. Adaptive artificial neural network is also used due to its robustness in extracting features such as fundamental frequency and harmonic amplitudes. The probable causes of the disturbances are contained in a database, and their association to each disturbance is made through a cause-effect relationship algorithm, which is used to diagnose. The software also allows the users to include information about the equipments installed in the system under analysis, resulting in the direct nomination of any installed equipment during the diagnostic phase. In order to prove the effectiveness of software, simulated and real signals were analyzed by PQMON showing its excellent performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4424176,no
Visualization for Software Evolution Based on Logical Coupling and Module Coupling,2007,"In large scale software projects, developers make much software during long term. The source codes of software are frequently revised in the projects. The source codes evolve to become complex. Measurements of software complexity have been proposed, such as module coupling and logical coupling. In the case of the module coupling, even if developers copy pieces of source codes to a new module, the module coupling can not detect relationship between the pieces of the source codes although the pieces of the two modules have strong coupling. On the other hand, in the logical coupling, if two modules are accidentally revised at same time by a same developer, the logical coupling will judge strong coupling between the two modules although the modules have no relation. Therefore, we proposed a visualization technique and software complexity metrics for software evolution. A basic idea is that modules including strong module coupling should have strong logical coupling. If a gap between a set of modules including strong module couplings and a set of modules including strong logical couplings is large, the software complexity will be large. In addition, our visualization technique helps developers understand changes of software complexity. As a result of experiments in open source projects, we confirmed that the proposed metrics and visualization technique were able to detect high risky project with many bugs.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4425857,no
A Large-Scale Empirical Comparison of Object-Oriented Cohesion Metrics,2007,"Cohesion is an attribute of software design quality for which many metrics have been proposed. The different proposals have been made largely on theoretical grounds, with little evidence of actual use. This makes it difficult to provide advice to software developers as to how to interpret the measurements any given metric produces. This paper presents the first large-scale empirical study of object- oriented cohesion metrics. We apply 16 metrics from the literature, as well as a number of variations, to 92 open source and industry Java applications ranging in size from a few classes to several thousand, over 100,000 classes in all. Our results show that by and large applications have similar distributions of measurements according to any given metric, but that the distributions can be quite different across metrics. This provides useful information for the ongoing empirical validation efforts for cohesion metrics.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4425882,no
Improving Effort Estimation Accuracy by Weighted Grey Relational Analysis During Software Development,2007,"Grey relational analysis (GRA), a similarity-based method, presents acceptable prediction performance in software effort estimation. However, we found that conventional GRA methods only consider non-weighted conditions while predicting effort. Essentially, each feature of a project may have a different degree of relevance in the process of comparing similarity. In this paper, we propose six weighted methods, namely, non-weight, distance-based weight, correlative weight, linear weight, nonlinear weight, and maximal weight, to be integrated into GRA. Three public datasets are used to evaluate the accuracy of the weighted GRA methods. Experimental results show that the weighted GRA performs better precision than the non-weighted GRA. Specifically, the linearly weighted GRA greatly improves accuracy compared with the other weighted methods. To sum up, the weighted GRA not only can improve the accuracy of prediction but is an alternative method to be applied to software development life cycle.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4425897,no
A Novel Approach of Prioritizing Use Case Scenarios,2007,"Modern softwares are very large and complex. As the size and complexity of software increases, software developers feel an urgent need for a better management of different activities during the course of software development. In this paper, we present an approach of use case scenario prioritization suitable for project planning at an early phase of the software development. We consider only use case model in our work. For prioritization, we focus on how critical a scenario path is, which essentially depends on density of overlapping of sub path of a scenario path with other scenario path(s) of a use case. Our proposed approach provides an analytical solution on use case scenario prioritization and is very much effective in project management related activities as substantiated by our experimental results.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4425898,no
A Model-Driven Simulation for Performance Evaluation of 1xEV-DO,2007,"Finding an appropriate approach to evaluate the capacity of a radio access technology with respect to a wide range of parameters (radio signal quality, quality of service, user mobility, network resources) has become increasingly important in today's wireless network planning. In this paper, we propose a model- based simulation to assess the capabilities of the IxEV- DO radio access technology. Results are expressed in terms of throughputs and signal quality (C/I and Ec/Io) for the requested services and applications.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4426113,no
Extended Model Design for Quality Factor Based Web Service Management,2007,"Recently web services has played a key role in implementing enterprise-level applications based on Service Oriented Architecture (SOA). To achieve this goal, a realistic web service meets both functional and non-functional requirements of its consumers. But currently there is no standard that is capable of accurately representing quality factors of web services. To manage web services, it needs to monitors quality of them periodically or aperiodically when changing their status like availability, performance and security policy. We present an extended web services framework based on SOA structure for providing information about quality of web services and build a prototype- WebServiceBot for applying quality factors. To build the prototype, we firstly define a service description by using annotated WSDL with quality factors. The extended web services framework is used to determine appropriate web services which meet non-functional requirements. Using the proposed framework, we can improve worst-case predictability of applications using totally uncontrollable web services.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4426288,no
Intelligent Selection of Fault Tolerance Techniques on the Grid,2007,"The emergence of computational grids has lead to an increased reliance on task schedulers that can guarantee the completion of tasks that are executed on unreliable systems. There are three common techniques for providing task-level fault tolerance on a grid: retrying, replicating, and checkpointing. While these techniques are varyingly successful at providing resilience to faults, each of them presents a tradeoff between performance and resource cost. As such, tasks having unique urgency requirements would ideally be placed using one of the techniques; for example, urgent tasks are likely to prefer the replication technique, which guarantees timely completion, whereas low priority tasks should not incur any extra resource cost in the name of fault tolerance. This paper introduces a placement and selection strategy which, by computing the utility of each fault tolerance technique in relation to a given task, finds the set of allocation options which optimizes the global utility. Heuristics which take into account the value offered by a user, the estimated resource cost, and the estimated response time of an option are presented. Simulation results show that the resulting allocations have improved fault tolerance, runtime, profit, and allow users to prioritize their tasks.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4426873,no
A Case-based Reasoning with Feature Weights Derived by BP Network,2007,"Case-based reasoning (CBR) is a methodology for problem solving and decision-making in complex and changing environments. This study investigates the performance of a hybrid case-based reasoning method that integrates a multi-layer BP neural network with case-based reasoning (CBR) algorithms for derivatives feature weights. This approach is applied to fault detection and diagnosis (FDD) system involves the examination of several criteria. The correct identification of the underlying mechanism of a fault is an important step in the entire fault analysis process. The trained BP neural network provides the basis to obtain attribute weights, whereas CBR serves as a classifier to identify the fault mechanism. Different parameters of the hybrid methods were varied to study their effect. The results indicate that better performance could be achieved by the proposed hybrid method than that using conventional CBR alone.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4426957,no
A Fast PageRank Convergence Method based on the Cluster Prediction,2007,"In recent years, search engines have already played the key roles among Web applications, and link analysis algorithms are the major methods to measure the important values of Web pages. These algorithms employ the conventional flat Web graph built by Web pages and link relations of Web pages to obtain the relative importance of Web objects. Previous researches have observed that PageRank-like link analysis algorithms have a bias against newly created Web pages. A new ranking algorithm called Page Quality was then proposed to solve this issue. Page Quality predicates future ranking values by the difference rate between the current ranking value and the previous ranking value. In this paper, we propose a new algorithm called DRank to diminish the bias of PageRank-like link analysis algorithms, and attain the better performance than Page Quality. In this algorithm, we model Web graph as a three-layer graph which includes Host Graph, Directory Graph and Page Graph by using the hierarchical structure of URLs and the structure of link relation of Web pages. We calculate the importance of Hosts, Directories and Pages by weighted graph we built and then the clustering distribution of PageRank values of pages within directories is observed. We can then predicate the more accurate values of page importance to diminish the bias of newly created pages by the clustering characteristic of PageRank. Experiment results show that DRank algorithm works well on predicating future ranking values of pages and outperform Page Quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4427158,no
A PC-Based System for Automated Iris Recognition under Open Environment,2007,"This paper presents an entirely automatic system designed to realize accurate and fast personal identification from the iris images acquired under open environment. The acquisition device detects the appearance of a user at any moment using an ultrasonic transducer, guides the user in positioning himself in the acquisition range and acquires the best iris image of the user through quality evaluation. Iris recognition is done using the bandpass characteristic of wavelets and wavelet transform principles for detecting singularities to extract iris features and adopting Hamming distance to match iris codes. The authentication service software can enroll a user's iris image into a database and perform verification of a claimed identity or identification of an unknown entity. The identification rate is high and the recognition result is available about 6 s starting at iris image acquisition. This system is promising to be used in applications requiring personal identification.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4428237,no
Fast global elimination algorithm and low-cost VLSI design for motion estimation,2007,"In this paper, an efficient fast full search motion estimation algorithm, which is accelerated by sub-sampled preprocess, is proposed to reduce the computation costs of block matching algorithm in video compressions. The proposed fast global elimination algorithm (FGEA) can skip unnecessary search positions by the preprocessing unit. We employ the preprocessing unit to calculate the sub-sampled sum of subblock absolute difference (sub-sampled SSAD) for each candidate macro-block (MB) in the search range, where the preprocessing can select a number of probable candidate motion blocks. Then we calculate the sum of absolute difference (SAD) from these selected candidate blocks and find out the final motion vector. The proposed FGEA which uses the sub-sampled skill, reduces the computation cost of preprocessing and maintains the same compensated video quality in comparison with the previous GEA, which almost provides the same quality as the well-known full search block matching algorithm (FSBMA). Software simulation results show that the proposed fast block matching motion estimation algorithm can reduce about 38% absolute difference (AD) costs and perform average 1.64 runtime faster than the previous GEA method on the basis of the same compensated video quality with the FSBMA Through VLSI implementations, the realization of the FGEA only needs 22.1 k gate counts to achieve real-time process 30 frames/sec CIF-format video sequences.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4428873,no
An embedded CPU based automatic ranging system for vehicles,2007,"Based on the structure and theory of optical-mechanical-electrical automatic ranging system of vehicles, paper has a discussion and research of the architecture and design of auto ranging system with risk estimation and decision making basing on embedded CPU. The solution of embedded system has got a good real-time performance and it can promote the data processing capability of the system. Paper mainly introduces the defects of SCM (single chip Micyoco) based ranging system and the hardware architecture, software architecture and some other key technology of the embedded system based ranging system, etc. It also introduces the solution of using two lidars light detection and ranging method.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4433700,no
On discrete event diagnosis methods for continuous systems,2007,"Fault detection and isolation is a key component of any safety-critical system. Although diagnosis methods based on discrete event systems have been recognized as a promising framework, they cannot be easily applied to systems with complex continuous dynamics. This paper presents a novel approach for discrete event system diagnosis of continuous systems based on a qualitative abstraction of the measurement deviations from the nominal behavior. We systematically derive a diagnosis model, provide diagnosability analysis, and design a diagnoser. Our results show that the proposed approach is easily applicable and can be used for online diagnosis of abrupt faults in continuous systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4433844,no
Commissioning of the ATLAS offline software with cosmic rays,2007,"The ATLAS experiment of the LHC is now taking its first data by collecting cosmic ray events. The full reconstruction chain including all sub-systems (inner detector, calorimeters and muon spectrometer) is being commissioned with this kind of data for the first time. Detailed analysis are being performed in order to provide ATLAS with its first alignment and calibration constants and to study the combined muon performance. Combined monitoring tools and event displays have also been developed to ensure good data quality. A simulation of cosmic events according to the different detector and trigger setups has also been provided to verify it gives a good description of the data.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4436316,no
Validation of GATE Monte Carlo simulation of the performance characteristics of a GE eXplore VISTA small animal PET system,2007,"In this work, we validate the application of GATE (Geant4 Application for Tomographic Emission) Monte Carlo simulation software to model a GE eXplore VISTA small animal PET system. GATE was used to build our three-dimensional PET simulation model. The simulated acquired data were stored in a list-mode file and processed by the same reconstruction scheme as that used in real scanner. To validate the simulation results, several phantoms used in performance assessment experiments were modeled for simulating the system sensitivity, spatial resolution, scatter fraction and count rate response. A realistic voxelized mouse phantom was applied to present the overall quality of our model and compared with mouse images obtained from real exam.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4436803,no
ACCE: Automatic correction of control-flow errors,2007,"Detection of control-flow errors at the software level has been studied extensively in the literature. However, there has not been any published work that attempts to correct these errors. Low-cost correction of CFEs is important for real-time systems where checkpointing is too expensive or impossible. This paper presents automatic correction of control-flow errors (ACCE), an efficient error correction algorithm involving addition of redundant code to the program. ACCE has been implemented by modifying GCC, a widely used C compiler, and performance measurements show that the overhead is very low. Fault injection experiments on SPEC and MiBench benchmark programs compiled with ACCE show that the correct output is produced with high probability and that CFEs are corrected with a latency of a few hundred instructions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4437639,no
A methodology for detecting performance faults in microprocessors via performance monitoring hardware,2007,"Speculative execution of instructions boosts performance in modern microprocessors. Control and data flow dependencies are overcome through speculation mechanisms, such as branch prediction or data value prediction. Because of their inherent self-correcting nature, the presence of defects in speculative execution units does not affect their functionality (and escapes traditional functional testing approaches) but impose severe performance degradation. In this paper, we investigate the effects of performance faults in speculative execution units and propose a generic, software-based test methodology, which utilizes available processor resources: hardware performance monitors and processor exceptions, to detect these faults in a systematic way. We demonstrate the methodology on a publicly available fully pipelined RISC processor that has been enhanced with the most common speculative execution unit, the branch prediction unit. Two popular schemes of predictors built around a Branch Target Buffer have been studied and experimental results show significant improvements on both cases fault coverage of the branch prediction units increased from 80% to 97%. Detailed experiments for the application of a functional self-testing methodology on a complete RISC processor incorporating both a full pipeline structure and a branch prediction unit have not been previously given in the literature.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4437646,no
High Performance Software-Hardware Network Intrusion Detection System,2007,"Network intrusion detection systems (NIDS) and quality of service (QoS) demands have been steadily increasing over the past few years. Current solutions using software become inefficient running on high speed high volume networks and will end up dropping packets. Hardware solutions are available and result in much higher efficiency but present problems such as flexibility and cost. Our proposed system uses a modified version of Snort, a robust widely deployed open-sourced NIDS. It has been found that Snort spends at least 30%-60% of its processing time doing pattern matching. Our proposed system runs Snort in software until it gets to the pattern matching function and then offloads that processing to the field programmable gate array (FPGA). The software can then go on to other processing while it waits for the results from the FPGA. The hardware is able to process data at upto 1.7 GB/s on one Xilinx XC2VP100 FPGA. The design is scaleable and will allow for multiple FPGAs to be used in parallel to increase the processing speed even further.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4439273,no
A parallel controls software approach for PEP II: AIDA & MATLAB middle layer,2007,"The controls software in use at PEP II (Stanford control program - SCP) had originally been developed in the eighties. It is very successful in routine operation but due to its internal structure it is difficult and time consuming to extend its functionality. This is problematic during machine development and when solving operational issues. Routinely, data has to be exported from the system, analyzed offline, and calculated settings have to be reimported. Since this is a manual process, it is time consuming and error-prone. Setting up automated processes, as is done for MIA (model independent analysis), is also time consuming and specific to each application. Recently, there has been a trend at light sources to use MATLAB [1] as the platform to control accelerators using a ""MATLAB middle layer"" [2] (MML), and so called channel access (CA) programs to communicate with the low level control system (LLCS). This has proven very successful, especially during machine development time and trouble shooting. A special CA code, named AIDA (Accelerator Independent Data Access [3]), was developed to handle the communication between MATLAB, modern software frameworks, and the SCP. The MML had to be adapted for implementation at PEP II. Colliders differ significantly in their designs compared to light sources, which poses a challenge. PEP II is the first collider at which this implementation is being done. We will report on this effort, which is still ongoing.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4440280,no
The LHC beam pipe waveguide mode reflectometer,2007,"The waveguide-mode reflectometer for obstacle detection in the LHC beam pipe has been intensively used for more than 18 months. The ""Assembly"" version is based on the synthetic pulse method using a modern vector network analyzer. It has mode selective excitation couplers for the first TE and TM mode and uses a specially developed waveguide mode dispersion compensation algorithm with external software. In addition there is a similar ""In Situ"" version of the reflectometer which uses permanently installed microwave couplers at the end of each of the nearly 3 km long LHC arcs. During installation a considerable number of unexpected objects have been found in the beam pipes and subsequently removed. Operational statistics and lessons learned are presented and the overall performance is discussed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4440830,no
Variable block size error concealment scheme based on H.264/AVC non-normative decoder,2007,"As the newest video coding standard, H.264/AVC can achieve high compression efficiency. At the same time, due to the high-efficiently predictive coding and the variable length entropy coding, it is more sensitive to transmission errors. So error concealment (EC) in H.264 is very important when compressed video sequences are transmitted over error-prone networks and erroneously received. To achieve higher EC performance, this paper proposes variable block size error concealment scheme (VBSEC) by utilizing the new concept of variable block size motion estimation (VBSME) in H.264 standard. This scheme provides four EC modes and four sub-block partitions. The whole corrupted macro-block (MB) will be divided into variable block size adaptively according to the actual motion. More precise motion vectors (MV) will be predicted for each sub-block. We also produce a more accurate distortion function based on spatio-temporal boundary matching algorithm (STBMA). By utilizing VBSEC scheme based on our STBMA distortion function, we can reconstruct the corrupted MB in the inter frame more accurately. The experimental results show that our proposed scheme can obtain maximum PSNR gain up to 1.72 dB and 0.48 dB, respectively compared with the boundary matching algorithm (BMA) adopted in the JM11.0 reference software and STBMA.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4445834,no
Hardware friendly background analysis based complexity reduction in H.264/AVC multiple reference frames motion estimation,2007,"In H.264 standard, multiple reference frame motion estimation (MRF- ME) can help to generate small residues and improve the performance. However, MRF-ME is also a computation intensive task for video coding system. Many software oriented fast algorithms have been proposed to shorten MRF-ME process. For hardwired real-time encoder, the division of ME part into two pipeline stages degrades the efficiency of many fast algorithms. This paper gives one hardware friendly MRF-ME algorithm to reduce computation complexity in MRF-ME procedure. The proposed algorithm is based on the analysis of macroblock's (MB's) feature and restricts search range for static background MB. Through experiment results, with negligible video quality degradation, the proposed background analysis based MRF-ME algorithm can averagely reduce 41.75% ME time for sequences with static background. Moreover, the proposed algorithm is compatible to other fast algorithms and friendly to hardware implementation of H.264 real-time encoder.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4445957,no
A policy controlled IPv4/IPv6 network emulation environment,2007,"In QoS enabled IP-based networks, QoS signaling and policy control are used to control the access to network resources and their usage. The IETF proposed standard protocol for policy control is Common Open Policy Service (COPS) protocol, which has also been adopted in 3GPP IP Multimedia Subsystem (IMS) Release 5. This paper presents a prototype for policy-controlled IPv4/IPv6 network emulation environment, in which it is possible to specify the policy control and emulate, over a period of time, QoS parameters such as bandwidth, packet delay, jitter, and packet discard probability for media flows within an IP multimedia session. The policy control is handled by COPS, and IP channel emulation uses two existing network emulation tools, NIST Net and ChaNet, supporting IPv4 and IPv6 protocols, respectively. The scenario-based approach allows reproducible performance measurements and running various experiments by using the same network behavior. A graphical user interface has been developed to make the scenario specification more user-friendly. We demonstrate the functionality of the prototype emulation environment for IPv6 and analyze its performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4446098,no
Using pipeline technology to improve web serverâ€™s QoS,2007,"To meet increasing QoS requirements of Web applications it is vital to design an efficient Web server. This article describes a pipeline server architecture, which can get intra-request parallelism. Based on control theory, we adopt predictor and feedback control methods to allocate threads of each stage dynamically. As a result, server can satisfy variant workloads adaptively. With the help of these ideas, we implement a novel kernel web server, KETA. Experiment results prove that KETA is an efficient kernel Web server with high performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4446104,no
Weighted Centroid Localization in Zigbee-based Sensor Networks,2007,"Localization in wireless sensor networks gets more and more important, because many applications need to locate the source of incoming measurements as precise as possible. Weighted centroid localization (WCL) provides a fast and easy algorithm to locate devices in wireless sensor networks. The algorithm is derived from a centroid determination which calculates the position of devices by averaging the coordinates of known reference points. To improve the calculated position in real implementations, WCL uses weights to attract the estimated position to close reference points provided that coarse distances are available. Due to the fact that Zigbee provides the link quality indication (LQI) as a quality indicator of a received packet, it can also be used to estimate a distance from a node to reference points.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4447528,no
An extensive review on accessing quality information,2007,"The WWW has become one of the fastest growing electronic information sources. In the Web, people are engaging in interaction with more and more diverse information than ever before, so that the problem of information quality is more significant in the Web than any other information system, especially considering the rate of growth in the number of documents. Yet, despite a decade of active research, Information quality lacks comprehensive methodology for its assessment and improvement and a few researches have presented practical way for measuring IQ criteria. This paper attempts to address some of the issues involved in information quality assessment by classifying and discussing existing researches of information quality frameworks. While many of pervious works have concentrated on a dimension of information quality assessment, in our classification has been considered three dimensions: criteria, models and validation methods of the models and criteria. This classification prepares a bed for developing practical and at the same time valid information quality model.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4448591,no
Estimates of Radial Current Error from High Frequency Radar using MUSIC for Bearing Determination,2007,Quality control of surface current measurements from high frequency (HF) radar requires understanding of individual error sources and their contribution to the total error. Radial velocity error due to uncertainty of the bearing determination technique employed by HF radar is observed with both direction finding and phased array techniques. Surface current estimates utilizing Multiple Signal Classification (MUSIC) direction finding algorithm with a compact antenna design are particularly sensitive to the radiation pattern of the receive and transmit antennas. Measuring the antenna pattern is a common and straightforward task that is essential for accurate surface current measurements. Radial current error due to the a distorted antenna pattern is investigated by applying MUSIC to simulated HF radar backscatter for an idealized ocean surface current. A Monte Carlo type treatment of distorted antenna patterns is used to provide statistics of the differences between simulated and estimated surface current. RMS differences between the simulated currents and currents estimated using distorted antenna patterns are 3-12 cm/s greater than those using perfect antenna patterns given a simulated uniform current of 50 cm/s. This type of analysis can be used in conjunction with antenna modeling software to evaluate possible error due to the antenna patterns before installing a HF radar site.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4449257,no
Predicting performance of software systems during feasibility study of software project management,2007,"Software performance is an important nonfunctional attribute of software systems for producing quality software. Performance issues must be considered throughout software project development. Predicting performance early in the life cycle is addressed by many methodologies, but the data collected during feasibility study not considered for predicting performance. In this paper, we consider the data collected (technical and environmental factors) during feasibility study of software project management to predict performance. We derive an algorithm to predict the performance metrics and simulate the results using a case study on banking application.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4449845,no
Software Tool for Real Time Power Quality Disturbance Analysis and Classification,2007,"Real time detection and classification of power quality disturbances is important for quick diagnosis and mitigation of such disturbances. This paper presents the development of a software tool based on MatLab for power quality disturbance analysis and classification. Prior to the development of the software tool, the disturbance signals are captured and processed in real-time using the TMS320C6711DSP starter kit. Digital signal processor is used to provide fast data capture, fast data processing and signal processing flexibility with increased system performance and reduced system cost. The developed software tool can be used for real-time and off-line disturbance analysis by displaying the detected disturbance, % harmonics of a signal, total harmonic distortion and results of the S-transform, fast Fourier transform and continuous wavelet transform analyses. In addition, graphical representation of the input signal, power quality indices including sag and swell magnitudes are also displayed on the graphical user interface. PQ disturbance classification results show that accurate disturbance classification can be obtained with a total % of correct classification of 99.3%. Such a software tool can serve as a simple and reliable means for PQ disturbance detection and classification.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4451390,no
Efficient Development Methodology for Multithreaded Network Application,2007,"Multithreading is becoming increasingly important for modern network programming. In inter-process communication platform, multithreaded applications have much benefit especially to improve application's throughput, responsiveness and latency. However, developing good quality of multithreaded codes is difficult, because threads may interact with each others in unpredictable ways. Although modern compilers can manage threads well, but in practice, synchronization errors (such as: data race errors, deadlocks) required careful management and good optimization method. The goal of this work is to discover common pitfalls in multithreaded network applications, present a software development technique to detect errors and optimize efficiently multithreaded applications. We compare performance of a single threaded network application with multithreaded network applications, use tools called Intelreg VTunetrade Performance Analyzer, Intelreg Thread Checker and our method to efficiently fix errors and optimize performance. Our methodology is divided into three phases: First phase is performance analysis using Intelreg VTunetrade Performance Analyzer with the aim is to identify performance optimization opportunities and detect bottlenecks. In second phase, with Intelreg Thread Checker we allocate data races, memory leakage and debug the multithreaded applications. In third phase, we apply tuning and optimization to the multithreaded applications. With the understanding of the common pitfalls in multithreaded network applications, through the development and debugging methodology aforementioned above, developers are able to optimize and tune their applications efficiently.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4451394,no
DCPD acquisition and analysis for HV storage capacitor based on Matlab,2007,"High-voltage storage capacitor is the key device in weapon system, high reliability is required. In the process of storage and application, effective measurements are needed to ensure the insulation capability of capacitors. Usually, partial discharge (PD) is used to detect the insulation status of capacitors. Under DC there does not exist fundamental parameter Phi , so a new parameter delta (t) is introduced .Adopting data acquisition and analysis system with single trigger based on Matlab software, PD of four typical defect models under DC condition is obtained. Data of DCPD signals was analyzed through q, n, delta (t) distribution figures, from which obvious differences can be obtained. All results can provide data foundation for pattern recognition of high-voltage storage capacitors.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4451532,no
Defect prediction for embedded software,2007,"As ubiquitous computing becomes the reality of our lives, the demand for high quality embedded software in shortened intervals increases. In order to cope with this pressure, software developers seek new approaches to manage the development cycle: to finish on time, within budget and with no defects. Software defect prediction is one area that has to be focused to lower the cost of testing as well as to improve the quality of the end product. Defect prediction has been widely studied for software systems in general, however there are very few studies which specifically target embedded software. This paper examines defect prediction techniques from an embedded software point of view. We present the results of combining several machine learning techniques for defect prediction. We believe that the results of this study will guide us in finding better predictors and models for this purpose.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4456886,no
An application of a rule-based model in software quality classification,2007,"A new rule-based classification model (RBCM) and rule-based model selection technique are presented. The RBCM utilizes rough set theory to significantly reduce the number of attributes, discretation to partition the domain of attribute values, and Boolean predicates to generate the decision rules that comprise the model. When the domain values of an attribute are continuous and relatively large, rough set theory requires that they be discretized. The subsequent discretized domain must have the same characteristics as the original domain values. However, this can lead to a large number of partitions of the attribute's domain space, which in turn leads to large rule sets. These rule sets tend to form models that over-fit. To address this issue, the proposed rule-based model adopts a new model selection strategy that minimizes over-fitting for the RBCM. Empirical validation of the RBCM is accomplished through a case study on a large legacy telecommunications system. The results demonstrate that the proposed RBCM and the model selection strategy are effective in identifying the classification model that minimizes over-fitting and high cost classification errors.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4457232,no
Presentation of Information Synchronized with the Audio Signal Reproduced by Loudspeakers using an AM-based Watermark,2007,Reproducing stego audio signal via a loudspeaker and detecting embedded data from a recorded sound from a microphone are challenging with respect to the application of data hiding. A watermarking technique using subband amplitude modulation was applied to a system that displays text information synchronously with the watermarked audio signal transmitted in the air. The robustness of the system was evaluated by a computer simulation in terms of the correct rate of data transmission under reverberant and noisy conditions. The results showed that the performance of detection and the temporal precision of synchronization were sufficiently high. Objective measurement of the watermarked audio quality using the PEAQ method revealed that the mean objective difference grade obtained from 100 watermarked music samples exhibited an intermediate value between the mean ODGs of 96-kbps and 128-kbps MP3 encoded music samples.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4457704,no
Ensuring the Quality Testing of Web Using a New Methodology,2007,"As Web sites becoming a fundamental component of businesses, quality of service will be one of the top management concerns. Users, normally, does not care about site failures, traffic jams, network bandwidth [1], or other indicators of system failures. To an online customer, quality of service means fast, predictable response service, level of a Web site noted in a real time. User measures the quality by response time, availability, reliability, predictability, and cost. Poor quality implies that the customer will no longer visit the site and hence the organization may loose business. The issues that affect the quality are broken pages and faulty images, CGI-bin error messages, complex colour combinations, no back link, multiple and frequent links, etc. So we try to build a good program that can scan Web site for broken links, broken images, broken pages and other common Web site faults. As Web site cannot test as a whole in one attempt we rely on decomposing the behavior of the Web site into testable components and mapping these components onto testable objects. Moreover we verify our method by using Jmeter tool [2] to measure the performance of Web, which indicate that broken components on a Web site has a bad effect in its performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4458215,no
PISRAT: Proportional Intensity-Based Software Reliability Assessment Tool,2007,"In this paper we develop a software reliability assessment tool, called PISRAT: Proportional intensity-based software reliability assessment tool, by using several testing metrics data as well as software fault data observed in the testing phase. The fundamental idea is to use the proportional intensity-based software reliability models proposed by the same authors. PISRAT is written in Java language with 54 classes and 8.0 KLOC, where JDK1.5.0_9 and JFreeChart are used as the development kit and the chart library, respectively. This tool can support (i) the parameter estimation of software reliability models via the method of maximum likelihood, (ii) the goodness-of-fit test under several optimization criteria, (iii) the assessment of quantitative software reliability and prediction performance. To our best knowledge, PISRAT is the first freeware for dynamic software reliability modeling and measurement with time- dependent testing metrics.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4459637,no
Predicting Defective Software Components from Code Complexity Measures,2007,"The ability to predict defective modules can help us allocate limited quality assurance resources effectively and efficiently. In this paper, we propose a complexity- based method for predicting defect-prone components. Our method takes three code-level complexity measures as input, namely Lines of Code, McCabe's Cyclomatic Complexity and Halstead's Volume, and classifies components as either defective or non-defective. We perform an extensive study of twelve classification models using the public NASA datasets. Cross-validation results show that our method can achieve good prediction accuracy. This study confirms that static code complexity measures can be useful indicators of component quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4459644,no
A new methodology for Web testing,2007,"As Web sites becoming a fundamental component of businesses, quality of service will be one of the top management concerns. Users, normally, does not care about site failures, traffic jams, network bandwidth, or other indicators of system failures. To an online customer, quality of service means fast, predictable response service, level of a Web site noted in a real time. User measures the quality by response time, availability, reliability, predictability, and cost. Poor quality implies that the customer will no longer visit the site and hence the organization may loose business. The issues that affects the quality are, broken pages and faulty images, CGI-bin error messages, complex colour combinations, no back link, multiple and frequent links, etc. So we try to build a good program that can scan web site for broken links, broken images, broken pages and other common web sit e faults. Because web site cannot test as a whole in one attempt we rely in our implementation to decompose the behavior of the web site into testable components then mapping these components onto testable objects. Then we prove by using Jmeter performance testing tool that broken components on a web site has a bad effect in its performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4475621,no
Joint Adaptive GOP and SCD Coding for Improving H.264 Scalable Video Coding,2007,"In order to concurrently provide different video services for diverse access devices, the scalable quality and complexity services in a single bit stream offered by H.264-based scalable video coding (H.264/SVC) has gained a lot of attentions recently. However, the motion-compensated temporal filtering (MCTF) structure consumes a large amount of operation time when combined with Core Experiment 2 (CE2), which implements adaptive GOP detection (AGD). Besides, a simple scene change detection (SCD) method is required to decrease the mis-position motion estimation and yield robust AGD. In this paper, we suggest a video content variation (VCV) feature to implement simple AGD and SCD methods by using block-based classification before executing MCTF. Based on the JSVM 7.0 software, the simulation results show that the proposed method not only reduces about 67% of the operation time consumed but also improves about 0.1 dB PSNR on average as compared to the CE2, where the proposed SCD can attain a detection rate of 89% to help for performance improvement.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4475956,no
Modeling of ITU-T G.729 codec with bit-width optimization for intensive computation blocks,2007,"In this work, ITU-T G.729 speech codec is precisely modeled using basic building blocks of SIMULINK such as simple adders and multipliers. Therefore, a golden model of the codec which best suits as a reference for its hardware implementation is developed. In a custom hardware design, the precision of the computations, i.e. the optimum number of the bits of the computational blocks should be calculated. The optimum number of the bits of the computational blocks is the minimum word-width that maintains the quality of the output with minimum chip area. Three blocks are selected to extract their bit-true models. The optimum bit-width of the coefficients and the internal computations are measured. These blocks are the pre-processing block, the open loop pitch estimator, and the fixed codebook search block. The former is important in terms of error propagation to the output and the two others are the blocks that demand the most computational power respectively. As such, these blocks are suitable choices to be hardware implemented in a hardware-software co-design project.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4497659,no
Multiple Parametric Circuit Analysis Tool for Detectability Estimation,2007,In this paper a software tool is presented which is capable of producing testability metrics for analog and mixed-signal circuits. These metrics are obtained by performing probabilistic analysis techniques. The presented tool acts as a shell utilizing the power of state of the art external schematic and simulation programs and offers to the user a graphical interface for circuit design and test. This interface is transparent to the user elaborately hiding all the complex task manipulations. Preliminary results are presented showing the effectiveness of the proposed scheme.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4511189,no
Component Based Proactive Fault Tolerant Scheduling in Computational Grid,2007,"Computational Grids have the capability to provide the main execution platform for high performance distributed applications. Grid resources having heterogeneous architectures, being geographically distributed and interconnected via unreliable network media are extremely complex and prone to different kinds of errors, failures and faults. Grid is a layered architecture and most of the fault tolerant techniques developed on grids use its strict layering approach. In this paper, we have proposed a cross-layer design for handling faults proactively. In a cross-layer design, the top- down and bottom-up approach is not strictly followed, and a middle layer can communicate with the layer below or above it [1]. At each grid layer there would be a monitoring component that would decide on predefined factors that the reliability of that particular layer is high, medium or low. Based on Hardware Reliability Rating (HRR) and Software Reliability Rating (SRR), the Middleware Monitoring Component / Cross- Layered Component (MMC/CLC) would generate a Combined Rating (CR) using CR calculation matrix rules. Each grid participating node will have a CR value generated through cross layered communication using HMC, MMC/CLC and SMC. All grid nodes will have their CR information in the form of a CR table and high rated machines would be selected for job execution on the basis of minimum CPU load along with different intensities of check pointing. Handling faults proactively at each layer of grid using cross communication model would result in overall improved dependability and increased performance with less overheads of check pointing.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4516328,no
Power System Flicker Analysis and Numeric Flicker Meter Emulation,2007,"This paper presents a methodology for flicker propagation analysis, numeric IEC flickermeter emulation and flicker source modeling. The main results of this study are: ldr To create a distribution load model by which the flicker propagation from HV to MV can be studied, ldr To build numeric IEC flickermeter with improved algorithm (demodulator and nonlinear classification), ldr To build simplified actual disturbance source models (electric arc furnace, welding machine, motor starter, etc), which can be used in frequency domain and load-flow flicker assessment. The complexity of nonlinear models for simulating the dynamic behavior of arc furnaces and welding machines is well known. For this reason, previous works are generally based on time domain solutions. In this paper, a simplified approach has been studied by using RMS value-based modeling. The models have been validated against experimental results and site measurements. After integration of these models and a numeric flickermeter in a frequency domain software, it is possible to simulate almost all low frequency electrical disturbances with very short computing time and to survey interactions among them, for example, between voltage dip and flicker.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4538543,no
New EPS state estimation algorithms based on the technique of test equations and PMU measurements,2007,"Large system blackouts that have occurred in different countries in the first five years of the new millenium prove the need of updating and improvement of software for EPS monitoring and control (SCADA/EMS-applications). The updating of software for EPS monitoring and control (SCADA/EMS-applications) has become possible on a qualitatively new level owing to WAMS (wide-area measurement system), that allows the EPS state to be controlled synchronously and with high accuracy. The phasor measurement units (PMU) are the main measurement equipment in these systems. One of the applications, which will be significantly affected by the introduction of PMU, is the state estimation (SE). State estimation is an important procedure that provides reliable and qualitative information for EPS control. The state estimation results in calculation of steady state (current conditions) of EPS on the basis of measured state variables and data on the state of scheme topology. The measurements used for state estimation include mainly the measurements received from SCADA system. As compared to a standard set of measurements received from SCADA, PMU installed in the node can measure voltage phasor in this node and current phasors in some or all branches adjacent to this node with high accuracy. The use of additional high accuracy measurements improves observability of calculated network, enhances the efficiency of methods for bad data detection, increases accuracy and reliability of the obtained estimates.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4538566,no
Estimation of the Distributed Generation Impacts on the Angle Stability of the Two-Machine Scheme,2007,"The paper discusses the results of the case studies devoted to the impact of distributed generators connected to a distribution network on the transient stability of transmission network, with the critical fault clearing time applied as an performance index. The case studies are based on the two- machine scheme, employing also a model of the Baltic power system. The authors analyse a wide technological range of the distributed generators and a variety of the modelled operating conditions. The transient stability is estimated using EUROSTAG software for simulation of power systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4538620,no
Utilizing data mining algorithms for identification and reconstruction of sensor faults: a Thermal Power Plant case study,2007,"This paper describes a procedure of identifying sensor faults and reconstructing the erroneous measurements. Data mining algorithms are successfully applied for deriving models that estimate the value of one variable based on correlated others. The estimated values can then be used instead of the recorded ones of a measuring instrument with false reading. The aim is to reassure the correctness of data entered to an optimization software application under development for the Thermal Power Plants of Western Macedonia, Greece.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4538639,no
Power Network Port Measurements Synchronization,2007,"This work introduces a new procedure for software synchronization of electrical power signals collected over a large geographical area. The idea is to use not necessarily synchronized, but stable, local clocks, and synchronize the sparsely collected data windows by an offline procedure. An approach for the estimation of non monitored internal system buses serves as a basis for the synchronization process. The electrical power network description yields the desired phase corrections, compensating the loss of data window synchronism. Typical sub determined situations are avoided by a proposed alternative network monitoring scheme, illustrated by a case example. Simulation results emphasize the method robustness with respect to usual network parameters uncertainties.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4538649,no
Standstill frequency response measurement of generator parameters,2007,"Connell Wagner has estimated the parameters of large generators on a number of occasions and this paper will outline the method and results of this work with respect to using standstill frequency response methods and load rejection methods. A number of issues involving the testing methods, ability to determine parameters from tests and the applicability to current modelling software will be discussed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4548121,no
Impact of harmonics on tripping time of overcurrent relays,2007,Theoretical and experimental analyses are used to investigate the effects of harmonics on the operation of overcurrent relays. Two analytical approaches based on relay characteristics provided by the manufacturer and simulations using PSCAD software package are used to estimate tripping times under non-sinusoidal operating conditions. The tests were conducted such that the order and the magnitude of harmonics could be controlled by employing a computer-based single-phase harmonic source. Computed and measured tripping times are compared and suggestions on the application of overcurrent relays in harmonically polluted environments are provided.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4548125,no
CSM-autossociators combination for degraded machine printed character recognition,2007,"This paper presents an OCR method that combines the complementary similarity measure (CSM) method with a set of autossociators for degraded character recognition. In the serial combination, the first classifier must achieve lower errors and be very well suited for rejection, whereas the second classifier must allow only low errors and rejects. We introduce a rejection criterion mode used as a quality measurement of the degraded character which makes the CSM-based classifier very powerful and very well suited for rejection. We report experimental results for a comparison of three methods: the CSM method, the autoassociator-based classifier and the proposed combined architecture. Experimental results show an achievement of 99.59% of recognition rate on poor quality bank check characters, which confirm that the proposed approach can be successfully used for effective degraded printed character recognition.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4555587,no
Compiler-assisted architectural support for program code integrity monitoring in application-specific instruction set processors,2007,"(ASIPs) are being increasingly used in mobile embedded systems, the ubiquitous networking connections have exposed these systems under various malicious security attacks, which may alter the program code running on the systems. In addition, soft errors in microprocessors can also change program code and result in system malfunction. At the instruction level, all code modifications are manifested as bit flips. In this work, we present a generalized methodology for monitoring code integrity at run-time in ASIPs, where both the instruction set architecture (ISA) and the underlying microarchitecture can be customized for a particular application domain. Based on the microoperation-based monitoring architecture that we have presented in previous work, we propose a compiler-assisted and application-controlled management approach for the monitoring architecture. Experimental results show that compared with the OS-managed scheme and other compiler-assisted schemes, our approach can detect program code integrity compromises with much less performance degradation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4601899,no
Interactive Mining of Functional MRI Data,2007,"Discovery of the image voxels of the brain that represent real activity is, in general, very difficult because of a weak signal-to-noise ratio and the presence of artifacts. The first tests of the classical data mining algorithms in this field showed low performances and weak quality of recognition. In this article, a new interactive data-driven approach to functional magnetic resonance imagery mining is presented, allowing the observation of cerebral activity. Several non-supervised classification algorithms have been developed and tested on sequences of fMRI images. The results of the tests have shown that the number of classes, signal-to-noise ratio, and volumes of activated and explored zones have a strong influence on the classifier performances.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4618871,no
Early Models for System-Level Power Estimation,2007,"Power estimation and verification have become important aspects of System-on-Chip (SoC) design flows. However, rapid and effective power modeling and estimation technologies for complex SoC designs are not widely available. As a result, many SoC design teams focus the bulk of their efforts on using detailed low-level models to verify power consumption. While such models can accurately estimate power metrics for a given design, they suffer from two significant limitations: (1) they are only available late in the design cycle, after many architectural features have already been decided, and (2) they are so detailed that they impose severe limitations on the size and number of workloads that can be evaluated. While these methods are useful for power verification, architects require information much earlier in the design cycle, and are therefore often limited to estimating power using spreadsheets where the expected power dissipation of each module is summed up to predict total power. As the model becomes more refined, the frequency that each module is exercised may be added as an additional parameter to further increase the accuracy. Current spreadsheets, however, rely on aggregate instruction counts and do not incorporate either time or input data and thus have inherent inaccuracies. Our strategy for early power estimation relies on (i) measurements from real silicon, (ii) models built from those measurements models that predict power consumption for a variety of processor micro-architectural structures and (iii)FPGA-based implementations of those models integrated with an FPGA-based performance simulator/emulator. The models will be designed specifically to be implemented within FPGAs. The intention is to integrate the power models with FPGA-based full-system, functional and performance simulators/emulators that will provide timing and functional information including data values. The long term goal is to provide relative power accuracy and power trends useful to arc- - hitects during the architectural phase of a project, rather than precise power numbers that would require far more information than is available at that time. By implementing the power models in an FPGA and driving those power models with a system simulator/emulator that can feed the power models real data transitions generated by real software running on top of real operating systems, we hope to both improve the quality of early stage power estimation and improve power simulation performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4620146,no
Evaluating the EASY-backfill job scheduling of static workloads on clusters,2007,"This research aims at improving our understanding of backfilling job scheduling algorithms. The most frequently used algorithm, EASY-backfilling, was selected for a performance evaluation by scheduling static workloads of parallel jobs on a computer cluster. To achieve the aim, we have developed a batch job scheduler for Linux clusters, implemented several scheduling algorithms including ARCA and EASY-Backfilling, and carried out their performance evaluation by running well known MPI applications on a real cluster. Our performance evaluation carried out for EASY-Backfilling serves two purposes. First, the performance results obtained from our evaluation can be used to validate other researcherspsila results generated by simulation, and second, the methodology used in our evaluation has alleviated many problems existed in the simulations presented in the current literature.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4629218,no
Reliability-aware resource allocation in HPC systems,2007,"Failures and downtimes have severe impact on the performance of parallel programs in a large scale High Performance Computing (HPC) environment. There were several research efforts to understand the failure behavior of computing systems. However, the presence of multitude of hardware and software components required for uninterrupted operation of parallel programs make failure and reliability prediction a challenging problem. HPC run-time systems like checkpoint frameworks and resource managers rely on the reliability knowledge of resources to minimize the performance loss due to unexpected failures. In this paper, we first analyze the Time Between Failure (TBF) distribution of individual nodes from a 512-node HPC system. Time varying distributions like Weibull, lognormal and gamma are observed to have better goodness-of-fit as compared to exponential distribution. We then present a reliability-aware resource allocation model for parallel programs based on one of the time varying distributions and present reliability-aware resource allocation algorithms to minimize the performance loss due to failures. We show the effectiveness of reliability-aware resource allocation algorithms based on the actual failure logs of the 512 node system and parallel workloads obtained from LANL and SDSC. The simulation results indicate that applying reliability-aware resource allocation techniques reduce the overall waste time of parallel jobs by as much as 30%. A further improvement by 15% in waste time is possible by considering the job run lengths in reliability-aware scheduling.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4629245,no
Hybrid predictive base station (HPBS) selection procedure in IEEE 802.16e-based WMAN,2007,"According to the mobility framework of IEEE 802.16e, a mobile station (MS) should scan the neighbouring base stations (BSs), for selecting the best BS for a potential handover activity. However, the standard does not specify the number of BSs to be scanned leaving room for unnecessary scanning. Moreover, prolonged scanning also interrupts data transmissions thus degrading the QoS of an ongoing connection. Reducing unnecessary scanning is an important issue. This paper proposes a scheme to reduce the number of BSs to scan (thus improving the overall handover performance). Simulation results have shown that this hybrid predictive BS selection scheme for potential scanning activities is more effective than the conventional IEEE 802.16e handover scheme in terms of handover delay and resource wastages.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4665310,no
A Fuzzy Multi-Criteria Decision Approach for Enhanced Auto-Tracking of Seismic Events,2007,"The main activity carried out by the geophysicist when interpreting seismic data, in terms of both importance and time spent is tracking (or picking) seismic events. In practice, this activity turns out to be rather challenging, particularly when the targeted event is interrupted by discontinuities such as geological faults or exhibits lateral changes in seismic character. In recent years, several automated schemes, known as auto-trackers, have been developed to assist the interpreter in this tedious and time-consuming task. The automatic tracking tool available in modern interpretation software packages often employs artificial neural networks (ANNs) to identify seismic picks belonging to target events through a pattern recognition process. The ability of ANNs to track horizons across discontinuities largely depends on how reliably data patterns characterise these horizons. While seismic attributes are commonly used to characterise amplitude peaks forming a seismic horizon, some researchers in the field claim that inherent seismic information is lost in the attribute extraction process and advocate instead the use of raw data (amplitude samples). This paper investigates the performance of ANNs using either characterisation methods, and demonstrates how the complementarity of both seismic attributes and raw data can be exploited in conjunction with other geological information in a fuzzy inference system (FIS) to achieve an enhanced auto-tracking performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4728573,no
Performance under failures of high-end computing,2007,"Modern high-end computers are unprecedentedly complex. Occurrence of faults is an inevitable fact in solving large-scale applications on future Petaflop machines. Many methods have been proposed in recent years to mask faults. These methods, however, impose various performance and production costs. A better understanding of faults' influence on application performance is necessary to use existing fault tolerant methods wisely. In this study, we first introduce some practical and effective performance models to predict the application completion time under system failures. These models separate the influence of failure rate, failure repair, checkpointing period, checkpointing cost, and parallel task allocation on parallel and sequential execution times. To benefit the end users of a given computing platform, we then develop effective fault-aware task scheduling algorithms to optimize application performance under system failures. Finally, extensive simulations and experiments are conducted to evaluate our prediction models and scheduling strategies with actual failure trace.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5348807,no
Application development on hybrid systems,2007,"Hybrid systems consisting of a multitude of different computing device types are interesting targets for high-performance applications. Chip multiprocessors, FPGAs, DSPs, and GPUs can be readily put together into a hybrid system; however, it is not at all clear that one can effectively deploy applications on such a system. Coordinating multiple languages, especially very different languages like hardware and software languages, is awkward and error prone. Additionally, implementing communication mechanisms between different device types unnecessarily increases development time. This is compounded by the fact that the application developer, to be effective, needs performance data about the application early in the design cycle. We describe an application development environment specifically targeted at hybrid systems, supporting data-flow semantics between application kernels deployed on a variety of device types. A specific feature of the development environment is the availability of performance estimates (via simulation) prior to actual deployment on a physical system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5348809,no
Performance and cost optimization for multiple large-scale grid workflow applications,2007,"Scheduling large-scale applications on the Grid is a fundamental challenge and is critical to application performance and cost. Large-scale applications typically contain a large number of homogeneous and concurrent activities which are main bottlenecks, but open great potentials for optimization. This paper presents a new formulation of the well-known NP-complete problems and two novel algorithms that addresses the problems. The optimization problems are formulated as sequential cooperative games among workflow managers. Experimental results indicate that we have successfully devised and implemented one group of effective, efficient, and feasible approaches. They can produce soultuins of significantly better performance and cost than traditional algorithms. Our algorithms have considerably low time complexity and can assign 1,000,000 activities to 10,000 processors within 0.4 second on one Opteron processor. Moreover, the solutions can be practically performed by workflow managers, and the violation of QoS can be easily detected, which are critical to fault tolerance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5348835,no
Investigation of Hyper-NA Scanner Emulation for Photomask CDU Performance,2007,"As the semiconductor industry moves toward immersion lithography using numerical apertures above 1.0 the quality of the photomask becomes even more crucial. Photomask specifications are driven by the critical dimension (CD) metrology within the wafer fab. Knowledge of the CD values at resist level provides a reliable mechanism for the prediction of device performance. Ultimately, tolerances of device electrical properties drive the wafer linewidth specifications of the lithography group. Staying within this budget is influenced mainly by the scanner settings, resist process, and photomask quality. Tightening of photomask specifications is one mechanism for meeting the wafer CD targets. The challenge lies in determining how photomask level metrology results influence wafer level imaging performance. Can it be inferred that photomask level CD performance is the direct contributor to wafer level CD performance? With respect to phase shift masks, criteria such as phase and transmission control are generally tightened with each technology node. Are there other photomask relevant influences that effect wafer CD performance? A comprehensive study is presented supporting the use of scanner emulation based photomask CD metrology to predict wafer level within chip CD uniformity (CDU). Using scanner emulation with the photomask can provide more accurate wafer level prediction because it inherently includes all contributors to image formation related to the 3D topography such as the physical CD, phase, transmission, sidewall angle, surface roughness, and other material properties. Emulated images from different photomask types were captured to provide CD values across chip. Emulated scanner image measurements were completed using an AIMS(TM)45-193i with its hyper-NA, through-pellicle data acquisition capability including the Global CDU Map(TM) software option for AIMS(TM) tools. The through-pellicle data acquisition capability is an essential prerequisite for capturing final CD- - U data (after final clean and pellicle mounting) before the photomask ships or for re-qualification at the wafer fab. Data was also collected on these photomasks using a conventional CD-SEM metrology system with the pellicles removed. A comparison was then made to wafer prints demonstrating the benefit of using scanner emulation based photomask CD metrology.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5760346,no
Automated Contingency Management for Propulsion Systems,2007,"Increasing demand for improved reliability and survivability of mission-critical systems is driving the development of health monitoring and Automated Contingency Management (ACM) systems. An ACM system is expected to adapt autonomously to fault conditions with the goal of still achieving mission objectives by allowing some degradation in system performance within permissible limits. ACM performance depends on supporting technologies like sensors and anomaly detection, diagnostic/prognostic and reasoning algorithms. This paper presents the development of a generic prototype test bench software framework for developing and validating ACM systems for advanced propulsion systems called the Propulsion ACM (PACM) Test Bench. The architecture has been implemented for a Monopropellant Propulsion System (MPS) to demonstrate the validity of the approach. A Simulink model of the MPS has been developed along with a fault injection module. It has been shown that the ACM system is capable of mitigating the failures by searching for an optimal strategy. Furthermore, few relevant experiments have been presented to show proof of concepts.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7068389,no
"On power control for wireless sensor networks: System model, middleware component and experimental evaluation",2007,"In this paper, we investigate strategies for radio power control for wireless sensor networks that guarantee a desired packet error probability. Efficient power control algorithms are of major concern for these networks, not only because the power consumption can be significantly decreased but also because the interference can be reduced, allowing for higher throughput. An analytical model of the Received Signal Strength Indicator (RSSI), which is link quality metric, is proposed. The model relates the RSSI to the Signal to Interference plus Noise Ratio (SINR), and thus provides a connection between the powers and the packet error probability. Two power control mechanisms are studied: a Multiplicative-Increase Additive-Decrease (MIAD) power control described by a Markov chain, and a power control based on the average packet error rate. A component-based software implementation using the Contiki operating system is provided for both the power control mechanisms. Experimental results are reported for a test-bed with Telos motes.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7069054,no
A Strategy for fault tolerant control in Networked Control Systems in the presence of Medium Access Constraints,2007,"This paper deals with the problem of fault-tolerant control of a Network Control System (NCS) for the case in which the sensors, actuators and controller are interconnected via various Medium Access Control protocols which define the access scheduling and collision arbitration policies in the network and employing the so-called periodic communication sequence. A new procedure for controlling a system over a network using the concept of an NCS-Information-Packet is described which comprises an augmented vector consisting of control moves and fault flags. The size of this packet is used to define a Completely Fault Tolerant NCS. The fault-tolerant behaviour and control performance of this scheme is illustrated through the use of a process model and controller. The plant is controlled over a network using Model-based Predictive Control and implemented via MATLABÂ© and LABVIEWÂ© software.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7069068,no
VLSI friendly edge gradient detection based multiple reference frames motion estimation optimization for H.264/AVC,2007,"In H.264/AVC standard, motion estimation can be processed on multiple reference frames (MRF) to improve the video coding performance. The computation is also increased in proportion to the reference frame number. Many software oriented fast multiple reference frames motion estimation (MRF-ME) algorithms have been proposed. However, for the VLSI real-time encoder, the heavy computation of fractional motion estimation (FME) makes the integer motion estimation (IME) and FME must be scheduled in two macro block (MB) pipeline stages, which makes many fast MRF-ME algorithms inefficient. In this paper, one edge gradient detection based algorithm is provided to reduce the computation of MRF-ME. The image being rich of texture and sharp edges contains much high frequency signal and this nature makes MRF-ME essential. Through analyzing the edges' gradient, we just perform MRF-ME on those blocks with sharp edges, so the redundant ME computation can be efficiently reduced. Experimental results show that average 26.43% computation can be saved by our approach with the similar coding quality as the reference software. This proposed algorithm is friendly to hardwired encoder implementation. Moreover, the provided fast algorithms can be combined with other fast ME algorithms to further improve the performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7099119,no
Software developed for obtaining GPS-derived total electron content values,2007,"This paper describes a complex technique with its built-in cycle slip correction procedures that have been developed for ionospheric space research to obtain high-quality and high-precision GPS-derived total electron content (TEC) values. Thus, to correct GPS anomalies while the signatures of space weather features detected in the dual-frequency 30-s rate GPS data are preserved is the main aim of this technique. Its main requirement is to complete fully automatically all the tasks required to turn the observational data to the desired final product. Its major tasks include curve fitting, cycle slip detection and correction in the slant relative TEC data, residual error detection and correction in the vertical TEC data, and vertical TEC data filtering for quantifying data smoothness and GPS phase fluctuations. A detailed description of these two data correction methods is given. Validation tests showing weaknesses and strengths of the methods developed are also included and discussed. Versatility and accuracy of the methods are demonstrated with interesting and real-world examples obtained from smooth midlatitude and from dynamic low- and high-latitude data. Results indicate that errors can be detected and corrected more reliably in the vertical TEC data than in the slant TEC data because of the lower rate of change of vertical TEC over a 30-s sampling period. Future work includes the development of a complex software package wherein the individual FORTRAN algorithms, described in this paper, will be incorporated into one main (FORTRAN, Matlab, or C++) program to provide professional and customized GPS data processing for ionospheric space research.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7771479,no
Long-term observations of Schumann resonances at Modra Observatory,2007,"The paper presents a summary of more than 4 years of continuous Schumann resonance (SR) monitoring of the vertical electric component at Modra Observatory. Principal parameters (peak frequency, amplitude, and quality factor) are determined for four resonances from 7 to 30 Hz, i.e., for modes one through four. Attention is also given to the less frequently compiled mode four. The resonance parameters are computed from 48 daily measurements and are represented as the mean monthly values for each time of the detection. Fitting of spectral peaks by Lorentz function is the main method used in data postprocessing. Diurnal, seasonal variations and the indication of interannual variations of these parameters (especially peak frequency) are discussed. The results are compared with other observatory measurements. Our observations confirm that variations in peak frequency of the lower-SR modes can be attributed mainly to the source-observer distance effect.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7771486,no
Using the Conceptual Cohesion of Classes for Fault Prediction in Object-Oriented Systems,2008,"High cohesion is a desirable property of software as it positively impacts understanding, reuse, and maintenance. Currently proposed measures for cohesion in Object-Oriented (OO) software reflect particular interpretations of cohesion and capture different aspects of it. Existing approaches are largely based on using the structural information from the source code, such as attribute references, in methods to measure cohesion. This paper proposes a new measure for the cohesion of classes in OO software systems based on the analysis of the unstructured information embedded in the source code, such as comments and identifiers. The measure, named the Conceptual Cohesion of Classes (C3), is inspired by the mechanisms used to measure textual coherence in cognitive psychology and computational linguistics. This paper presents the principles and the technology that stand behind the C3 measure. A large case study on three open source software systems is presented which compares the new measure with an extensive set of existing metrics and uses them to construct models that predict software faults. The case study shows that the novel measure captures different aspects of class cohesion compared to any of the existing cohesion measures. In addition, combining C3 with existing structural cohesion metrics proves to be a better predictor of faulty classes when compared to different combinations of structural cohesion metrics.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4384505,yes
"Defect Prediction using Combined Product and Project Metrics - A Case Study from the Open Source ""Apache"" MyFaces Project Family",2008,"The quality evaluation of open source software (OSS) products, e.g., defect estimation and prediction approaches of individual releases, gains importance with increasing OSS adoption in industry applications. Most empirical studies on the accuracy of defect prediction and software maintenance focus on product metrics as predictors that are available only when the product is finished. Only few prediction models consider information on the development process (project metrics) that seems relevant to quality improvement of the software product. In this paper, we investigate defect prediction with data from a family of widely used OSS projects based both on product and project metrics as well as on combinations of these metrics. Main results of data analysis are (a) a set of project metrics prior to product release that had strong correlation to potential defect growth between releases and (b) a combination of product and project metrics enables a more accurate defect prediction than the application of one single type of measurement. Thus, the combined application of project and product metrics can (a) improve the accuracy of defect prediction, (b) enable a better guidance of the release process from project management point of view, and (c) help identifying areas for product and process improvement.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4725724,yes
A comparative analysis of the efficiency of change metrics and static code attributes for defect prediction,2008,"In this paper we present a comparative analysis of the predictive power of two different sets of metrics for defect prediction. We choose one set of product related and one set of process related software metrics and use them for classifying Java files of the Eclipse project as defective respective defect-free. Classification models are built using three common machine learners: logistic regression, naive Bayes, and decision trees. To allow different costs for prediction errors we perform cost-sensitive classification, which proves to be very successful: >75% percentage of correctly classified files, a recall of >80%, and a false positive rate <30%. Results indicate that for the Eclipse data, process metrics are more efficient defect predictors than code metrics.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4814129,yes
The influence of organizational structure on software quality,2008,"Often software systems are developed by organizations consisting of many teams of individuals working together. Brooks states in the Mythical Man Month book that product quality is strongly affected by organization structure. Unfortunately there has been little empirical evidence to date to substantiate this assertion. In this paper we present a metric scheme to quantify organizational complexity, in relation to the product development process to identify if the metrics impact failure-proneness. In our case study, the organizational metrics when applied to data from Windows Vista were statistically significant predictors of failure-proneness. The precision and recall measures for identifying failure-prone binaries, using the organizational metrics, was significantly higher than using traditional metrics like churn, complexity, coverage, dependencies, and pre-release bug measures that have been used to date to predict failure-proneness. Our results provide empirical evidence that the organizational metrics are related to, and are effective predictors of failure-proneness.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4814163,yes
Predicting defects using network analysis on dependency graphs,2008,"In software development, resources for quality assurance are limited by time and by cost. In order to allocate resources effectively, managers need to rely on their experience backed by code complexity metrics. But often dependencies exist between various pieces of code over which managers may have little knowledge. These dependencies can be construed as a low level graph of the entire system. In this paper, we propose to use network analysis on these dependency graphs. This allows managers to identify central program units that are more likely to face defects. In our evaluation on Windows Server 2003, we found that the recall for models built from network measures is by 10% points higher than for models built from complexity metrics. In addition, network measures could identify 60% of the binaries that the Windows developers considered as critical-twice as many as identified by complexity metrics.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4814164,yes
Overhead Analysis of Scientific Workflows in Grid Environments,2008,"Scientific workflows are a topic of great interest in the grid community that sees in the workflow model an attractive paradigm for programming distributed wide-area grid infrastructures. Traditionally, the grid workflow execution is approached as a pure best effort scheduling problem that maps the activities onto the grid processors based on appropriate optimization or local matchmaking heuristics such that the overall execution time is minimized. Even though such heuristics often deliver effective results, the execution in dynamic and unpredictable grid environments is prone to severe performance losses that must be understood for minimizing the completion time or for the efficient use of high-performance resources. In this paper, we propose a new systematic approach to help the scientists and middleware developers understand the most severe sources of performance losses that occur when executing scientific workflows in dynamic grid environments. We introduce an ideal model for the lowest execution time that can be achieved by a workflow and explain the difference to the real measured grid execution time based on a hierarchy of performance overheads for grid computing. We describe how to systematically measure and compute the overheads from individual activities to larger workflow regions and adjust well-known parallel processing metrics to the scope of grid computing, including speedup and efficiency. We present a distributed online tool for computing and analyzing the performance overheads in real time based on event correlation techniques and introduce several performance contracts as quality-of-service parameters to be enforced during the workflow execution beyond traditional best effort practices. We illustrate our method through postmortem and online performance analysis of two real-world workflow applications executed in the Austrian grid environment.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4359427,no
RivWidth: A Software Tool for the Calculation of River Widths From Remotely Sensed Imagery,2008,"RivWidth is an implementation in ITT Visual Information Solutions IDL of a new algorithm that automates the calculation of river widths using raster-based classifications of inundation extent derived from remotely sensed imagery. The algorithm utilizes techniques of boundary definition to extract a river centerline, derives a line segment that is orthogonal to this line at each centerline pixel, and then computes the total river width along each orthogonal. The output of RivWidth is comparable in quality to measurements derived using manual techniques; yet, it continuously generates thousands of width values along an entire stream course, even in multichannel river systems. Uncertainty in RivWidth principally depends on the quality of the water classification used as an input, though pixel resolution and the values of input parameters play lesser roles. Source code for RivWidth can be obtained by visiting http://pavelsky.googlepages.com/rivwidth.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4382932,no
Software Reliability Analysis and Measurement Using Finite and Infinite Server Queueing Models,2008,"Software reliability is often defined as the probability of failure-free software operation for a specified period of time in a specified environment. During the past 30 years, many software reliability growth models (SRGM) have been proposed for estimating the reliability growth of software. In practice, effective debugging is not easy because the fault may not be immediately obvious. Software engineers need time to read, and analyze the collected failure data. The time delayed by the fault detection & correction processes should not be negligible. Experience shows that the software debugging process can be described, and modeled using queueing system. In this paper, we will use both finite, and infinite server queueing models to predict software reliability. We will also investigate the problem of imperfect debugging, where fixing one bug creates another. Numerical examples based on two sets of real failure data are presented, and discussed in detail. Experimental results show that the proposed framework incorporating both fault detection, and correction processes for SRGM has a fairly accurate prediction capability.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4385745,no
Classifying Software Changes: Clean or Buggy?,2008,"This paper introduces a new technique for predicting latent software bugs, called change classification. Change classification uses a machine learning classifier to determine whether a new software change is more similar to prior buggy changes or clean changes. In this manner, change classification predicts the existence of bugs in software changes. The classifier is trained using features (in the machine learning sense) extracted from the revision history of a software project stored in its software configuration management repository. The trained classifier can classify changes as buggy or clean, with a 78 percent accuracy and a 60 percent buggy change recall on average. Change classification has several desirable qualities: 1) The prediction granularity is small (a change to a single file), 2) predictions do not require semantic information about the source code, 3) the technique works for a broad array of project types and programming languages, and 4) predictions can be made immediately upon the completion of a change. Contributions of this paper include a description of the change classification approach, techniques for extracting features from the source code and change histories, a characterization of the performance of change classification across 12 open source projects, and an evaluation of the predictive power of different groups of features.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4408585,no
Electronic Scheme for Multiplexed Dynamic Behavior Excitation and Detection of Piezoelectric Silicon-Based Micromembranes,2008,"A new concept for a precise compensation of the static capacitance of piezoelectric silicon-based micromembranes is proposed. Combining analog and digital field-programmable gate array hardware elements with specific software treatment, this system enables the parallel excitation and detection of the resonant frequencies (and the quality factors) of matrices of piezoelectric micromembranes integrated on the same chip. The frequency measurement stability is less than 1 ppm (1-2 Hz) with a switching capability of 4 micromembranes/sec and a measurement bandwidth of 1.5 MHz. The real-time multiplexed tracking of the resonant frequency and quality factor on several micromembranes is performed in different liquid media, showing the high capability of measurement on dramatically attenuated signals. Prior to these measurements, calibration in air is done making use of silica microbeads successive depositions onto piezoelectric membranes surface. The mass sensitivity in air is, thus, estimated at, in excellent agreement with the theoretical corresponding value.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4443123,no
Component-based performance-sensitive real-time embedded software,2008,"The software complexity is continuously increasing and the competition in the software market is becoming more intensive than ever. Therefore, it is crucial to improve the software quality, and meanwhile, minimize software development cost, and reduce software delivery time in order to gain competitive advantages. Recently, Component-Based Software Development (CBSD) was proposed and has now been applied in various industry and business applications as a possible way to achieve this goal. As verified by numerous practical applications in different fields, CBSD is able to increase software development productivity as well as improve software quality. Modern embedded real-time systems have both strict functional and non-functional requirements and they are essentially safety-critical, real-time, and embedded software-intensive systems. In particular, the crucial end-to-end quality-of-service (QoS) properties should be assured in embedded systems such as timeliness and fault tolerance. Herein, I first introduce the modern component technologies and commonly used component models. Then, the middleware in distributed real-time embedded systems is discussed. Further, adaptive system resource management is elaborated upon. Finally, the prospects of a component-based approach in implementing modern embedded real-time software is discussed and future research directions are suggested.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4444486,no
Safety Issues in Modern Bus Standards,2008,Selecting and designing bus standards for safety-critical applications requires careful analysis of error-detection and fault-handling mechanisms. This analysis must be based on the revision of standard specifications and an experimental evaluation that covers representative fault classes. The traditional approach to evaluating a bus design measures its two most important performance parameters: data throughput and data latency.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4445613,no
A High Performance Online Storage System for the LHCb Experiment,2008,"This document describes the architecture of the LHCb storage system, and discusses the key criteria which formed the basis of the evaluation of the system. The configuration and implementation of the current solution and its capabilities are also described, accompanied by performance figures.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4448454,no
Data Quality Monitoring Framework for the ATLAS Experiment at the LHC,2008,"Data quality monitoring (DQM) is an integral part of the data taking process of HEP experiments. DQM involves automated analysis of monitoring data through user-defined algorithms and relaying the summary of the analysis results to the shift personnel while data is being processed. In the online environment, DQM provides the shifter with current run information that can be used to overcome problems early on. During the offline reconstruction, more complex analysis of physics quantities is performed by DQM, and the results are used to assess the quality of the reconstructed data. The ATLAS data quality monitoring framework (DQMF) is a distributed software system providing DQM functionality in the online environment. The DQMF has a scalable architecture achieved by distributing the execution of the analysis algorithms over a configurable number of DQMF agents running on different nodes connected over the network. The core part of the DQMF is designed to have dependence only on software that is common between online and offline (such as ROOT) and therefore the same framework can be used in both environments. This paper describes the main requirements, the architectural design, and the implementation of the DQMF.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4448489,no
The CMS Muon System and Its Performance in the CMS Cosmic Challenge,2008,"The CMS Muon System exploits three different detection technologies in both the central and forward regions, in order to provide a good muon identification and efficient event selection via a multi-level trigger. In summer 2006 the CMS solenoid has been switched on for the first time, generating successfully the designed magnetic field of B = 4 T. During this period also a slice of the CMS detector was operational (the ""CMS Cosmic Challenge"") including a subset of detectors from all three muon subsystems along with the muon hardware alignment. A dedicated CMS cosmic muon trigger had to be set-up, which was based on the Level-1 trigger inputs from the muon subsystems. The CMS subsystems were integrated in the central DAQ, detector control and data quality monitoring. Cosmic data were taken in different trigger configurations in order to study the behavior of muon detectors in the magnetic field, combined detector operations and trigger synchronization. The recorded data are used to study alignment with tracks, to verify and tune the reconstruction software. The layout and status of the CMS muon system is presented and results of the recent cosmic challenge data taking are discussed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4448515,no
Memory-centric video processing,2008,"This work presents a domain-specific memory subsystem based on a two-level memory hierarchy. It targets the application domain of video post-processing applications including video enhancement and format conversion. These applications are based on motion compensation and/or broad class of content adaptive filtering to provide the highest quality of pictures. Our approach meets the required performance and has sufficient flexibility for the application domain. It especially aims at the implementation-wise most challenging applications: compute-intensive and bandwidth-demanding applications that provide the highest quality at high picture resolutions. The lowest level of the memory hierarchy, closest to the processing element, the L0 scratchpad, is organized specifically to enable fast retrieval of an arbitrarily positioned 2-D block of pixels to the processing element. To guarantee the performance, most of its addressing logic is hardwired, leaving a user a set of API for initialization and storing/loading the data to/from the L0 scratchpad. The next level of the memory hierarchy, the L1 scratchpad, minimizes the off-chip memory bandwidth requirements. The L1 scratchpad is organized specifically to enable efficient aligned block-based accesses. With lower data rates compared to the L0 scratchpad and aligned block access, software-based addressing is used to enable full flexibility. The two-level memory hierarchy exploits prefetching to further improve the performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4453849,no
A Framework for Estimating the Impact of a Distributed Software System's Architectural Style on its Energy Consumption,2008,"The selection of an architectural style for a given software system is an important factor in satisfying its quality requirements. In battery-powered environments, such as mobile and pervasive systems, efficiency with respect to energy consumption has increasingly been recognized as an important quality attribute. In this paper, we present a framework that (1) facilitates early estimation of the energy consumption induced by an architectural style in a distributed software system, and (2) consequently enables an engineer to use energy consumption estimates along with other quality attributes in determining the most appropriate style for a given distributed application. We have applied the framework on five distributed systems styles to date, and have evaluated it for precision and accuracy using a particular middleware platform that supports the implementation of those styles. In a large number of application scenarios, our framework exhibited excellent precision, in that it was consistently able to correctly rank the five styles and estimate the relative differences in their energy consumptions. Moreover, the framework has also proven to be accurate: its estimates were within 7% of the different style implementations ' actually measured energy consumptions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4459169,no
Model-Based Gaze Direction Estimation in Office Environment,2008,"In this paper, we present a model-based approach for gaze direction estimation in office environment. An overlapped elliptical model is used in detection of head, and Bayesian network model is used in estimation of gaze direction. The head consists of two regions which are face and hair region, and it can be represented by two overlapped ellipses. We use its spatial layout based on relative angle of two ellipses and size ratio of two ellipses as prior information for gaze direction estimation. In an image, the face regions are detected based on color and shape information, the hair regions are detected based on color information. The head is tracked by mean shift algorithm and adjustment method for image sequence. The performance of the proposed approach is illustrated on various image sequences obtained from office environment, and we show goodness of gaze direction estimation quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4459595,no
A Display Simulation Toolbox for Image Quality Evaluation,2008,"The output of image coding and rendering algorithms are presented on a diverse array of display devices. To evaluate these algorithms, image quality metrics should include more information about the spatial and chromatic properties of displays. To understand how to best incorporate such display information, we need a computational and empirical framework to characterize displays. Here we describe a set of principles and an integrated suite of software tools that provide such a framework. The display simulation toolbox (DST) is an integrated suite of software tools that help the user characterize the key properties of display devices and predict the radiance of displayed images. Assuming that pixel emissions are independent, the DST uses the sub-pixel point spread functions, spectral power distributions, and gamma curves to calculate display image radiance. We tested the assumption of pixel independence for two liquid crystal device (LCD) displays and two cathode-ray tube (CRT) displays. For the LCD displays, the independence assumption is reasonably accurate. For the CRT displays it is not. The simulations and measurements agree well for displays that meet the model assumptions and provide information about the nature of the failures for displays that do not meet these assumptions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4463754,no
Dynamic coupling measurement of object oriented software using trace events,2008,"Software metrics are increasingly playing a central role in the planning and control of software development projects. Coupling measures have important applications in software development and maintenance. They are used to reason about the structural complexity of software and have been shown to predict quality attributes such as fault-proneness, ripple effects of changes and changeability. Coupling or dependency is the degree to which each program module relies on each one of the other modules. Coupling measures characterize the static usage dependencies among the classes in an object-oriented system. Traditional coupling measures take into account only ""static"" couplings. They do not account for ""dynamic"" couplings due to polymorphism and may significantly underestimate the complexity of software and misjudge the need for code inspection, testing and debugging. This is expected to result in poorer predictive accuracy of the quality models that utilize static coupling measurement. In this paper, We propose dynamic coupling measurement techniques. First the source code is introspected and all the functions are added with some trace events. Then the source code is compiled and allowed to run. During runtime the trace events are logged. This log report provides the actual function call information (AFCI) during the runtime. Based on AFCI the source code is filtered to arrive the actual runtime used source code (ARUSC). The ARUSC is then given for any standard coupling technique to get the dynamic coupling.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4469179,no
An Adaptive Digital Front-End for Multimode Wireless Receivers,2008,"The ongoing evolution from 2G to 3G and beyond requires multimode operation of wireless cellular transceivers. This paper discusses the design of an adaptive wireless receiver for multimode operation and power efficiency. This is achieved by sharing digital signal processing stages for all operation modes and adapting them such that performance requirements are met at a minimum power consumption. The flexibility a digital front-end introduces in combination with an analog zero-intermediate-frequency receiver is studied with respect to the main cellular communication standards (Global System for Mobile Communication, Enhanced Data Rates for Global Evolution, Wide-Band Code-Division Multiple Access (WCDMA)/High-Speed Downlink Packet Access, and CDMA2000 and satellite navigation systems (Galileo), with considerations of impacts on the capability of an implementation of a software-defined radio. The proposed approach is verified by simulations exhibiting an error-vector magnitude of 2.9% in Universal Mobile Telecommunications System mode while the estimated power consumption can be reduced by 62% versus full featured mode at reasonable degradation in modulation quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4472697,no
Performance of Three Mode-Meter Block-Processing Algorithms for Automated Dynamic Stability Assessment,2008,The frequency and damping of electromechanical modes offer considerable insight into the dynamic stability properties of a power system. The performance properties of three mode-estimation block-processing algorithms from the perspective of near real-time automated stability assessment are demonstrated and examined. The algorithms are: the extended modified Yule Walker (YW); extended modified Yule Walker with spectral analysis (YWS); and sub-space system identification (N4SID). The YW and N4SID have been introduced in previous publications while the YWS is introduced here. Issues addressed include: stability assessment requirements; automated subset selecting identified modes; using algorithms in an automated format; data assumptions and quality; and expected algorithm estimation performance.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4476159,no
Performance analysis of multi-homed transport protocols with network failure tolerance,2008,"The performance of multi-homed transport protocols tolerant of network failure is studied. It evaluates the performance of different retransmission policies combined with path failure detection thresholds, infinite or finite receive buffers for various path bandwidths, delays and loss rate conditions through stream control transmission protocol simulation. The results show that retransmission policies perform differently with different path failure detection threshold configurations. It identifies that retransmission of all data on an alternate path with the path failure detection threshold set to zero performs the best in symmetric path conditions but its performance degrades acutely in asymmetric path conditions even when the alternate path delay is shorter than the primary path delay. It illustrates that retransmission of all data on the same path with the path failure detection threshold set to one or zero gives the most stable performance in all path configurations.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4479528,no
An On-Demand Test Triggering Mechanism for NoC-Based Safety-Critical Systems,2008,"As embedded and safety-critical applications begin to employ many-core SoCs using sophisticated on-chip networks, ensuring system quality and reliability becomes increasingly complex. Infrastructure IP has been proposed to assist system designers in meeting these requirements by providing various services such as testing and error detection, among others. One such service provided by infrastructure IP is concurrent online testing (COLT) of SoCs. COLT allows system components to be tested in-field and during normal operation of the SoC However, COLT must be used judiciously in order to minimize excessive test costs and application intrusion. In this paper, we propose and explore the use of an anomaly-based test triggering unit (ATTU) for on-demand concurrent testing of SoCs. On-demand concurrent testing is a novel solution to satisfy the conflicting design constraints of fault-tolerance and performance. Ultimately, this ensures the necessary level of design quality for safety-critical applications. To validate this approach, we explore the behavior of the ATTU using a NoC-based SoC simulator. The test triggering unit is shown to trigger tests from test infrastructure IP within 1 ms of an error occurring in the system while detecting 81% of errors, on average. Additionally, the ATTU was synthesized to determine area and power overhead.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4479723,no
Quantified Impacts of Guardband Reduction on Design Process Outcomes,2008,"The value of guardband reduction is a critical open issue for the semiconductor industry. For example, due to competitive pressure, foundries have started to incent the design of manufacturing-friendly ICs through reduced model guardbands when designers adopt layout restrictions. The industry also continuously weighs the economic viability of relaxing process variation limits in the technology roadmap [2]. Our work gives the first-ever quantification of the impact of modeling guardband reduction on outcomes from the synthesis, place and route (SP&R) implementation flow. We assess the impact of model guard- band reduction on various metrics of design cycle time and design quality, using open-source cores and production (specifically, ARM/TSMC) 90 nm and 65 nm technologies and libraries. Our experimental data clearly shows the potential design quality and turnaround time benefits of model guardband reduction. For example, we typically (i.e., on average) observe 13% standard-cell area reduction and 12% routed wirelength reduction as the consequence of a 40% reduction in library model guardband; 40% is the amount of guardband reduction reported by IBM for a variation-aware timing methodology [8]. We also assess the impact of guardband reduction on design yield. Our results suggest that there is justification for the design, EDA and process communities to enable guardband reduction as an economic incentive for manufacturing-friendly design practices.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4479839,no
Greensand Mulling Quality Determination Using Capacitive Sensors,2008,"Cast iron foundries typically use molds made from a mixture of sand, clay, and water called greensand. The mixing of the clay and water into the sand requires the clay to be smeared around the sand particles in a very thin layer, which allows the mold to have strength while allowing gas to escape. This mixing is called mulling and takes place in a machine called a muller. At present, the industry uses electrical resistance measurements to determine water quantity in the muller. The resistance measurements can not accurately predict the quality of mulling due to binding of the water to sodium and calcium ions in the clay. Poorly mixed greensand has a high resistance when the water is concentrated in a few areas, then a medium mixed greensand has a lower resistance because the water present between the sensors, and a well mulled sand has a higher resistance when the clay binds the water. This paper investigates the feasibility of using capacitive sensors to measure mulling quality using the simulation software Ansoft Maxwell. A second investigation of this paper is to find the ability of capacitance sensors to determine the drying effect of molds delayed in the casting process.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4480245,no
Momentum-Based Motion Detection Methodology for Handoff in Wireless Networks,2008,"This paper presents a novel motion detection scheme by using the momentum of received signal sstrength (MRSS) to improve the quality of handoff in a general wireless network. MRSS can detect the motion state of a mobile node (MN) without assistance of any positioning service. Although MRSS is sensitive in detecting user's motion, it is static and fails to detect quickly the motion changes of users. Thus, a novel motion state dependent MRSS scheme called dynamic MRSS (DMRSS) algorithm is proposed to address this issue. Extensive simulation experiments were conducted to study performance of our presented algorithms. The simulation results show that MRSS and DMRSS can be used to assist a handoff algorithm in substantially reducing unnecessary handoff and saving power.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4482789,no
SOSRAID-6: A Self-Organized Strategy of RAID-6 in the Degraded Mode,2008,"The distinct benefit of RAID-6 is that it provides higher reliability than the other RAID levels for tolerating double disk failures. Whereas, when a disk fails the read/write operations on the failed disk will be redirected to all the surviving disks, which will increase the burden of the surviving disks, the probability of the disk failure and the energy consumption along with the degraded performance issue. In this paper, we present a Self-Organized Strategy (SOS) to improve the performance of RAID-6 in the degraded mode. SOS organizes the data on the failed disks to the corresponding parity locations on first access. Then the later accesses to the failed disks will be redirected to the parity locations rather than all the surviving disks. Besides the performance improvement the SOSRAID-6 reduces the failure probability of the survived disks and is more energy efficient compared with the Traditional RAID-6. With the theoretical evaluation we find that the SOSRAID-6 is more powerful than the TRAID-6.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4482907,no
eMuse: QoS Guarantees for Shared Storage Servers,2008,"Storage consolidation as a perspective paradigm inevitably leads to the extensive installations of shared storage servers in product environments. However, owing to the dynamics of both workloads and storage systems, it is pragmatic only if each workload accessing common storage servers can surely possess a specified minimum share of system resources even when competing with other workloads and consequently obtain predictable quality of service (QoS). This paper presents an I/O scheduling framework for shared storage servers. The eMuse algorithm in the framework employs a dynamic assignment mechanism that not only accommodates a weighted bandwidth share for every active workload, but also fulfills their latency requirements through a fair queuing policy. Experimental results demonstrate that our scheduling framework can accomplish performance isolation among multiple competing workloads as well as the effective utilization of system resources.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4482924,no
Semi-quantitative Modeling for Managing Software Development Processes,2008,"Software process modeling has become an essential technique for managing software development processes. However, purely quantitative process modeling requires a detailed understanding and accurate measurement of software process, which relies on reliable and precise history data. This paper presents a semi-quantitative process modeling approach to model and manage software development processes. It allows for the existence of uncertainty and contingency during software development, and facilitates a manager's qualitative and quantitative estimates and assessments of process progress. We demonstrate its value and flexibility by developing semi-quantitative models of the test-and-fix process of incremental software development. Results conclude that the semi-quantitative process modeling approach can support process or project management activities, including estimating, planning, tracking and decision making throughout the software development cycle.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4483194,no
Checklist Based Reading's Influence on a Developer's Understanding,2008,"This paper addresses the influence the checklist based reading inspection technique has on a developer's ability to modify inspected code. Traditionally, inspections have been used to detect defects within the development life cycle. This research identified a correlation between the number of defects detected and the successful code extensions for new functionality unrelated to the defects. Participants reported that having completed a checklist inspection, modifying the code was easier because the inspection had given them an understanding of the code that would not have existed otherwise. The results also showed a significant difference in how developers systematically modified code after completing a checklist inspection when compared to those who had not performed a checklist inspection. This study has shown that applying software inspections for purposes other than defect detection include software understanding and comprehension.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4483238,no
Automated Usability Testing Using HUI Analyzer,2008,"In this paper, we present an overview of HUI Analyzer, a tool intended for automating usability testing. The tool allows a user interface's expected and actual use to be captured unobtrusively, with any mismatches indicating potential usability problems being highlighted. HUI Analyzer also supports specification and checking of assertions governing a user interface's layout and actual user interaction. Assertions offer a low cost means of detecting usability defects and are intended to be checked iteratively during a user interface's development. Hotspot analysis is a feature that highlights the relative use of GUI components in a form. This is useful in informing form layout, for example to collocate heavily used components thereby reducing unnecessary scrolling or movement. Based on evaluation, we have found HUI Analyzer's performance in detecting usability defects to be comparable to conventional formal user testing. However the time taken by HUI Analyzer to automatically process and analyze user interactions is much less than that for formal user testing.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4483248,no
A Design Approach for Soft Error Protection in Real-Time Embedded Systems,2008,"The decreasing line widths employed in semiconductor technologies means that soft errors are an increasing problem in modern system on a chip designs. Approaches adopted so far have focused on recovery after detection. In real-time systems, though, that can easily lead to missed deadlines. This paper proposes a preventative approach. Specifically a design methodology that uses metrics in design space exploration that highlight where in the structure of the systems model and at what point in its behaviour, protection is needed against soft errors. The approach does not eliminate the impact of soft errors completely, but aims to significantly reduce their impact.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4483256,no
Test Suite for the LHCb Muon Chambers Quality Control,2008,"This paper describes the apparatus and the procedures implemented to test the front-end (FE) electronics of the LHCb muon detector multi wire proportional chambers (MWPC). Aim of the test procedure is to diagnose every FE channel of a given chamber by performing an analysis of the noise rate versus threshold and of the performances at the operational thresholds. Measurements of the key noise parameters, obtained while performing quality tests on the MWPC chambers before the installation on the experiment, are presented. The test suite proved to be an automatic, fast and user-friendly system for mass production tests of chambers. It provided the electronic identification of every chamber and FE board, and the storage and bookkeeping of test results that will be made available to the experiment control system during data taking.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4484240,no
Thermal Balancing Policy for Streaming Computing on Multiprocessor Architectures,2008,"As feature sizes decrease, power dissipation and heat generation density exponentially increase. Thus, temperature gradients in multiprocessor systems on chip (MPSoCs) can seriously impact system performance and reliability. Thermal balancing policies based on task migration have been proposed to modulate power distribution between processing cores to achieve temperature flattening. However, in the context of MPSoC for multimedia streaming computing, where timeliness is critical, the impact of migration on quality of service must be carefully analyzed. In this paper we present the design and implementation of a lightweight thermal balancing policy that reduces on-chip temperature gradients via task migration. This policy exploits run-time temperature and load information to balance the chip temperature. Moreover, we assess the effectiveness of the proposed policy for streaming computing architectures using a cycle-accurate thermal-aware emulation infrastructure. Our results using a real-life software defined radio multitask benchmark show that our policy achieves thermal balancing while keeping migration costs bounded.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4484766,no
SLOT: A Fast and Accurate Technique to Estimate Available Bandwidth in Wireless IEEE 802.11,2008,"Accuracy of available estimated bandwidth and convergence delay algorithm are researchersâ€?challenges in wireless network measurements. Currently a variety of tools are developed to estimate the end-to-end network available bandwidth such as TOPP (Train Of Packet Pair), SLoPS (Self Loading Periodic Stream), Spurce and Variable Packet Size etc. These tools are important to improve the QoS (Quality of Service) of demanding applications. However the performances of the previous tools are very different in term of probing time and estimation accuracy. In this paper we present a fast and accurate bandwidth estimation technique named SLOT. By using NS2 the performance of SLOT technique will be evaluated and the obtain results are analysed by MATLAB software and compared with the TOPP and the SLoPS ones. We show that SLOT has shorter probing time than the TOPP one, and it provides more accurate available estimated bandwidth than the SLoPS one.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4488904,no
A Coverage-Based Handover Algorithm for High-speed Data Service,2008,"4G supports various types of services. The coverage of high-speed data service is smaller than that of low-speed data service, which make the high-speed data service users occur dropping before reaching the handover area. In order to solve the problem, this paper proposes A Coverage-based Handover Algorithm for High-speed Data Service (CBH), which extends the coverage of high-speed data service by reducing source rate and makes these users acquire "";transient coverage"";. Meanwhile, FSES (Faint Sub-carrier Elimination Strategy) is introduced, which utilizes the "";transient handover QoS""; and reduces the handover effect to target cell for high-speed data service users. The simulation results shows that the new algorithm can improve the whole system performance, reduce the handover dropping probability and new call blocking probability, enhance the resource utilization ratio.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4489556,no
CiCUTS: Combining System Execution Modeling Tools with Continuous Integration Environments,2008,"System execution modeling (SEM) tools provide an effective means to evaluate the quality of service (QoS) of enterprise distributed real-time and embedded (DRE) systems. SEM tools facilitate testing and resolving performance issues throughout the entire development life-cycle, rather than waiting until final system integration. SEM tools have not historically focused on effective testing. New techniques are therefore needed to help bridge the gap between the early integration capabilities of SEM tools and testing so developers can focus on resolving strategic integration and performance issues, as opposed to wrestling with tedious and error-prone low-level testing concerns. This paper provides two contributions to research on using SEM tools to address enterprise DRE system integration challenges. First, we evaluate several approaches for combining continuous integration environments with SEM tools and describe CiCUTS, which combines the CUTS SEM tool with the CruiseControl.NET continuous integration environment. Second, we present a case study that shows how CiCUTS helps reduce the time and effort required to manage and execute integration tests that evaluate QoS metrics for a representative DRE system from the domain of shipboard computing. The results of our case study show that CiCUTS helps developers and testers ensure the performance of an example enterprise DRE system is within its QoS specifications throughout development, instead of waiting until system integration time to evaluate QoS.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4492388,no
Estimation of Defects Based on Defect Decay Model: ED^{3}M,2008,"An accurate prediction of the number of defects in a software product during system testing contributes not only to the management of the system testing process but also to the estimation of the product's required maintenance. Here, a new approach called ED<sup>3</sup>M is presented that computes an estimate of the total number of defects in an ongoing testing process. ED<sup>3</sup>M is based on estimation theory. Unlike many existing approaches the technique presented here does not depend on historical data from previous projects or any assumptions about the requirements and/or testers' productivity. It is a completely automated approach that relies only on the data collected during an ongoing testing process. This is a key advantage of the ED<sup>3</sup>M approach, as it makes it widely applicable in different testing environments. Here, the ED<sup>3</sup>M approach has been evaluated using five data sets from large industrial projects and two data sets from the literature. In addition, a performance analysis has been conducted using simulated data sets to explore its behavior using different models for the input data. The results are very promising; they indicate the ED<sup>3</sup>M approach provides accurate estimates with as fast or better convergence time in comparison to well-known alternative techniques, while only using defect data as the input.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4492790,no
Reengineering Idiomatic Exception Handling in Legacy C Code,2008,"Some legacy programming languages, e.g., C, do not provide adequate support for exception handling. As a result, users of these legacy programming languages often implement exception handling by applying an idiom. An idiomatic style of implementation has a number of drawbacks: applying idioms can be fault prone and requires significant effort. Modern programming languages provide support for structured exception handling (SEH) that makes idioms largely obsolete. Additionally, aspect-oriented programming (AOP) is believed to further reduce the effort of implementing exception handling. This paper investigates the gains that can be achieved by reengineering the idiomatic exception handling of a legacy C component to these modern techniques. First, we will reengineer a C component such that its exception handling idioms are almost completely replaced by SEH constructs. Second, we will show that the use of AOP for exception handling can be beneficial, even though the benefits are limited by inconsistencies in the legacy implementation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4493308,no
Visual Detection of Design Anomalies,2008,"Design anomalies, introduced during software evolution, are frequent causes of low maintainability and low flexibility to future changes. Because of the required knowledge, an important subset of design anomalies is difficult to detect automatically, and therefore, the code of anomaly candidates must be inspected manually to validate them. However, this task is time- and resource-consuming. We propose a visualization-based approach to detect design anomalies for cases where the detection effort already includes the validation of candidates. We introduce a general detection strategy that we apply to three types of design anomaly. These strategies are illustrated on concrete examples. Finally we evaluate our approach through a case study. It shows that performance variability against manual detection is reduced and that our semi-automatic detection has good recall for some anomaly types.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4493326,no
Modularity-Oriented Refactoring,2008,"Refactoring, in spite of widely acknowledged as one of the best practices of object-oriented design and programming, still lacks quantitative grounds and efficient tools for tasks such as detecting smells, choosing the most appropriate refactoring or validating the goodness of changes. This is a proposal for a method, supported by a tool, for cross-paradigm refactoring (e.g. from OOP to AOP), based on paradigm and formalism-independent modularity assessment.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4493330,no
Assessing quality of web based systems,2008,"This paper proposes an assessment model for Web-based systems in terms of non-functional properties of the system. The proposed model consists of two stages: (i) deriving quality metrics using goal-question-metric (GQM) approach; and (ii) evaluating the metrics to rank a Web based system using multi-element component comparison analysis technique. The model ultimately produces a numeric rating indicating the relative quality of a particular Web system in terms of selected quality attributes. We decompose the quality objectives of the web system into sub goals, and develop questions in order to derive metrics. The metrics are then assessed against the defined requirements using an assessment scheme.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4493613,no
An evaluation method for aspectual modeling of distributed software architectures,2008,"Dealing with crosscutting requirements in software development usually makes the process more complex. Modeling and analyzing of these requirements in the software architecture facilitate detecting architectural risks early. Distributed systems have more complexity and so these facilities are much useful in development of such systems. Aspect oriented Architectural Description Languages (ADD) have emerged to represent solutions for discussed problems; nevertheless, imposing radical changes to existing architectural modeling methods is not easily acceptable by architects. Software architecture analysis methods, furthermore, intend to verify that the quality requirements have been addressed properly. In this paper, we enhance ArchC# through utilization of aspect features with an especial focus on Non-Functional Requirements (NFR). ArchC# is mainly focused on describing architecture of distributed systems; in addition, it unifies software architecture with an object- oriented implementation to make executable architectures. Moreover, in this paper, a comparative analysis method is presented for evaluation of the result. All of these materials are illustrated along with a case study.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4493639,no
Derivation of Fault Tolerance Measures of Self-Stabilizing Algorithms by Simulation,2008,"Fault tolerance measures can be used to distinguish between different self-stabilizing solutions to the same problem. However, derivation of these measures via analysis suffers from limitations with respect to scalability of and applicability to a wide class of self-stabilizing distributed algorithms. We describe a simulation framework to derive fault tolerance measures for self-stabilizing algorithms which can deal with the complete class of self-stabilizing algorithms. We show the advantages of the simulation framework in contrast to the analytical approach not only by means of accuracy of results, range of applicable scenarios and performance, but also for investigation of the influence of schedulers on a meta level and the possibility to simulate large scale systems featuring dynamic fault probabilities.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4494419,no
An Innovative Transient-Based Protection Scheme for MV Distribution Networks with Distributed Generation,2008,This paper presents a new transient based scheme to detect single phase to ground faults (or grounded faults in general) in distribution systems with high penetration of distributed generation. This algorithm combines the fault direction based approach and the distance estimation based approach in order to determine the faulted section. The wavelet coefficients of the transient fault currents measured at the interconnection points of the network are used to determine the direction of fault currents and to estimate the distance of the fault. The simulations have been carried out by using DigSilent software package and the results have been processed in MATLAB using the Wavelet Toolbox.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4496999,no
Quality-Aware Retrieval of Data Objects from Autonomous Sources for Web-Based Repositories,2008,"The goal of this paper is to develop a framework for designing good data repositories for Web applications. The central theme of our approach is to employ statistical methods to predict quality metrics. These prediction quantities can be used to answer important questions such as: How soon should the local repository be synchronized to have a quality of at least 90% precision with certain confidence level? Suppose the local repository was synchronized three days ago, how many objects could have been deleted at the remote source since then?",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4497600,no
The Infamous Ratio Measure,2008,Examines the use of ratios and other derived metrics in software measurement.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4497752,no
Surveillance and Quality Evaluation System for Real-Time Audio Signal Streaming,2008,"This paper deals with the design of an algorithm and application for an automatic system of real-time audio signal quality surveillance on personal computers. This system works without any knowledge of how the audio signal is processed. The audio signal quality and possible signal failures are monitored by being compared with the original signal in the time-frequency domain. The system supports several audio formats from mono to 5.1 and downmix for comparison between different audio formats. For high performance, SIMD technologies are used. The whole system consists of several local surveillance applications (clients) and a server application with SNMP client that monitors the local application states using the TCP/IP network.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4498220,no
Modeling Resource Sharing Dynamics of VoIP Users over a WLAN Using a Game-Theoretic Approach,2008,"We consider a scenario in which users share an access point and are mainly interested in VoIP applications. Each user is allowed to adapt to varying network conditions by choosing the transmission rate at which VoIP traffic is received. We denote this adaptation process by end-user congestion control, our object of study. The two questions that we ask are: (1) what are the performance consequences of letting the users to freely choose their rates? and (2) how to explain the adaptation process of the users? We set a controlled lab experiment having students as subject to answer the first question, and we extend an evolutionary game-theoretic model to address the second. Our partial answers are the following: (1) free users with local information can reach an equilibrium which is close to optimal from the system perspective. However, the equilibrium can be unfair; (2) the adaptation of the users can be explained using a game theoretic model. We propose a methodology to parameterize the latter, which involves active network measurements, simulations and an artificial neural network to estimate the QoS perceived by the users in each of the states of the model.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4509739,no
Speech privacy for modern mobile communication systems,2008,"Speech privacy techniques are used to scramble clear speech into an unintelligible signal in order to avoid eavesdropping. Some analog speech-privacy equipments (scramblers) have been replaced by digital encryption devices (comsec), which have higher degree of security but require complex implementations and large bandwidth for transmission. However, if speech privacy is wanted in a mobile phone using a modern commercial codec, such as the AMR (adaptive multirate) codec, digital encryption may not be an option due to the fact that it requires internal hardware and software modifications. If encryption is applied before the codec, poor voice quality may result, for the vocoder would handle digitally encrypted signal resembling noise. On the other hand, analog scramblers may be placed before the voice encoder without causing much penalty to its performance. Analog scramblers are intended in applications where the degree of security is not too critical and hardware modifications are prohibitive due to its high cost. In this article we investigate the use of different techniques of voice scramblers applied to mobile communications vocoders. We present our results in terms of LPC and cepstral distances, and PESQ values.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4517975,no
Quality Attributes - Architecting Systems to Meet Customer Expectations,2008,"This paper addresses the use of quality attributes as a mechanism for making objective decisions about architectural tradeoffs and for providing reasonably accurate predictions about how well candidate architectures will meet customer expectations. Typical quality attributes important to many current systems of interest include: performance, dependability, security, and safety. This paper begins with an examination of how quality attributes and architectures are related, including some the seminal work in the area, and a survey of the current standards addressing product quality and evaluation. The implications for both the customer and the system developer of employing a quality- attribute-based approach to architecture definition and trade-off are then briefly explored. The paper also touches on the relationship of an architectural quality-attribute-based approach to engineering process and process maturity. Lastly the special concerns of architecting for system assurance are addressed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4518988,no
System of Systems Issues for the 2008 U. S. National Healthcare Information Network Remote Patient Monitoring Requirements,2008,"This paper describes a number of new system of systems engineering (SoSE) issues that must be addressed in order to design and deploy a safe, secure, and private informatics infrastructure for remote patient monitoring systems that are interoperable with the emerging U.S. National Healthcare Information Network (NHIN). Motivations for NHIN's ambitious remote patient monitoring goals - such as reducing the cost of care and improving medical care quality for chronically ill patients - are introduced, and the technological requirements and challenges that arise are described. This paper demonstrates the use of SoSE modeling, simulation, verification and validation techniques similar to the emerging draft of INCOSE's SoSE Engineering Guide to improve the success of such projects. The tools used include event driven modeling and software simulation tools to assist in the design and evaluation of predictive software models that simulate key safety and performance aspects of proposed remote patient monitoring systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4519018,no
The Role of System Behavior in Diagnostic Performance,2008,"The diagnostic performance of system built-in-test has historically suffered from deficiencies such as high false alarm rates, high undetected failure rates and high fault isolation ambiguity. In general these deficiencies impose a burden on maintenance resources and can affect mission readiness and effectiveness. Part of the problem has to do with the blurred distinction between physical faults and the test failures used to detect those faults. A greater part of the problem has to do with the test limits used to establish pass/fail criteria. If the limits do not reflect system behavior that is to be expected, given its current no fault (or fault) status, then a test fail result can often be a false alarm, and a test pass result can often constitute an undetected fault. A model based approach to prediction of system behavior can do much to alleviate the problem.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4519031,no
A Methodology for Performance Modeling of Distributed Event-Based Systems,2008,"Distributed event-based systems (DEBS) are gaining increasing attention in new application areas such as transport information monitoring, event-driven supply-chain management and ubiquitous sensor-rich environments. However, as DEBS increasingly enter the enterprise and commercial domains, performance and quality of service issues are becoming a major concern. While numerous approaches to performance modeling and evaluation of conventional request/reply-based distributed systems are available in the literature, no general approach exists for DEBS. This paper is the first to provide a comprehensive methodology for workload characterization and performance modeling of DEBS. A workload model of a generic DEBS is developed and operational analysis techniques are used to characterize the system traffic and derive an approximation for the mean event delivery latency. Following this, a modeling technique is presented that can be used for accurate performance prediction. The paper is concluded with a case study of a real life system demonstrating the effectiveness and practicality of the proposed approach.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4519556,no
Automatic simulation of transmission line backflashover rate base on ATP,2008,"The design of such a novel software application that can be used to calculate the backflashover rate of transmission line is presented. The software application was developed by combining ATP with VC++. The software structure and the input/output interface are provided, as well as the implementation details. In the calculation, compares the insulators' volt-time curves to the overvoltage to judge the insulators' behavior, considers the system voltage and the induced voltage's influence on backflash lightning withstanding level. A total line backflashover rate calculation example shows that it's convenient for estimating the backflashover rate of a total line and studying the effect of lightning performance improvement methods such as reducing grounding resistance with the software application. This software application's design way is suitable for other transient calculation which using ATP.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4523591,no
Three-phase four-wire DSTATCOM based on a three-dimensional PWM algorithm,2008,"A modified voltage space vector pulse-width modulated (PWM) algorithm for three-phase four-wire distribution static compensator (DSTATCOM) is described in this paper. The mathematical model of shunt-connected three-leg inverter in three-phase four-wire system is studied in a-b-c frame. The p-q-o theory based on the instantaneous reactive power theory is applied to detect the reference current. A fast and generalized applicable three-dimensional space vector modulation (3DSVM) is proposed for controlling a three-leg inverter. The reference voltage vector is decomposed into an offset vector and a two-level vector. So identification of neighboring vectors and dwell times calculation are all settled by a general two-level 3DSVM control. This algorithm can also be applied to multilevel inverter. The zero-sequence component of each vector is considered in order to implement the neutral current compensation. The simulation is performed by EMTDC/PSCAD software. The neutral current, harmonics current, unbalance current and reactive current can be compensated. The result shows that the validity of the proposed 3DSVM can be applied to compensate power quality problems in three-phase four-wire system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4523748,no
Interactive Software and Hardware Faults Diagnosis Based on Negative Selection Algorithm,2008,"Both hardware and software of computer systems are subject to faults. However, traditional methods, ignoring the relationship between software fault and hardware fault, are ineffective to diagnose complex faults between software and hardware. On the basis of defining the interactive effect to describe the process of the interactive software and hardware fault, this paper present a new matrix-oriented negative selection algorithm to detect faults. Furthermore, the row vector distance and matrix distance are constructed to measure elements between the self set and detector set. The experiment on a temperature control system indicates that the proposed algorithm has good fault detection ability, and the method is applicable to diagnose interactive software and hardware faults with small samples.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4525255,no
Intelligibility and Space-based Voice with Relaxed Delay Constraints,2008,"In this paper, we leverage the long range end-to-end scenarios envisioned for lunar and beyond voice conversations by allowing non-traditional additional processing delay for quasi-real-time voice conversations. The concept of improving the quality of end-to-end voice conversations for long delay environments is considered by utilizing Luby transforms on the conjugate-structure algebraic-code-excited linear-prediction (CS-ACELP) codec. In addition, this paper also examines the use of automated speech recognition software as a means of generating a quantifiable metric for speech intelligibility in the spirit of the diagnostic rhyme test (DRT).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4526329,no
Verification of a Byzantine-Fault-Tolerant Self-Stabilizing Protocol for Clock Synchronization,2008,"This paper presents the mechanical verification of a simplified model of a rapid byzantine-fault-tolerant self-stabilizing protocol for distributed clock synchronization systems. This protocol does not rely on any assumptions about the initial state of the system except for the presence of sufficient good nodes, thus making the weakest possible assumptions and producing the strongest results. This protocol tolerates bursts of transient failures, and deterministically converges within a time bound that is a linear function of the self-stabilization period. A simplified model of the protocol is verified using the symbolic model verifier (SMV). The system under study consists of 4 nodes, where at most one of the nodes is assumed to be Byzantine faulty. The model checking effort is focused on verifying correctness of the simplified model of the protocol in the presence of a permanent byzantine fault as well as confirmation of claims of determinism and linear convergence with respect to the self-stabilization period. Although model checking results of the simplified model of the protocol confirm the theoretical predictions, these results do not necessarily confirm that the protocol solves the general case of this problem. Modeling challenges of the protocol and the system are addressed. A number of abstractions are utilized in order to reduce the state space.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4526337,no
Automated Generation and Assessment of Autonomous Systems Test Cases,2008,"Verification and validation testing of autonomous spacecraft routinely culminates in the exploration of anomalous or faulted mission-like scenarios. Prioritizing which scenarios to develop usually comes down to focusing on the most vulnerable areas and ensuring the best return on investment of test time. Rules-of-thumb strategies often come into play, such as injecting applicable anomalies prior to, during, and after system state changes; or, creating cases that ensure good safety-net algorithm coverage. Although experience and judgment in test selection can lead to high levels of confidence about the majority of a system's autonomy, it's likely that important test cases are overlooked. One method to fill in potential test coverage gaps is to automatically generate and execute test cases using algorithms that ensure desirable properties about the coverage. For example, generate cases for all possible fault monitors, and across all state change boundaries. Of course, the scope of coverage is determined by the test environment capabilities, where a faster-than-real-time, high-fidelity, software-only simulation would allow the broadest coverage. Even real-time systems that can be replicated and run in parallel, and that have reliable set-up and operations features provide an excellent resource for automated testing. Making detailed predictions for the outcome of such tests can be difficult, and when algorithmic means are employed to produce hundreds or even thousands of cases, generating predicts individually is impractical, and generating predicts with tools requires executable models of the design and environment that themselves require a complete test program. Therefore, evaluating the results of large number of mission scenario tests poses special challenges. A good approach to address this problem is to automatically score the results based on a range of metrics. Although the specific means of scoring depends highly on the application, the use of formal scoring - metrics has high value in identifying and prioritizing anomalies, and in presenting an overall picture of the state of the test program. In this paper we present a case study based on automatic generation and assessment of faulted test runs for the Dawn mission, and discuss its role in optimizing the allocation of resources for completing the test program.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4526484,no
Lights-Out Scenario Testing for the New Horizons Autonomous Operations Subsystem,2008,"New horizons is a NASA sponsored mission to explore Pluto and its largest moon Charon. The new horizons spacecraft, designed, built and operated by the Johns Hopkins University Applied Physics Laboratory (APL), was successfully launched in January 2006 and will perform its primary mission at Pluto in the summer of 2015. To support this mission, the spacecraft is equipped with onboard software that provides a rule based expert system for performing autonomous fault detection and recovery. This system has been updated nine times since launch and is continuously being tested to ascertain its performance in various spacecraft states. The test approach for the autonomous fault protection subsystem is to perform a combination of unit-level tests and full system scenario tests. For the scenario tests, we have developed a ""lights out"" test method and have been using it to reduce the time required to run each fault scenario test. This approach reduces the time it takes to develop a test, reduces the number of man hours required to run the test, and decouples the initial spacecraft state from the mechanisms used to inject faults. Decoupling the initial state from the fault injection allows for easy expansion in the number of initial state/fault combinations that can be tested. This significantly improves the test coverage of the scenario test suite. Using this approach, we will be able to run more tests and increase our working knowledge of the performance of the fault protection subsystem. This paper describes the evolution, benefits and cautions of the ""lights out"" scenario test process.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4526485,no
An Introspection Framework for Fault Tolerance in Support of Autonomous Space Systems,2008,"This paper describes a software system designed for the support of future autonomous space missions by providing an infrastructure for runtime monitoring, analysis, and feedback. The objective of this research is to make mission software executing on parallel on-board architectures fault tolerant through an introspection mechanism that provides automatic recovery minimizing the loss of function and data. Such architectures are essential for future JPL missions because of their increased need for autonomy along with enhanced on-board computational capabilities while in deep space or time-critical situations. The standard framework for introspection described in this paper integrates well with existing flight software architectures and can serve as an enabling technology for the support of such systems. Furthermore, it separates the introspection capability from applications and the underlying system, providing a generic framework that can be also applied to a broad range of problems beyond fault tolerance, such as behavior analysis, intrusion detection, performance tuning, and power management.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4526592,no
False Alarm Mitigation of Vibration Diagnostic Systems,2008,"False alarms in legacy aircraft diagnostic systems have negatively impacted fleet maintenance costs and mission readiness. As the industry moves towards more advanced prognostic and health management (PHM) solutions, a reduction in false alarms is needed to reduce the cost and readiness burdens that have plagued legacy systems. It is therefore important to understand why these false alarms occur and how they are generated so appropriate mitigation solutions can be included in next-generation diagnostic systems. This paper examines four major sources of false alarms in the development of vibration diagnostics (faulty sensor performance, transient system operating conditions, improper health indicator selection, and inadequate fault detection logic) and details a solution designed to mitigate their impact. An overview of the developed false alarm statistics toolbox for PHM (FAST PHMtrade) software is also provided to illustrate how the software guides design engineers through the processes of verifying data, processing for diagnostic features, analyzing feature performances, developing ""virtual"" features through fusion, and deriving statistically optimized feature thresholds. The developed approach will improve the overall performance, robustness, and reliability of vibration diagnostic and prognostics systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4526620,no
Software Maintenance Implications on Cost and Schedule,2008,"Software Maintenance Implications on Cost and Schedule. The dictionary defines maintenance as, ""The work of keeping something in proper order."" However, this definition does not necessarily fit for software. Software maintenance is different from hardware maintenance because software doesn't physically wear out, but often gets less useful with age. Software is typically delivered with undiscovered flaws. Therefore, software maintenance is: ""The process of modifying existing operational software while leaving its primary functions intact."" Maintenance typically exceeds fifty percent of the systems' life cycle cost. Additionally, software is highly dependent on defined maintenance rigor and operational life expectancy. Software maintenance generally includes sustaining engineering and new function development; corrective changes (fixing bugs); adapting to new requirements (OS upgrade, new processor); perfecting or improving existing functions (improve speed, performance); enhancing application with (minor) new functions (new feature.) Since software maintenance costs can be somewhat set by definition, the implications on cost and schedule must be evaluated. Development decisions, processes, and tools can impact maintenance costs. But, generally even a perfectly delivered system quickly needs upgrades. While software maintenance can be treated as a level of effort activity, there are consequences on quality, functionality, reliability, cost and schedule that can be mitigated through the use of parametric estimation techniques.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4526688,no
A Methodology for Performance Modeling and Simulation Validation of Parallel 3-D Finite Element Mesh Refinement With Tetrahedra,2008,"The design and implementation of parallel finite element methods (FEMs) is a complex and error-prone task that can benefit significantly by simulating models of them first. However, such simulations are useful only if they accurately predict the performance of the parallel system being modeled. The purpose of this contribution is to present a new, practical methodology for validation of a promising modeling and simulation approach for parallel 3-D FEMs. To meet this goal, a parallel 3-D unstructured mesh refinement model is developed and implemented based on a detailed software prototype and parallel system architecture parameters in order to simulate the functionality and runtime behavior of the algorithm. Estimates for key performance measures are derived from these simulations and are validated with benchmark problem computations obtained using the actual parallel system. The results illustrate the potential benefits of the new methodology for designing high performance parallel FEM algorithms.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4526979,no
Benchmarking Classification Models for Software Defect Prediction: A Proposed Framework and Novel Findings,2008,"Software defect prediction strives to improve software quality and testing efficiency by constructing predictive classification models from code attributes to enable a timely identification of fault-prone modules. Several classification models have been evaluated for this task. However, due to inconsistent findings regarding the superiority of one classifier over another and the usefulness of metric-based classification in general, more research is needed to improve convergence across studies and further advance confidence in experimental results. We consider three potential sources for bias: comparing classifiers over one or a small number of proprietary data sets, relying on accuracy indicators that are conceptually inappropriate for software defect prediction and cross-study comparisons, and, finally, limited use of statistical testing procedures to secure empirical findings. To remedy these problems, a framework for comparative software defect prediction experiments is proposed and applied in a large-scale empirical comparison of 22 classifiers over 10 public domain data sets from the NASA Metrics Data repository. Overall, an appealing degree of predictive accuracy is observed, which supports the view that metric-based classification is useful. However, our results indicate that the importance of the particular classification algorithm may be less than previously assumed since no significant performance differences could be detected among the top 17 classifiers.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4527256,no
FEDC: Control Flow Error Detection and Correction for Embedded Systems without Program Interruption,2008,"This paper proposes a new technique called CFEDC to detect and correct control flow errors (CFEs) without program interruption. The proposed technique is based on the modification of application software and minor changes in the underlying hardware. To demonstrate the effectiveness of CFEDC, it has been implemented on an OpenRISC 1200 as a case study. Analytical results for three workload programs show that this technique detects all CFEs and corrects on average about 81.6% of CFEs. These figures are achieved with zero error detection /correction latency. According to the experimental results, the overheads are generally low as compared to other techniques; the performance overhead and the memory overhead are on average 8.5% and 9.1%, respectively. The area overhead is about 4% and the power dissipation increases by the amount of 1.5% on average.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4529318,no
Quantitative Assessment of Enterprise Security System,2008,In this paper we extend a model-based approach to security management with concepts and methods that provide a possibility for quantitative assessments. For this purpose we introduce security metrics and explain how they are aggregated using the underlying model as a frame. We measure numbers of attack of certain threats and estimate their likelihood of propagation along the dependencies in the underlying model. Using this approach we can identify which threats have the strongest impact on business security ob jectives and how various security controls might differ with regard to their effect in reducing these threats.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4529442,no
Admon: ViSaGe Administration And Monitoring Service For Storage Virtualization in Grid Environment,2008,"This work is part of the ViSaGe project. This project concerns the storage virtualization applied in the grid environment. Its goal is to create and manage virtual storage resources by aggregating geographically distributed physical storage resources shared on a Grid. To these shared resources, a quality of service will be associated and related to the data storage performance. In a Grid environment, sharing resources may improve performances if these resources are well managed and if the management software obtains sufficient knowledge about the grid resources workload (computing nodes, storage nodes and links). The grid resources workload is mainly perceived by a monitoring system. Several existing monitoring systems are available for monitoring the grid resources and applications. Each one provides informations according to its aim. For example, Network Weather Service [6] is designed to monitor grid computing nodes and links. On the other hand, Netlogger [8] monitors grid resources and applications in order to detect application's bottleneck. These systems are useful for a post mortem analysis. However, in ViSaGe, we need a system that analyzes the necessary nodes state during execution time. This system is represented by the ViSaGe Administration and monitoring service ""Admon"". In this paper, we present our scalable distributed system: Admon. Admon traces applications, and calculates the system resources consumption (CPU, Disks, Links). Its originality consists in providing a new opportunity to improve virtual data storage performance by using workload's constraints (e.g., maximum CPU usage percentage). Many constraints will be discussed (such as time's constraint). These constraints allow performance improvement by assigning nodes to the ViSaGe's jobs in an effective manner.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4530214,no
Efficient Approximate Wordlength Optimization,2008,"In this paper, the problem of bounding the performance of good wordlength combinations for fixed-point digital signal processing flowgraphs is addressed. By formulating and solving an approximate optimization problem, a lower bounding curve on attainable cost/quality combinations is rapidly calculated. This curve and the associated wordlength combinations are useful in several situations, and can serve as starting points for real design searches. A detailed design example that utilizes these concepts is given.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4531730,no
Adaptive Fault Management of Parallel Applications for High-Performance Computing,2008,"As the scale of high-performance computing (HPC) continues to grow, failure resilience of parallel applications becomes crucial. In this paper, we present FT-Pro, an adaptive fault management approach that combines proactive migration with reactive checkpointing. It aims to enable parallel applications to avoid anticipated failures via preventive migration and, in the case of unforeseeable failures, to minimize their impact through selective checkpointing. An adaptation manager is designed to make runtime decisions in response to failure prediction. Extensive experiments, by means of stochastic modeling and case studies with real applications, indicate that FT-Pro outperforms periodic checkpointing, in terms of reducing application completion times and improving resource utilization, by up to 43 percent.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4531733,no
Localization of IP Links Faults Using Overlay Measurements,2008,"Accurate fault detection and localization is essential to the efficient and economical operation of ISP networks. In addition, it affects the performance of Internet applications such as VoIP, and online gaming. Fault detection algorithms typically depend on spatial correlation to produce a set of fault hypotheses, the size of which increases by the existence of lost and spurious symptoms, and the overlap among network paths. The network administrator is left with the task of accurately locating and verifying these fault scenarios, which is a tedious and time-consuming task. In this paper, we formulate the problem of finding a set of overlay paths that can debug the set of suspected faulty IP links. These overlay paths are chosen from the set of existing measurement paths, which will make overlay measurements meaningful and useful for fault debugging. We study the overlap among overlay paths using various real-life Internet topologies of the two major service carriers in the U.S. We found that with a reasonable number of concurrent failures, it is possible to identify the location of the IP links faults with 60% to 95% success rate. Finally, we identify some interesting research problems in this area.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4534091,no
Fault Tolerance Management for a Hierarchical GridRPC Middleware,2008,"The GridRPC model is well suited for high performance computing on grids thanks to efficiently solving most of the issues raised by geographically and administratively split resources. Because of large scale, long range networks and heterogeneity, Grids are extremely prone to failures. GridRPC middleware are usually managing failures by using 1) TCP or other link network layer provided failure detector, 2) automatic checkpoints of sequential jobs and 3) a centralized stable agent to perform scheduling. Most recent developments have provided some new mechanisms like the optimal Chandra & Toueg & Aguillera failure detector, most numerical libraries now providing their own optimized checkpoint routine and distributed scheduling GridRPC architectures. In this paper we aim at adapting to these novelties by providing the first implementation and evaluation in a grid system of the optimal fault detector, a novel and simple checkpoint API allowing to manage both service provided checkpoint and automatic checkpoint (even for parallel services) and a scheduling hierarchy recovery algorithm tolerating several simultaneous failures. All those mechanisms are implemented and evaluated on a real grid in the DIET middleware.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4534253,no
Fault Tolerance and Recovery of Scientific Workflows on Computational Grids,2008,"In this paper, we describe the design and implementation of two mechanisms for fault-tolerance and recovery for complex scientific workflows on computational grids. We present our algorithms for over-provisioning and migration, which are our primary strategies for fault-tolerance. We consider application performance models, resource reliability models, network latency and bandwidth and queue wait times for batch-queues on compute resources for determining the correct fault-tolerance strategy. Our goal is to balance reliability and performance in the presence of soft real-time constraints like deadlines and expected success probabilities, and to do it in a way that is transparent to scientists. We have evaluated our strategies by developing a Fault-Tolerance and Recovery (FTR) service and deploying it as a part of the Linked Environments for Atmospheric Discovery (LEAD) production infrastructure. Results from real usage scenarios in LEAD show that the failure rate of individual steps in workflows decreases from about 30% to 5% by using our fault-tolerance strategies.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4534303,no
Application Resilience: Making Progress in Spite of Failure,2008,"While measures such as raw compute performance and system capacity continue to be important factors for evaluating cluster performance, such issues as system reliability and application resilience have become increasingly important as cluster sizes rapidly grow. Although efforts to directly improve fault-tolerance are important, it is also essential to accept that application failures will inevitably occur and to ensure that progress is made despite these failures. Application monitoring frameworks are central to providing application resilience. As such, the central theme of this paper is to address the impact that application monitoring detection latency has on the overall system performance. We find that immediate fault detection is not necessary in order to obtain substantial improvement in performance. This conclusion is significant because it implies that less complex, highly portable, and predominately less expensive failure detection schemes would provide adequate application resilience.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4534305,no
Integrating Static Analysis into a Secure Software Development Process,2008,"Software content has grown rapidly in all manner of electronic systems. Meanwhile, society has become increasingly dependent upon the safe and secure operation of these electronic systems. We depend on software for our telecommunications, critical infrastructure, avionics, financial systems, medical information systems, automobiles, and more. Unfortunately, our ability to develop secure software has not improved at the same rate, resulting in increasing reliability and security vulnerabilities. The increase in software vulnerability poses a serious threat to national and homeland security. Vulnerabilities have caused or contributed to blackouts, air traffic control failures, traffic light system breaches, and other well publicized security breaches in critical infrastructure. This threat demands new approaches to secure software development. Static analysis has emerged as a promising technology for improving the security of software and systems. Static analysis tools analyze software to find defects that may go undetected using traditional techniques, such as compilers, human code reviews, and testing. A number of limitations, however, have prevented widespread adoption in software development. Static analysis tools often take prohibitively long to execute and are not well integrated into the software development environment. This paper will introduce a new approach - the integrated static analyzer (ISA) - that solves many of these problems. Specific metrics will be provided to demonstrate how the new approach makes the use of static analysis tools practical and effective for everyday embedded software development. In addition to traditional analysis, the ISA approach enables detection of a new class of security flaws not otherwise practicable.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4534479,no
"SNAP, Small-world Network Analysis and Partitioning: An open-source parallel graph framework for the exploration of large-scale networks",2008,"We present SNAP (small-world network analysis and partitioning), an open-source graph framework for exploratory study and partitioning of large-scale networks. To illustrate the capability of SNAP, we discuss the design, implementation, and performance of three novel parallel community detection algorithms that optimize modularity, a popular measure for clustering quality in social network analysis. In order to achieve scalable parallel performance, we exploit typical network characteristics of small-world networks, such as the low graph diameter, sparse connectivity, and skewed degree distribution. We conduct an extensive experimental study on real-world graph instances and demonstrate that our parallel schemes, coupled with aggressive algorithm engineering for small-world networks, give significant running time improvements over existing modularity-based clustering heuristics, with little or no loss in clustering quality. For instance, our divisive clustering approach based on approximate edge betweenness centrality is more than two orders of magnitude faster than a competing greedy approach, for a variety of large graph instances on the Sun Fire T2000 multicore system. SNAP also contains parallel implementations of fundamental graph-theoretic kernels and topological analysis metrics (e.g., breadth-first search, connected components, vertex and edge centrality) that are optimized for small- world networks. The SNAP framework is extensible; the graph kernels are modular, portable across shared memory multicore and symmetric multiprocessor systems, and simplify the design of high-level domain-specific applications.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4536261,no
Efficient software checking for fault tolerance,2008,"Dramatic increases in the number of transistors that can be integrated on a chip make processors more susceptible to radiation-induced transient errors. For commodity chips which are cost- and energy-constrained, software approaches can play a major role for fault detection because they can be tailored to fit different requirements of reliability and performance. However, software approaches add a significant performance overhead because they replicate the instructions and add checking instructions to compare the results. In order to make software checking approaches more attractive, we use compiler techniqes to identify the ""unnecessary"" replicas and checking instructions. In this paper, we present three techniques. The first technique uses boolean logic to identify code patterns that correspond to outcome tolerant branches. The second technique identifies address checks before loads and stores that can be removed with different degrees of fault coverage. The third technique identifies the checking instructions and shadow registers that are unnecessary when the register file is protected in hardware. By combining the three techniques, the overheads of software approaches can be reduced by an average 50%.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4536435,no
PLP: Towards a realistic and accurate model for communication performances on hierarchical cluster-based systems,2008,"Today, due to many reasons, such as the inherent heterogeneity, the diversity, and the continuous evolving of actual computational supports, writing efficient parallel applications on such systems represents a great challenge. One way to answer this problem is to optimize communications of such applications. Our objective within this work is to design a realistic model able to accurately predict the cost of communication operations on execution environments characterized by both heterogeneity and hierarchical structure. We principally aim to guarantee a good quality of prediction with a neglected additional overhead. The proposed model was applied on point-to-point and collective communication operations and showed by achieving experiments on a hierarchical cluster-based system with heterogeneous resources that the predicted performances are close to measured ones.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4536486,no
Performance assessment of OMG compliant data distribution middleware,2008,"Event-driven architectures (EDAs) are widely used to make distributed mission critical software systems more- efficient and scalable. In the context of EDAs, data distribution service (DDS) is a recent standard by the object management group that offers a rich support for quality- of-service and balances predictable behavior and implementation efficiency. The DDS specification does not outline how messages are delivered, so several architectures are nowadays available. This paper focuses on performance assessment of OMG DDS-compliant middleware technologies. It provides three contributions to the study of evaluating the performance of DDS implementations: 1) describe the challenges to be addressed; 2) propose possible solutions; 3) define a representative workload scenario for evaluating the performance and scalability of DDS platforms. At the end of the paper, a case study of DDS performance assessment, performed with the proposed benchmark, is presented.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4536566,no
Constructing Node-Disjoint Paths in Biswapped Networks (BSNs),2008,"The recent biswapped networks (BSNs) offer a systematic scheme for building large, scalable, modular, and robust parallel architectures of arbitrary basis networks. BSNs are related to well-known swapped or OTIS networks, and are promising because of their attractive performance attributes including structural symmetry and algorithmic efficiency. In this paper we present an efficient general algorithm for constructing a maximal number of node-disjoint paths between two distinct nodes in a BSN built of an arbitrary basis network and analyze its performance. From this algorithm, we prove that a BSN is maximally fault tolerant if its basis network is connected. As an example application, we show that this algorithm finds the desirable node disjoint paths of length at most 1.5D+3 in 0(log<sup>3</sup>N) time when applied to an N-node BSN of diameter D built of a cube, and also estimate the average performance concerning the maximum path length by computer simulation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539357,no
"Relationships between Test Suites, Faults, and Fault Detection in GUI Testing",2008,"Software-testing researchers have long sought recipes for test suites that detect faults well. In the literature, empirical studies of testing techniques abound, yet the ideal technique for detecting the desired kinds of faults in a given situation often remains unclear. This work shows how understanding the context in which testing occurs, in terms of factors likely to influence fault detection, can make evaluations of testing techniques more readily applicable to new situations. We present a methodology for discovering which factors do statistically affect fault detection, and we perform an experiment with a set of test-suite- and fault-related factors in the GUI testing of two fielded, open-source applications. Statement coverage and GUI-event coverage are found to be statistically related to the likelihood of detecting certain kinds of faults.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539528,no
On the Predictability of Random Tests for Object-Oriented Software,2008,"Intuition suggests that random testing of object-oriented programs should exhibit a significant difference in the number of faults detected by two different runs of equal duration. As a consequence, random testing would be rather unpredictable. We evaluate the variance of the number of faults detected by random testing over time. We present the results of an empirical study that is based on 1215 hours of randomly testing 27 Eiffel classes, each with 30 seeds of the random number generator. Analyzing over 6 million failures triggered during the experiments, the study provides evidence that the relative number of faults detected by random testing over time is predictable but that different runs of the random test case generator detect different faults. The study also shows that random testing quickly finds faults: the first failure is likely to be triggered within 30 seconds.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539534,no
Prioritizing User-Session-Based Test Cases for Web Applications Testing,2008,"Web applications have rapidly become a critical part of business for many organizations. However, increased usage of Web applications has not been reciprocated with corresponding increases in reliability. Unique characteristics, such as quick turnaround time, coupled with growing popularity motivate the need for efficient and effective Web application testing strategies. In this paper, we propose several new test suite prioritization strategies for Web applications and examine whether these strategies can improve the rate of fault detection for three Web applications and their preexisting test suites. We prioritize test suites by test lengths, frequency of appearance of request sequences, and systematic coverage of parameter-values and their interactions. Experimental results show that the proposed prioritization criteria often improve the rate of fault detection of the test suites when compared to random ordering of test cases. In general, the best prioritization metrics either (1) consider frequency of appearance of sequences of requests or (2) systematically cover combinations of parameter-values as early as possible.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539541,no
The Use of Intra-Release Product Measures in Predicting Release Readiness,2008,"Modern business methods apply micro management techniques to all aspects of systems development. We investigate the use of product measures during the intra-release cycles of an application as a means of assessing release readiness. The measures include those derived from the Chidamber and Kemerer metric suite and some coupling measures of our own. Our research uses successive monthly snapshots during systems re-structuring, maintenance and testing cycles over a two year period on a commercial application written in C++. We examine the prevailing trends which the measures reveal at both component class and application level. By applying criteria to the measures we suggest that it is possible to evaluate the maturity and stability of the application thereby facilitating the project manager in making an informed decision on the application's fitness for release.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539550,no
An Empirical Study on Bayesian Network-based Approach for Test Case Prioritization,2008,"A cost effective approach to regression testing is to prioritize test cases from a previous version of a software system for the current release. We have previously introduced a new approach for test case prioritization using Bayesian Networks (BN) which integrates different types of information to estimate the probability of each test case finding bugs. In this paper, we enhance our BN-based approach in two ways. First, we introduce a feedback mechanism and a new change information gathering strategy. Second, a comprehensive empirical study is performed to evaluate the performance of the approach and to identify the effects of using different parameters included in the technique. The study is performed on five open source Java objects. The obtained results show relative advantage of using feedback mechanism for some objects in terms of early fault detection. They also provide insight into costs and benefits of the various parameters used in the approach.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539555,no
The Role of Stability Testing in Heterogeneous Application Environment,2008,"This paper presents an approach to system stability tests performed in Motorola private radio networks (PRN) department. The stability tests are among crucial elements of the department's testing strategy, such as functional testing, regression testing and stress testing. The gravity of the subject is illustrated with an example of a serious system defect: memory leak, whicht was detected in Solaris operating system during system stability tests. The paper provides technical background essential to understand the problem, as well as emphasizes the role of the tests in the problem solving. The following approaches to memory leaks detection are discussed: code review, memory debugging and system stability tests. The article presents several guidelines on stability test implementation and mentions the crucial elements of the PRN Department testing strategy: load definition, testing period and the system monitoring method.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539571,no
Collaborative Target Detection in Wireless Sensor Networks with Reactive Mobility,2008,"Recent years have witnessed the deployments of wireless sensor networks in a class of mission-critical applications such as object detection and tracking. These applications often impose stringent QoS requirements including high detection probability, low false alarm rate and bounded detection delay. Although a dense all-static network may initially meet these QoS requirements, it does not adapt to unpredictable dynamics in network conditions (e.g., coverage holes caused by death of nodes) or physical environments (e.g., changed spatial distribution of events). This paper exploits reactive mobility to improve the target detection performance of wireless sensor networks. In our approach, mobile sensors collaborate with static sensors and move reactively to achieve the required detection performance. Specifically, mobile sensors initially remain stationary and are directed to move toward a possible target only when a detection consensus is reached by a group of sensors. The accuracy of final detection result is then improved as the measurements of mobile sensors have higher signal-to-noise ratios after the movement. We develop a sensor movement scheduling algorithm that achieves near-optimal system detection performance within a given detection delay bound. The effectiveness of our approach is validated by extensive simulations using the real data traces collected by 23 sensor nodes.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539679,no
Adaptation of high level behavioral models for stuck-at coverage analysis,2008,"There has been increasing effort in the years for defining test strategies at the behavioral level. Due to the lack of suitable coverage metrics and tools to assess the quality of a testbench, these strategies have not been able to play an important role in stuck-at fault simulation. The work we are presenting here proposes a new coverage metric that employs back-annotation of post-synthesis design properties into pre-synthesis simulation models to estimate the stuck-at fault coverage of a testbench. The effectiveness of this new metric is evaluated for several example circuits. The results show that the new metric provides a good evaluation of high level testbenches for detection of stuck-at faults.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4540254,no
Adaptive Kalman Filtering for anomaly detection in software appliances,2008,"Availability and reliability are often important features of key software appliances such as firewalls, web servers, etc. In this paper we seek to go beyond the simple heartbeat monitoring that is widely used for failover control. We do this by integrating more fine grained measurements that are readily available on most platforms to detect possible faults or the onset of failures. In particular, we evaluate the use of adaptive Kalman Filtering for automated CPU usage prediction that is then used to detect abnormal behaviour. Examples from experimental tests are given.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4544581,no
A Survey of Automated Techniques for Formal Software Verification,2008,"The quality and the correctness of software are often the greatest concern in electronic systems. Formal verification tools can provide a guarantee that a design is free of specific flaws. This paper surveys algorithms that perform automatic static analysis of software to detect programming errors or prove their absence. The three techniques considered are static analysis with abstract domains, model checking, and bounded model checking. A short tutorial on these techniques is provided, highlighting their differences when applied to practical problems. This paper also surveys tools implementing these techniques and describes their merits and shortcomings.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4544862,no
Assessment of a New High-Performance Small-Animal X-Ray Tomograph,2008,"We have developed a new X-ray cone-beam tomograph for in vivo small-animal imaging using a flat panel detector (CMOS technology with a microcolumnar CsI scintillator plate) and a microfocus X-ray source. The geometrical configuration was designed to achieve a spatial resolution of about 12 lpmm with a field of view appropriate for laboratory rodents. In order to achieve high performance with regard to per-animal screening time and cost, the acquisition software takes advantage of the highest frame rate of the detector and performs on-the-fly corrections on the detector raw data. These corrections include geometrical misalignments, sensor non-uniformities, and defective elements. The resulting image is then converted to attenuation values. We measured detector modulation transfer function (MTF), detector stability, system resolution, quality of the reconstructed tomographic images and radiated dose. The system resolution was measured following the standard test method ASTM E 1695 -95. For image quality evaluation, we assessed signal-to-noise ratio (SNR) and contrast-to-noise ratio (CNR) as a function of the radiated dose. Dose studies for different imaging protocols were performed by introducing TLD dosimeters in representative organs of euthanized laboratory rats. Noise figure, measured as standard deviation, was 50 HU for a dose of 10 cGy. Effective dose with standard research protocols is below 200 mGy, confirming that the system is appropriate for in vivo imaging. Maximum spatial resolution achieved was better than 50 micron. Our experimental results obtained with image quality phantoms as well as with in-vivo studies show that the proposed configuration based on a CMOS flat panel detector and a small micro-focus X-ray tube leads to a compact design that provides good image quality and low radiated dose, and it could be used as an add-on for existing PET or SPECT scanners.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4545226,no
Effective and Open System for Wavelengths Monitoring,2008,"Today there are very few open source optical network monitoring solutions available for network operators or optical paths users. Most of the available optical monitoring solutions are proprietary and single vendor solutions only (firmware). Even the existing software is often limited in functionality to one-domain homogeneous network environment only and extremely difficult to apply in many operators multi-domain heterogeneous environment. Unfortunately, network operators as well as users of optical paths (or client trails in SDH) can only successfully monitor and use wavelengths infrastructures spanning over multiple domains if they have efficient monitoring and fault detection tools available to them. In this paper we present an effective and open source tool for monitoring wavelengths. Our approach recognises and facilitates the ability of independent networks to set policies while facilitating the status and performance monitoring by users interested in optical paths connecting distributed resources.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4545536,no
Notice of Violation of IEEE Publication Principles<BR>Dynamic Binding Framework for Adaptive Web Services,2008,"Notice of Violation of IEEE Publication Principles<BR><BR>""Dynamic Binding Framework for Adaptive Web Services,""<BR>by A. Erradi, P. Maheshwari<BR>in the Proceedings of the Third International Conference on Internet and Web Applications and Services, 2008. ICIW '08, June 2008, pp. 162-167<BR><BR>After careful and considered review of the content and authorship of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>This paper contains significant portions of original text from the paper cited below. The original text was copied without attribution (including appropriate references to the original author(s) and/or paper title) and without permission.<BR><BR>""Foundations of Software Engineering,""<BR>by A. Michlmayr, F. Rosenberg, C. Platzer, M. Treiber, S. Dustdar<BR>in the Proceedings of the 2nd International Workshop on Service Oriented Software Engineering: In Conjunction with the 6th ESEC/FSE Joint Meeting, Sept 2007, pp. 22-28<BR><BR>Dynamic selection and composition of autonomous and loosely-coupled Web services is increasingly used to automate business processes. The typical long-running characteristic of business processes imposes new management challenges such as dynamic adaptation of running process instances. To address this, we developed a policy-based framework, named manageable and adaptable service compositions (MASC) , to declaratively specify policies that govern: (1) discovery and selection of services to be used, (2) monitoring to detect the need for adaptation, (3) reconfiguration and adaptation of the process to handle special cases (e.g., context-dependant behavior) and recover from typical faults in service-based processes. The identified constructs are executed by a lightweight service-oriented management middleware named MASC middleware. We implemented a MASC proof-of-concept prototype and evaluated it on stock trading case study scenarios. We conducted exten- sive studies to demonstrate the feasibility of the proposed techniques and illustrate the benefits of our approach in providing adaptive composite services using the policy-based approach. Our performance and scalability studies indicate that MASC middleware is scalable and the introduced overhead are acceptable.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4545608,no
Improving Efficiency of IC Burn-In Testing,2008,"Burn-in (i.e., electronic test performed under elevated temperature and other stress conditions) plays an important role in integrated circuits (ICs) manufacturing process to ensure required high quality and reliability of the produced semiconductors before shipping them to final users. Burn-in aims at accelerating detection and screening out of so-called 'infant mortalities' (early-life latent failures). It is normally associated with lengthy test time and high cost, often making it a bottleneck of the entire IC manufacturing process. It is no surprise therefore, that much attention and efforts have been dedicated towards possible ways of improving efficiency of the entire burn-in process. This paper looks at improving efficiency of practical burn-in testing process in the IC fabrication setting. The paper presents development of applied algorithms and relevant embedded software the burn-in and binning processes automation. In addition to the improved burn-in test efficiency, the developed tools lead also to yield improvement by providing more comprehensive test results data logging and analysis.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4547315,no
Prediction-based Haptic Data Reduction and Compression in Tele-Mentoring Systems,2008,"In this paper, a novel haptic data reduction and compression technique to reduce haptic data traffic in networked haptic tele-mentoring systems is presented. The suggested method follows a two-step procedure: (1) haptic data packets are not transmitted when they can be predicted within a predefined tolerable error; otherwise, (2) data packets are compressed prior to transmission. The prediction technique relies on the least-squares method. Knowledge from human haptic perception is incorporated into the architecture to assess the perceptual quality of the prediction results. Packet-payload compression is performed using uniform quantization and adaptive Golomb-Rice codes. The preliminary experimental results demonstrate the algorithm's effectiveness as great haptic data reduction and compression is achieved, while preserving the overall quality of the tele-mentoring environment.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4547342,no
Traffic and Quality Characterization of Single-Layer Video Streams Encoded with the H.264/MPEG-4 Advanced Video Coding Standard and Scalable Video Coding Extension,2008,"The recently developed H.264/AVC video codec with scalable video coding (SVC) extension, compresses non-scalable (single-layer) and scalable video significantly more efficiently than MPEG-4 Part 2. Since the traffic characteristics of encoded video have a significant impact on its network transport, we examine the bit rate-distortion and bit rate variability-distortion performance of single-layer video traffic of the H.264/AVC codec and SVC extension using long CIF resolution videos. We also compare the traffic characteristics of the hierarchical B frames (SVC) versus classical B frames. In addition, we examine the impact of frame size smoothing on the video traffic to mitigate the effect of bit rate variabilities. We find that compared to MPEG-4 Part 2, the H.264/AVC codec and SVC extension achieve lower average bit rates at the expense of significantly increased traffic variabilities that remain at a high level even with smoothing. Through simulations we investigate the implications of this increase in rate variability on <i>(i)</i> frame losses when transmitting a single video, and <i>(ii)</i> on a bufferless statistical multiplexing scenario with restricted link capacity and information loss. We find increased frame losses, and rate-distortion/rate-variability/encoding complexity tradeoffs. We conclude that solely assessing bit rate-distortion improvements of video encoder technologies is not sufficient to predict the performance in specific networked application scenarios.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4547482,no
Integrated Evaluation Model for Software Process Modeling Methods,2008,"In order to help developers choose suitable modeling method according to specific modeling environment and requirement for achieving the best modeling effect, it is important to make reasonable assessment for software process modeling methods. An evaluation system for software process modeling methods is presented, and an evaluation model for evaluating modeling methods is established using method that combines uncertain analytic hierarchy process (AHP) with fuzzy integrated evaluation technology. Further, by an example it proves that using the model can evaluate modeling methods soundly, so it has practical value in software project development.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4548290,no
Development of a Weather Forecasting Code: A Case Study,2008,"Computational science is increasingly supporting advances in scientific and engineering knowledge. The unique constraints of these types of projects result in a development process that differs from the process more traditional information technology projects use. This article reports the results of the sixth case study conducted under the support of the Darpa High Productivity Computing Systems Program. The case study aimed to investigate the technical challenges of code development in this environment, understand the use of development tools, and document the findings as concrete lessons learned for other developers' benefit. The project studied here is a major component of a weather forecasting system of systems. It includes complex behavior and interaction of several individual physical systems (such as the atmosphere and the ocean). This article describes the development of the code and presents important lessons learned.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4548409,no
Structure and Interpretation of Computer Programs,2008,"Call graphs depict the static, caller-callee relation between ""functions "" in a program. With most source/target languages supporting functions as the primitive unit of composition, call graphs naturally form the fundamental control flow representation available to understand/develop software. They are also the substrate on which various inter- procedural analyses are performed and are integral part of program comprehension/testing. Given their universality and usefulness, it is imperative to ask if call graphs exhibit any intrinsic graph theoretic features - across versions, program domains and source languages. This work is an attempt to answer these questions: we present and investigate a set of meaningful graph measures that help us understand call graphs better; we establish how these measures correlate, if any, across different languages and program domains; we also assess the overall, language independent software quality by suitably interpreting these measures.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4549888,no
Full automated packaging of high-power diode laser bars,2008,"Full automated packaging of high power diode laser bars on passive or micro channel heat sinks requires a high precision measurement and handling technology. The metallurgic structure of the solder and intrinsic stress of the laser bar are largely influenced by the conditions of the mounting process. To avoid thermal deterioration the tolerance for the overhang between laser bar and heat sink is about a few microns maximum. Due to an increase of growing applications and growing number of systems there is a need for automatic manufacturing not just for cost efficiency but also for yield and product quality reasons. In this paper we describe the demands on fully automated packaging, the realized design and finally the test results of bonded devices. The design of the automated bonding systems includes an air cushioned, 8 axes system on a granite frame. Each laser bar is picked up by a vacuum tool from a special tray or directly out of the gel pak. The reflow oven contains a ceramic heater with low thermal capacity and reaches a maximum of 400degC with a heating rate up to 100 K/s and a cooling rate up to 20 K/s. It is suitable for all common types of heat sinks and submounts which are fixed onto the heater by vacuum. The soldering process is performed under atmospheric pressure, during the oven is filled up with inert gas. Additionally, reactive gases can be used to proceed the reduction of the solder. Three high precision optical sensors for distance measurement detect the relative position of laser bar and heat sink. The high precision alignment uses a special algorithm for final positioning. For the alignment of the tilt and roll angles between the laser bar and the heat sink two optical distance sensors and the two goniometers below the oven are used. To detect the angular orientation of the heat sinks upper surface a downwards looking optical sensor system is used. The upwards pointing optical sensor mounted is used to measure the orientation of the laser bars lo- wer side. These measurements provide the data needed to calculate the angles that the heat sink needs to be tilted and rolled by the two goniometers, in order to get its upper surface parallel to the lower surface of the laser bar. For the measurement of the laser bar overhang and yaw an optical distance sensor is mounted in front of the oven. Overhang and yaw are aligned by using high precision rotary and translation stages. A software tool calculates the displacement necessary to get a parallel orientation and a desired overhang of the laser bar relative to the heat sink. A post bonding accuracy of +/- 1 micron and of +/- 0,2 mrad respectively is achieved. To demonstrate the performance and reliability of the bonding system the bonded devices were characterized by tests like smile test, shear test, burn in test. The results will be presented as well as additional aspects of automated manufacturing like part identification and part tracking.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4550096,no
Clustering Analysis for the Management of Self-Monitoring Device Networks,2008,"The increasing computing and communication capabilities of multi-function devices (MFDs) have enabled networks of such devices to provide value-added services. This has placed stringent QoS requirements on the operations of these device networks. This paper investigates how the computational capabilities of the devices in the network can be harnessed to achieve self-monitoring and QoS management. Specifically, the paper investigates the application of clustering analysis for detecting anomalies and trends in events generated during device operation, and presents a novel decentralized cluster and anomaly detection algorithm. The paper also describes how the algorithm can be implemented within a device overlay network, and demonstrates its performance and utility using simulated as well as real workloads.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4550827,no
Goal-Centric Traceability: Using Virtual Plumblines to Maintain Critical Systemic Qualities,2008,"Successful software development involves the elicitation, implementation, and management of critical systemic requirements related to qualities such as security, usability, and performance. Unfortunately, even when such qualities are carefully incorporated into the initial design and implemented code, there are no guarantees that they will be consistently maintained throughout the lifetime of the software system. Even though it is well known that system qualities tend to erode as functional and environmental changes are introduced, existing regression testing techniques are primarily designed to test the impact of change upon system functionality rather than to evaluate how it might affect more global qualities. The concept of using goal-centric traceability to establish relationships between a set of strategically placed assessment models and system goals is introduced. This paper describes the process, algorithms, and techniques for utilizing goal models to establish executable traces between goals and assessment models, detect change impact points through the use of automated traceability techniques, propagate impact events, and assess the impact of change upon systemic qualities. The approach is illustrated through two case studies.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4553719,no
Module Prototype for Online Failure Prediction for the IBM Blue Gene/L,2008,"The growing complexity of scientific applications has led to the design and deployment of large-scale parallel systems. The IBM Blue Gene/L can hold in excess of 200 K processors and it has been designed for high performance and reliability. However, failures in this large-scale parallel system are a major concern, since it has been demonstrated that a failure will significantly reduce the performance of the system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4554349,no
Quality of service investigation for multimedia transmission over UWB networks,2008,"In this paper, the Quality of Service (QoS) for multimedia traffic of the Medium Access Control (MAC) protocol for Ultra Wide-Band (UWB) networks is investigated. A protocol is proposed to enhance the network performance and increase its capacity. This enhancement comes from using Wise Algorithm for Link Admission Control (WALAC). The QoS of multimedia transmission is determined in terms of average delay, loss probability, utilization, and the network capacity.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4554404,no
Performance engineering of replica voting protocols for high assurance data collection systems,2008,"Real-time data collection in a distributed embedded system requires dealing with failures such as data corruptions by malicious devices and arbitrary message delays in the network. Replication of data collection devices is employed to deal with such failures, with voting among the replica devices to move a correct data to the end-user. Here, the data being voted upon can be large-sized and/or take long time to be compiled (such as images in a terrain surveillance system and transaction histories in an intrusion detection system). The goal of our paper is to engineer the voting protocols to achieve good performance while meeting the reliability requirements of data delivery in a high assurance setting. The performance metrics are the data transfer efficiency (DTE) and the time-to-complete a data delivery (TTC). DTE captures the network bandwidth wasted and/or the energy drain in wireless-connected devices; whereas, TTC depicts the degradation in user-level QoS due to delayed and/or missed data deliveries. So, improving both DTE and TTC is a goal of our performance engineering exercise. Our protocol-level optimizations focus on reducing: i) the movement of user-level data between voters, ii) the number of voting actions/messages generated, and iii) the latency caused by the voting itself. The paper describes these optimizations, along with the experimental results from a prototype voting system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4554452,no
Qos based handover layer for a multi-RAT mobile terminal in UMTS and Wi-MAX networks,2008,"To meet the growing demand for broadband wireless connectivity, integrating 3G and IEEE 802.16 networks is a clear and viable solution. In this paper, we present a novel mobile client architecture called HDEL (handover decision and execution layer) that provides seamless mobility while maintaining connectivity across heterogeneous wireless networks and provides better quality of service (QoS) support. Our Proposed HDEL will monitor the QoS parameters and ensure seamless mobility to support continuous data transfer while the subscriber is moving across UMTS and Wi-MAX networks. This HDEL consists of different functional layers that are used for measuring the basic signal strength as well as the QoS parameters like delay, jitter, packet loss and throughput. To ensure QoS support during seamless handover, the proposed HDEL incorporates the procedures for activating a QoS session and ensures proper translation for network-specific QoS classification.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4554460,no
A QoS routing protocol for delay-sensitive applications in mobile ad hoc networks,2008,"The Ad hoc On Demand Distance Vector (AODV) routing protocol is intended for use by mobile nodes in ad hoc networks. To provide quality of service, extensions can be added to the messages used during route discovery. These extensions specify the service requirements which must be met by nodes rebroadcasting a route request (RREQ) or returning a route reply (RREP) for a destination. In order to provide quality delivery to delay sensitive applications such as voice and video, it is extremely important that mobile ad hoc networks provide quality of service (QoS) support in terms of bandwidth and delay. Most existing routing protocols for mobile ad hoc networks are designed to search for the shortest path with minimum hop count. However, the shortest routes do not always provide the best performance, especially when there are congested nodes along these routes. In this paper we propose an on demand delay based quality of service (QoS) routing protocol (AODV-D) to ensure that delay does not exceed a maximum value for mobile ad hoc networks. This protocol will take into consideration MAC layer channel contention information and number of packets in the interface queue in addition to minimum hops. The protocol is implemented and simulated using GlomoSim simulator. Performance comparisons of the proposed AODV-D protocol against AODV, and QS-AODV is presented and shown that the proposed algorithm performs well.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4554506,no
Pre-emption based call admission control with QoS and dynamic bandwidth reservation for cellular networks,2008,"Call admission protocol (CAC) is a very important process in the provision of good quality of service (QoS) in cellular mobile networks. With micro/Pico cellular architectures that are now used to provide higher capacity, the cell size decreases with a drastic increase in the handoff rate. In this paper, we present modeling and simulation results to help in better understanding of the performance and efficiency of CAC in cellular networks. Handoff prioritization is a common characteristic, which is achieved through the threshold bandwidth reservation policy framework. Combined with this framework, we use pre-emptive call admission scheme and elastic bandwidth allocation for data calls in order to gain a near optimal QoS. In this paper, we also use a genetic algorithm (GA) based approach to optimize the fitness function, which we obtained by calculating the mean square error of predicted rejection values and the actual ones. The predicted values are calculated using a linear model, which relates the rejection ratios with different threshold values.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4554510,no
Online Dependability Assessment through Runtime Monitoring and Prediction,2008,"Computer science and engineering have, in comparison to other areas, focused on abstraction and technology, chasing the ever-changing artifacts, namely, computer and communication hardware and software. At the beginning, a focus was on functionality, a decade later on performance, next the focus was widened by including cost minimization and later quality-of-service.The result is that the time for modeling, measurements and assessment is limited and once some hypotheses are verified in part, they frequently become obsolete due new technologies, environments and applications. Not surprisingly, computer science community can boast a relatively small number of useful ""laws"" and principles not only due to dynamicity but also due to immense diversity of applications. Nevertheless, we are an optimistic folk, always trying to solve all the world's problems, generalize to death (proposing, rarely useful, general models, meta-models and meta-meta-models) as in, for example, software development process where there are no limitations so one can easily promise anything and defy any laws of feasibility or reasonableness (unlike hardware engineers who are constrained by physics).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556003,no
Checklist Inspections and Modifications: Applying Bloom's Taxonomy to Categorise Developer Comprehension,2008,"Software maintenance can consume up to 70% of the effort spent on a software project, with more than half of this devoted to understanding the system. Performing a software inspection is expected to contribute to comprehension of the software. The question is: at what cognition levels do novice developers operate during a checklist-based code inspection followed by a code modification? This paper reports on a pilot study of Bloom's taxonomy levels observed during a checklist-based inspection and while adding new functionality unrelated to the defects detected. Bloom's taxonomy was used to categorise think-aloud data recorded while performing these activities. Results show the checklist-based reading technique facilitates inspectors to function at the highest cognitive level within the taxonomy and indicates that using inspections with novice developers to improve cognition and understanding may assist integrating developers into existing project teams.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556135,no
Evaluating the Reference and Representation of Domain Concepts in APIs,2008,"As libraries are the most widespread form of software reuse, the usability of their APIs substantially influences the productivity of programmers in all software development phases. In this paper we develop a framework to characterize domain-specific APIs along two directions: 1) how can the API users reference the domain concepts implemented by the API; 2) how are the domain concepts internally represented in the API. We define metrics that allow the API developer for example to assess the conceptual complexity of his API and the non-uniformity and ambiguities introduced by the API's internal representations of domain concepts, which makes developing and maintaining software that uses the library difficult and error-prone. The aim is to be able to predict these difficulties already during the development of the API, and based on this feedback be able to develop better APIs up front, which will reduce the risks of these difficulties later.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556138,no
Multi Background Memory Testing,2008,"To coverage Pattern Sensitive Faults (PSF) the multiple run March test algorithms have been used. The key element of multiple run March test algorithms are memory backgrounds. In this paper the constructive algorithm for optimal set of memory backgrounds selection is proposed. The backgrounds selection it based on the binary vectors dissimilarity measures. The optimal solutions have been obtained for the cases of two, three and four runs memory testing. Theoretical and experimental analysis has been done which allow to prove the efficiency of proposed technique.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4557853,no
A novel approach for image fusion based on Markov Random Fields,2008,Markov random field (MRF) model is a powerful tool to model image characteristics accurately and has been successfully applied to a large number of image processing applications. This paper investigates the problem of image fusion based on MRF models. A fusion algorithm under the maximum a posteriori (MAP) criterion is developed under this model. Experimental results are provided to demonstrate the improvement of fusion performance by our algorithm.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4558549,no
A scalable Software Defined Radio receiver for video and audio broadcasting on personal computers,2008,"Software defined radios (SDR) can be found in every area of wireless signal processing. SDRs realized so far are mostly part of communication technology, including audio broadcast receivers. Image- and video processing in SDRs is not common due to the fact that this type of real-time application requires a huge amount of processing resources. It can be foreseen that SDRs for video processing will be the next attractive approach for flexible and cost-effective media systems. We started with functional diagrams of the required modules for TV receivers, selected suitable algorithms to estimate their computational complexity and investigated their resource-quality scaling properties. Thereafter benchmark tests were done for a variety of common general purpose processors (GPPs), showing that real-time performance will be possible in the near future.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4559479,no
A fast CIS still image stabilization method without parallax and moving object problems,2008,"In this paper, we propose a fast still image stabilization method for CMOS image sensors (CIS) which can solve the parallax and moving object problems in image registration. CIS still image stabilization is a software technique that prevents motion blurs caused by minute shaking of a CIS camera, and it is realized by seamlessly registering several rapidly exposed CIS images. Our method consists of two stages of image registration: a global registration and then, a local registration. Since a CIS image can be distorted by the motion of a CIS camera having a rolling shutter mechanism, we present a way of removing CIS distortions in the global registration. Also, the local misalignments are reduced using a mesh-based local registration. Since the parallax or moving object regions cannot be correctly aligned in the image registration process, we propose a two-stage post-processing method that removes the misaligned regions by measuring the registration quality based on the labeling method. The computational load in the image stabilization is reduced by selectively performing our local registration based on a real-time global motion estimation method. The experiment results show the necessity of CIS distortion analysis in the CIS image registration and the effectiveness of our stabilization method even for the moving object and parallax problems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4560075,no
A Dangerousness-Based Investigation Model for Security Event Management,2008,"The current landscape of security management solutions for large scale networks is limited by the lack of supporting approaches capable to deal with the huge number of alarms and events that are generated on current networks. In this paper we propose a security management architecture, capable to reconstruct causal dependencies from captured network and service alarms. The key idea is based on mapping events in semantic spaces, where a novel algorithm can determine such dependencies. We have implemented a prototype and tested it on a operational network within an outsourced security management suite protecting multiple networks.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4561335,no
Improving Learning Objects Quality with Learning Styles,2008,"The traditional learning style theory is being used in the development of e-learning materials progressively. Usually the process of working with learning styles begins with the development of tests or questionnaires that are focus on determining the preferences of students to learn, and then prepare the material according to them. But once the content is implemented there is no tool to measure or estimate if it complies with the desired learning styles. We use a different approach and in this paper we perform the evaluation of existing pedagogical materials in order to determine the dominant learning style theory if there is any. Then the experimental result shows the classification of all the contents.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4561747,no
New technique for fault location in interconnected networks using phasor measurement unit,2008,"Application of PMU for fault location is conducted through a driven algorithm and is applied to different study systems through computer numerical simulation.The algorithm estimates the fault location based on synchronized phasors from both ends of the transmission line whether phase measurement units are installed to both ends or to only one end and the other end is calculated from synchronized phasors from another side. This algorithm allows for accurate estimation of fault location irrespective of fault resistance, load currents, and source impedance. A computer simulation using PSCAD program of the transmission line under study with various fault types and different locations is carried out. A transformation modal is used in the algorithm. Different fault types are simulated with different fault locations to more than one line in the Egyptian network, which has phase measurement units installed according to a selected allocating technique. The results obtained from applying the considered technique shows high levels of accuracy in locating the fault of different faults types. Hence, it is strongly recommended to use the PMU in the Egyptian Network.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4562381,no
Grey Theory Based Evaluation for SAR Image Denoising,2008,"To evaluate the quality of denoised SAR images, this paper discusses some performance parameters, and proposes a grey-theory-based method. The main idea of the method is to extract some comparative sequences and a reference sequence, respectively standing for quality of the denoised SAR images and the noise-free image, after all the denoised images involved are concerned. And then, an overall performance index is acquired from the grey relational degrees between the two kinds of sequences. The feature of the method is that it not only does not require any real images, but also is inexpensive in processing time. Pilot experiments show that the method is effective and precise. Potential application includes automatic selection of the optimal algorithm and intellectual comparison among SAR denoising algorithms.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4566227,no
A Generic Metamodel For Security Policies Mutation,2008,We present a new approach for mutation analysis of security policies test cases. We propose a metamodel that provides a generic representation of security policies access control models and define a set of mutation operators at this generic level. We use Kermeta to build the metamodel and implement the mutation operators. We also illustrate our approach with two successful instantiation of this metamodel: we defined policies with RBAC and OrBAC and mutated these policies.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4567021,no
Building a reliable internet core using soft error prone electronics,2008,"This paper describes a methodology for building a reliable internet core router that considers the vulnerability of its electronic components to single event upset (SEU). It begins with a set of meaningful system level metrics that can be related to product reliability requirements. A specification is then defined that can be effectively used during the system architecture, silicon and software design process. The system can then be modeled at an early stage to support design decisions and trade-offs related to potentially costly mitigation strategies. The design loop is closed with an accelerated measurement technique using neutron beam irradiation to confirm that the final product meets the specification.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4567283,no
The Checkpoint Interval Optimization of Kernel-Level Rollback Recovery Based on the Embedded Mobile Computing System,2008,"Due to the limited resources of the embedded mobile computing systems, such as wearable computers, PDAs or sensor nodes, reducing the overhead of the software implemented fault tolerance mechanism is a key factor in reliability design. Two checkpoint interval optimization techniques of kernel level rollback recovery mechanism are discussed. Step checkpointing algorithm modulates checkpoint intervals on-line according to the characteristics of software or hardware environment-dependent system that the failure rate fluctuates acutely shortly after the system fails. Checkpoint size monitoring and threshold-control technique adjusts the checkpoint interval by predicting the amount of data to be saved. Combining these two techniques can effectively improve the performance of the embedded mobile computer system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4568557,no
Test results for the WAAS Signal Quality Monitor,2008,"The signal quality monitor (SQM) is an integrity monitor for the wide area augmentation system (WAAS). The monitor detects L1 signal waveform deformation of a GPS or a geosynchronous (GEO) satellite monitored by WAAS should that event occur. When a signal deformation occurs, the L1 correlation function measured by the receiver becomes distorted. The distortion will result in an error in the L1 pseudorange. The size of the error depends on the design characteristics of the user receiver. This paper describes test results for the WAAS SQM conducted using prototype software. There are two groups of test cases: the nominal testing and the negative path testing. For nominal test cases, recorded data are collected from a test facility in four 5-day periods. These four data sets include SQM correlation values for SV-receiver pairs, and satellite error bounds for satellites. They are used as input to the prototype. The prototype processes these data sets, executes the algorithm, and records test results. Parameters such as the ""maximum median-adjusted detection metric over threshold"" (i.e., the maximum detection test), ""UDRE forwarded from upstream integrity monitors,"" and ""UDRE supported by SQM"" are shown and described. The magnitude of the maximum detection test for all GPS and GEO satellites are also shown. For negative path testing, this paper describes two example simulated signal deformation test cases. A 2-day data set is collected from the prototype. A few example ICAO signal deformations are simulated based on this data set and are inserted in different time slots in the 2-day period. The correlator measurements for selected satellites are pre- processed to simulate the signal deformation. The results demonstrate the sensitivity of the Signal Quality Monitor to the simulated deformation, and shows when the event is detected and subsequently cleared. It also shows that the SQM will not adversely affect WAAS performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4570015,no
System-Level Performance Estimation for Application-Specific MPSoC Interconnect Synthesis,2008,"We present a framework for development of streaming applications as concurrent software modules running on multi-processors system-on-chips (MPSoC). We propose an iterative design space exploration mechanism to customize MPSoC architecture for given applications. Central to the exploration engine is our system-level performance estimation methodology, that both quickly and accurately determine quality of candidate architectures. We implemented a number of streaming applications on candidate architectures that were emulated on an FPGA. Hardware measurements show that our system-level performance estimation method incurs only 15% error in predicting application throughput. More importantly, it always correctly guides design space exploration by achieving 100% fidelity in quality-ranking candidate architectures. Compared to behavioral simulation of compiled code, our system-level estimator runs more than 12 times faster, and requires 7 times less memory.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4570792,no
Measuring Package Cohesion Based on Context,2008,"Packages play a critical role to understand, construct and maintain large-scale software systems. As an important design attribute, cohesion can be used to predict the quality of packages. Although a number of package cohesion metrics have been proposed in the last decade, they mainly converge on intra-package data dependences between components, which are inadequate to represent the semantics of packages in many cases. To address this problem, we propose a new cohesion metric for package called SCC on the assumption that two components are related tightly if they have similar contexts. Compared to existing works, SCC uses the common context of two components to infer whether they have close relation or not, which involves both inter- and intra- package data dependences. It is hence able to reveal semantic relations between components. We demonstrate the effectiveness of SCC by case studies.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4570828,no
ORTEGA: An Efficient and Flexible Software Fault Tolerance Architecture for Real-Time Control Systems,2008,"Fault tolerance is an important aspect in real-time computing. In real-time control systems, tasks could be faulty due to various reasons. Faulty tasks may compromise the performance and safety of the whole system and even cause disastrous consequences. In this paper, we describe ORTEGA (On-demand Real-TimE GuArd), a new software fault tolerance architecture for real-time control systems. ORTEGA has high fault coverage and reliability. Compared with existing real-time fault tolerance architectures, such as Simplex, ORTEGA allows more efficient resource utilizations and enhances flexibility. These advantages are achieved through the on-demand detection and recovery of faulty tasks. ORTEGA is applicable to most industrial control applications where both efficient resource usage and high fault coverage are desired.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4573109,no
Predictable Code and Data Paging for Real Time Systems,2008,"There is a need for using virtual memory in real-time applications: using virtual addressing provides isolation between concurrent processes; in addition, paging allows the execution of applications whose size is larger than main memory capacity, which is useful in embedded systems where main memory is expensive and thus scarce. However, virtual memory is generally avoided when developing real-time and embedded applications due to predictability issues. In this paper we propose a predictable paging system in which the page loading and page eviction points are selected at compile-time. The contents of main memory is selected using an Integer Linear Programming (ILP) formulation. Our approach is applied to code, static data and stack regions of individual tasks. We show that the time required for selecting memory contents is reasonable for all applications including the largest ones, demonstrating the scalability of our approach. Experimental results compare our approach with a previous one, based on graph coloring. It shows that quality of page allocation is generally improved, with an average improvement of 30% over the previous approach. Another comparison with a state-of-the-art demand-paging system shows that predictability does not come at the price of performance loss.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4573122,no
Reputation-Based Service Discovery in Multi-agents Systems,2008,"Reputation has recently received considerable attention within a number of disciplines such as distributed artificial intelligence, economics, evolutionary biology, and among others. Most papers about reputation provide an intuitive approach to reputation which appeals to common experiences without clarifying whether their use of reputation is similar or different from those used by others. DF provides a Yellow Pages service. Agents in the FIPA-compliant agent system can provide services to others, and store these services in the DF of the multiagent system. However, existing DF cannot detect the fake service which is registered by malicious agent. So,a user may search these fault services. In this paper, we analyze the DFpsilas problem and propose the solution. We describe the Reputation mechanism for searching these fake services. Reputation function assumes the presence of other agent who can provide ratings for other agents that are reflective of the performance or behavior of the corresponding agents.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4573164,no
Towards autonomic risk-aware security configuration,2008,"Security of a network depends on a number of dynamically changing factors. These include emergence of new vulnerabilities and threats, policy structure and network traffic. Due to the dynamic nature of these factors, identifying security metrics that measure objectively the quality of security configuration pose a major challenge. Moreover, this evaluation must be done dynamically to handle real time changes in the threat toward the network. In this paper, we extend our security metric framework that identifies and quantifies objectively the most significant security risk factors, which include existing vulnerabilities, historical trend of vulnerabilities of remotely accessible services, prediction of potential vulnerabilities for any general network service and their estimated severity and finally propagation of an attack within the network. We have implemented this framework as a user-friendly tool called <i>Risk</i> <i>based</i> <i>prOactive</i> <i>seCurity</i> <i>cOnfiguration</i> <i>maNAger</i> <i>(ROCONA)</i> and showed how this tool simplifies security configuration management using risk measurement and mitigation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4575198,no
Real-time problem localization for synchronous transactions in HTTP-based composite enterprise applications,2008,"Loosely-coupled composite enterprise applications based on modern Web technologies are becoming increasingly popular. While composing such applications is appealing for a number of reasons, the distributed nature of the applications makes problem determination difficult. Stringent service level agreements in these environments require rapid localization of failing and poorly performing services. We present in this paper a method that performs real-time transaction level problem determination by tracking synchronous transaction flows in HTTP based composite enterprise applications. Our method relies on instrumentation of service requests and responses to transmit downstream path and monitoring information in realtime. Further, our method applies change-point based techniques on monitored information at the point of origin of a transaction, and quickly detects anomalies in the performance of invoked services. Since our method performs transaction level monitoring, it avoids the pitfalls associated with techniques that use aggregate performance metrics. Additionally, since we use change-point based techniques to detect problems, our method is more robust than error-prone static threshold based techniques.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4575216,no
Assessing Web Applications Consistently: A Context Information Approach,2008,"In order to assess Web applications in a more consistent way we have to deal not only with non-functional requirement specification, measurement and evaluation (M&E) information but also with the context information about the evaluation project. When organizations record the collected data from M&E projects, the context information is very often neglected. This can jeopardize the validity of comparisons among similar evaluation projects. We highlight this concern by introducing a quality in use assessment scenario. Then, we propose a solution by representing the context information as a new add-in to the INCAMI M&E framework. Finally, we show how context information can improve Web application evaluations, particularly, data analysis and recommendation processes.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4577886,no
Available Bandwidth Estimation in Wireless Ad Hoc Network: Accuracy and Probing Time,2008,"Accuracy of available estimated bandwidth and convergence delay algorithm are researchers' challenges in wireless network measurements. Currently a variety of tools are developed to estimate the end-to-end network available bandwidth such as TOPP (train of packet pair), SLoPS (self loading periodic stream), Spurce and Variable Packet Size etc. These tools are important to improve the QoS (quality of service) of demanding applications. However the performances of the previous tools are very different in term of probing time and estimation accuracy. In this paper we study two active probing techniques: TOPP and SLoPS. TOPP provides mo.re accurate estimation of available bandwidth than the second one. However SLoPS technique has faster response delay than TOPP. Thus, we decided to combine the two techniques to get benefit from their respective advantages and propose a new and fast bandwidth estimation technique providing the same accuracy as TOPP. To valid this new technique, NS2 was used. The performance of SLOT technique will be evaluated and the obtain results are analysed by MATLAB software and compared with the TOPP and the SLoPS ones. We show that SLOT has shorter probing time than the TOPP one, and it provides more accurate available estimated bandwidth than the SLoPS one.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4578256,no
Specifying Flexible Charging Rules for Composable Services,2008,"Where services are offered on a commercial basis, the manner in which charges for service usage are calculated is of key importance. Services typically have associated with them a charging scheme specifying the rules for charge calculation; schemes can range from the simple (such fixed charge per service invocation) to highly complex (where charges are calculated dynamically in order to influence customer demand and thereby optimise overall system performance). Typically, charging schemes are manually configured and verified prior to services being made available to customers - typically, a time consuming and expensive process. In environments where service compositions can be rapidly built and offered to customers, manual specification of a charging scheme for the service composition becomes untenable. In this paper we describe how charge modification rules associated with individual services can be used to flexibly govern how a service is charged for when it is used in the context of a composed service.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4578350,no
wsrbench: An On-Line Tool for Robustness Benchmarking,2008,"Testing Web services for robustness is a difficult task. In fact, existing development support tools do not provide any practical mean to assess Web services robustness in the presence of erroneous inputs. Previous works proposed that Web services robustness testing should be based on a set of robustness tests (i.e., invalid Web services call parameters) that are applied in order to discover both programming and design errors. Web services can be classified based on the failure modes observed. In this paper we present and discuss the architecture and use of an on-line tool that provides an easy interface for Web services robustness testing. This tool is publicly available and can be used by both web services providers (to assess the robustness of their Web services code) and consumers (to select the services that best fit their requirements). The tool is demonstrated by testing several Web services available in the Internet.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4578524,no
Modeling Service Quality for Dynamic QoS Publishing,2008,"As Web services are widely adopted, the nonfunctional requirements (i.e. QoS constraints) have become a significant factor in implementing trustworthy and predictable service applications. In this paper, we analyze the QoS publishing necessity, and categorize QoS parameters to refine their characteristics. Then we propose a conceptual publishing model, which is adaptive for the distributed and decentralized service-oriented computing environment. We further introduce the perception service quality measurement models, and extend P-E model for web services. The extended model takes both dynamic QoS parameters and customer expectations into consideration. A compensation mechanism is also incorporated into the model for trade-off when nonfunctional requirements cannot be fully satisfied.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4578538,no
Towards a User-perceived Service Availability Metric,2008,"Web services availability has been regarded as one of the key properties for (critical) service-oriented applications. Based on analyzing the limitations of current metrics for ""User-perceived"" availability, we propose a service status based availability metric and a corresponding estimating approach. Experiments demonstrate that this metric and the corresponding estimation approach could get reasonable measurement on web services' availability.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4578576,no
3C-Silicon Carbide Nanowire FET: An Experimental and Theoretical Approach,2008,"Experimental and simulated I-V characteristics of silicon carbide (SiC) nanowire-based field-effect transistors (NWFETs) are presented. SiC NWs were fabricated by using the vapor-liquid-solid mechanism in a chemical vapor deposition system. The diameter of fabricated SiC NWs varied from 60 up to 100 nm while they were some micrometers long. Their I-V characteristics were simulated with SILVACO software, and special attention was paid to explore the role of NW doping level and NW/dielectric interface quality. The fabricated SiC-based NWFETs exhibit a mediocre gating effect and were not switched-off by varying the gate voltage. Based on the simulations, this is a result of the high unintentional doping (estimated at 1times10<sup>19</sup> cm<sup>-3</sup>) and the poor NW/dielectric interface quality. Moreover, a homemade algorithm was used to investigate the ideal properties of SiC-based NWFETs in ballistic transport regime, with NW lengths of 5-15 nm and a constant diameter of 4 nm for which the carrier transport is fully controlled by quantum effects. This algorithm self-consistently solves the Poisson equation with the quantum nonequilibrium Green function formalism. In the ballistic regime, devices with undoped SiC NWs exhibit superior theoretical performances (transconductance: ~43.2times10<sup>-6</sup> A/V and I<sub>ON</sub>/I<sub>OFF</sub>=1.6times10<sup>5</sup> for a device with 9-nm NW length) based on their simulated characteristics.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4578882,no
Statistical Measurement Analysis of Automated Test Systems,2008,"This paper addressed the application of the statistical paired t-test of observed parametric data, nested GR&R analysis for evaluating ATS and TPS capability, and ANOVA analysis on ATS instrument measurement variation. The applicability of these methods provides a sound approach for analyzing data results and can be further expanded across other TPSs with additional data for improved estimation. In addition, further research should address the additional instruments and UUTs to model the entire the ATS.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4579266,no
Defect Prevention and Detection in Software for Automated Test Equipment,2008,"This paper describes a test application development tool designed with a high degree of defect prevention and detection built-in. While this tool is specific to a particular tester, the PT3800, the approach that it uses may be employed for other ATE. The PT3800 tester is the successor of a more than 20 year-old tester, the PT3300. The development of the PT3800 provided an opportunity to improve the test application development experience. The result was the creation of a test application development tool known as the PT3800 AM creation, revision and archiving tool, or PACRAT (AM refers to automated media, specifically test application source code). This paper details the built-in defect prevention and detection techniques employed by PACRAT.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4579267,no
APART: Low Cost Active Replication for Multi-tier Data Acquisition Systems,2008,"This paper proposes APART (a posteriori active replication), a novel active replication protocol specifically tailored for multi-tier data acquisition systems. Unlike existing active replication solutions, APART does not rely on a-priori coordination schemes determining a same schedule of events across all the replicas, but it ensures replicas consistency by means of an a-posteriori reconciliation phase. The latter is triggered only in case the replicated servers externalize their state by producing an output event towards a different tier. On one hand, this allows coping with non-deterministic replicas, unlike existing active replication approaches. On the other hand, it allows attaining striking performance gains in the case of silent replicated servers, which only sporadically, yet unpredictably, produce output events in response to the receipt of a (possibly large) volume of input messages. This is a common scenario in data acquisition systems, where sink processes, which filter and/or correlate incoming sensor data, produce output messages only if some application relevant event is detected. Further, the APART replica reconciliation scheme is extremely lightweight as it exploits the cross-tier communication pattern spontaneously induced by the application logic to avoid explicit replicas coordination messages.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4579633,no
Historical Value-Based Approach for Cost-Cognizant Test Case Prioritization to Improve the Effectiveness of Regression Testing,2008,"Regression testing has been used to support software testing activities and assure the acquirement of appropriate quality through several versions of a software program. Regression testing, however, is too expensive because it requires many test case executions, and the number of test cases increases sharply as the software evolves. In this paper, we propose the Historical Value-Based Approach, which is based on the use of historical information, to estimate the current cost and fault severity for cost-cognizant test case prioritization. We also conducted a controlled experiment to validate the proposed approach, the results of which proved the proposed approachpsilas usefulness. As a result of the proposed approach, software testers who perform regression testing are able to prioritize their test cases so that their effectiveness can be improved in terms of average percentage of fault detected per cost.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4579792,no
Which Spot Should I Test for Effective Embedded Software Testing?,2008,"Today, the embedded industry is changing fast - systems have become larger, more complex, and more integrated. Embedded system consists of heterogeneous layers such as hardware, HAL, device driver, OS kernel, and application. These heterogeneous layers are usually customized for special purpose hardware. Therefore, various hardware and software components of embedded system are mostly integrated together under unstable status. That is, there are more possibilities of faults in all layers unlike package software.In this paper, we propose the embedded software interface as two essential parts: interface function that represents the statement of communication between heterogeneous layers, and interface variable that represents software and/or hardware variable which are defined in different layer from integrated software and used to expected output for decision of fault. Also, we statically investigate various views of embedded software interface and demonstrate that proposed interface should be new criterion for effective embedded software testing.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4579805,no
A New Method for Measuring Single Event Effect Susceptibility of L1 Cache Unit,2008,"Cache SEE susceptibility measurements are required for predicting processorpsilas soft error rate in space missions. Previous dynamic or static real beam test based approaches are only tenable for processors which have optional cache operating modes such as disable(bypass)/enable, frozen, etc. As L1 cache are indispensable to the processorpsilas total performance, some newly introduced processors no longer have such cache management schemes, thus make the existed methods inapplicable. We propose a novel way to determine cache SEE susceptibility for any kind of processors, whether cache bypass mode supported or not, by combining heavy ion dynamic testing with software implemented fault injection approaches.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4579825,no
Early Reliability Prediction: An Approach to Software Reliability Assessment in Open Software Adoption Stage,2008,Conventional software reliability models are not adequate to assess the reliability of software system in which OSS (Open Source Software) adopted as a new feature add-on because OSS can be modified while the inside of COTS(Commercial Off-The-Shelf) products cannot be changed. This paper presents an approach to software reliability assessment of OSS adopted software system in the early stage. We identified the software factors that affect the reliability of software system when a large software system adopts OSS and assess software reliability using those factors. They are code modularity and code maintainability in software modules related with system requirements. We used them to calculate the initial fault rate with weight index (correlated value between requirement and module) which represents the degree of code modification. We apply the proposed initial fault rate to reliability model to assess software reliability in the early stage of a software life cycle. Early software reliability assessment in OSS adoption helps to make an effective development and testing strategies for improving the reliability of the whole system.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4579834,no
Artificial Neural Network Classification of Power Quality Disturbances Using Time-Frequency Plane in Industries,2008,"The rapid growth of industrialization has increased use of power systems tremendously in process and software industries. Therefore, there is thrust on industries to improve power quality with power monitoring and protection system by classifying voltage and current disturbances. This paper presents a new approach for classifying the events that represent or lead to the degradation of power quality. A neural network with feed forward structure is chosen as the classifier with modified Fisherpsilas Discriminant Ratio Kernel for feature extraction. The potential of developing a more powerful fuzzy classification method based on this algorithm is also discussed. This novel combination of methods shows promise for further development of a fully automated power quality monitoring system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4579964,no
New condition monitoring techniques for reliably drive systems operation,2008,"The dominant application of electronics today is to process information. The computer industry is the biggest user of semiconductor devices and consumer electronics. Due to the successful development of semiconductors, electronic system and controls have gained wide acceptance in power and computing technology and due to the continuous use of drive systems (rotating machines, controlling thyristors and associated electronic components) in industry and in power stations, and the need to keep such systems running reliably, the detection of defects and anomalies is of increasing importance, and on-line monitoring to detect any fault in these systems is now a strong possibility and certainly periodic monitoring of a drive systems in strategic situations. The principal aim of the paper is to use both software and hardware and develop a fault diagnosis knowledge-based system, which will analyze and manipulate the output obtained from sensors using a microcomputer for acquiring the plant condition data and subsequently interpreting them, data collected can be analyzed using suitable computer programs, and any trends can be identified and compared with the knowledge base. The probability of certain condition can then be diagnosed and compared, providing the necessary information on which subsequent decisions can be based and provide any necessary alarms to the operator. To achieve this objective, the simulation and experimental technique considered is to use sensors placed in the wedges closing the stator slots to sense the induced voltage. The induced voltage and for each fault is shown to have a unique voltage pattern, thus, the fault identification through voltage pattern recognition were the basic rules for the development of the knowledge base. The predicted results are verified by measurements on a model system in which known faults can be established.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4580397,no
A wavelet transform technique for de-noising partial discharge signals,2008,"Partial discharge (PD) diagnosis is essential to identify the nature of insulation defects causing discharge. The problem of PD signal recognition has been approached in a number of ways. Most of the approaches are based on laboratory experiments or on signals acquired during off-line tests of industrial apparatus. On-line testing is vastly preferable as the equipment can remain in service, and the operators can monitor the insulation condition continuously. Interferences from noise sources have been a persistent problem, which have increased with the advent of solid-state power switching electronics. Use of wavelet transform technique offers many advantages over conventional digital filters and is ideally suited to process non- stationary signals (transients) often encountered in high voltage testing and measurements. In this paper, an empirical wavelet- based method is proposed to recover PD pulses mixed with excessive noise/interference. A critical assessment of the proposed method is carried out by processing simulated PD signals along with noise signals using MAT LAB software.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4580476,no
Identifying learners robust to low quality data,2008,"Real world datasets commonly contain noise that is distributed in both the independent and dependent variables. Noise, which typically consists of erroneous variable values, has been shown to significantly affect the classification performance of learners. In this study, we identify learners with robust performance in the presence of low quality (noisy) measurement data. Noise was injected into five class imbalanced software engineering measurement datasets, initially relatively free of noise. The experimental factors considered included the learner used, the level of injected noise, the dataset used (each with unique properties), and the percentage of minority instances containing noise. No other related studies were found that have identified learners that are robust in the presence of low quality measurement data. Based on the results of this study, we recommend using the random forest learner for building classification models from noisy data.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4583028,no
Code and carrier divergence technique to detect ionosphere anomalies,2008,"A single and dual frequency smoothing techniques implemented to detect ionosphere anomalies for GBAS system (ground based augmentation system) were discuss in this paper. As a dominant threat for using differential navigation satellites systems in landing applications an ionosphere storm is considered. To detect these occurrences the number of algorithms is developed. Some of them are addressed to meet the integrity requirements of CAT III landing and base on the multi frequency GPS techniques. Depending on the combination of frequency used during code and carrier phase measurements the smoothed pseudorange achieves a different level of accuracy. From this reason the most popular algorithms e.g. the divergence free and ionosphere free smoothing algorithms are analyzed and compared. In the article the works realized in the Institute of Radioelectronics connected with GBAS application are presented, too. The investigations were conducted by using actually GPS signals and signals from GNSS simulator. The self prepared software was used to analyze the results.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4585767,no
Case studies in arc flash reduction to improve safety and productivity,2008,"With the advent of new power system analysis software, a more detailed arc flash analysis can be performed under various load conditions. These new ldquotoolsrdquo can also evaluate equipment damage, design systems with lower arc flash, and predict electrical fire locations based on high arc flash levels. This paper demonstrates how arc flash levels change with available utility MVA (mega volt amperes), additions in connected load, and selection of system components. This paper summarizes a detailed analysis of several power systems to illustrate possible misuses of 2004 NFPA 70E Risk Category Classification Tables while pointing toward future improvements of the Standards. In particular, findings indicate upstream protection may not open quick enough for fault on the secondary of a transformer or at the far end of a long cable due to the increase in system impedance. Several examples of how these problem areas can be dealt with are described in detail.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4585806,no
Fuzzy fault detection and diagnosis under severely noisy conditions using feature-based approaches,2008,"This paper introduces an approach to fault detection and diagnosis scheme which uses fuzzy reference models to describe the symptoms of both faulty and fault-free plant operation. Recently, some approaches have been combined with fuzzy logic to enhance its performance in particular applications such as fault detection and diagnosis. The reference models are generated from training data which are produced by computer simulation of typical plant. A fuzzy matching scheme compares the parameters of a fuzzy partial model, identified using on-line data collected from the real plant, with the parameters of the reference models. The reference models are also compared to each other to take account of the ambiguity which arises at some operating points when the symptoms of correct and faulty operations are similar. Independent components analysis (ICA) is used to extract the exact data from variables under severe noisy conditions. A fuzzy self organizing feature map is applied to the data obtained from ICA for obtaining more accurate and precise features representing different conditions of the system. The results are then applied to the model-based fuzzy procedure for diagnosis goals. Results are presented which demonstrate the applicability of the scheme.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4587004,no
Intensity statistics-based HSI diffusion for color photo denoising,2008,"This paper presents a new image denoising model for real color photo noise removal. Our model is implemented in the hue, saturation and intensity (HSI) space. The hue and saturation denoising are combined and implemented as a complex total variation (TV) diffusion. The intensity denoising is based on a diffusion flow to minimize a new energy functional, which is constructed with intensity component statistics. Besides the common gradient-based edge stopping functions for anisotropic diffusion, specifically for color photo denoising, we incorporate an intensity-based brightness adjusting term in the new energy, which corresponds to the noise disturbance with respect to photo intensity. In addition, we use the gradient vector flow (GVF) as the new diffusion directions for more accurate and robust denoising. Compared with previous diffusion flows only based on regular image gradients, this model provides more accurate image structure and intensity noise characterization for better denoising. Comprehensive quantitative and qualitative experiments on color photos demonstrate the improved performance of the proposed model when compared with 14 recognized approaches and 2 commercial software.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4587415,no
Adaptive control strategies for a class of anaerobic depollution bioprocesses,2008,"This paper presents the design and the analysis of some nonlinear adaptive control strategies for a class of anaerobic depollution processes that are carried out in continuous stirred tank bioreactors. The controller design is based on the input-output linearization technique. The adaptive control structure is based on the nonlinear model of the process and is combined with a state observer and a parameter estimator which play the role of the software sensors for the on-line estimation of biological states and parameter variables of interest of the bioprocess. The resulted control methods are applied in depollution control problem in the case of the anaerobic digestion bioprocess for which dynamical kinetics are strongly nonlinear and not exactly known, and not all the state variables are measurable. The effectiveness and performance of both estimation and control algorithms are illustrated by simulation results.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4588813,no
A virtual instrumentation-based on-line determination of a single/two phase induction motor drive characteristics at coarse start-up,2008,"The single/two phase induction motors (S/TPIM) are used in various low-power, low-cost applications. A representative example is the washing machine drum's main drive. Both manufacturers and designers have augmented their preoccupations and researches to optimize the efficiency of this drive due to the large and increasing number of devices in use and also stimulated by the market requests. At the same time, their efforts have been directed to maintain the drive's main advantage i. e. its low cost. A simple strategy to optimize the S/TPIM drive's efficiency is to replace the fixed capacitance of the starting capacitor of the motor with an electronically switched capacitor. In this approach, an accurate dynamic characteristic of the actuator, i.e. electric motor and washing machine's drum, need to be determined for the implementation of the starting capacitance's command law. In this paper an estimation method of the drive's mechanical strain at coarse start-up is presented. The dynamic model of the drive is based on the direct-and quadrature-axis theory; this approach provides accurate estimates of the drive's response. The on-line measurements of the input voltages and currents are performed with a fast data acquisition board and a virtual instrumentation software environment providing noise-free measurements. The main advantage of this method consists in its simple implementation on the washing machine and to provide torque predictions both in stationary and in dynamic regimes in real operating conditions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4588959,no
Behavioral Dependency Measurement for Change-Proneness Prediction in UML 2.0 Design Models,2008,"During the development and maintenance of object-oriented (OO) software, the information on the classes which are more prone to be changed is very useful. Developers and maintainers can make a more flexible software by modifying the part of classes which are sensitive to changes. Traditionally, most change-proneness prediction has been studied based on source codes. However, change-proneness prediction in the early phase of software development can provide an easier way for developing a stable software by modifying the current design or choosing alternative designs before implementation. To address this need, we present a systematic method for calculating the behavioral dependency measure (BDM) which helps to predict change-proneness in UML 2.0 models. The proposed measure has been evaluated on a multi-version medium size open-source project namely JFreeChart. The obtained results show that the BDM is an useful indicator and can be complementary to existing OO metrics for change-proneness prediction.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4591537,no
A Systematic Approach for Integrating Fault Trees into System Statecharts,2008,"As software systems are encompassing a wide range of fields and applications, software reliability becomes a crucial step. The need for safety analysis and test cases that have high probability to uncover plausible faults are necessities in proving software quality. System models that represent only the operational behavioral of a system are incomplete sources for deriving test cases and performing safety analysis before the implementation process. Therefore, a system model that encompasses faults is required. This paper presents a technique that formalizes a safety model through the incorporation of faults with system specifications. The technique focuses on introducing semantic faults through the integration of fault trees with system specifications or statechart. The method uses a set of systematic transformation rules that tries to maintain the semantics of both fault trees and statechart representations during the transformation of fault trees into statechart notations.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4591544,no
Teaching Team Software Process in Graduate Courses to Increase Productivity and Improve Software Quality,2008,"This paper presents a case study that describes TSPi teaching (introduction to the team software process) to 4th year students, grouped by teams, at the Computer Science School, Polytechnic University of Madrid (UPM). The achievements of the teams, due to training and the use of TSPi, were analyzed and discussed. This paper briefly discusses the approach to the teaching and some of the issues that were identified. The teams collected data on the projects developed. They reviewed the schedule and quality status weekly. The metrics selected to analyze the impact on the students were: size, effort, productivity, costs and defects density. These metrics were chosen to analyze teams 'performance evolution through project development. This paper also presents a study related to the evolution of estimation, quality and productivity improvements these teams obtained. This study will prove that training in TSPi has a positive impact on getting better estimations, reducing costs, improving productivity, and decreasing defect density. Finally, the teams 'performance are analyzed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4591594,no
Error Modeling in Dependable Component-Based Systems,2008,"Component-based development (CBD) of software, with its successes in enterprise computing, has the promise of being a good development model due to its cost effectiveness and potential for achieving high quality of components by virtue of reuse. However, for systems with dependability concerns, such as real-time systems, a major challenge in using CBD consists of predicting dependability attributes, or providing dependability assertions, based on the individual component properties and architectural aspects. In this paper, we propose a framework which aims to address this challenge. Specifically, we present a revised error classification together with error propagation aspects, and briefly sketch how to compose error models within the context of component-based systems (CBS). The ultimate goal is to perform the analysis on a given CBS, in order to find bottlenecks in achieving dependability requirements and to provide guidelines to the designer on the usage of appropriate error detection and fault tolerance mechanisms.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4591772,no
New DAQB and associated virtual library included in LabVIEW for environmental parameters monitoring,2008,"In this work a data acquisition board (DAQB) with data transfer by serial port and the associated virtual library included into LabVIEW software are presented. The DAQB developed around a National Semiconductor LM 12458 devices, have the capability to perform tasks that diminish the host processor work and is capable to communicate with the host computer by using a set of drivers associated. Highly integrated device circuit that has into it the most components of the board, facility in data handling, good metrological performance and a very low cost are the benefits of the proposed system. Using the LabView environment, we have realized a virtual instrument able to get from the prototype data acquisition board for environmental monitoring parameters the information about air pollution factors like CO, H2S, SO2, NO, NO2 etc. In order to get effective information about those factors and the monitoring points, this intelligent measurement system, compound from portable computer, and Gas detector. This system can be used to map the information about the air pollution factors dispersion in order to answer to the needs of residential and industrial areas expansion.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4592765,no
Research on double-end cooperative congestion control mechanism of random delay network,2008,"Propagation delay is the main factor of affecting the network performance. Random propagation delay has an adverse impact on the stability of feedback control mechanism. We argue that some existing schemes which try to control the node-end-systems queue donpsilat work well when random propagation delay acts on the models. To find a solution to this problem, a double-end cooperative congestion control mechanism is proposed and analyzed. We have studied the performance of the control mechanism via simulations on OPNET software. Simulations show that the mechanism can improve network performance under random propagation delay. And in the node-end-systems, the probability of cells discard is lesser.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4594176,no
Application of detection method based on energy function against faults in motion system,2008,"There are many possibilities of faults in motion systems which are related to driver equipments, based on the torque monitor signal supplied by systemic driver, a new fault diagnosis method was brought forward which was profited from a nonlinear energy operator. The principles of using it to performing fault diagnosis were presented, the keynotes about its application were also analyzed, and the detail flow chart of diagnosis was given. After the software simulation using Matlab and diagnosing experiments on X-Y-Z motion platform under numeric control arithmetic, the validity of this method was proved. Aiming at different kinds of energy functions and the choices of their parameters, after particular analysis, some guidelines about fault diagnosis are concluded.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4594407,no
A software tool to relate technical performance to user experience in a mobile context,2008,"Users in todaypsilas mobile ICT environment are confronted with more and more innovations and an ever increasing technical quality, which makes them more demanding and harder to please. It is often hard to measure and to predict the user experience during service consumption. This is nevertheless a very important dimension that should be taken into account while developing applications or frameworks. In this paper we demonstrate a software tool that is integrated in a wireless living lab environment in order to validate and quantify actual user experience. The methodology to assess the user experience combines both technological and social assets. User experience of a Wineguide application on a PDA is related to signal strength, monitored during usage of the applications. Higher signal strengths correspond with a better experience (e.g. speed). Finally, difference in the experience among users will be discussed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4594902,no
SDTV Quality Assessment Using Energy Distribution of DCT Coefficients,2008,"The VQM (Video Quality Measurement) scheme is a methodology that measures the difference of quality between the distorted video signal and the reference video signal. In this paper, we propose a novel video quality measurement method that extracts features in DCT (Discrete Cosine Transform) domain of H.263 SDTV. Main idea of the proposed method is to utilize the texture pattern and edge oriented information that is generated in DCT domain. For this purpose, the energy distribution of the reodered DCT coefficients is considered to obtain unique information of each video file. Then, we measure the difference of probability distribution of context information between original video and distorded one. The simulation results show that the proposed algorithm can represent correctly the video quality and give a high correlation with the video DMOS.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4595547,no
A novel high-capability control-flow checking technique for RISC architectures,2008,"Nowadays more and more small transistors make microprocessors more susceptible to transient faults, and then induce control-flow errors. Software-based signature monitoring is widely used for control-flow error detection. When previous signature monitoring techniques are applied to RISC architectures, there exist some branch-errors that they can not detect. This paper proposes a novel software-based signature monitoring technique: CFC-End (Control-Flow Checking in the End). One property of CFC-End is that it uses two global registers for storing the run-time signature alternately. Another property of CFC-End is that it compares the run-time signature with the assigned signature in the end of every basic block. CFC-End is better than previous techniques in the sense that it can detect any single branch-error when applied to RISC architectures. CFC-End has similar performance overhead in comparison with the RCF (Region based Control-Flow checking) technique, which has the highest capability of branch-error detection among previous techniques.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4595567,no
Chaotic Routing: A Set-based Broadcasting Routing Framework for Wireless Sensor Networks,2008,"Data communication in wireless sensor networks (WSNs) exhibits distinctive characteristics. Routing in WSNs still relies on simple variations of traditional distance vector or link state based protocols, thus suffering low throughput and less robustness. Drawing intuitions from the Brownian motions where localized momentum exchanges enable global energy diffusion, we propose an innovative routing protocol, chaotic routing (CR), which achieves efficient information diffusion with seemingly chaotic local information exchanges. Leveraging emerging networking concepts such as potential based routing, opportunistic routing and network coding, CR improves throughput via accurate routing cost estimation, opportunistic data forwarding and localized node scheduling optimizing information propagation in mesh structures. Through extensive simulations, we prove that CR outperforms, in terms of throughput, best deterministic routing scheme (i.e. best path routing) by a factor of around 300% and beats the best opportunistic routing scheme (i.e. MORE) by a factor of around 200%. CR shows stable performance over wide range of network densities, link qualities and batch sizes.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4595586,no
Online Measurement of the Capacity of Multi-Tier Websites Using Hardware Performance Counters,2008,"Understanding server capacity is crucial for system capacity planning, configuration, and QoS-aware resource management. Conventional stress testing approaches measure the server capacity in terms of application-level performance metrics like response time and throughput. They are limited in measurement accuracy and timeliness. In a multitier website, resource bottleneck often shifts between tiers as client access pattern changes. This makes the capacity measurement even more challenging. This paper presents a measurement approach based on hardware performance counter metrics. The approach uses machine learning techniques to infer application-level performance at each tier. A coordinated predictor is induced over individual tier models to estimate system-wide performance and identify the bottleneck when the system becomes overloaded. Experimental results demonstrate that this approach is able to achieve an overload prediction accuracy of higher than 90% for a priori known input traffic patterns and over 85% accuracy even for traffic causing frequent bottleneck shifting. It costs less than 0.5% runtime overhead for data collection and no more than 50 ms for each on-line decision.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4595945,no
An alternative fault location algorithm based on Wavelet Transforms for three-terminal lines,2008,"This paper presents the study and development of a complete fault location scheme for three-terminal transmission lines using wavelet transforms (WT). The methodology is based on the low and high frequency components of the transient signals originated from a fault situation registered in the terminals of a system. By processing these signals and using the WT, it is possible to determine the time of traveling waves of voltages and/or currents from the fault point to the terminals, as well as to estimate the fundamental frequency components. As a consequence, both faulted leg and fault location can be estimated with reference to one of the system terminals. A new approach is presented to develop a reliable and accurate fault location scheme combining the best the methods can offer. The main idea is to have a decision routine in order to select which method should be used in each situation presented to the algorithm. The algorithm was tested for different fault conditions by simulations using ATP (alternative transients program) software. The results obtained are promising and demonstrate a highly satisfactory degree of accuracy and reliability of the proposed method.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4596274,no
Transmission systems power quality monitors allocation,2008,"This work develops and tests a branch and bound algorithm for solving optimum allocation of power quality monitors in a transmission power system. The optimization problem is solved by using 0-1 integer programming techniques and depends highly on network topology. The algorithm, which is implemented in Matlab software, minimizes the total cost of the monitoring system and found the optimum number and locations for monitors on the network studied, under a set of given network observability constraints. Case studies are presented for IEEE test networks and for CEMIG actual transmission power systems. Current and voltage values are estimated by using monitored variables to validate the obtained results.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4596309,no
Performance evaluation of a connection-oriented Internet service based on a queueing model with finite capacity,2008,"The operating mechanism of a connection-oriented Internet service is analyzed. Considering the finite buffer in connection-oriented Internet service, we establish a Geom/G/1/K queueing model with setup-close delay-close down for user-initiated session. Using the approach of an embedded Markovian chain and supplementary variables, we derive the probability distribution for the steady queue length and the probability generating function for the waiting time. Correspondingly, we study the performance measures of quality of service (QoS) in terms of system throughput, system response time, and system blocking probability. Based on the simulation results, we discuss the influence of the upper limit of close delay period and the capacity of the queueing model on the performance measures, which have potential application in the design, resource assigning, and optimal setting for the next generation Internet.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4598236,no
Assuring information quality in Web service composition,2008,"As organizations have begun increasingly to communicate and interact with consumers via the Web, so the information quality (IQ) of their offerings has become a central issue since it ensures service usability and utility for each visitor and, in addition, improves server utilization. In this article, we present an IQ-enable Web service architecture, IQEWS, by introducing a IQ broker module between service clients and providers (servers). The functions of the IQ broker module include assessing IQ about servers, making selection decisions for clients, and negotiating with servers to get IQ agreements. We study an evaluation scheme aimed at measuring the information quality of Web services used by IQ brokers acting as the front-end of servers. This methodology is composed of two main components, an evaluation scheme to analyze the information quality of Web services and a measurement algorithm to generate the linguistic recommendations.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4598537,no
"An Empirical Study on the Relationship Between Software Design Quality, Development Effort and Governance in Open Source Projects",2008,"The relationship among software design quality, development effort, and governance practices is a traditional research problem. However, the extent to which consolidated results on this relationship remain valid for open source (OS) projects is an open research problem. An emerging body of literature contrasts the view of open source as an alternative to proprietary software and explains that there exists a continuum between closed and open source projects. This paper hypothesizes that as projects approach the OS end of the continuum, governance becomes less formal. In turn a less formal governance is hypothesized to require a higher-quality code as a means to facilitate coordination among developers by making the structure of code explicit and facilitate quality by removing the pressure of deadlines from contributors. However, a less formal governance is also hypothesized to increase development effort due to a more cumbersome coordination overhead. The verification of research hypotheses is based on empirical data from a sample of 75 major OS projects. Empirical evidence supports our hypotheses and suggests that software quality, mainly measured as coupling and inheritance, does not increase development effort, but represents an important managerial variable to implement the more open governance approach that characterizes OS projects which, in turn, increases development effort.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4599582,no
Assessment driven process modeling for software process improvement,2008,"Software process improvement (SPI) is used to develop processes to meet more effectively the software organizationpsilas business goals. Improvement opportunities can be exposed by conducting an assessment. A disciplined process assessment evaluates organizationpsilas processes against a process assessment model, which usually includes good software practices as indicators. Many benefits of SPI initiatives have been reported but some improvement efforts have failed, too. Our aim is to increase the probability to success by integrating software process modeling with assessments. A combined approach is known to provide more accurate process ratings and higher quality process models. In this study we have revised the approach by extending the scope of modeling further. Assessment Driven Process Modeling for SPI uses assessment evidence to create a descriptive process model of the assessed processes. The descriptive model is revised into a prescriptive process model, which illustrates an organizationpsilas processes after the improvements. The prescriptive model is created using a process library that is based on the indicators of the assessment model. Modeling during assessment is driven by both process performance and process capability indicators.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4599774,no
Cross-Layer Transmission Scheme with QoS Considerations for Wireless Mesh Networks,2008,"IEEE 802.11 wireless networks utilizes a hard handoff scheme when a station is travelling from one area of coverage to another one within a transmission duration. In IEEE 802.11s wireless mesh networks, the handoff procedure the transmitted data will first be buffered in the source MAP and be not relayed to the target MAP until the handoff procedure is finished. Besides, there are multi-hop in the path between the source station and the destination station. In each pair of neighboring MAPs, contention is needed to transmit data. The latency for successfully transmitting data is seriously lengthened so that the deadlines of data frames are missed with high probabilities. In this paper, we propose a cross-layer transmission (CLT) scheme with QoS considerations for IEEE 802.11 mesh wireless networks. By utilizing CLT, the ratios of missing deadlines will be significantly improved to conform strict time requirements for real-time multimedia applications. We develop a simulation model to investigate the performance of CLT. The capability of the proposed scheme is evaluated by a series of experiments, for which we have encouraging results.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4599919,no
A fault tolerant approach in cluster computing system,2008,"A long-term trend in high performance computing is the increasing number of nodes in parallel computing platforms, which entails a higher failure probability. Hence, fault tolerance becomes a key property for parallel application running on parallel computing systems. The message passing interface (MPI) is currently the programming paradigm and communication library most commonly used on parallel computing platforms. MPI applications may be stopped at any time during their execution due to an unpredictable failure. In order to avoid complete restarts of an MPI application because of only one failure, a fault tolerant MPI implementation is essential. In this paper, we propose a fault tolerant approach in cluster computing system. Our approach is based on reassignment of tasks to the remaining system and message logging is used for message losses. This system consists of two main parts, failure diagnosis and failure recovery. Failure diagnosis is the detection of a failure and failure recovery is the action needed to take over the workload of a failed component. This fault tolerant approach is implemented as an extension of the message passing interface.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4600394,no
MUSIC: Mutation-based SQL Injection Vulnerability Checking,2008,"SQL injection is one of the most prominent vulnerabilities for web-based applications. Exploitation of SQL injection vulnerabilities (SQLIV) through successful attacks might result in severe consequences such as authentication bypassing, leaking of private information etc. Therefore, testing an application for SQLIV is an important step for ensuring its quality. However, it is challenging as the sources of SQLIV vary widely, which include the lack of effective input filters in applications, insecure coding by programmers, inappropriate usage of APIs for manipulating databases etc. Moreover, existing testing approaches do not address the issue of generating adequate test data sets that can detect SQLIV. In this work, we present a mutation-based testing approach for SQLIV testing. We propose nine mutation operators that inject SQLIV in application source code. The operators result in mutants, which can be killed only with test data containing SQL injection attacks. By this approach, we force the generation of an adequate test data set containing effective test cases capable of revealing SQLIV. We implement a MUtation-based SQL Injection vulnerabilities Checking (testing) tool (MUSIC) that automatically generates mutants for the applications written in Java Server Pages (JSP) and performs mutation analysis. We validate the proposed operators with five open source web-based applications written in JSP. We show that the proposed operators are effective for testing SQLIV.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4601530,no
Does Adaptive Random Testing Deliver a Higher Confidence than Random Testing?,2008,"Random testing (RT) is a fundamental software testing technique. Motivated by the rationale that neighbouring test cases tend to cause similar execution behaviours, adaptive random testing (ART) was proposed as an enhancement of RT, which enforces random test cases evenly spread over the input domain. ART has always been compared with RT from the perspective of the failure-detection capability. Previous studies have shown that ART can use fewer test cases to detect the first software failure than RT. In this paper, we aim to compare ART and RT from the perspective of program-based coverage. Our experimental results show that given the same number of test cases, ART normally has a higher percentage of coverage than RT. In conclusion, ART outperforms RT not only in terms of the failure-detection capability, but also in terms of the thoroughness of program-based coverage. Therefore, ART delivers a higher confidence of the software under test than RT even when no failure has been revealed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4601538,no
How to Measure Quality of Software Developed by Subcontractors (Short Paper),2008,"In Japan, where the multiple subcontracting is very common in software development, it is difficult to measure the quality of software developed by subcontractors. Even if we request them for process improvement based on CMMI including measurement and analysis, it will not work immediately. Using ""the unsuccessful ratio in the first time testing pass"" as measure to assess software quality, we have had good results. We can get these measures with a little effort of both outsourcer and subcontractor. With this measure, we could identify the defect-prone programs and conduct acceptance testing for these programs intensively. Thus, we could deliver the system on schedule. The following sections discuss why we devised this measure and its trial results.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4601558,no
Towards a Method for Evaluating the Precision of Software Measures (Short Paper),2008,"Software measurement currently plays a crucial role in software engineering given that the evaluation of software quality depends on the values of the measurements carried out. One important quality attribute is measurement precision. However, this attribute is frequently used indistinctly and confused with accuracy in software measurement. In this paper, we clarify the meaning of precision and propose a method for assessing the precision of software measures in accordance with ISO 5725. This method was used to assess a functional size measurement procedure. A pilot study was designed for the purpose of revealing any deficiencies in the design of our study.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4601559,no
A Method for Measuring the Size of a Component-Based System Specification,2008,"The system-level size measures are particularly important in software project management as tasks such as planning and estimating the cost and schedule of software development can be performed more effectively when a size estimate of the entire system is available. However, due to the black-box nature of components, traditional software measures are not adequate as system-level measures for component-based systems (CBS). Thus, if a system-level size is required, alternate measures should be used for sizing CBS. In this paper, we present a function point like approach, named component point to measure the system-level size of a CBS using the CBS specification written in UML. The component point approach integrates two software measures and extends an existing size measure from the more matured object-oriented paradigm to the related and relatively young CBS discipline. We also suggest a customized set of general system characteristics so as to make our measure more relevant to CBS.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4601562,no
Path and Context Sensitive Inter-procedural Memory Leak Detection,2008,This paper presents a practical path and context sensitive inter-procedural analysis method for detecting memory leaks in C programs. A novel memory object model and function summary system are used. Preliminary experiments show that the method is effective. Several memory leaks have been found in real programs including which and wget.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4601571,no
Achievements and Challenges in Cocomo-Based Software Resource Estimation,2008,"This article summarizes major achievements and challenges of software resource estimation over the last 40 years, emphasizing the Cocomo suite of models. Critical issues that have enabled major achievements include the development of good model forms, criteria for evaluating models, methods for integrating expert judgment and statistical data analysis, and processes for developing new models that cover new software development approaches. The article also projects future trends in software development and evolution processes, along with their implications and challenges for future software resource estimation capabilities.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4602679,no
An Expectation Trust Benefit Driven Algorithm for Resource Scheduling in Grid Computing,2008,"In traditional grid resource allocation solutions, the jobs with high-quality required may not be executed when they were allocated to low-quality offered nodes because of the separation of trust mechanism and job schedule mechanisms. Therefore, this paper combines the two mechanisms, proposes grid job schedule arithmetic driven by the expectation trust benefit. The value of the expectation trust benefit function is the prediction benefit of a certain job which executes in grid. Simulation experiments prove that expectation trust benefit driven arithmetic is better than conversional min-min arithmetic in benefit and it also has a good performance in trust benefit.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4603277,no
On the Trend of Remaining Software Defect Estimation,2008,"Software defects play a key role in software reliability, and the number of remaining defects is one of most important software reliability indexes. Observing the trend of the number of remaining defects during the testing process can provide very useful information on the software reliability. However, the number of remaining defects is not known and has to be estimated. Therefore, it is important to study the trend of the remaining software defect estimation (RSDE). In this paper, the concept of RSDE curves is proposed. An RSDE curve describes the dynamic behavior of RSDE as software testing proceeds. Generally, RSDE changes over time and displays two typical patterns: 1) single mode and 2) multiple modes. This behavior is due to the different characteristics of the testing process, i.e., testing under a single testing profile or multiple testing profiles with various change points. By studying the trend of the estimated number of remaining software defects, RSDE curves can provide further insights into the software testing process. In particular, in this paper, the Goel-Okumoto model is used to estimate this number on actual software failure data, and some properties of RSDE are derived. In addition, we discuss some theoretical and application issues of the RSDE curves. The concept of the proposed RSDE curves is independent of the selected model. The methods and development discussed in this paper can be applied to any valid estimation model to develop and study its corresponding RSDE curve. Finally, we discuss several possible areas for future research.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4604821,no
A Novel Embedded Fingerprints Authentication System Based on Singularity Points,2008,"In this paper a novel embedded fingerprints authentication system based on core and delta singularity points detection is proposed. Typical fingerprint recognition systems use core and delta singularity points for classification tasks. On the other hand, the available optical and photoelectric sensors give high quality fingerprint images with well defined core and delta points, if they are present. In the proposed system, fingerprint matching is based on singularity points position, orientation, and relative distance detection. As result, fingerprint matching involves the comparison between few features leading to a very fast system with recognition rates comparable to the standard minutiae based recognition systems. The whole system has been prototyped on the Celoxica RC203E board, equipped with a Xilinx VirtexII FPGA. The prototype has been tested with the FVC databases. Hardware system performance show high recognition rates and low execution time: FAR = 1.2% and FRR=2.6%, while each fingerprint elaboration requires 34.82 ms. Considering both FAR, FRR indexes and the high speed, the proposed system could be used for identification tasks in large fingerprint databases. To the best of our knowledge, this is the first authentication system based on singularity points.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4606665,no
Towards a Multidimensional Visualization of Environmental Soft Sensors Predictions,2008,"Soft sensors have been successfully applied to simulate physical and chemical measurements in specific locations of a monument. They allowed monitoring the quality conditions of the monument surface for long periods of time. This is a not invasive process that provides a huge set of multidimensional data. They have been analyzed by the Cultural Heritage experts to find physical or chemical critical condition which could generate some degradation process. Here we propose several multidimensional visualization techniques to represent the predictions of environmental parameters, given by several soft sensors, in a comprehensive and compact way. Visualization tools using both shape variation (glyph) and color are developed to realize an homogeneous communication paradigm. Moreover, each tool uses a 3D navigable model of the monument as visual support.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4606762,no
Fast motion estimation for H.264/AVC using image edge features,2008,"The key to high performance in video coding lies on efficiently reducing the temporal redundancies. For this purpose, H.264/AVC coding standard has adopted variable block size motion estimation on multiple reference frames to improve the coding gain. However, the computational complexity of motion estimation is also increased in proportion to the product of the reference frame number and the inter mode number. The mathematical analysis reveals that the prediction errors mainly depend on the image edge gradient amplitude and quantization parameter. Consequently, this paper proposed the image content based early termination algorithm, which outperforms the method adopted by JVT reference software, especially at high and moderate bit rates. In light of rate-distortion theory, this paper also relates the homogeneity of image to the quantization parameter. For the homogenous block, its search computation for the futile reference frames and inter modes can be efficiently discarded. Then the computation saving performance increases with quantization parameter. These content based fast algorithms were integrated with Unsymmetrical-cross Multihexagon-grid Search (UMHexagonS) algorithm. Compared to the original UMHexagonS fast matching algorithm, 26.14-54.97% search time can be saved with an average of 0.0369dB coding quality degradation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4607370,no
Automatic pixel-shift detection and restoration in videos,2008,"A common form of serious defect in video is pixel-shift. It is caused by the consecutive pixels loss introduced by video transmission systems. Pixel-shift means a large amount of pixel shifts one by one due to a small quantity of image data loss. The damaged region in affected frame is usually large, and thus the visual effect is often very disturbing. So far there is no method of automatically treating pixel-shift. This paper focuses on a difficult issue to locate pixel-shift in videos. We propose an original algorithm of automatically detecting and restoring pixel-shift. Pixel-shift frames detection relies on spatio-temporal information and motion estimation. Accurate measure of pixels shift is best achieved based on the analysis of temporal-frequency information. Restoration is accomplished by reversing the pixels shift and spatio-temporal interpolation. Experimental results show that our completely automatic algorithm can achieve very good performances.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4607741,no
Automatic detection of pronunciation errors in CAPT systems based on confidence measure,2008,"Computer aided pronunciation training (CAPT) systems aim at listening to a learner's utterance, judging the overall pronunciation quality and pointing out any pronunciation errors in the utterance. However, the performance of the current error detection techniques can not satisfy the users' expectation. In this paper, we introduce confidence measures and anti-models into the error detection algorithm. The basic theory and the roles of confidence measures and anti-models are discussed. We then propose a new algorithm, and evaluate it on an English learning system. Experiments are conducted based on the TIMIT database and an adaptive database, which involves 40 Chinese undergraduates. The results show that confidence measures can be utilized to effectively improve the performance of the CAPT system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4608055,no
Using the TSPi Defined Process and Improving the Project Management Process,2008,"Team software process is an integrated framework that guides development teams in producing high-quality software-intensive systems. This paper analyzes the effects of TSPi training and the improvements achieved by 44 teams, comprising fourth-year students on the software engineering degree programme, corresponding to two academic years. This study shows the benefits of using defined models such as TSPi to improve the students' skills and knowledge. The metrics of size and effort estimation, defects, productivity and costs are collected to measure the improvements through two development cycles.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4609426,no
Inaccessible Area and Effort Consumption Dynamics,2008,"The estimation of project effort E and project duration D is based on several simple empirical rules (laws). The laws are almost independent although some dependencies must exist. An example is the existence of so called inaccessible area (IA). IA is the set of pairs (D, E) that cannot appear. IA is related to the changes of effort consumption dynamics (ECD) for tight project durations D. The widely used Rayleigh model RM of ECD does not imply the existence of IA and the behavior of project efforts for projects having (D,E) near their lA's. RM has further deficiencies. Let t<sub>max</sub> be the time when effort intensity E(t) is maximal RM require that the effort E expended before t<sub>max</sub> is near to 0.52E. Another drawback is that RM implies almost no effort consumption for t > 3.5t<sub>max</sub>. It therefore wrongly implies that zero defect software could be a feasible goal. It is known that it is not true. We propose a new model M inspired by physics having no such drawbacks. The model can be used to refine estimations of D and E after t<sub>max</sub> is passed. The model implies several mutual dependencies of empirical laws of software development. M for example implies the behavior of effort E near inaccessibility area and implies a realistic estimation of project duration.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4609430,no
A New Classifier to Deal with Incomplete Data,2008,"Classification is a very important research topic in knowledge discovery and machine learning. Decision-tree is one of the well-known data mining methods that are used in classification problems. But sometimes the data set for classification contains vectors missing one or more of the feature values, and is called as incomplete data. Generally, the existence of incomplete data will degrade the learning quality of classification models. If the incomplete data can be dealt well, the classifier can be used to real life applications. So handling incomplete data is important and necessary for building a high quality classification model. In this paper a new decision tree is proposed to solve the incomplete data classification problem and it has a very good performance. At the same time, the new method solves two other important problems: rule refinement problem and importance preference problem, which ensures the outstanding advantages of the proposed approach. Significantly, this is the first classifier which can deal with all these problems at the same time.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4617356,no
Privacy Preserving of Associative Classification and Heuristic Approach,2008,"In the era of data explosion, privacy preserving has become a necessary task for any data mining task. Therefore, data transformation to ensure privacy preservation is needed. Meanwhile, the transformed data must have quality to be used in the intended data mining task, i.e. the impact on the data quality with regard to the data mining task must be minimized. However, the data transformation problem to preserve the data privacy while minimizing the impact has been proven as an NP-hard. Also, for classification mining, each classification approach may use different approach to deliver knowledge. Therefore, data quality metric for the classification task should be tailored to a specific type of classification. In this paper, we focus on maintaining the data quality in the scenarios which the transformed data will be used to build associative classification models. We propose a data quality metric for such the associative classification. Also, we propose a heuristic approach to preserve the privacy and maintain the data quality. Subsequently, we validate our proposed approaches with experiments.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4617410,no
A Crossover Game Routing Algorithm for Wireless Multimedia Sensor Networks,2008,"The multi-constrained QoS-based routing problem of wireless multimedia sensor networks is an NP hard problem. Genetic algorithms (GAs) have been used to handle these NP hard problems in wireless networks. Because the crossover probability is a key factor of GAs' action and performance, and affects the convergence of GAs, and the selection of crossover probability is very difficult, so we propose a novel method - crossover game instead of probability crossover. The crossover game in routing problems is based of each node has restricted energy, and each node trend to get maximal whole network benefit but pay out minimum cost. The players of crossover game are individual routes. The individual would perform crossover operator if it is Nash equilibrium in crossover game. The Simulation results demonstrate that this method is effective and efficient.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4617437,no
An integrated framework of the modeling of failure-detection and fault-correction processes in software reliability analysis,2008,"The failure-detection and fault-correction are critical processes in attaining good performance of software quality. In this paper, we propose several improvements on the conventional software reliability growth models (SRGMs) to describe actual software development process by eliminating some unrealistic assumptions. Most of these models have focused on the failure detection process and not given equal priority to modeling the fault correction process. But, most latent software errors may remain uncorrected for a long time even after they are detected, which increases their impact. The remaining software faults are often one of the most unreliable reasons for software quality. Therefore, we develop a general framework of the modeling of the failure detection and fault correction processes. Furthermore, we also analyze the effect of applying the delay-time non-homogeneous Poisson process (NHPP) models. Finally, numerical examples are shown to illustrate the results of the integration of the detection and correction process.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4618163,no
Graphical Representation as a Factor of 3D Software User Satisfaction: A Metric Based Approach,2008,"During the last few years, an increase in the development and research activity on 3D applications, mainly motivated by the rigorous growth of the game industry, is observed. This paper deals with assessing user satisfaction, i.e. a critical aspect of 3D software quality, by measuring technical characteristics of virtual worlds. Such metrics can be easily calculated in games and virtual environments of different themes and genres. In addition to that, the metric suite would provide an objective mean of comparing 3D software. In this paper, metrics concerning the graphical representation of a virtual world are introduced and validated through a pilot experiment.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4621546,no
A BBN Based Approach for Improving a Telecommunication Software Estimation Process,2008,"This paper describes analytically a methodology for improving the estimation process of a small-medium telecommunication (TLC) company. All the steps required for the generation of estimates such as data collection, data transformation, estimation model extraction and finally exploitation of the knowledge explored are described and demonstrated as a case study involving a Greek TLC company. Based on this knowledge certain interventions are suggested in the current process of the company under study in order to include formal estimation procedures in each development phase.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4621548,no
Dynamic Multipath Allocation in Ad Hoc Networks,2008,"Ad hoc networks are characterized by fast dynamic changes in the topology of the network. A known technique to improve QoS is to use multipath routing where packets (voice/video/...) from a source to a destination travel in two or more maximal disjoint paths. We observe that the need to find a set of maximal disjoint paths can be relaxed by finding a set of paths S wherein only bottlenecked links are bypassed. In the proposed model we assume that there is only one edge along a path in S is a bottleneck and show that by selecting random paths in S the probability that bottlenecked edges get bypassed is high. We implemented this idea in the MRA system which is a highly accurate visual ad hoc simulator currently supporting two routing protocols AODV and MRA. We have extended the MRA protocol to use multipath routing by maintaining a set of random routing trees from which random paths can be easily selected. Random paths are allocated/released by threshold rules monitoring the session quality. The experiments show that: (1) session QoS is significantly improve, (2) the fact that many sessions use multiple paths in parallel does not depredate overall performances, (3) the overhead in maintaining multipath in the MRA algorithm is negligible.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4622762,no
Development of Fault Detection System in Air Handling Unit,2008,"Monitoring systems used at present to operate air handling unit(AHU) optimally do not have a function that enables to detect faults properly when there are faults of such as operating plants or performance falling, so they are unable to manage faults rapidly and operate optimally. In this paper, we have developed a classified rule-based fault detection system which can be inclusively used in AHU system of a building by installation of sensor which is composed of AHU system and required low costs compare to the model based fault detection system which can be used only in a special building or system. In order to experiment this algorithm, it was applied to AHU system which is installed inside environment chamber(EC), verified its own practical effect, and confirmed its own applicability to the related field in the future.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4622840,no
A Scalable Method for Improving the Performance of Classifiers in Multiclass Applications by Pairwise Classifiers and GA,2008,"In this paper, a new combinational method for improving the recognition rate of multiclass classifiers is proposed. The main idea behind this method is using pairwise classifiers to enhance the ensemble. Because of more accuracy of them, they can decrease the error rate in error-prone feature space. Firstly, a multiclass classifier has been trained. Then, regarding to confusion matrix and evaluation data, the pair-classes that have the most error have been derived. After that, pairwise classifiers have been trained and added to ensemble of classifiers. Finally, weighted majority vote for combining the primary results is applied. In this paper, multi layer perceptron is used as base classifier. Also, GA determines the optimized weights in final classifier. This method is evaluated on a Farsi digit handwritten dataset. Using proposed method, the recognition rate of simple multiclass classifier has been improved from 97.83 to 98.89 which shows an adequate improvement.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4624131,no
Software Dependability Evaluation Model Based on Fuzzy Theory,2008,"Through the foundation of a universal dependability indicator system and making use of the fuzzy synthesized evaluation method, a dependability evaluation model is established to solve the problem of software dependability evaluation. The feasibility and effectiveness of this model is proved by a representative example. It provides an effective method for software dependability evaluation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4624841,no
Performance models for Cluster-enabled OpenMP implementations,2008,"A key issue for cluster-enabled OpenMP implementations based on software distributed shared memory (sDSM) systems, is maintaining the consistency of the shared memory space. This forms the major source of overhead for these systems, and is driven by the detection and servicing of page faults. This paper investigates how application performance can be modelled based on the number of page faults. Two simple models are proposed, one based on the number of page faults along the critical path of the computation, and one based on the aggregated numbers of page faults. Two different sDSM systems are considered. The models are evaluated using the OpenMP NAS parallel benchmarks on an 8-node AMD-based Gigabit Ethernet cluster. Both models gave estimates accurate to within 10% in most cases, with the critical path model showing slightly better accuracy; accuracy is lost if the underlying page faults cannot be overlapped, or if the application makes extensive use of the OpenMP flush directive.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4625433,no
"Algorithm for PM2.5 Mapping over Penang Island, Malaysia, Using SPOT Satellite Data",2008,"Air pollution is an important issue being monitored and regulated in industrial and developing cities. In this study, we explored the relationship between particulate matters of size less than 2.5 micron (PM 2.5) derived from the SPOT using regression technique. The aim of this study was to evaluate the high spatial resolution satellite data for air quality mapping by using FLAASH software. The corresponding PM 2.5 data were measured simultaneously with the acquired satellite scene and their locations were determined using a handheld Global Positioning System (GPS). Due to the fact that the current commercial aerosol retrieval method (FLAASH) was designed to work reasonably well over land, but is not accurate for water applications, adjustments had to be made to the software to optimize it for reflectance retrieval over water. In-situ reflectance spectra will be plot against spectra derived from the atmospherically corrected images. In this study, we ensure that all the atmospherically corrected spectra match reasonably with the in-situ spectra. We have developed a new algorithm that can effectively estimate the spatial distribution of atmospheric aerosols and retrieve surface reflectance from remotely sensed imagery under general atmospheric and surface conditions. The algorithm was developed base on the aerosol characteristics in the atmosphere. The efficiency of the developed algorithm, in comparison to other forms of algorithm, will be investigated in this study. Results indicate that there is a good correlation between the satellites derived PM 2.5 and the measured PM 2.5. This study shows the potential of using the thermal infrared data for air quality mapping. The finding obtained by this study indicates that the FLAASH can be used to retrieve air quality information for remotely sensed data.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4627036,no
Improving the Efficiency of Misuse Detection by Means of the q-gram Distance,2008,"Misuse detection-based intrusion detection systems (IDS) perform search through a database of attack signatures in order to detect whether any of them are present in incoming traffic. For such testing, fault-tolerant distance measures are needed. One of the appropriate distance measures of this kind is constrained edit distance, but the time complexity of its computation is too high. We propose a two-phase indexless search procedure for application in misuse detection-based IDS that makes use of q-gram distance instead of the constrained edit distance. We study how well q-gram distance approximates edit distance with special constraints needed in IDS applications. We compare the performances of the search procedure with the two distances applied in it. Experimental results show that the procedure with the q-gram distance implemented achieves for higher values of q almost the same accuracy as the one with the constrained edit distance implemented, but the efficiency of the procedure that implements the q-gram distance is much better.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4627086,no
DAST: A QoS-Aware Routing Protocol for Wireless Sensor Networks,2008,"In wireless sensor networks (WSNs), a challenging problem is how to advance network QoS. Energy-efficiency, network communication traffic and failure-tolerance, these important factors of QoS are closely related with the applied performance of WSNs. Hence a QoS-aware routing protocol called directed alternative spanning tree (DAST) is proposed to balance the above three factors of QoS. A directed tree-based model is constructed to bring data transmission more motivated and efficient. Based on Markov, a communication state predicted mechanism is proposed to choose reasonable parent, and packet transmission to double-parent is submitted with alternative algorithm. For enhancing network failure-tolerance, routing reconstruction is studied on. With the simulations, the proposed protocol is evaluated in comparison with the existing protocols from energy efficiency to the failure-tolerance. The performance of DAST is verified to be efficient and available, and it is competent for satisfying QoS of WSNs.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4627168,no
Design and Analysis of Embedded GPS/DR Vehicle Integrated Navigation System,2008,"Global Position system (GPS) is a positioning system with superior long-term error performance, while Dead Reckoning (DR) system has good positioning precision in short-term, through advantage complementation, a GPS/DR integration provides position data with high reliability for vehicle navigation system. This paper focuses on the design of the embedded GPS/DR vehicle integrated navigation system using the nonlinear Kalman filtering approach. The signal's observation gross errors are detected and removed at different resolution levels based on statistic 3sigma- theory, and navigation data are solved with Extended Kalman filter in real-time, the fault tolerance and precision of the vehicle integrated navigation system are improved greatly.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4627169,no
Tempest: Towards early identification of failure-prone binaries,2008,"Early estimates of failure-proneness can be used to help inform decisions on testing, refactoring, design rework etc. Often such early estimates are based on code metrics like churn and complexity. But such estimates of software quality rarely make their way into a mainstream tool and find industrial deployment. In this paper we discuss about the Tempest tool that uses statistical failure-proneness models based on code complexity and churn metrics across the Microsoft Windows code base to identify failure-prone binaries early in the development process. We also present the tool architecture and its usage as of date at Microsoft.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4630079,no
"An integrated approach to resource pool management: Policies, efficiency and quality metrics",2008,"The consolidation of multiple servers and their workloads aims to minimize the number of servers needed thereby enabling the efficient use of server and power resources. At the same time, applications participating in consolidation scenarios often have specific quality of service requirements that need to be supported. To evaluate which workloads can be consolidated to which servers we employ a trace-based approach that determines a near optimal workload placement that provides specific qualities of service. However, the chosen workload placement is based on past demands that may not perfectly predict future demands. To further improve efficiency and application quality of service we apply the trace-based technique repeatedly, as a workload placement controller. We integrate the workload placement controller with a reactive controller that observes current behavior to i) migrate workloads off of overloaded servers and ii) free and shut down lightly-loaded servers. To evaluate the effectiveness of the approach, we developed a new host load emulation environment that simulates different management policies in a time effective manner. A case study involving three months of data for 138 SAP applications compares our integrated controller approach with the use of each controller separately. The study considers trade-offs between i) required capacity and power usage, ii) resource access quality of service for CPU and memory resources, and iii) the number of migrations. We consider two typical enterprise environments: blade and server based resource pool infrastructures. The results show that the integrated controller approach outperforms the use of either controller separately for the enterprise application workloads in our study. We show the influence of the blade and server pool infrastructures on the effectiveness of the management policies.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4630101,no
Hot-spot prediction and alleviation in distributed stream processing applications,2008,"Many emerging distributed applications require the real-time processing of large amounts of data that are being updated continuously. Distributed stream processing systems offer a scalable and efficient means of in-network processing of such data streams. However, the large scale and the distributed nature of such systems, as well as the fluctuation of their load render it difficult to ensure that distributed stream processing applications meet their Quality of Service demands. We describe a decentralized framework for proactively predicting and alleviating hot-spots in distributed stream processing applications in real-time. We base our hot-spot prediction techniques on statistical forecasting methods, while for hot-spot alleviation we employ a non-disruptive component migration protocol. The experimental evaluation of our techniques, implemented in our Synergy distributed stream processing middleware over PlanetLab, using a real stream processing application operating on real streaming data, demonstrates high prediction accuracy and substantial performance benefits.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4630103,no
A recurrence-relation-based reward model for performability evaluation of embedded systems,2008,"Embedded systems for closed-loop applications often behave as discrete-time semi-Markov processes (DTSMPs). Performability measures most meaningful to iterative embedded systems, such as accumulated reward, are thus difficult to solve analytically in general. In this paper, we propose a recurrence-relation-based (RRB) reward model to evaluate such measures. A critical element in RRB reward models is the notion of state-entry probability. This notion enables us to utilize the embedded Markov chain in a DTSMP in a novel way. More specifically, we formulate state-entry probabilities, state-occupancy probabilities, and expressions concerning accumulated reward solely in terms of state-entry probability and its companion term, namely the expected accumulated reward at the point of state entry. As a result, recurrence relations abstract away all the intermediate points that lack the memoryless property, enabling a solvable model to be directly built upon the embedded Markov chain. To show the usefulness of RRB reward models, we evaluate an embedded system for which we leverage the proposed notion and methods to solve a variety of probabilistic measures analytically.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4630124,no
A new technique for assessing the diversity of close-Pareto-optimal front,2008,"The quality of an approximation set usually includes two aspects-- approaching distance and spreading diversity. This paper introduces a new technique for assessing the diversity of an approximation to an exact Pareto-optimal front. This diversity is assessed by using an ldquoexposure degreerdquo of the exact Pareto-optimal front against the approximation set. This new technique has three advantages: Firstly, The ldquoexposure degreerdquo combines the uniformity and the width of the spread into a direct physical sense. Secondly, it makes the approaching distance independent from the spreading diversity at the most. Thirdly, the new technique works well for problems with any number of objectives, while the widely used diversity metric proposed by Deb would work poor in problems with 3 objectives or over. Experimental computational results show that the new technique assesses the diversity well.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4630820,no
A Grammatical Swarm for protein classification,2008,"We present a grammatical swarm (GS) for the optimization of an aggregation operator. This combines the results of several classifiers into a unique score, producing an optimal ranking of the individuals. We apply our method to the identification of new members of a protein family. Support vector machine and naive Bayes classifiers exploit complementary features to compute probability estimates. A great advantage of the GS is that it produces an understandable algorithm revealing the interest of the classifiers. Due to the large volume of candidate sequences, ranking quality is of crucial importance. Consequently, our fitness criterion is based on the area under the ROC curve rather than on classification error rate. We discuss the performances obtained for a particular family, the cytokines and show that this technique is an efficient means of ranking the protein sequences.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4631142,no
Software quality modeling: The impact of class noise on the random forest classifier,2008,"This study investigates the impact of increasing levels of simulated class noise on software quality classification. Class noise was injected into seven software engineering measurement datasets, and the performance of three learners, random forests, C4.5, and Naive Bayes, was analyzed. The random forest classifier was utilized for this study because of its strong performance relative to well-known and commonly-used classifiers such as C4.5 and Naive Bayes. Further, relatively little prior research in software quality classification has considered the random forest classifier. The experimental factors considered in this study were the level of class noise and the percent of minority instances injected with noise. The empirical results demonstrate that the random forest obtained the best and most consistent classification performance in all experiments.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4631321,no
Module documentation based testing using Grey-Box approach,2008,"Testing plays an important role to assure the quality of software. Testing is a process of detecting errors that can be highly effective if performed rigorously. The use of formal specifications provides significant opportunity to develop effective testing techniques. Grey-box testing approach usually based on knowledge obtains from specification and source code while seldom the design specification is concerned. In this paper, we propose an approach for testing a module with internal memory from its formal specification based on grey-box approach. We use formal specifications that are documented using Parnasâ€™s Module Documentation (MD) method. The MD provides us with the information of external and internal view of a module that can be useful in greybox testing approach.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4631651,no
The sliced Gaussian mixture filter for efficient nonlinear estimation,2008,"This paper addresses the efficient state estimation for mixed linear/nonlinear dynamic systems with noisy measurements. Based on a novel density representation - sliced Gaussian mixture density - the decomposition into a (conditionally) linear and nonlinear estimation problem is derived. The systematic approximation procedure minimizing a certain distance measure allows the derivation of (close to) optimal and deterministic estimation results. This leads to high-quality representations of the measurement-conditioned density of the states and, hence, to an overall more efficient estimation process. The performance of the proposed estimator is compared to state-of-the-art estimators, like the well-known marginalized particle filter.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4632188,no
Software quality prediction using Affinity Propagation algorithm,2008,"Software metrics are collected at various phases of the software development process. These metrics contain the information of the software and can be used to predict software quality in the early stage of software life cycle. Intelligent computing techniques such as data mining can be applied in the study of software quality by analyzing software metrics. Clustering analysis, which can be considered as one of the data mining techniques, is adopted to build the software quality prediction models in the early period of software testing. In this paper, a new clustering method called Affinity Propagation is investigated for the analysis of two software metric datasets extracted from real-world software projects. Meanwhile, K-Means clustering method is also applied for comparison. The numerical experiment results show that the Affinity Propagation algorithm can be applied well in software quality prediction in the very early stage, and it is more effective on reducing Type II error.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4634056,no
A novel method for DC system grounding fault monitoring on-line and its realization,2008,"On the basis of comparison and analysis of the present grounding fault monitoring methods such as method of AC injection, method of DC leakage and so on, this paper points out their shortcomings in practical applications. A novel method named method of different frequency signals for detecting grounding fault of DC system is advanced, which can overcome the bad influence of distributed capacitor between the ground and branches. Finally a new kind of detector based on the proposed method is introduced. The detector, with C8051F041 as kernel, adopting method of different frequency signals, realizes on-line grounding fault monitoring exactly. The principles, hardware and software design are introduced in detail. The experimental results and practical operations show that the detector has the advantages of high-precision, better anti-interference, high degree of automation, low cost, etc.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4636452,no
Type Highlighting: A Client-Driven Visual Approach for Class Hierarchies Reengineering,2008,"Polymorphism and class hierarchies are key to increasing the extensibility of an object-oriented program but also raise challenges for program comprehension. Despite many advances in understanding and restructuring class hierarchies, there is no direct support to analyze and understand the design decisions that drive their polymorphic usage. In this paper we introduce a metric-based visual approach to capture the extent to which the clients of a hierarchy polymorphically manipulate that hierarchy. A visual pattern vocabulary is also presented in order to facilitate the communication between analysts. Initial evaluation shows that our techniques aid program comprehension by effectively visualizing large quantities of information, and can help detect several design problems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4637553,no
Performance Prediction Model for Service Oriented Applications,2008,"Software architecture plays a significant role in determining the quality of a software system. It exposes important system properties for consideration and analysis. Performance related properties are frequently of interest in determining the acceptability of a given software design. This paper focuses mainly on developing an architectural model for applications that use service oriented architecture (SOA). This enables predicting the performance of the application even before it is completely developed. The performance characteristics of the components and connectors are modeled using queuing network model. This approach facilitates the performance prediction of service oriented applications. Further, it also helps in identification of various bottlenecks. A prototype service oriented application has been implemented and the actual performance is measured. This is compared against the predicted performance in order to analyze the accuracy of the prediction.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4637815,no
"Networks, multicore, and systems evolution - facing the timing beast",2008,"Embedded systems rapidly grow in several dimensions. They grow in size, from local isolated networks with simple protocols to network hierarchies to large open heterogeneous networks with complex communication behaviour. They grow in performance, from simple microcontrollers to superscalar to multicore systems with many levels of memory hierarchy. And, they expand in the time dimension by moving from static system functions to open and evolutionary functions that change over time and require new design methods and autonomous system functions. All these development contribute to an ever increasing behavioural complexity with equally complex timing. Nevertheless, the fundamental requirements to reliability and performance predictability have stayed and even been enhanced. Embedded system technology has responded with new integration methods and software architectures supported by platform control methods using new service quality metrics, and with composable formal methods that scale with system size. The talk will give an overview on this exiting scientific field and will give practical examples.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4638353,no
A Metrics Based Approach to Evaluate Design of Software Components,2008,"Component based software development approach makes use of already existing software components to build new applications. Software components may be available in-house or acquired from the global market. One of the most critical activities in this reuse based process is the selection of appropriate components. Component evaluation is the core of the component selection process. Component quality models have been proposed to decide upon a criterion against which candidate components can be evaluated and then compared. But none is complete enough to carry out the evaluation. It is advocated that component users need not bother about the internal details of the components. But we believe that complexity of the internal structure of the component can help estimating the effort related to evolution of the component. In our ongoing research, we are focusing on quality of internal design of a software component and its relationship to the external quality attributes of the component.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4638679,no
Test-Suite Augmentation for Evolving Software,2008,"One activity performed by developers during regression testing is test-suite augmentation, which consists of assessing the adequacy of a test suite after a program is modified and identifying new or modified behaviors that are not adequately exercised by the existing test suite and, thus, require additional test cases. In previous work, we proposed MATRIX, a technique for test-suite augmentation based on dependence analysis and partial symbolic execution. In this paper, we present the next step of our work, where we (I) improve the effectiveness of our technique by identifying all relevant change-propagation paths, (2) extend the technique to handle multiple and more complex changes, (3) introduce the first tool that fully implements the technique, and (4) present an empirical evaluation performed on real software. Our results show that our technique is practical and more effective than existing test-suite augmentation approaches in identifying test cases with high fault-detection capabilities.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4639325,no
An emergency Earthquake warning system for land mobile vehicles using the earthquake early warning,2008,"This paper describes a study of an emergency earthquake warning system for land mobile vehicles using the Earthquake early warning in Japan, First, estimation accuracy of seismic wave is evaluated at a local area, when using both the position information and the soil information database in addition to the Earthquake early warning, and it is shown that using those additional information is effective for indication of the earthquake warning to each vehicle. Second, an emergency warning system for land mobile vehicles is proposed and the distribution system of the warning messages in the proposed system is investigated. It results that the Ground digital broadcasting is effective for the distribution to vehicles of the warning messages because of high quality and usability on mobile, and the Dedicated short range communication can also be provided reliable distribution. Last, an automotive experimental software of the proposed system is developed and confirmation of the software is performed by using warnings.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4640895,no
Intelligent Java Analyzer,2008,"This paper presents a software metric working prototype to evaluate Java programmer's profiles. In order to automatically detect source code patterns, a Multi Layer Perceptron neural network is applied. Features determined from such patterns constitute the basis for systempsilas programmer profiling. Results presented here show that the proposed prototype is a confident approach for support in the software quality assurance process.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4641074,no
Software development for determination of power quality parameters,2008,"The Colombian Energy and Gas Regulating Commission -CREG- through resolution CREG 024 of 2005, requires monitoring some power quality disturbances in substations of distribution systems; the main objective of this resolution is determine electrical power quality supplied by the utility operators to costumers by means of measurements analysis. In this paper the authors proposes a computational tool for management and classification of power quality measurements required by Regulator Entity, This tool performs a power quality analysis in substations and shows information about conditions in power quality to conduct subsequent analysis.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4641776,no
Differential protection of three-phase transformers using Wavelet Transforms,2008,"This paper proposes a novel formulation for differential protection of three-phase transformers using Wavelet Transforms (WTs). The new proposed methodology implements the WTs to extract predominant transient signals originated by transformer internal faults and captured from the current transformers. The Wavelet Transform is an efficient signal processing tool used to study non stationary signals with fast transition (high frequency components), mapping the signal in time-frequency representation. The three phase differential currents are the input signals used on-line to detect internal faults. The performance of this algorithm is demonstrated through simulation of different internal faults and switching conditions on a power transformer using ATP/EMTP software. The analyzed data is obtained from simulation of different normal and faulty operating conditions such as internal faults - phase/phase, phase/ground-, magnetizing inrush and external faults. The case study shows that the new algorithm is highly accurate and effective.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4641818,no
Automatic generation of supervision system based on bond graph tool,2008,"The paper deals with an integrated approach for supervision systems design based on a mechatronic properties of a bond graph tool. The methodology is implemented in a specific software which automatically creates complex process dynamic models from a simple graphical interface, where system components can be dragged from a component data base and interconnected so as to produce the overall system, following the Piping and Instrumentation Diagram. Once the model has been created, the software checks its consistency and performs its structural analysis in order to automatically determine the diagnosis algorithms which should be implemented, and their fault detectability and isolability performances. The friendly graphical user interface allows to test several sensor configurations in order to optimise the diagnostic possibilities. The obtained model can also be used for the simulation of the process and its diagnosis algorithms in normal and faulty situations.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4648812,no
Web-based evaluation of Parkinson's Disease subjects: Objective performance capacity measurements and subjective characterization profiles,2008,"Parkinson's Disease (PD) is classified as a progressively degenerative movement disorder, affecting approximately 0.2% of the population and resulting in decreased performance in a wide variety of activities of daily living. Motivated by needs associated with the conduct of multi-center clinical trials, early detection, and the optimization of routine management of individuals with PD, we have developed a three-tiered approach to evaluation of PD and other neurologic diseases/disorders. One tier is characterized as â€œweb-based evaluationâ€? consisting of objective performance capacity tests and subjective questionnaires that target history and symptom evaluation. Here, we present the initial evaluation of three representative, self-administered, objective, web-based performance capacity tests (simple visual-hand response speed, rapid alternating movement quality, and upper extremity neuromotor channel capacity). Twenty-one subjects (13 with PD, 8 without neurologic disease) were evaluated. Generally good agreement was obtained with lab-based tests executed with an experienced test administrator. We conclude that objective performance capacity testing is a feasible component of a web-based evaluation for PD, providing a sufficient level of fidelity to be useful.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4649273,no
Ultrasound-based coordinate measuring system for estimating cervical dysfunction during functional movement,2008,"Cervical range of motion (ROM) is a part of the dynamic component of spine evaluation and can be used as an indication of dysfunction in anatomical structures as well as a diagnostic aid in patients with neck pain. Studies indicate that movement coordination of axial segments such as head in dynamic state, disrupted in pathologic conditions. In recent years, a number of non-invasive instruments with varying degrees of accuracy and repeatability have been utilized to measure active or passive range of motion in asymptomatic adults. The aim of this investigation is to design and implement a new method by evidence based approach for estimating the level of defect in segment stability and improvement after treatment by measuring quality or quantity of movement among cervical segment. Transmitter sensors which have been mounted on body send ultrasonic burst signal periodically and from the delay time it takes for this burst to reach three other sensors which arranged on a T-shape Mechanical base, three dimensional position of the transmitter can be calculated. After sending 3D coordination data to a PC via USB port, a complex and elaborative Visual Basic software calculate the angular dispersion and acceleration for each segment separately. This software also calculates the stabilization parameters such as anchoring index (AI) and cross-correlation function (CCF) between head and trunk.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4649651,no
Coal Management Module (CMM) for power plant,2008,"Importance of coal management in power plant is very much significant and also one of the most critical areas in view of plant operation as well as cost involvement, so it forms an important part of the management process in a power plant. It deals with the management of commercial, operational and administrative functions pertaining to estimating coal requirements, selection of coal suppliers, coal quality check, transportation and coal handling, payment for coal received, consumption and calculation of coal efficiency. The results are then used for cost benefit analysis to suggest further plant improvement. At various levels, management information reports need to be extracted to communicate the required information across various levels of management. The core processes of coal management involve a huge amount of paper work and manual labour, which makes it tedious, time-consuming and prone to human errors. Moreover, the time taken at each stage as well as the transparency of the relevant information has a direct bearing on the economics and efficient operation of the power plant. Both system performance and information transparency can be enhanced by the introduction of Information Technology in managing this area. This paper reports on the design & development of Coal Management Module (CMM) Software, which aims at systematic functioning of the Core Business Processes of Coal Management of a typical coal-fired power plant.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4651474,no
An Early Reliability Assessment Model for Data-Flow Software Architectures,2008,"Software architectures have emerged as a promising approach for managing, analyzing, building, integrating, reusing, and improving the quality of software systems. Specifically, early design decisions can be improved by the analysis of architectural models for different properties. This paper addresses the problem of estimating the reliability of data-flow architectures before the construction of the system. The proposed model uses an operational profile of the system and a set of component test profiles. A test profile is a set of test cases extended with information about the software intra-component execution. The analysis of the system is performed by composing the test points along the virtual execution among the components. This strategy overcomes the determination of intermediate operational profiles. In addition, metrics to select the best match in the execution trace and to evaluate the selection error in such kind of match are described.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4653235,no
A Study of Analogy Based Sampling for interval based cost estimation for software project management,2008,"Software cost estimation is one of the most challenging activities in software project management. Since the software cost estimation affects almost all activities of software project development such as: biding, planning, and budgeting, the accurate estimation is very crucial to the success of software project management. However, due to the inherent uncertainties in the estimation process and other factors, the accurate estimates are often obtained with great difficulties. Therefore, it is safer to generate interval based estimates with a certain probability over them. In the literature, many approaches have been proposed for interval estimation. In this study, we propose a navel method namely Analogy Based Sampling (ABS) and compare ABS against the well established Bootstrapped Analogy Based Estimation (BABE) which is the only existing variant of analogy based method with the capability to generate interval predictions. The results and comparisons show that ABS could improve the performance of BABE with much higher efficiencies and more accurate interval predictions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4654377,no
Software estimation tool based on three -layer model for software engineering metrics,2008,"In todaypsilas competitive world of software, cost estimation of software plays a major role because it determines the effort, time, quality etc. Lines of codes and function point are traditional method for estimating the various software metrics. But in todaypsilas world of object oriented programming function point calculation for C++, Java is a big challenge. We have developed a tool based on function point analysis which has three layer architecture model to calculate various software metrics project, especially for Java program. This tool has been used for estimating of various projects developed by students. Finally as a result we have shown the attributes of three layer models i.e. FP, LOC, productivity and Time etc.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4654437,no
A study of unavailability of software systems,2008,"System availability is a critical measure of complex system during the operational phase. It is especially important to those safety-critical systems. While in the last few decades many technologies have been developed for enhancing the hardware reliability and availability and there are plenty of literatures in this field, little has been done in the aspect of software availability. The objective of this paper is to investigate some simple models for the estimation of software unavailability and its impact on systems considering safety, risk and cost. In this paper, a stochastic framework to model software availability is proposed. We consider software availability as the percentage of time the software is operational and both the operation and repair process are studied under this framework. The unavailability is the percentage of downtime. The dynamic behaviours of software failure and maintenance are studied and the effects of scheduled service and break-down time on software unavailability are investigated. A numerical example is provided to show how software availability can be estimated.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4654549,no
Effect of color pre-processing on color-based object detection,2008,"Color-based object detection is receiving much interest recently for the potential applications in the traffic and security fields. While Color information is precious to facilitate and accelerate the object detection process, the spectral sensitivity functions of the color camera sensor have a strong effect on the quality of image colors and their appearance in the Hue -Saturation, H-S histogram of the object surface color. In this work we study the effect of color preprocessing by using a linear spectral sharpening transform, on the quality of image colors and their detection quality. It is shown that color-based object detection is severely impaired due to the spectral overlap of camera filters. We show the performance of a new spectral sharpening method, running on real images. The image color preprocessing resulted in a significant reduction of color correlation and hence clearer image colors. The transformed colors allow for fine definition of color zones in the H-S diagram and consequently enhances the detection process.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4654827,no
Statistical Analysis of Bayes Optimal Subset Ranking,2008,"The ranking problem has become increasingly important in modern applications of statistical methods in automated decision making systems. In particular, we consider a formulation of the statistical ranking problem which we call subset ranking, and focus on the discounted cumulated gain (DCG) criterion that measures the quality of items near the top of the rank-list. Similar to error minimization for binary classification, direct optimization of natural ranking criteria such as DCG leads to a nonconvex optimization problems that can be NP-hard. Therefore, a computationally more tractable approach is needed. We present bounds that relate the approximate optimization of DCG to the approximate minimization of certain regression errors. These bounds justify the use of convex learning formulations for solving the subset ranking problem. The resulting estimation methods are not conventional, in that we focus on the estimation quality in the top-portion of the rank-list. We further investigate the asymptotic statistical behavior of these formulations. Under appropriate conditions, the consistency of the estimation schemes with respect to the DCG metric can be derived.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4655444,no
The Ï€ measure,2008,"A novel software measure, called the pi measure, used for evaluating the fault-detection effectiveness of test sets, for measuring test-case independence and for measuring code complexity is proposed. The pi measure is interpreted as a degree of run-time control and data difference at the code level, resulting from executing a program on a set of test cases. Unlike other well-known static and dynamic complexity measures, the pi measure is an execution measure, computed using only run-time information. The Diversity Analyzer computes the pi measure for programs written in C, C++, C# and VB in .NET. The experimental data presented here show a correlation between the pi measure, test case independence and fault-detection rates.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4657464,no
Software reliability allocation of digital relay for transmission line protection using a combined system hierarchy and fault tree approach,2008,"Digital relay is a special purpose signal processing unit in which the samples of physical parameters such as current, voltage and other quantities are taken. With the proliferation of computer technology in terms of computational ability as well as reliability, computers are being used for such digital signal processing purposes. As far as computer hardware is concerned, it has been growing steadily in terms of power and reliability. Since power plant technology is now globally switching over to such computer-based relaying, software reliability naturally emerges as an area of prime importance. Recently, some computer-based digital relay algorithms have been proposed based on frequency-domain analysis using wavelet-neuro-fuzzy techniques for transmission line faults. A software reliability allocation scheme is devised for the performance evaluation of a multi-functional, multi-user digital relay that does detection, classification and location of transmission line faults.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4657466,no
A Rough Set Model for Software Defect Prediction,2008,High assurance software requires extensive and expensive assessment. Many software organizations frequently do not allocate enough resources for software quality. We research the defect detectors focusing on the data sets of software defect prediction. A rough set model is presented to deal with the attributes of data sets of software defect prediction in this paper. Appling this model to the most famous public domain data set created by the NASA's metrics data program shows its splendid performance.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4659587,no
Research and Design of Multifunctional Intelligent Melted Iron Analyzer,2008,"A multifunctional intelligent melted iron analyzer is researched and designed. This device combined the function of thermal analysis for quality analysis of melted iron and ultrasonic measuring for nodularity. By thermal analysis, equation of linear regression wasused and the percentage composition of carbon, silicon and phosphorus etc. were obtained. In addition, thermal analysis was used to predict gray iron inoculation result, structure and performance. Therefore, ultrasonic measuring was adopted to be applied in this study to survey the nodularity of spheroidal graphite iron. In order to protect the analyzer far away from boiling melted iron, wireless temperature collection was adopted. The system is composed of PC104, single chip SPCE061 and other peripheral circuits. Designed temperature collection and ultrasonic measuring module, the software of data analysis and management is programmed by VB6.0 based on PC104 and windows operation system. After running for more than one year, the analyzer goes very well in the foundry.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4659881,no
Molecular imaging of the myoskeletal system through Diffusion Weighted and Diffusion Tensor Imaging with parallel imaging techniques,2008,"Diffusion weighted imaging (DWI) and diffusion tensor imaging (DTI) are useful tools when used in combination with standard imaging methods that may offer a significant advantage in certain clinical applications such as orthopedics and myoskeletal tissue imaging. Incorporation of these tools in clinical practice is limited due to the considerable amount of user intervention that apparent diffusion coefficient (ADC) and anisotropy data require in terms of processing and quantification require and due to the importance of acquisition parameter optimization in image quality. In this work various acquisition parameters and their effects in DWI and DTI are investigated. To assess the quality of these techniques, a series of experiments were conducted using a phantom. The application of lipid suppression techniques and their compatibility with other parameters were also investigated. Artifacts were provoked to study the effects in imaging quality. All the data were processed with specialized software to analyze various aspects of the measurements and quantify various parameters such as signal to noise ratio (SNR), contrast to noise ratio (CNR), and the accuracy of ADC and fractional anisotropy values. The experience acquired from the experiments was applied in acquisition parameter optimization and improvement of clinical applications for the rapid screening and differential diagnosis of myoskeletal pathologies.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4659972,no
Assessment and optimization of TEA-PRESS sequences in 1H MRS and MRSI of the breast,2008,"Magnetic Resonance Spectroscopy (MRS) and Magnetic Resonance Spectroscopic Imaging (MRSI) are useful tools when used in combination with standard imaging methods that may offer a significant advantage in certain clinical applications such as cancer localization and staging. Incorporation of these tools in clinical practice is, however, limited due to the considerable amount of user intervention that spectrum processing and quantification requires and due to the importance of acquisition parameter optimization in spectrum quality. In this work various acquisition parameters and their effects in spectrum quality are investigated. In order to assess the quality of various spectroscopic techniques, a series of experiments were conducted using a standard solution. The application of water and fat suppression techniques and their compatibility with other parameters were also investigated. A number of artifacts were provoked to study the effects in spectrum quality. The stability of the equipment, the appearance of errors and artifacts and the reproducibility of the results were also examined to obtain useful conclusions for the interaction of acquisition parameters. All the data were processed with specialized computer software (jMRUI 2.2, FUNCTOOL) to analyze various aspects of the measurements and quantify various parameters such as signal to noise ratio (SNR), full width at half maximum (FWHM), peak height and j-modulation. The experience acquired from the conducted experiments was successfully applied in acquisition parameter optimization and improvement of clinical applications for the biochemical analysis of breast lesions by significantly improving the spectrum quality, SNR and spatial resolution.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4659988,no
Fault-tolerant event region detection in wireless sensor networks using statistical hypothesis test,2008,"Most existing algorithms for fault-tolerant event region detection only assume that events are spatially correlated, but we argue that events are usually both spatially and temporally correlated. By examining the temporal correlation of sensor measurements, we propose a detection algorithm by applying statistical hypothesis test (SHT). SHT-based algorithm is more accurate in detecting event regions, and is more energy efficient since it avoids measurement exchanges. To improve the capability of fault recognition, we extend SHT-based algorithm by examining both spatial and temporal correlations of sensor measurements. The extended SHT-based algorithm can recognize almost all faults when sensor network is densely deployed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4660039,no
Multiresolution sensor fusion approach to PCB fault detection and isolation,2008,"This paper describes a novel approach to printed circuit board (PCB) testing that fuses the products of individual, non-traditional sensors to draw conclusions regarding overall PCB health and performance. This approach supplements existing parametric test capabilities with the inclusion of sensors for electromagnetic emissions, laser Doppler vibrometry, off-gassing and material parameters, and X-ray and Terahertz spectral images of the PCB. This approach lends itself to the detection and prediction of entire classes of anomalies, degraded performance, and failures that are not detectable using current automatic test equipment (ATE) or other test devices performing end-to-end diagnostic testing of individual signal parameters. This greater performance comes with a smaller price tag in terms of non-recurring development and recurring maintenance costs over currently existing test program sets. The complexities of interfacing diverse and unique sensor technologies with the PCB are discussed from both the hardware and software perspective. Issues pertaining to creating a whole-PCB interface, not just at the card-edge connectors, are addressed. In addition, we discuss methods of integrating and interpreting the unique software inputs obtained from the various sensors to determine the existence of anomalies that may be indicative of existing or pending failures within the PCB. Indications of how these new sensor technologies may comprise future test systems, as well as their retrofit into existing test systems, will also be provided.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4662581,no
Regression analysis of automated measurment systems,2008,"Automated measurement systems are dependent upon successfully application of multiple integrated systems to perform measurement analysis on various units-under-tests (UUT)s. Proper testing, fault isolation and detection of a UUT are contingent upon accurate measurements of the automated measurement system. This paper extends previous presentation from 2007 AUTOTESTCON on the applicability of measurement system analysis for automated measurement systems. The motivation for this research was to reduce risk of transportability issues from legacy measurement systems to emerging systems. Improving regression testing utilizing parametric metadata for large scale automated measurement systems over existing regression testing techniques which provides engineers, developers and management increased confidence that mission performance is not compromised. The utilization of existing software statistical tools such as Minitab<sup>R</sup> provides the necessary statistical techniques to evaluate measurement capability of automated measurement systems. By applying measurement system analysis to assess the measurement variability between the US Navypsilas two prime automated test systems the Consolidated Automated Support System (CASS) and the Reconfigurable-Transportable Consolidated Automated Support System (RTCASS). Measurement system analysis shall include capability analysis between one selected CASS and RTCASS instrument to validate measurement process capability; general linear model to assess variability between stations, multivariate analysis to analyze measurement variability of UUTs between measurement systems, and gage repeatability and reproducibility analysis to isolate sources of variability at the UUT testing level.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4662676,no
Flexible Advance Reservation Impact on Backfilling Scheduling Strategies,2008,"Advance reservation mechanism was introduced in grid environments to provide time quality of service requirements for time critical applications. Also there are applications that need resource coordination namely co-allocation and workflow, which benefits from this capability. Contemporary trend in grid computing is toward flexibility, because of its advantages in advance reservation such as increasing accept rate. This mechanism should be supported by local scheduler which is responsible for normal jobs. Using backfilling methods with FCFS priority for scheduling normal jobs is the dominant approach. Therefore well investigation of performance impact of flexible Advance Reservation on backfilling scheduling is essential. In this paper incorporation of flexible Advance Reservation in major backfilling policies is investigated, regarding workload parameters impact on well known performance metrics. Here also the impact of increasing inaccuracy of user estimated job runtime is studied. Our experimental results indicate that aggressive backfilling algorithm is biased toward AR related performance metrics but conservative method has better job performance. Although more flexibility in advance reservation would improve AR performance but its impact on degrade of job performance metrics is noteworthy. Both parameter types' rates are less in conservative method. Moreover, usually increasing inaccuracy interestingly has opposing effect on backfilling methods.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4662857,no
ADOM: An Adaptive Objective Migration Strategy for Grid Platform,2008,"Object migration is the movement of objects from one machine to another during execution. It can be used to enhance the efficiency and the reliability of grid systems, such as to balance load distribution, to enable fault resilience, to improve system administration, and to minimize communication overhead. Most existing schemes apply fixed object migration strategies, which are unadaptable to changing requirements of applications. In this paper,we address the issue of object migration for large scale grid system with multiple object levels.First, we devise a probabilistic object tree model and formulate the object migration problem as an optimization problem. Then we proposed an adaptive object migration algorithm called ADOM to solve the problem. the ADOM algorithm applies the breadth first search scheme to traversal the object tree and migrates object adaptively according to their access probability. Finally we evaluate the performance of different object migration algorithms in our grid platform, which show that the ADOM algorithm outperforms other algorithms under large object tree size.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4662898,no
A Dynamic Fault Tolerant Algorithm Based on Active Replication,2008,"To the wide area network oriented distributed computing such as Web services, a slow service is equivalent to an unavailable service. It makes demands of effectively improving the performance without damaging availability to the replication algorithms. Aim for improving the performance of the active replication algorithm, we propose a new replication algorithm named AAR (adaptive active replication). Its basic idea is: all replicas receive requests, but only the fastest one returns the response. Its main advantages are: (1) The response is returned by the fastest replica; (2) The algorithm is based on the active replication algorithm, but it avoids the redundant nested invocation problem. We prove the advantages by analyzing and experiments.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4662916,no
Bridging Security and Fault Management within Distributed Workflow Management Systems,2008,"As opposed to centralized workflow management systems, the distributed execution of workflows can not rely on a trusted centralized point of coordination. As a result, basic security features including compliance of the overall sequence of workflow operations with the pre-defined workflow execution plan or traceability become critical issues that are yet to be addressed. Besides, the detection of security inconsistencies during the execution of a workflow usually implies the complete failure of the workflow although it may be possible in some situations to recover from the latter. In this paper, we present security solutions supporting the secure execution of distributed workflows. These mechanisms capitalize on onion encryption techniques and security policy models to assure the integrity of the distributed execution of workflows, to prevent business partners from being involved in a workflow instance forged by a malicious peer and to provide business partners identity traceability for sensitive workflow instances. Moreover, we specify how these security mechanisms can be combined with a transactional coordination framework to recover from faults that may be caught during their execution. The defined solutions can easily be integrated into distributed workflow management systems as our design is strongly coupled with the runtime specification of decentralized workflows.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4663051,no
Distributed and scalable control plane for next generation routers: A case study of OSPF,2008,"The growing traffic on the core Internet entails new requirements related to scalability and resiliency of the routers. One of the promising trends of router evolution is to build next generation routers with enhanced memory capacity and computing resources, distributed across a very high speed switching fabric. The main limitation of the current routing and signaling software modules, traditionally designed in a centralized manner, is that they do not scale in order to fully exploit such an advanced distributed hardware architecture. This paper discusses an implementation for an OSPF architecture for next generation routers, aiming at increasing the scalability and resiliency. The proposed architecture distributes the OSPF processing functions on router cards, i.e., on both control and line cards. Therefore, it reduces the bottlenecks and improves both the overall performance and the resiliency in the presence of faults. Scalability is estimated with respect to the CPU utilization and memory requirements.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4664205,no
Accurate real-time monitoring of bottlenecks and performance of packet trace collection,2008,Collection of packet traces for future analysis is a very meticulous work that must guarantee accurate traces in order for these traces to be valuable for analysis. Current platforms do not provide a means to measure this accuracy. This paper describes a real-time monitoring method to measure the quality of a collected trace. The method takes a system architecture approach monitoring different points of the system to account for all potential drops of the packet journey. A set of metadata is stored in metatraces to be analyzed together with the trace after the capturing. The primary information is taken from standard Ethernet counters which are available in all commodity hardware and therefore performs very well without expensive specific hardware. The paper presents the evaluation of the real-time monitoring method concluding that the processing overhead does not produce significant performance degradation and that it improves packet loss detection up to orders of magnitude depending on different scenarios.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4664298,no
Perceptual feature based music classification - A DSP perspective for a new type of application,2008,"Today, more and more computational power is available not only in desktop computers but also in portable devices such as smart phones or PDAs. At the same time the availability of huge non-volatile storage capacities (flash memory etc.) suggests to maintain huge music databases even in mobile devices. Automated music classification promises to allow keeping a much better overview on huge data bases for the user. Such a classification enables the user to sort the available huge music archives according to different genres which can be either predefined or user defined. It is typically based on a set of perceptual features which are extracted from the music data. Feature extraction and subsequent music classification are very computational intensive tasks. Today, a variety of music features and possible classification algorithms optimized for various application scenarios and achieving different classification qualities are under discussion. In this paper results concerning the computational needs and the achievable classification rates on different processor architectures are presented. The inspected processors include a general purpose P IV dual core processor, heterogeneous digital signal processor architectures like a Nomadik STn8810 (featuring a smart audio accelerator, SAA) as well as an OMAP2420. In order to increase classification performance, different forms of feature selection strategies (heuristic selection, full search and Mann-Whitney-Test) are applied. Furthermore, the potential of a hardware-based acceleration for this class of application is inspected by performing a fine as well as a coarse grain instruction tree analysis. Instruction trees are identified, which could be attractively implemented as custom instructions speeding up this class of applications.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4664851,no
Region based image appeal metric for consumer photos,2008,"Image appeal may be defined as the interest that a photograph generates when viewed by human observers, incorporating subjective factors on top of the traditional objective quality measures. User studies were conducted in order to identify the right features to use in an image appeal measure; these studies also revealed that a photograph may be appealing even if only a region/area of the photograph is actually appealing. Extensive experimentation helped identify a good set of low level features, which were combined into a set of image appeal metrics. The appealing region detection and the image appeal metrics were validated with experiments conducted on 2000 ground truth images, each graded by three human observers. Two main image appeal metrics are presented: one that ranks the images based on image appeal with a high correlation with the human observations; and a second one which performs better at retrieving highly appealing images from an image collection.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4665165,no
Design of a Pareto-optimization environment and its application to motion estimation,2008,"The characteristics of modern video signal processing algorithms are significantly influenced by a multitude of different configuration parameters. Hence, the selection of an optimum parameter set becomes a difficult task, since often various quality metrics have to be regarded, leading to a so-called multi-objective optimization problem. Furthermore, the high computational effort typically restricts the maximum possible number of simulated parameter configurations. Therefore, a flexible environment for multi-objective optimization of configuration parameters has been elaborated which uses evolutionary algorithms to efficiently explore the design-space. This environment is applied here for a detailed analysis and Pareto-optimization of a complex state-of-the-art motion estimation algorithm being used for frame rate conversion.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4665190,no
Novelty Detection in Time Series Through Self-Organizing Networks: An Empirical Evaluation of Two Different Paradigms,2008,"This paper addresses the issue of novelty or anomaly detection in time series data. The problem may be interpreted as a spatio-temporal classification procedure where current time series observation is labeled as normal or novel/abnormal according to a decision rule. In this work, the construction of the decision rules is formulated by means of two different self-organizing neural network (SONN) paradigms: one builds decision thresholds from quantization errors and the other one from prediction errors. Simulations with synthetic and real-world data show the feasibility of the two approaches.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4665904,no
Pattern Recognition by DTW and Series Data Mining in 3D Stratum Modelling and 3D Visualization,2008,"Three-dimension (3D) modeling and visualization of stratum plays important role in seismic active fault detection, of course in geoinformation science. Well-logging data of strata is taken as time series. Similarity measure of subsequence search is proposed based on dynamic time warping (DTW). Realizing time series match in different length of time series. The frequent pattern mining experiment is carried on to survey data of strata by multivariable combination analysis. We supply the stratum geophysics attribute by the depth (time) records sequence in the non-drill hole survey data's places using these frequent patterns,combining structure frame and mathematics geology interpolation technology, establish 3D geology model of the target area,and develop the underground geologic body 3D visualization software depending on visual studio.net and OpenGL graph packages, realize 3D visualization system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4666605,no
Asynchronous Distributed Parallel Gene Expression Programming Based on Estimation of Distribution Algorithm,2008,"In order to reduce the computation time and improve the quality of solutions of Gene Expression Programming (GEP), a new asynchronous distributed parallel gene expression programming based on Estimation of Distribution Algorithm (EDA) is proposed. The idea of introducing EDA into GEP is to accelerate the convergence speed. Moreover, the improved GEP is implemented by an asynchronous distributed parallel method based on the island parallel model on a message passing interface (MPI) environment. Some experiments are done on distributed network connected by twenty computers. The best results of sequential and parallel algorithms are compared, speedup and performance influence of some important parallel control parameters to this parallel algorithm are discussed. The experimental results show that it may approach linear speedup and has better ability to find optimal solution and higher stability than sequential algorithm.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4666883,no
DRESREM 2: An Analysis System for Multi-document Software Review Using Reviewers' Eye Movements,2008,"To build high-reliability software in software development, software review is essential. Typically, software review requires documents from multiple phases such as requirements specification, design document and source code to reveal the inconsistencies among them and to ensure the traceability of deliverables. However, most previous studies on software review (reading) techniques focus on finding defects in a single document in their experiments. In this paper, we propose a multi-document review evaluation system, DRESREM2. This system records reviewers' eye movements and mouse/keyboard operations for analysis. We conducted eye gaze analysis of reviewers in design document review with multiple documents (including requirements specification, design document, etc.) to confirm the usefulness of the system. For the performance analysis, we recorded defect detection ratio, detection time per defect, and fixation ratio of eye movements on each document. As a result, reviewers who concentrated their eye movements on requirements specification found more defects in the design document. We believe this result is good evidence to encourage developers to read high-level documents when reviewing lowlevel documents.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4668106,no
A Comparative Evaluation of Using Genetic Programming for Predicting Fault Count Data,2008,"There have been a number of software reliability growth models (SRGMs) proposed in literature. Due to several reasons, such as violation of models' assumptions and complexity of models, the practitioners face difficulties in knowing which models to apply in practice. This paper presents a comparative evaluation of traditional models and use of genetic programming (GP) for modeling software reliability growth based on weekly fault count data of three different industrial projects. The motivation of using a GP approach is its ability to evolve a model based entirely on prior data without the need of making underlying assumptions. The results show the strengths of using GP for predicting fault count data.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4668139,no
Hardware PSO for sensor network applications,2008,"This paper addresses the problem of emission source localization in an environment monitored by a distributed wireless sensor network. Typical application scenarios of interest include emergency response and military surveillance. A nonlinear least squares method is employed to model the problem of estimation of the emission source location and the intensity at the source. A particle swam optimization (PSO) approach to solve this problem produces solution qualities that compete well with other best known traditional approaches. Moreover, the PSO solution achieves the best runtime performance compared to the other methods investigated. However, when it is targeted on to low capacity embedded processors PSO itself suffers from poor execution performance. To address this problem a direct, flexible and efficient hardware implementation of the PSO algorithm is developed, resulting in tremendous speedup over software solutions on embedded processors.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4668308,no
Automatic evaluation of flickering sensitivity of fluorescent lamps caused by interharmonic voltages,2008,"Recent studies have shown that fluorescent lamps are also prone to light flickering due to the increasing level of inter-harmonics in the power systems. This possibility and the levels of problematic inter-harmonics were confirmed through laboratory tests. These tests were manually conducted, and therefore were laborious and time consuming. It would be convenient to automate the testing by using the advanced features of data acquisition system to save time and manpower. This paper describes the development of an automated flicker measurement and testing system. It consists of a lighting booth, a programmable AC source, photo-sensing circuit, a data acquisition device and automated flicker measurement and testing software. It is developed in accordance to the IEC 61000-4-15 standard. Tests were later conducted on various types of compact fluorescent lamps, confirming their sensitivity to interharominc voltages.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4668775,no
Real time â€œsystem-in-the-loopâ€?simulation of tactical networks,2008,"Military mission success probability is closely related to careful planning of communication infrastructures for Command and Control Information Systems (C2IS). Over recent years, the designers of tactical networks have realized more and more need for using simulation tools in the process of designing networks with optimal performances, in regard to terrain conditions. One of the most demanding simulation problems is the modeling of protocols and devices; especially on the application layer, because the credibility of simulation results mainly depends on the quality of modeling. A new branch of communications network simulations has appeared for resolving these kinds of problems the - simulations with real communication devices in the simulation loop. Such simulations initiate real-time into the simulation process. The results of our research are aimed at simulation methodology. Using this system, military command personnel, can perform realistic training on the real C2IS equipment connected to the simulation tool, by modeled wireless links over a virtual terrain. In our research work, we used the OPNET Modeler simulation tool, with additional modules.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4669461,no
Usage of Weibull and other models for software faults prediction in AXE,2008,There are several families for software quality prediction techniques in development projects. All of them can be classified in several subfamilies. Each of these techniques has its own distinctive feature and it may not give correct prediction of quality for a scenario different from the one for which the technique was designed. All these techniques for software quality prediction are dispersed. One of them is statistical and probabilistic technique. The paper deals with software quality prediction techniques in development projects. Four different models based on statistical and probabilistic approach is presented and evaluated for prediction of software faults in very large development projects.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4669471,no
Predicting Fault Proneness of Classes Trough a Multiobjective Particle Swarm Optimization Algorithm,2008,"Software testing is a fundamental software engineering activity for quality assurance that is also traditionally very expensive. To reduce efforts of testing strategies, some design metrics have been used to predict the fault-proneness of a software class or module. Recent works have explored the use of machine learning (ML) techniques for fault prediction. However most used ML techniques can not deal with unbalanced data and their results usually have a difficult interpretation. Because of this, this paper introduces a multi-objective particle swarm optimization (MOPSO) algorithm for fault prediction. It allows the creation of classifiers composed by rules with specific properties by exploring Pareto dominance concepts. These rules are more intuitive and easier to understand because they can be interpreted independently one of each other. Furthermore, an experiment using the approach is presented and the results are compared to the other techniques explored in the area.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4669800,no
Automatically Determining Compatibility of Evolving Services,2008,"A major advantage of Service-Oriented Architectures (SOA) is composition and coordination of loosely coupled services. Because the development lifecycles of services and clients are decoupled, multiple service versions have to be maintained to continue supporting older clients. Typically versions are managed within the SOA by updating service descriptions using conventions on version numbers and namespaces. In all cases, the compatibility among services description must be evaluated, which can be hard, error-prone and costly if performed manually, particularly for complex descriptions. In this paper, we describe a method to automatically determine when two service descriptions are backward compatible. We then describe a case study to illustrate how we leveraged version compatibility information in a SOA environment and present initial performance overheads of doing so. By automatically exploring compatibility information, a) service developers can assess the impact of proposed changes; b) proper versioning requirements can be put in client implementations guaranteeing that incompatibilities will not occur during run-time; and c) messages exchanged in the SOA can be validated to ensure that only expected messages or compatible ones are exchanged.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4670172,no
Invited Talk: The Role of Empiricism in Improving the Reliability of Future Software,2008,"This talk first bemoan the general absence of empiricism in the evolution of software system building and then go on to show the results of some experiments in attempting to understand how defects appear in software, what factors affect their appearance and their relationship to testing generally. It challenge a few cherished beliefs on the way and demonstrate in no particular order at least the following: 1) the equilibrium state of a software system appears to conserve defect; 2) there is strong evidence in quasi-equilibrated systems for xlogx growth in defects where x is a measure of the lines of code; 3) component sizes in OO and non-OO software systems appear to be scale-free, (this is intimately related to the first two bullet points); 4) software measurements, (also known rather inaccurately as metrics) are effectively useless in determining the defect behaviour of a software system; 5) most such measurements, (including the ubiquitous cyclomatic complexity) are almost as highly correlated with lines of code as the relationship between temperature in degrees Fahrenheit and degrees Centigrade measured with a slightly noisy thermometer. In other words, lines of code are just about as good as anything else when estimating defects; 6) 'gotos considered irrelevant'. The goto statement has no obvious relationship with defects even when studied over very long periods. It probably never did; 7) checklists in code inspections appear to make no significant difference to the efficiency of the inspection; and 8) when you find a defect, there is an increasing probability of finding another in the same component. This strategy is effective up to a surprisingly large number of defects in youthful systems but not at all in elderly systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4670293,no
Exploring the Relationship of a File's History and Its Fault-Proneness: An Empirical Study,2008,"Knowing which particular characteristics of software are indicators for defects is very valuable for testers in order to allocate testing resources appropriately. In this paper, we present the results of an empirical study exploring the relationship between history characteristics of files and their defect count. We analyzed nine open source Java projects across different versions in order to answer the following questions: 1)Do past defects correlate with a filepsilas current defect count? 2) Do late changes correlate with a filepsilas defect count? 3) Is the file's age a good indicator for its defect count? The results are partly surprising. Only 4 of 9 programs show moderate correlation between a file's defects in previous and in current releases in more than the half of analysed releases. In contrast to our expectations, the oldest files represent the most fault-prone files. Additionally, late changes influence filepsilas defect count only partly.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4670296,no
A Bayesian approach for software quality prediction,2008,"Many statistical algorithms have been proposed for software quality prediction of fault-prone and non fault-prone program modules. The main goal of these algorithms is the improvement of software development processes. In this paper, we introduce a new software prediction algorithm. Our approach is purely Bayesian and is based on finite Dirichlet mixture models. The implementation of the Bayesian approach is done through the use of the Gibbs sampler. Experimental results are presented using simulated data, and a real application for software modules classification is also included.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4670508,no
An intelligent FMEA system implemented with a hierarchy of back-propagation neural networks,2008,"This paper has used a series of back-progation neural networks (BPNs) to form a hierarchical framework adequate for the implementation of an intelligent FMEA (failure modes and effects analysis) system. Its aim is to apply this novel system as a tool to assist the reliability design required for preventing failures occurred in the operating periods of a system The hierarchical structure upgrades the classical statistic off-line FMEA performance. From the simulated experiments of the proposed BPN-based FMEA system (N-FMEA), it has found that the accuracy of the failure modes classification and the reliability calculation are knowledgeable and potential for performing pragmatic preventive maintenance activities. As a result, this paper conducts an effective FMEA process and contributes to help FMEA working teams to reduce their working loading, shorten design time and ensure system operating success.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4670758,no
Research on rotary dump health monitoring expert system based on causality diagram theory,2008,"Causality diagram theory is a kind of uncertainty reasoning theory based on the belief network. It expresses the knowledge and causality relationship by diagrammatic form and direct causality intensity. Furthermore, it resolves the shortages of the belief network, and realizes a hybrid model which can process discrete and continuous variations. The theory of causality diagram model and the steps of causality diagram reasoning methodology are studied in this paper, and a model of rotary dump health monitoring expert system is proposed. In addition, this paper establishes the causality diagram of rotary dump and converts it to the causality tree. According to the causality tree of rotary dump, the causality diagram reasoning methodology composed of four steps is described. Finally, an application of rotary dump health monitoring expert system is shown, and the system performance analysis is discussed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4670773,no
Design of a DSP-based CMOS imaging system for embedded computer vision,2008,"With the advance of image processing techniques and the fast reduction of image sensor costs, embedded computer vision is becoming a reality. This paper presents the design and implementation of an integrated CMOS imaging system based on DM642. The system can acquire VGA resolution image at 100 frame/s. The on-board 10 Mbps Ethernet connection and the UART as well as digital I/O ports provide convenience for direct interface to other intelligent devices. The on-board software provides image acquisition function, network-based control, network-based data transfer, JPEG compression, image pre-processing functions, edge detection function. Automatic exposure control and automatic white balance are realized in the system. Experiments show that the CMOS imaging system has a good performance in terms of imaging quality, with much potential for being used as intelligent vision system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4670862,no
Testing content addressable memories using instructions and march-like algorithms,2008,"CAM is widely used in microprocessors and SOC TLB modules. It gives great advantage for software development. And TLB operations become bottleneck of the microprocessor performance. The test cost of normal BIST approach of the CAM can not be ignored. The paper analyses the fault models of CAM and proposes an instruction suitable march-like algorithm. The algorithm requires 14N+2L operations, where N is the number of words of the CAM and L is the width of a word. The algorithm covers 100% targeted faults. Instruction-level test using the algorithm has not any test cost on area and performance. Moreover the algorithm can be used in BIST approaches and have less performance lost for microprocessors. The paper instances the algorithm in a MIPS compatible microprocessor and have good results.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4674968,no
Predicting the SEU error rate through fault injection for a complex microprocessor,2008,"This paper deals with the prediction of SEU error rate for an application running on a complex processor. Both, radiation ground testing and fault injection, were performed while the selected processor, a Power PC 7448, executed a software issued from a real space application. The predicted error rate shows that generally used strategies, based on static cross-section, significantly overestimate the application error rate.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677290,no
Event Driven RFID Based Exhaust Gas Detection Services Oriented System Research,2008,"The vehicle exhaust gas emission is directly related with the quality of air. This paper describes the research and development of an event driven RFID based exhaust gas detection system applying the service oriented architecture for the purpose of environmental protection. An edge engine supporting the EPCglobal ALE specification, a complex event processing engine dealing with the complex streaming RFID events and service bus, which are involved in the architecture of the system, are discussed both separately and in a whole picture. The edge engine is composed of four modules, which are kernel module, device management module, events filter module, and configuration module. It provides features to encapsulate the applications from device interfaces; to process the raw observations captured by the readers and sensors; and to provide an application-level interface for managing readers and querying RFID observations. Event processing engine consists of a network of event processing agents running in parallel that interact using a dedicated event processing infrastructure, which provides an abstract communication mechanism and allows dynamic reconfiguration of the communication topology between agents at run-time. Web Services and asynchronous APIs are pivotal system implementation for flexible components reuse, dynamic configuration and optimized performance. Service bus, through its service integration and management capabilities, is the backbone of the system. Lastly, the system is proven to be effective with high performance in the benchmark.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4678606,no
"Reliable system design: Models, metrics and design techniques",2008,"Design of reliable systems meeting stringent quality, reliability, and availability requirements is becoming increasingly difficult in advanced technologies. The current design paradigm, which assumes that no gate or interconnect will ever operate incorrectly within the lifetime of a product, must change to cope with this situation. Future systems must be designed with built-in mechanisms for failure tolerance, prediction, detection and recovery during normal system operation. This tutorial will focus on models and metrics for designing reliable systems, algorithms and tools for modeling and evaluating such systems, will discuss a broad spectrum of techniques for building such systems with support for concurrent error detection, failure prediction, error correction, recovery, and self-repair. Complex interplay between power, performance and reliability requirements in future systems, and associated constraints will also be discussed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4681534,no
Graphene nanoribbon FETs: Technology exploration and CAD,2008,"Graphene nanoribbon FETs (GNRFETs) have emerged as a promising candidate for nanoelectronics applications. This paper summarizes (i) current understanding and prospects for GNRFETs as ultimately scaled, ideal ballistic transistors, (ii) physics-based modeling of GNRFETs to support circuit design and CAD, and (iii) variability and defects in GNRs and their impact on GNRFET circuit performance and reliability.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4681607,no
Heterogeneous Architecture-Based Software Reliability Estimation: Case Study,2008,"With growing system complication, multiple structures are constructed in software, instead of single architecture style. As a result, the traditional architecture-based models that have considered only homogeneous software behaviors are not suitable to the complex system. In this paper a heterogeneous architecture-based model is presented. In order to verify the practicability, we apply the model to estimate the system reliability of the astronautics payload communication and monitoring system (APCMS). Multiple structures are constructed in the system, i.e., sequence, parallel and fault-tolerance. The study results demonstrate that the model is accurate when compared to the actual reliability. Theoretical research must be applied on actual systems to understand their applicability and accuracy.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4682255,no
Human-Intention Driven Self Adaptive Software Evolvability in Distributed Service Environments,2008,"Evolvability is essential to adapting to the dynamic and changing requirements in response to the feedback from context awareness systems. However, most of current context models have limited capability in exploring human intentions that often drive system evolution. To support service requirements analysis of real-world applications in distributed service environments, this paper focuses on human-intention driven software evolvability. In our approach, requirements analysis via an evolution cycle provides the means of speculating requirement changes, predicting possible new generations of system behaviors, and assessing the corresponding quality impacts. Furthermore, we also discuss evolvability metrics by observing intentions from user contexts.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4683114,no
Assessing the Quality of Software Requirements Specifications,2008,"Software requirements specifications (SRS) are hard to compare due to the uniqueness of the projects they were created in. In practice this means that it is not possible to objectively determine if a projects SRS fails to reach a certain quality threshold. Therefore, a commonly agreed-on quality model is needed. Additionally, a large set of empirical data is needed to establish a correlation between project success and quality levels. As there is no such quality model, we had to define our own based on the goal-question-metric (GQM) method. Based on this we analyzed more than 40 software projects (student projects in undergraduate software engineering classes), in order to contribute to the empirical part. This paper contributes in three areas: Firstly, we outline our GQM plan and our set of metrics. They were derived from widespread literature, and hence could lead to a discussion of how to measure requirements quality. Practitioners and researchers can profit from our experience, when measuring the quality of their requirements. Secondly, we present our findings. We hope that others find these valuable when comparing them to their own results. Finally,we show that the results of our quality assessment correlate to project success. Thus, we give an empirical indication for the correlation of requirements engineering and project success.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4685703,no
Towards supporting evolution of service-oriented architectures through quality impact prediction,2008,"The difficulty in evolving service-oriented architectures with extra-functional requirements seriously hinders the spread of this paradigm in critical application domains. This work tries to offset this disadvantage by introducing a design-time quality impact prediction and trade-off analysis method, which allows software engineers to predict the extra-functional consequences of alternative design decisions and select the optimal architecture without costly prototyping.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4686297,no
Architecting for evolvability by means of traceability and features,2008,"The frequent changes during the development and usage of large software systems often lead to a loss of architectural quality which hampers the implementation of further changes and thus the systemspsila evolution. To maintain the evolvability of such software systems, their architecture has to fulfil particular quality criteria. Available metrics and rigour approaches do not provide sufficient means to evaluate architectures regarding these criteria, and reviews require a high effort. This paper presents an approach for an evaluation of architectural models during design decisions, for early feedback and as part of architectural assessments. As the quality criteria for evolvability, model relations in terms of traceability links between feature model, design and implementation are evaluated. Indicators are introduced to assess these model relations, similar to metrics, but accompanied by problem resolution actions. The indicators are defined formally to enable a tool-based evaluation. The approach has been developed within a large software project for an IT infrastructure.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4686323,no
Aspect Oriented Modeling of Component Architectures Using AADL,2008,"Dependable embedded software system design is fastidious because designers have to understand and handle multiple, interdependent, pervasive dependability concerns such as fault tolerance, timeliness, performance, security. Because these concerns tend to crosscut application architecture, understanding and changing their descriptions can be difficult. Separating theses concerns at architectural level allow the designers to locate them, to understand them and thus to preserve the required properties when making the change in order to keep the architecture consistent. That separation of concerns leads to better understanding, reuse, analysis and evolution of these concerns during design. The Architecture Analysis and Design Language (AADL) is a standard architecture description language in use by a number of organizations around the world to design, analyze embedded software architectures and generate application code. In this paper we explain how aspect oriented modeling (AOM) techniques and AADL can be used to model dependability aspects of component architecture separately from other aspects. The AOM architectural model used to illustrate the approach in this paper consists of a component primary view describing the base architecture and a component template aspect model describing a fault tolerance concern that provides error detection and recovery services.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4689080,no
A General QoS Error Detection and Diagnosis Framework for Accountable SOA,2008,"Accountability is a composite measure for different but related quality aspects. To be able to ensure accountability in practice, it is required to define specific quality attributes of accountability, and metrics for each quality attribute. In this paper, we propose a quality detection and diagnosis framework for the service accountability. We first identify types of quality attributes which are essential to manage QoS in accountability framework. We then present a detection and diagnosis model for problematic situations in services system. In this model, we design situation link representing dependencies among quality attributes, and provide information to detect and diagnose problems and their root causes. Based on the model, we propose an integrated model-based and case-based diagnosis method using the situation link.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4690621,no
A Novel Protocol in Intermittent Connected Mobile Networks,2008,"Intermittent connected mobile sensor networks (ICMSN) have emerged in recent studies, e.g. underwater sensor networks, wildlife tracking and human-oriented flu virus monitoring. In these networks, there are no end to end connections from sensor nodes to sink nodes due to low node density and node mobility. Therefore, conventional routing protocols in WSN are not practical for ICMSN since packets will be dropped when no complete route to the sink is available. In this paper, we propose an efficient routing protocol for ICMSN, which is composed of buffer management and message forwarding. Buffer management consists of two related components: queuing mechanism and purging mechanism. The former determines priority of messages to transmit or discard based on the importance factor, which indicates the QoS requirement of the messages. The latter utilizes a death vector generated by the sink to purge useless messages that have been delivered from the network. Message forwarding makes decision on which sensors are qualified for next hop based on node delivery probability, which synthesizes the meeting predictability to the sink, current buffer state and residual energy and increases the likelihood that the sensor can deliver the message to the sink. Our experimental results show that the proposed routing protocol not only supports the QoS requirement of different content but also achieves the good performance tradeoff between delivery ratio, delay and resource consumption.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4690741,no
A Redundancy Mechanism under Single Chip Multiprocessor Architecture,2008,"Chip multiprocessor (CMP) will become more popular in embedded systems. As clock frequency increases, dependency among core blocks rises, which therefore tends to reduce performance of system and be more unstable. Redundancy is a way to improve the dependability of the system. But the traditional methods cannot make full use of the computing capacity of CMP. In this paper, we propose a new redundancy mechanism. The whole system is reorganized using lightweight virtualization. A group of processors and a block of memory are treated as a single logical computing unit each of which corresponds to a processer separately. In this way, only a distributor and an arbiter need to be added into operating system with least effort. The distributor dispatches one task to all the logical computing units. After execution, the arbiter gathers all results to analyze whether they are dependable. Because the distributor and the arbiter are very simple, total overhead of the system are very low. Experiment results show the proposed method is practical with average overhead less than 8%.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4690757,no
Gumshoe: Diagnosing Performance Problems in Replicated File-Systems,2008,"Replicated file-systems can experience degraded performance that might not be adequately handled by the underlying fault-tolerant protocols. We describe the design and implementation of Gumshoe, a system that aims to diagnose performance problems in replicated file-systems. Gumshoe periodically gathers OS and protocol metrics and then analyzes these metrics to automatically localize the performance problem to the culprit node(s). We describe our results and experiences with problem diagnosis in two replicated file-systems (replicated-CoreFS and BFS) using two file-system benchmarks (Postmark and IOzone).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4690808,no
The model calibration protocol for parameter estimation of activated sludge model,2008,"In this paper, a standardized model calibration method for the optimal parameter estimation of the ASM is proposed. We developed a kind of the calibration protocol of ASM 1 model based on parameter selection, design of experiments and parameter optimization using multiple response surface methodology. In this research, two softwares of WEST<sup>reg</sup> and MINITAP are used to model a waste water treatment process and optimize the model parameter and design of experiment. First, the most sensitive parameter set is determined by a new sensitivity analysis for considering the effluent quality index. Second, a multiple response surface methodology (MRS) is conducted for optimizing parameter estimation of ASM1 model. Because the proposed method is a multi-response model which is the suitable methods to estimate the model parameters in the ASM, it can simultaneously optimize the key parameters in the aspect of input-output model performance. The result of the model calibration protocol shows that it can select the key parameters of the ASM model and minimize the model error of the ASM model by a systematic sequence, which can save the much time in the modeling a process, improve the modeling performance of the ASM model and the prediction result of ASM. Since the proposed method is a kind of the calibration methodology, that is, protocol, it can be easily applied to a other ASM models, i.e., ASM2, 2d, 3 and also a full-scale plant.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4694298,no
A study on position detection of Rolling Stock,2008,"We developed a measurement system for on-line test and performance evaluation of rolling stock. The measurement system is composed of software part and hardware part. Perfect interface between multi-users is possible. Nowadays, position data inputs to pulse signal from wheel. Perfect position measurement was limited to slip and slide of vehicle. This measurement makes up for the weak points, position detection system using GPS develops. By using the system, rolling stock is capable of accurate fault position detection.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4694469,no
Development of prototype Reference Station and Integrity Monitors (RSIM) for maritime application,2008,IMO (International Maritime Organization) and IALA (International Association of Marine Aids to Navigation and Lighthouse Authorities) recommend establish the maritime DGPS (Differential GPS) for ship safety located in coast and narrow channel using GNSS (Global Navigation Satellite System). This paper analysis RSIM (Reference Stations and Integrity Monitors) prototype fault detection system and verify RSIM prototype performance.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4694598,no
Design of intelligent testing device for airplane navigation radar,2008,"Modern airborne radar systems are very complex electronic equipment systems, high reliability is demanded, and fine functions of automatic detect is needed for guarantee. In this dissertation, we have studied the detect method of a new kind of airborne radar systems. With Embed machinery control as its core, adoption unit wooden blocks type construction, examining the faults of radar one by one by exciting the fault models input and checking out the response, to get the faults localized. Being programmed by Visual Basic6.0, the software can be enlarged and advanced and it provides the users with an intelligent and automatic testing environment and amicable interface. Under the guidance of testing interface, testers can complete the fault localization of radar circuit automatically. By testing, this intelligent and synthesis detect system holds well-found function advanced techniques and predominant capabilities. It can proceed the all-directions performance test to airplane navigation radar. So it has important significance for ensuring flight safety and increasing combat effectiveness.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4694806,no
Evaluation of an efficient control-oriented coverage metric,2008,"Dynamic verification, the use of simulation to determine design correctness, is widely used due to its tractability for large designs. A serious limitation of dynamic techniques is the difficulty in determining whether or not a test sequence is sufficient to detect all likely design errors. Coverage metrics are used to address this problem by providing a set of goals to be achieved during the simulation process; if all coverage goals are satisfied then the test sequence is assumed to be complete. Many coverage metrics have been proposed but no effort has been made to identify a correlation between existing metrics and design quality. In this paper we present a technique to evaluate a coverage metric by examining its ability to ensure the detection of real design errors. We apply our evaluation technique to our control-oriented coverage metric to verify its ability to reveal design errors.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4695895,no
Bottom up approach to enhance top level SoC verification,2008,SoCs today rely heavily on behavioral models of analog circuits for Top Level Verification. The minimum modeling requirement is to model the functional behavior of the circuit. A lot of ongoing work is also focused on modeling analog circuits to predict the system performance of the SoC. This paper presents a methodology to enhance the quality of SoC verification by using a bottom up approach to verify the equivalence of building blocks and then work at higher levels to increase coverage. It is shown that this methodology can be used to verify functional and performance equivalence of behavioral models.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4695928,no
Analyzing Performance of Web-Based Metrics for Evaluating Reliability and Maintainability of Hypermedia Applications,2008,"This paper has been designed to identify the Web metrics for evaluating the reliability and maintainability of hypermedia applications. In the age of information and communication technology (ICT), Web and the Internet, have brought significant changes in information technology (IT) and their related scenarios. Therefore in this paper an attempt has been made to trace out the Web-based measurements towards the creation of efficient Web centric applications. The dramatic increase in Web site development and their relative usage has led to the need of Web-based metrics. These metrics will accurately assess the efforts in the Web-based applications. Here we promote the simple, but elegant approaches to estimate the efforts needed for designing Web-based applications with the help of user behavior model graph (UBMG), Web page replacement algorithms, and RS Web Application Effort Assessment (RSWAEA) method. Effort assessment of hyperdocuments is crucial for Web-based systems, where outages can result in loss of revenue and dissatisfied customers. Here we advocate a simple, but elegant approach for effort estimation for Web applications from an empirical point of view. The proposed methods and models have been designed after carrying out an empirical study with the students of an advanced university class and Web designers that used various client-server based Web technologies. Our first aim was to compare the relative importance of each Web-based metric and method. Second, we also implemented the quality of the designs obtained based by constructing the User Behavior Model Graphs (UBMGs) to capture the reliability of Web-based applications. Thirdly, we use Web page replacement algorithms for increasing the Web site usability index, maintainability, reliability, and ranking. The results obtained from the above Web-based metrics can help us to analytically identify the effort assessment and failure points in Web-based systems and makes the evaluation of reliability of thes- - e systems simple.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4696136,no
Active Throughput Estimation Using RTT of Differing ICMP Packet Sizes,2008,"Network measurements always play a vital role for good network management purposes and are essential for quality of service (QoS) requirements. Packet delay derivations are critical measurement parameters in diagnosing simple network performance measures. The traditional ping tool is popular for target host connectivity testing and average delay measurements. However, ping is a poor indicator of round trip time (RTT) measurements at the network layer. In this paper, an active round trip time (RTT) packet delay measurements using Internet control message protocol (ICMP) data packets of varying sizes is proposed. It was observed that in the proposed RTT measurement technique, peak throughput values were obtained compared to the ping measurements. The proposed approach eliminates the need for special purpose cooperation and/or software applications at the destination in the measured network path when using the ICMP Echo Request/ Reply technique.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4696152,no
Rough Set Granularity in Mobile Web Pre-caching,2008,"Mobile Web pre-caching (Web prefetching and caching) is an explication of performance enhancement and storage limitation of mobile devices. In this paper, we present the granularity of rough sets (RS) and RS based inductive learning in reducing the size of rules to be stored in mobile devices. The conditional attributes such as 'timestamp', 'size document' and 'object retrieval time' are presented and the provided decision is granted. Decision rules are obtained using RS based inductive learning to grant the ultimate judgment either to cache or not to cache the conditional attributes and objects. However, in mobile clients, these rules need to be specified so as the specific sessions can be kept in their mobile storage as well as the proxy caching administrations. Consequently, these rules will notify the objects in Web application request and priority level to the clients accordingly. The results represent that the granularity of RS in mobile Web pre-caching is vital to improve the latest Web caching technology by providing virtual client and administrator feedback; hence making Web pre-caching technologies practical, efficient and powerful.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4696272,no
Efficient implementation of biomedical hardware using open source descriptions and behavioral synthesis,2008,"Medical diagnostics are changing rapidly, aided by a new generation of portable equipment and handheld devices that can be carried to the patientpsilas bedside. Processing solutions for such equipment must offer high performance, low power consumption and also, minimize board space and component counts. Such a multi-objective optimization can be performed with behavioral hardware synthesis, offering design quality with significantly reduced design time. In this paper, open source code of a QRS detection algorithm is implemented in hardware using an advanced behavioral synthesis framework. Experimental results show that with this approach performance improvements are introduced with a fraction of design time, reducing dramatically time-to-market for modern diagnostic devices.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4696771,no
Improving the performance of speech recognition systems using fault-tolerant techniques,2008,"In this paper, using of fault tolerant techniques are studied and experimented in speech recognition systems to make these systems robust to noise. Recognizer redundancy is implemented to utilize the strengths of several recognition methods that each one has acceptable performance in a specific condition. Duplication-with-comparison and NMR methods are experimented with majority and plurality voting on a telephony Persian speech-enabled IVR system. Results of evaluations present two promising outcomes, first, it improves the performance considerably; second, it enables us to detect the outputs with low confidence.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4697199,no
A Theoretical Model of the Effects of Losses and Delays on the Performance of SIP,2008,The session initiation protocol (SIP) is widely used for VoIP communication. Losses caused by network or server overload would cause retransmissions and delays in the session establishment and would hence reduce the perceived service quality of the users. In order to be able to take counter measures network and service planers require detailed models that would allow them to predict such effects in advance. This paper presents a theoretical model of SIP that can be used for determining various parameters such as the delay and the number of messages required for establishing a SIP session when taking losses and delays into account. The model is restricted to the case when SIP is transported over UDP. The theoretical results are then verified using measurements.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4698067,no
"The IV<sup>2</sup> Multimodal Biometric Database (Including Iris, 2D, 3D, Stereoscopic, and Talking Face Data), and the IV<sup>2</sup>-2007 Evaluation Campaign",2008,"Face recognition finds its place in a large number of applications. They occur in different contexts related to security, entertainment or Internet applications. Reliable face recognition is still a great challenge to computer vision and pattern recognition researchers, and new algorithms need to be evaluated on relevant databases. The publicly available IV<sup>2</sup> database allows monomodal and multimodal experiments using face data. Known variabilities, that are critical for the performance of the biometric systems (such as pose, expression, illumination and quality) are present. The face and subface data that are acquired in this database are: 2D audio-video talking-face sequences, 2D stereoscopic data acquired with two pairs of synchronized cameras, 3D facial data acquired with a laser scanner, and iris images acquired with a portable infrared camera. The IV<sup>2</sup> database is designed for monomodal and multimodal experiments. The quality of the acquired data is of great importance. Therefore as a first step, and in order to better evaluate the quality of the data, a first internal evaluation was conducted. Only a small amount of the total acquired data was used for this evaluation: 2D still images, 3D scans and iris images. First results show the interest of this database. In parallel to the research algorithms, open-source reference systems were also run for baseline comparisons.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4699323,no
Visibility Classification of Rocks in Piles,2008,"Size measurement of rocks is usually performed by manual sampling and sieving techniques. Automatic on-line analysis of rock size based on image analysis techniques would allow non-invasive, frequent and consistent measurement. In practical measurement systems based on image analysis techniques, the surface of rock piles will be sampled and therefore contain overlapping rock fragments. It is critical to identify partially visible rock fragments for accurate size measurements. In this research, statistical classification methods are used to discriminate rocks on the surface of a pile between entirely visible and partially visible rocks. The feature visibility ratio is combined with commonly used 2D shape features to evaluate whether 2D shape features can improve classification accuracies to minimize overlapped particle error.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700022,no
Software Reliability - 40 Years of Avoiding the Question,2008,"Summary form only given. During the past years, there has been an explosion of technical complexity of both hardware and software. Hardware, characterized by physics and empirical data, provides the reliability and systems engineers with a capability to estimate the expected reliability. Software has however managed to avoid this type of model development in part because the factors affecting reliability are not measurable by physical data. Software reliability is characterized by data gathered during systems integration and test. This data has attributes and parameters such as defect density, capability of programming of the software engineering team, the experience of the engineering team, the understanding of the application be the designers, the language used and more. Software reliability is more than the processes advocated by CMMI (Capability Maturity Modelreg Integration) and is susceptible to esoteric and infinitely harder parameters to measure. The author discusses some of the elements that affect software reliability and compares some of the differences when trying to estimate reliability of today's systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700302,no
Predicting Defect Content and Quality Assurance Effectiveness by Combining Expert Judgment and Defect Data - A Case Study,2008,"Planning quality assurance (QA) activities in a systematic way and controlling their execution are challenging tasks for companies that develop software or software-intensive systems. Both require estimation capabilities regarding the effectiveness of the applied QA techniques and the defect content of the checked artifacts. Existing approaches for these purposes need extensive measurement data from his-torical projects. Due to the fact that many companies do not collect enough data for applying these approaches (es-pecially for the early project lifecycle), they typically base their QA planning and controlling solely on expert opinion. This article presents a hybrid method that combines commonly available measurement data and context-specific expert knowledge. To evaluate the methodpsilas applicability and usefulness, we conducted a case study in the context of independent verification and validation activities for critical software in the space domain. A hybrid defect content and effectiveness model was developed for the software requirements analysis phase and evaluated with available legacy data. One major result is that the hybrid model provides improved estimation accuracy when compared to applicable models based solely on data. The mean magni-tude of relative error (MMRE) determined by cross-validation is 29.6% compared to 76.5% obtained by the most accurate data-based model.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700306,no
Trace Normalization,2008,"Identifying truly distinct traces is crucial for the performance of many dynamic analysis activities. For example, given a set of traces associated with a program failure, identifying a subset of unique traces can reduce the debugging effort by producing a smaller set of candidate fault locations. The process of identifying unique traces, however, is subject to the presence of irrelevant variations in the sequence of trace events, which can make a trace appear unique when it is not. In this paper we present an approach to reduce inconsequential and potentially detrimental trace variations. The approach decomposes traces into segments on which irrelevant variations caused by event ordering or repetition can be identified, and then used to normalize the traces in the pool. The approach is investigated on two well-known client dynamic analyses by replicating the conditions under which they were originally assessed, revealing that the clients can deliver more precise results with the normalized traces.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700311,no
Cost Curve Evaluation of Fault Prediction Models,2008,"Prediction of fault prone software components is one of the most researched problems in software engineering. Many statistical techniques have been proposed but there is no consensus on the methodology to select the ""best model"" for the specific project. In this paper, we introduce and discuss the merits of cost curve analysis of fault prediction models. Cost curves allow software quality engineers to introduce project-specific cost of module misclassification into model evaluation. Classifying a software module as fault-prone implies the application of some verification activities, thus adding to the development cost. Misclassifying a module as fault free carries the risk of system failure, also associated with cost implications. Through the analysis of sixteen projects from public repositories, we observe that software quality does not necessarily benefit from the prediction of fault prone components. The inclusion of misclassification cost in model evaluation may indicate that even the ""best"" models achieve performance no better than trivial classification. Our results support a recommendation to adopt cost curves as one of the standard methods for software quality model performance evaluation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700324,no
Using Statistical Models to Predict Software Regressions,2008,"Incorrect changes made to the stable parts of a software system can cause failures - software regressions. Early detection of faulty code changes can be beneficial for the quality of a software system when these errors can be fixed before the system is released. In this paper, a statistical model for predicting software regressions is proposed. The model predicts risk of regression for a code change by using software metrics: type and size of the change, number of affected components, dependency metrics, developerpsilas experience and code metrics of the affected components. Prediction results could be used to prioritize testing of changes: the higher is the risk of regression for the change, the more thorough testing it should receive.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700331,no
An Effective Software Reliability Analysis Framework for Weapon System Development in Defense Domain,2008,"Software reliability has been regarded as one of the most important quality attributes for software intensive systems, especially in defense domain. As most of weapon systems complicated functionalities and controls are implemented by software which is embedded in hardware systems, it became more critical to assure high reliability for software itself. However, many software development organizations in Korea defense domain have had problems in performing reliability engineered processes for developing mission-critical and/or safety-critical weapon systems. In this paper, we propose an effective framework with which software organizations can identify and select metrics associated software reliability, analyze the collected data, appraise software reliability, and develop software reliability prediction/estimation model based on the result of data analyses.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700335,no
The Effect of the Number of Defects on Estimates Produced by Capture-Recapture Models,2008,Project managers use inspection data as input to capture-recapture (CR) models to estimate the total number of faults present in a software artifact. The CR models use the number of faults found during an inspection and the overlap of faults among inspectors to calculate the estimate. A common belief is that CR models underestimate the number of faults but their performance can be improved with more input data. This paper investigates the minimum number of faults that has to be present in an artifact before the CR method can be used. The result shows that the minimum number of faults varies from ten faults to twenty-three faults for different CR estimators.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700348,no
Software Reliability Modeling with Logistic Test Coverage Function,2008,"Test coverage is a good indicator for testing completeness and effectiveness. This paper utilizes the logistic function to describe the test coverage growth behavior. Based on the logistic test coverage function, a model that relates test coverage to fault detection is presented and fitted to one actual data set. The experimental results show that, compared with three existing models, the evaluation performance of this new model is the best at least with the experimental data. Finally, the logistic test coverage function is applied to the NHPP software reliability modeling for further research.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700355,no
On Reliability Analysis of Open Source Software - FEDORA,2008,"Reliability analyses of software systems often focus only on the number of faults reported against the software. Using a broader set of metrics, such as problem resolution times and field software usage levels, can provide a more comprehensive view of the product. Some of these metrics are more readily available for open source products. We analyzed a suite of FEDORA releases and obtained some interesting findings. For example, we show that traditional reliability models may be used to predict problem rates across releases. We also show that security related reports tend to have a different profile than non-security related problem reporting and repair.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700358,no
Requirement Model-Based Mutation Testing for Web Service,2008,"Web services present a new promising software technology. However, some new issues and challenges in testing of them come out due to their characteristics of distribution, source code invisibility etc. This paper discusses the traditional mutation testing and then a new methodology of OWL-S requirement model-based web service mutation testing is brought forward. The traits of this methodology are as follows. Firstly, requirements are used effectively to reduce the magnitude of mutants. Secondly, mutants are generated by AOP technology conveniently and promptly. Thirdly, to reducing testing cost, using business logic implied in OWL-S requirement model as assistant of the process of killing the mutants. Fourthly, two sufficient measurement criteria are employed to evaluate the testing process. Finally, our empirical results have shown the usefulness of this testing method.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700384,no
A Practical Monitoring Framework for ESB-Based Services,2008,"Services in service-oriented computing (SOC) are often black-box since they are typically developed by 3rd party developers and deployed only with their interface specifications. Therefore, internal details of services and implementation details of service components are not readily available. Also, services in SOC are highly evolvable since new services can be registered into repositories, and existing services maybe modified for their logic and interfaces, or they may suddenly disappear. As the first and most essential step for service management, service monitoring is to acquire useful data and information about the services and to assess various quality of service (QoS). Being able to monitor services is a strong prerequisite to effective service management. In this paper, we present a service monitoring framework for ESB-based services, as a sub-system of our Open Service Management Framework (OSMaF). We firstly define the criteria for designing QoS monitoring framework, and present the architecture and key components of the framework. Then, we illustrate the key techniques used to efficiently monitor services and compute QoS metrics. Finally, an implementation of the framework is presented to show its applicability.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700499,no
Using static analysis to improve communications infrastructure,2008,"Static analysis is a promising technique for improving the safety and reliability of software used in avionics infrastructure. Source code analyzers are effective at locating a significant class of defects that are not detected by compilers during standard builds and often go undetected during runtime testing as well. Related to bug finders are a number of other static code improvement tasks, including automated unit test generation, programmer and software metrics tracking, and coding standards enforcement. However, adoption of these tools for everyday avionics software developer has been low. This paper will discuss the major barriers to adoption of these important tools and provide advice regarding how they can be effectively promulgated across the enterprise. Case studies of popular open source applications will be provided for illustration.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4702765,no
HyperMIP: Hypervisor Controlled Mobile IP for Virtual Machine Live Migration across Networks,2008,"Live migration provides transparent load-balancing and fault-tolerant mechanism for applications. When a Virtual Machine migrates among hosts residing in two networks, the network attachment point of the Virtual Machine is also changed, thus the Virtual Machine will suffer from IP mobility problem after migration. This paper proposes an approach called Hypervisor controlled Mobile IP to support live migration of Virtual Machine across networks, which enables virtual machine live migration over distributed computing resources. Since Hypervisor is capable of predicting exact time and destination host of Virtual Machine migration, our approach not only can improve migration performance but also reduce the network restoration latency. Some comprehensive experiments have been conducted and the results show that the HyperMIP brings negligible overhead to network performance of Virtual Machines. The network restoration time of HyperMIP supported migration is about only 3 second. HyperMIP is a promising essential component to provide reliability and fault tolerant for network application running in Virtual Machine.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4708866,no
Formal Support for Quantitative Analysis of Residual Risks in Safety-Critical Systems,2008,"With the increasing complexity in software and electronics in safety-critical systems new challenges to lower the costs and decrease time-to-market, while preserving high assurance have emerged. During the safety assessment process, the goal is to minimize the risk and particular, the impact of probable faults on system level safety. Every potential fault must be identified and analysed in order to determine which faults that are most important to focus on. In this paper, we extend our earlier work on formal qualitative analysis with a quantitative analysis of fault tolerance. Our analysis is based on design models of the system under construction. It further builds on formal models of faults that have been extended for estimated occurence probability allowing to analyse the system-level failure probability. This is done with the help of the probabilistic model checker PRISM. The extension provides an improvement in the costly process of certification in which all forseen faults have to be evaluated with respect to their impact on safety and reliability. We demonstrate our approach using an application from the avionic industry: an Altitude Meter System.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4708874,no
Detection and Diagnosis of Recurrent Faults in Software Systems by Invariant Analysis,2008,"A correctly functioning enterprise-software system exhibits long-term, stable correlations between many of its monitoring metrics. Some of these correlations no longer hold when there is an error in the system, potentially enabling error detection and fault diagnosis. However, existing approaches are inefficient, requiring a large number of metrics to be monitored and ignoring the relative discriminative properties of different metric correlations. In enterprise-software systems, similar faults tend to reoccur. It is therefore possible to significantly improve existing correlation-analysis approaches by learning the effects of common recurrent faults on correlations. We present methods to determine the most significant correlations to track for efficient error detection, and the correlations that contribute the most to diagnosis accuracy. We apply machine learning to identify the relevant correlations, removing the need for manually configured correlation thresholds, as used in the prior approaches. We validate our work on a multi-tier enterprise-software system. We are able to detect and correctly diagnose 8 of 10 injected faults to within three possible causes, and to within two in 7 out of 8 cases. This compares favourably with the existing approaches whose diagnosis accuracy is 3 out of 10 to within 3 possible causes. We achieve a precision of at least 95%.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4708890,no
Partheno-Genetic Algorithm for Test Instruction Generation,2008,"Test case generation is the classic method in finding software defects, and test instruction generation is one of its typical applications in embedded chipset systems.In this paper, the optimized partheno-genetic algorithm(PGA) is proposed after a 0-1 integer programming model is set up for instruction-set test cases generation problem. Based on simulation, the proposed model and algorithm achieve a convincing computational performance, in most cases 50%~70%, instruction-set test cases with better ability of error detecting obtained using this algorithm could save the execution time up to 3 seconds. Besides, it also avoids the problem of using complicated crossover and mutation operations that traditional genetic algorithm shave.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4709142,no
An Experience-Based Approach for Test Execution Effort Estimation,2008,"Software testing is becoming more and more important as it is a widely used activity to ensure software quality. Testing is now an essential phase in software development life cycle. Test execution becomes an activity in the critical path of project development. In this case, a good test execution effort estimation approach can benefit both tester managers and software projects.This paper proposes an experience-based approach for test execution effort estimation. In the approach, we characterize a test suite as a 3-dimensions vector which combines test case number, test execution complexity and its tester together. Based on the test suite execution vector model, we set up an experience database, and then a machine learning algorithm is applied to estimate efforts for given test suite vectors. We evaluate the approach through an empirical study on projects from a financial software company.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4709143,no
Adaptive block-based approach to image stabilization,2008,"The objective of image stabilization is to prevent image blurring caused by the relative motion between the camera and the scene during the image integration time. In this paper we propose a software approach to image stabilization based on capturing and fusing multiple short exposed image frames of the same scene. Due to their short exposure, the individual frames are noisy, but they are less corrupted by motion blur than it would be a single long exposed frame. The proposed fusion method is designed such that to compensate for the misalignment between the individual frames, and to prevent the blur caused by object motion in front of the camera during the multi-frame image acquisition. Various natural images acquired with camera phones have been used to evaluate the proposed image stabilization system. The results reveal the ability of the system to improve the image quality by simulating longer exposure times. In addition the system has the ability to reduce the effect of noise and outliers present in the individual short exposed frames.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4711806,no
Determination of a failure probability prognosis based on PD - diagnostics in gis,2008,"In complex high voltage components local insulation defects can cause partial discharges (PD). Especially in highly stressed gas insulated switchgear (GIS) these PD affected defects can lead to major blackouts. Today each PD activity on important insulation components causes an intervention of an expert, who has to identify and analyze the PD source and has to decide: Are the modules concerned to be switched off or can they stay in service? To reduce these cost and time intensive expert interventions, this contribution specifies a proposal which combines an automated PD defect identification procedure with a quantifiable diagnosis confidence. A risk assessment procedure is described, which is based on measurements of phase resolved PD pulse sequence data and a subsequent PD source identification. A defect specific risk is determined and then integrated within a failure probability software using the Farmer diagram. The risks of failure are classified into three levels. The uncertainty of the PD diagnosis is assessed by applying different PD sources and comparisons with other evaluation concepts as well as considering system theoretical investigations. It is shown that the PD defect specific risk is the key aspect of this approach which depends on the so called criticality range and the main PD impact aspects PD location, time dependency, defect property and (over) voltage dependency.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4712675,no
Internet traffic management based on AMCC network processor,2008,"The remarkable and continuous increase in link speed and application variety of Internet calls for new QoS provision technology that is both efficient and flexible. Being a kind of application-specific processor optimized for network computation, network processor (NP) combines the high performance of hardware and the flexibility of software, which exactly meets the demands of nowadays Internet traffic management and QoS provision. This paper originally presents an efficient traffic management mechanism based on AMCC network processor. The data-plane software architecture, the manipulation process, especially some key components such as accurate traffic classification, flexible access control and three-step scheduling are discussed in detail. We implement the traffic management mechanism based on NP3450 network processor and test it using Spirent AX/4000 broadband test system. The test results show that our proposal can efficiently provide QoS guarantee for various traffic category on OC-48 links.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4716108,no
Integrated tactile system for dynamic 3D contact force mapping,2008,"We present the first integrated tactile system that is based on dynamic, spatially distributed, three-axial contact force data. Compared to general pressure mapping systems, our devices measure not only one, but all three components of contact forces (normal and shear) with up to 64 independent micromachined force sensing elements integrated on a single chip. The spatially distributed shear force sensing adds new dimensions and directions to tactile data analysis, including pre-slip detection, enhanced robotic grasping or high quality tactile texture classification. In this paper we briefly describe the components of the novel system: the sensor arrays, the data acquisition methods and the data analysis software. We also present two example applications that exploit the advantages of real time three-dimensional contact force mapping.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4716560,no
Comparative study of cognitive complexity measures,2008,"Complexity metrics are used to predict critical information about reliability and maintainability of software systems. Cognitive complexity measure based on cognitive informatics, plays an important role in understanding the fundamental characteristics of software, therefore directly affects the understandability and maintainability of software systems. In this paper, we compared available cognitive complexity measures and evaluated cognitive weight complexity measure in terms of Weyukerpsilas properties.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4717939,no
Learning software engineering principles using open source software,2008,"Traditional lectures espousing software engineering principles hardly engage studentspsila attention due to the fact that students often view software engineering principles as mere academic concepts without a clear understanding of how they can be used in practice. Some of the issues that contribute to this perception include lack of experience in writing and understanding large programs, and lack of opportunities for inspecting and maintaining code written by others. To address these issues, we have worked on a project whose overarching goal is to teach students a subset of basic software engineering principles using source code exploration as the primary mechanism. We attempted to espouse the following software engineering principles and concepts: role of coding conventions and coding style, programming by intention to develop readable and maintainable code, assessing code quality using software metrics, refactoring, and reverse engineering to recover design elements. Student teams have examined the following open source Java code bases: ImageJ, Apache Derby, Apache Lucene, Hibernate, and JUnit. We have used Eclipse IDE and relevant plug-ins in this project.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4720643,no
Research and Assessment of the Reliability of a Fault Tolerant Model Using AADL,2008,"In order to solve the problem of the assessment of the reliability of the fault tolerant system, the work in this paper is devoted to analyze a subsystem of ATC (air traffic control system), and use AADL (architecture analysis and design language) to build its model. After describing the various software and hardware error states and as well as error propagation from hardware to software, the work builds the AADL error model and convert it to GSPN (general stochastic Petri net). Using current Petri Net technology to assess the reliability of the fault tolerant system which is based on ATC as the background, this paper receives good result of the experiment.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721310,no
The Software Failure Prediction Based on Fractal,2008,"Reliability is one of the most important qualities of software, and failure analysis is an important part of the research of software reliability. Fractals are mathematical or natural objects that are made of parts similar to the whole in certain ways. A fractal has a self-similar structure that occurs at different scales. In this paper the failure data of software are analyzed, the fractals are discovered in the data, and the method of software failure prediction based on fractals is proposed. Analyzing the empirical failure data (three data sets including two of Musa's) validates the validity of the model. It should be noticed that the analyses and research methods in this paper are differ from the conventional methods in the past, and a new idea for the research of the software failure mechanism is presented.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721317,no
Towards High-Level Parallel Programming Models for Multicore Systems,2008,"Parallel programming represents the next turning point in how software engineers write software. Multicore processors can be found today in the heart of supercomputers, desktop computers and laptops. Consequently, applications will increasingly need to be parallelized to fully exploit multicore processors throughput gains now becoming available. Unfortunately, writing parallel code is more complex than writing serial code. This is where the threading building blocks (TBB) approach enters the parallel computing picture. TBB helps developers create multithreaded applications more easily by using high-level abstractions to hide much of the complexity of parallel programming, We study the programmability and performance of TBB by evaluating several practical applications. The results show very promising performance but parallel programming with TBB is still tedious and error-prone.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721348,no
The Implementation of Analysis System of Vehicle Impact Based on Dynamic Image Sequence,2008,"This paper presents the way to realize the analysis system of vehicle impact based on dynamic image sequence, and discusses some key technologies: import the method of image recovering based on atmospheric physical model to reduce the effect on image quality made by atmosphere; present the amalgamation method of background subtraction and temporal difference to realize fast detection to moving object (vehicle); establish a complete camera model based on perspective projection and distortion correction, and fulfill the 3D spatial coordinate measurement based on 2-step method. The qualitative and quantitative analysis of the impact process can be reached using this system, and the result approves its efficiency.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721748,no
Probability-Based Binary Particle Swarm Optimization Algorithm and Its Application to WFGD Control,2008,"Sulfur dioxide is an air pollutant and an acid rain precursor. Coal-fired power generating plants are major sources of sulfur dioxide, so the limestone-gypsum wet flues gas desulphurization (WFGD) technology has been widely used in thermal power plant in China nowadays to reduce the emission of sulfur dioxide and protect the environment. The absorber slurry pH value control is very important for limestone-gypsum WFGD technique since it directly determine the desulphurization performance and the quality of product. However, it is hard to achieve the satisfactory adjustment performance for the traditional PID controller because of the complexity of the absorber slurry pH control. To tackle this problem, a novel probability-based binary particle swarm optimization (PBPSO) is proposed to tuning the PID parameters. The simulation results show PBPSO is valid and outperform the traditional binary PSO algorithm in terms of easy implementation and global optimal search ability. And it also presents that the proposed PBPSO algorithm can search the optimal PID parameters and achieves the expected control performance with PID controller by constructing the proper fitness function of PID tuning.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721782,no
An Effective Method for Support Vectors Selection in Kernel Space,2008,"In Kernel Space, Support Vectors selection is an important issue for Support Vector Machines (SVMs). But, at present most sample selection methods have a common disadvantage that the candidate set for Support Vectors is the whole sample space, so, it may select interior samples or ldquooutliersrdquo that have little or even bad effect on the classifying quality. To tackle it, two improved methods based on effective candidate set are proposed in the paper. By using these two methods, the effective candidate set is firstly identified through ldquoremoving centerrdquo and eliminating ldquooutlinersrdquo, and then Support Vectors are selected in this effective candidate set. Experimental results show that the methods reserved effective candidate samples undoubtedly, and also improved the performance of the SVMs classifier in kernel space.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721888,no
An Effective Algorithm of Image Splicing Detection,2008,"To implement image splicing detection a blind, passive and effective splicing detection scheme was proposed in this paper. The model was based on moment features extracted from the multi-size block discrete cosine transform (MBDCT) and some image quality metrics (IQMs) extracted from the given test image, which are sensitive to spliced image. This model can measure statistical differences between original image and spliced image. Experimental results demonstrate that this new splicing detection algorithm is effective and reliable; indicating that the proposed approach has a broad application prospect.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721929,no
An Implementation Scheme of Power Saving Mechanism for IEEE 802.11e,2008,"U-APSD is the power save procedure defined in IEEE802.11e to improve the QoS performance of multimedia applications. U-APSD is specially suited for bi-directional traffic streams. But in some scenario without synchronized uplink and downlink traffic, defects in U-APSD procedure will result in some problems. In order to address this issue, this paper develops a low power implementation scheme of U-APSD by exploiting the downlink traffic rate estimation algorithm, and using clock gating technique as circuit-level approach to save dynamic power. Our experiments show that the scheme brings significant reduction in power consumption of NIC with little impact to the performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721963,no
Mining Change Patterns in AspectJ Software Evolution,2008,"Understanding software change patterns during evolution is important for researchers concerned with alleviating change impacts. It can provide insight to understand the software evolution, predict future changes, and develop new refactoring algorithms. However, most of the current research focus on the procedural programs like C, or object-oriented programs like Java; seldom effort has been made for aspect-oriented software. In this paper, we propose an approach for mining change patterns in AspectJ software evolution. Our approach first decomposes the software changes into a set of atomic change representations, then employs the apriori data mining algorithm to generate the most frequent itemsets. The patterns we found reveal multiple properties of software changes, including their kind, frequency, and correlation with other changes. In our empirical evaluation on several non-trivial AspectJ benchmarks, we demonstrate that those change patterns can be used as measurement aid and fault predication for AspectJ software evolution analysis.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722012,no
An Architectural Quality Assessment for Domain-Specific Software,2008,"With growing scale and complexity of software-intensive systems, the requirements to evaluating the effects of components on software quality will dramatically increase. Existing quality metrics for software architecture demand users to perform a tedious and high-cost process so that they cannot meet the needs for a rapid, simple and preliminary evaluation. In this paper, a description technique for software architecture is introduced. Through separating the control flow from component, the description technique makes it easy to analyze and evaluate software architecture. Based on the description technique, a quality metric is proposed, which is exploited to preliminarily evaluate the effects of component on the quality of software architecture. Finally, an experiment is given to show the simplicity, usability and effectiveness of the metric.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722021,no
Some Metrics for Accessing Quality of Product Line Architecture,2008,"Product line architecture is the most important core asset of software product line. vADL, a product line architecture description languages, can be used for specifying product line architecture, and also provide enough information for measuring quality of product line architecture. In this paper, some new metrics are provided to assess similarity, variability, reusability, and complexity of product line architecture. The main feature of our approach is to assess the quality of product line architecture by analyzing its formal vADL specification, and therefore the process of metric computation can be automated completely.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722101,no
Defect Tracing System Based on Orthogonal Defect Classification,2008,"With the increase of the software complexity, the defect measurement becomes a task of high priority. We give a new software defect analytical methodology based on orthogonal classification. This method has two folds. Then a set of orthogonal defect classification (ODC) reference model is given which includes activity, trigger, severity, origin, content and type of defect. In the end, it gives a support tool and concrete workflow of defect tracing. In contrast with the traditional method, this method not only has the advantages of popularity, haleness and low cost, but also improves the accuracy of identifying defects notably. Thus it offers a strong support for the prevention of defects in software products.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722117,no
Cost Model Based on Software-Process and Process Oriented Cost System,2008,"The paper attempted to establish a software process oriented cost system depend upon cost model based on software process. It considers that it is impossible to realize the cost oriented process control and process improvement in view of software artifact is a complete unit. Firstly, cost model based on software process is introduced, different from the cost model based on product unit; the paper defines the reusable software process as the object of cost-metric. And then the paper provides ontology of software process and cost-metric in order to give a semantic background of cost-metric object and metric data. The research of this paper emphasizes on establishing cost management system oriented to software process, realizing the software process related cost measure, data record, and thereafter realizing cost control during the project execution and cost estimation in the phase of software process modeling. At last the paper expounds the method of integrating cost systems into PSEEs.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722119,no
Software Productivity Analysis with CSBSG Data Set,2008,"The aim is to report on an initial exploratory statistical analysis of the relationship between software productivity and its influencing factors in China. We analyzed of a data set containing project measurement data which were collected from lots of IT companies in China by China Software Benchmarking Standards Group (CSBSG), established by China Software Process Improvement Network (CSPIN), China.Productivity with its influencing factors was investigated using a exploratory statistical analysis. Only 133 closed projects with high ratings could be used in this investigation. Furthermore, confidentiality and data quality, data filling ratio and sizing method were considered as major issues during our current work and also for future investigation. Overall, we observed that influencing factors - project size, project type and business area - do affect productivity with different level statistical significance. However, no clear evidence can be observed that factors, like maximum team size and programming languages affect productivity. Ongoing analysis of productivity and its influencing factors. Based on current productivity analysis results, comparative analysis between counties and estimation modeling will be investigated in future.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722120,no
Reliability Growth of Open Source Software Using Defect Analysis,2008,We examine two active and popular open source products to observe whether or not open source software has a different defect arrival rate than software developed in-house. The evaluation used two common models of reliability growth models; concave and S-shaped and this analysis shows that open source has a different profile of defect arrival. Further investigation indicated that low level design instability is a possible explanation of the different defect growth profile.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722138,no
Fault Tree Based Prediction of Software Systems Safety,2008,"In our modern world, software controls much of the hardware (equipment, electronics, and instruments) around us. Sometimes hardware failure can lead to a loss of human life. When software controls, operates, or interacts with such hardware, software safety becomes a vital concern. To assure the safety of software controlling system, prediction of software safety should be done at the beginning of systempsilas design. The paper focused on safety prediction using the key node property of fault trees. This metric use parameter ""s"" related to the fault tree to predict the safety of the software control systems. This metric allow designers to measure and the safety of software systems early in the design process. An applied example is shown in the paper.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722141,no
Research on a New Fabric Defect Identification Method,2008,"A new method of recognition the fabric defects features is proposed for alleviating the difficulties of extracting to complicated fabric defects features. First, fast Fourier transform and self-adaptive power spectrum decomposition are performed. Sector-regional energy of spectrum is extracted and its mean and standard deviations is calculated as fabric features. Then, the spectral energy distribution was objected to direction Y, and local peak was extracted as defects recognition features after objection. Fabric defects recognition using the proposed method shows a high performance in the on-line detection.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722173,no
An Improved Three-Step Search Algorithm with Zero Detection and Vector Filter for Motion Estimation,2008,"In this paper, a new fast block matching algorithm is developed for motion estimation. The motion estimation algorithm use improved 3-step block matching (ITSS) algorithm that can produces better quality performance and less computational time compared with three-step search (TSS) algorithm and correct the motion vector by using Zero Detection and vector filter. From the experimental results, the proposed algorithm is superior to TSS in both quality performance and computational complexity.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722211,no
A Constrained QoS Multicast Routing Algorithm in Ad Hoc,2008,This paper introduces a constrained QoS multicast routing algorithm in Ad hoc (CQMRA). The key idea of proposed algorithm is to construct the new metric-entropy and select the stability path with the help of entropy metric to reduce the number of route reconstruction so as to provide QoS guarantee in Adhoc. The simulation results show that the proposed approach and parameters provide an accurate and efficient method of estimating and evaluating the route stability in Adhoc .,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722307,no
QoS-Satisfied Pathover Scheme in FMIPv6 Environment,2008,"Using the application of bluck data transfer, we investigate the performance of QoS-satisfied pathover for transport layer mobility scheme such as mSCTP in FMIPv6 envirmonment. We find that existing scheme has some defects in aspect of pathover and throughput. Based on this, we make a potential change to mSCTP by adding QoS-Measurement-Chunk, which is used to take into account information about wireless link condition in reselection/handover process of FMIPv6 network, we proposed a scheme with an algorithm named congestion-oriented pathover (COPO) to detect congestion of the primary path using back-to-back RTTs and adapt change of the wireless link parameters. A demonstrate using simulation is provided showing how the proposed scheme provides better performance to pathover and throughput.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722312,no
Reliable and Efficient Adaptive Streaming Mechanism for Multi-user SVC VoD System over GPRS/EDGE Network,2008,"In this paper we propose and then implement a novel H.264/AVC Scalable Video Coding multi-user video-on-demand system for hand-held devices over GPRS/EDGE network. We design a reliable and efficient adaptive streaming mechanism for QoS support. The major contributions come from three folds: firstly, accumulation-based congestion control model is used for bandwidth estimation to ensure fairness among all the users. Secondly, for the purpose of handling time-varying bitrate, we propose a PLS algorithm which performs rate adaptation as well as optimizes the video quality in a R-D sense under the bandwidth constraint. Finally, to minimize adaptation delay, IDR frames are online inserted in the pre-encoded stream at the server by using transcoding. Furthermore, to improve system userspsila overall performance, transmission of packets is optimally scheduled through exploiting the multi-user diversity. More importantly, a practical system was implemented and further tested over existing GPRS/EDGE network deployed in China. The results demonstrate that the proposed mechanism achieves outstanding performance in terms of higher perceived video quality, smoother video playback, higher utilization of wireless link and fairer share of resources, compared to RTP Usage Model defined in 3 GPP Release7.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722330,no
Subjective Evaluation of Sound Quality for Mobile Spatial Digital Audio,2008,"In the past decades, technical developments have enabled the delivery of sophisticated mobile spatial audio signals to consumers, over links that range very widely in quality, requiring decisions to be made about the tradeoffs between different aspects of audio quality. It is therefore important to determine the most important spatial quality attributes of reproduced sound fields and to find ways of predicting perceived sound quality on the basis of objective measurements.This paper first briefly reviews several subjective quality measures developed for streaming realtime audio over mobile network communications and in digital audio broadcasts. Then a experimental design on the application of the subjective listening test for mobile spatial audio is described. Finally, the conclusion is analysed and some future research directions are identified.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722334,no
Space-Time Correlation Based Fault Correction of Wireless Sensor Networks,2008,"The nodes within wireless sensor network (WSN) have low reliability, and are easy to act abnormally and produce erroneous data. To address the problem, we propose a distributed fault correction algorithm, which makes use of correlation among data of adjacent nodes and the correlation between current data and historical data on a single node. The algorithm could correct measurement errors every time nodes take measures. Simulation results show that the algorithm could help correct lots of errors, and only introduce very few errors, while still keep effective for nodes near event region border where many existed algorithms failed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722366,no
Dynamic Service Replica on Distributed Data Mining Grid,2008,"Data mining is an important technique in summarize and prediction in different industry. With the help of grid computer, the capability for data storage and efficiency of data mining process can be highly increased. Nowadays, most of studies focus on data replication mechanism and replica selection method, but ignore the part of service replica mechanism which is also important. In the paper, we propose a dynamic service replica process (DSRP) system based on service-oriented architecture (SOA). DSRP has a mechanism to create and delete service replica automatically to achieve better load-balancing and performance. In the process, each activity in data mining process is viewed as a Web service and placed in different computer on data mining grid. User can define different data mining task by selecting proper services and the replication mechanism is sprung from the result of usage. The Web services provide data extracting, data preprocessing, data mining algorithms and analysis.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722367,no
The Portable PCB Fault Detector Based on ARM and Magnetic Image,2008,"Traditional fault detector technique can hardly adapt to the modern electronic technique. This paper introduces how to set up military electronic equipmentspsila portable fault detector based on magnetic image, which can fast detect the electronic equipment in the scene. The detector has made use of ARM and uC/OS-II as the developing platform, taking MiniGUI as the figure interface.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722506,no
Damaged Mechanism Research of RS232 Interface under Electromagnetic Pulse,2008,"RS232 interface executes the role of Transportation and Communication, which has became the important interface between MCU of embedded system and peripheral equipment. Because RS232 mainly work in bottom of communication protocol, so it is important to protect the infrastructure of RS232. In test, pulse double electromagnetic is pulled into RS232 data transmission lines by coupling clamp, simulated differential-mode pulse voltage, basing on the test data, it will get the damaged mechanism of RS232 interface, meanwhile presenting the Logistic model of injected voltage pulse and probability of damage to the port interface, and getting the performance evaluation of RS232 interface, moreover estimating the voltage pulse range of upper and lower bounds at the port in the normal and damage state.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722537,no
Design of a New Integrated Real Time Device to Monitor Power Harmonic Pollution,2008,"A new integrated device is put forward to monitor harmonic pollution. This system takes a 8-bit microcontroller MPC860 chip, a PowerPC TM real time system controller that produced by Motorola's company as the nucleus and is composed of multi-channels data synchronous acquisition system in the Capacitor Voltage Transformer (CVT), which can suppress the ferromagnetic resonance, satisfy the long-range monitor and show a good frequency response. The harmonic analysis algorithm adopts Fast Fourier Transform (FFT). The measured quantities are analysed and compared with international standards. This new compact system has resolved problems of traditional models with inefficient performance and complicated program. It also can communicate with monitor center through telephone line, serial port and wireless, etc. This new device is carried out in order to identify the sources of harmonics generation, estimate the level of distortion and monitor electric power signal variations very efficiently. Simulation results show the new device helps to facilitate the supervision of electric power signals and many kinds of electric power quality disturbances and harmonic pollution are tested thoroughly.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722593,no
Data Mining from Simulation of Six Sigma in Manufacturing Company,2008,"With the quickly development of the information system and the applications of six sigma in manufacturing company, six sigma faces the problem how to analyze the vast of data effectively. Data mining from simulation outputs is performed in this paper; it focuses on techniques for extracting knowledge from simulation outputs for a production and optimizing devices and labors with certain target. This paper first gives a brief definition of six sigma. Then we set up one simulation model for the production process and construct optimization objective. Then we set up one data mining model based on WITNESS Miner. It then explains six sigma project modeling with WITNESS incorporating the calculation of sigma ratings for processes and the export of key statistics. The WITNESS optimizer six sigma algorithm is explained and an example project illustrating the use of WITNESS Miner (data mining) is included. The mining results show that the model is able to fund important information affecting target, make manager diagnose the bottlenecks of the beer production process, and help manager to make decisions rapidly under uncertainty.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722649,no
A Novel Dynamic Rate Based System for Matching and Retrieval in Images and Video Sequences,2008,In this paper we describe a system for improved object matching and retrieval in real world video sequences. We propose a dynamic frame rate model for selecting the best frames from a video based on the frame quality. A novel blur metric based approach is used for frame selection. A method for calculating this no-reference blur parameter by selecting only the features contributing to the real blur and discarding unwanted ones is proposed. Precision and Re-call figures are provided for trademark matching in sports videos. Results show that the proposed technique is an efficient and simple way of improving object matching and retrieval in video sequences.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722681,no
A Novel Hybrid Approach of KPCA and SVM for Crop Quality Classification,2008,"Quality evaluation and classification is very important for crop market price determination. A lot of methods have been applied in the field of quality classification including principal component analysis (PCA) and artificial neural network (ANN) etc. The use of ANN has been shown to be a cost-effective technique. But their training is featured with some drawbacks such as small sample effect, black box effect and prone to overfitting. This paper proposes a novel hybrid approach of kernel principal component analysis (KPCA) with support vector machine (SVM) for developing the accuracy of quality classification. The tobacco quality data is evaluated in the experiment. Traditional PCA-SVM, SVM and ANN are investigated as comparison basis. The experimental results show that the proposed approach can achieve better performance in crop quality classification.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722724,no
Image Blind Forensics Using Artificial Neural Network,2008,"With the advent of digital technology, digital image has gradually taken the place of the original analog photograph, and the forgery of digital image has become increasingly easy and indiscoverable. To implement image splicing blind detection, this paper proposes a new splicing detection model. Image splicing detection can be treated as a two-class pattern recognition problem, which builds the model using moment features and some image quality metrics (IQMs) extracted from the given test image. Artificial neural network (ANN) is chosen as a classifier to train and test the given images. Experimental results demonstrate that the proposed approach has a high accuracy rate, and the network selected can work properly, proving that the ANN is effective and suitable for this model.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722751,no
Widest K-Shortest Paths Q-Routing: A New QoS Routing Algorithm in Telecommunication Networks,2008,"Actually, various kinds of sources (such as voice, video or data) with diverse traffic characteristics and quality of service requirements (QoS), which are multiplexed at very high rates, leads to significant traffic problems such as packet losses, transmission delays, delay variations, etc, caused mainly by congestion in the networks. The prediction of these problems in real time is quite difficult, making the effectiveness of ""traditional"" methodologies based on analytical models questionable. This article proposed and evaluates a QoS routing policy in packets topology and irregular traffic of communications network called widest K-shortest paths Q-routing. The technique used for the evaluation signals of reinforcement is Q-learning. Compared to standard Q-routing, the exploration of paths is limited to K best non loop paths in term of hops number (number of routers in a path) leading to a substantial reduction of convergence time. In this work a proposal for routing which improves the delay factor and is based on the reinforcement learning is concerned. We use Q-learning as the reinforcement learning technique and introduce K-shortest idea into the learning process. The proposed algorithm are applied to two different topologies. The OPNET is used to evaluate the performance of the proposed algorithm. The algorithm evaluation is done for two traffic conditions, namely low load and high load.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722795,no
Adaptive Spectrum Selection for Cognitive Radio Networks,2008,"While essentially all the frequency is allocated to different wireless applications, observations provide evidence that usage of the spectrum is actually quite limited. Fortunately, cognitive radio technology offers an attractive solution to the reuse of under-utilized licensed spectrum. Therefore, proper channel selection mechanism and channel quality metric are essential to exploit the benefits of spectrum sharing. We propose a free time ratio (FTR) metric that captures channel quality. In addition, we also propose a distributed collaborative sensing scheme to ensure the robustness of the whole network. Simulation results show that the proposed scheme significantly outperforms the existing channel selection methods.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722845,no
Bearing Fault Diagnosis Based on Feature Weighted FCM Cluster Analysis,2008,"A new method of fault diagnosis based on feature weighted FCM is presented. Feature-weight assigned to a feature indicates the importance of the feature. This paper shows that an appropriate assignment of feature-weight can improve the performance of fuzzy c-means clustering. Feature evaluation based on class separability criterion is discussed in this paper. Experiment shows that the algorithm is able to reliably recognize not only different fault categories but also fault severities. Therefore, it is a promising approach to fault diagnosis of rotating machinery.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722953,no
Harmonic Detection in Electric Power System Based on Wavelet Multi-resolution Analysis,2008,"Because of widespread uses of nonlinear load, electric power system network is injected with much harmonic current, which does great harm to consumer equipment. In order to prevent harmonic current from influencing safety of systemÃ‚Â¿s operation, we should know well how much distorted harmonic wave contained and take corresponding measurement to control or compensate it. Inthis paper, based on a kind of multi-resolution wavelet method, we use seven levels of restructuring through using Daubechies wavelet (db24) to decompose the electric current signal into fundamental wave and higher harmonic. By experiment analysis of simulation software MATLAB, separated fundamental wave error is less than 1%, which realizes harmonic effective tracking and gives accuracy compensation by making use of total distorted component.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4723124,no
Color Reproduction Quality Metric on Printing Images Based on the S-CIELAB Model,2008,"The just-perceived color difference in a pair of printing images, the reproduction and its original, has been confirmed, subjectively by a paired-comparison psychological experiment and objectively by the S-CIELAB color quality metric. For one color image, a total number of 53 pairs of test images, simulating a number of varieties in C, M, Y, and K ink amounts, were produced, and determined their corresponding color difference recognizing probabilities by visual paired-comparison. Also, the image color difference Delta Es values, presented in the S-CIELAB model, for each pairs of images were calculated and correlated with their color difference recognizing probability. The results showed that the just-perceived image color difference Delta Es, when 0.9 color difference recognizing probability being considered as the just-perceived level, was about 1.4 Delta Eab units for the experiment image, being the image color fidelity threshold parameter.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4723255,no
A Novel Optimum Data Duplication Approach for Soft Error Detection,2008,"Soft errors are a growing concern for computer reliability. To mitigate the effects of soft errors, a variety of software-based fault tolerance methodologies have been proposed for their low costs. Data duplication techniques have the advantage of flexible and general implementation with strong capacity for error detection. However, the trade-off between reliability, performance and memory overhead should be carefully considered before employing data duplication techniques. In this paper, we first introduce an analytical model, named PRASE (Program Reliability Analysis with Soft Errors), which is able to access the impact of soft errors for the reliability of a program. Furthermore, the analytical result of PRASE points out a factor about data reliability weight, which meters the criticality of data for the overall reliability of the target program. Based on PRASE, we propose a novel data duplication approach, called ODD, which can provide the optimum error coverage under system performance constraints. To illustrate the effectiveness of our method, we perform several fault injection experiments and performance evaluations on a set of simple benchmark programs using the SimpleScalar tool set.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724544,no
Mining Individual Performance Indicators in Collaborative Development Using Software Repositories,2008,"A better understanding of the individual developersÃ‚Â¿ performance has been shown to result in benefits such as improved project estimation accuracy and enhanced software quality assurance. However, new challenges of distinguishing the individual activities involved in software evolution arise when considering collaborative development environments. Since software repositories such as version control systems (VCS) and bug tracking systems (BTS) are available for most software projects and hold a detailed and rich record of the historical development information, this paper presents our experiences mining individual performance indicators in collaborative development environments by using these repositories. The base of our key idea is to identify the complexity metrics (in the code base) and field defects (from bug tracking system) at individual-level by incorporating the historical data from version control system. We also remotely measure and analyze these indicators mined from a libre project jEdit, which involves around one hundred developer. The results show that these indicators are feasible and instructive in the understanding of the individual performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724554,no
A Design Quality Model for Service-Oriented Architecture,2008,"Service-Oriented Architecture (SOA) is emerging as an effective solution to deal with rapid changes in the business environment. To handle fast-paced changes, organizations need to be able to assess the quality of its products prior to implementation. However, literature and industry has yet to explore the techniques for evaluating design quality of SOA artifacts. To address this need, this paper presents a hierarchical quality assessment model for early assessment of SOA system quality. By defining desirable quality attributes and tracing necessary metrics required to measure them, the approach establishes an assessment model for identification of metrics at different abstraction levels. Using the model, design problems can be detected and resolved before they work into the implemented system where they are more difficult to resolve. The model is validated against an empirical study on an existing SOA system to evaluate the quality impact from explicit and implicit changes to its requirements.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724572,no
Test Case Prioritization Based on Analysis of Program Structure,2008,"Test case prioritization techniques have been empirically proved to be effective in improving the rate of fault detection in regression testing. However, most of previous techniques assume that all the faults have equal severity, which dose not meet the practice. In addition, because most of the existing techniques rely on the information gained from previous execution of test cases or source code changes, few of them can be directly applied to non-regression testing. In this paper, aiming to improve the rate of severe faults detection for both regression testing and non-regression testing, we propose a novel test case prioritization approach based on the analysis of program structure. The key idea of our approach is the evaluation of testing-importance for each module (e.g., method) covered by test cases. As a proof of concept, we implement $Apros$, a test case prioritization tool, and perform an empirical study on two real, non-trivial Java programs. The experimental result represents that our approach could be a promising solution to improve the rate of severe faults detection.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724580,no
Early Estimate the Size of Test Suites from Use Cases,2008,"Software quality becomes an increasingly important factor in software marketing. It is well known that software testing is an important activity to ensure software quality. Despite the important role that software testing plays, little is known about the prediction of test suites size. Estimation of testing size is a crucial activity among the tasks of testing management. Work plan and subsequent estimations of the effort required are made based on the estimation of test suites size. The earlier test suites size estimation we do, the more benefit we will get in the process of testing. This paper presents an experience-based approach for the test suites size estimation. The main findings are: (1) Model of use case verification points. (2) Linear relationship between use case verification points and test case number. The test case number prediction model deduced from the data of real projects in a financial software company.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724582,no
Theoretical Maximum Prediction Accuracy for Analogy-Based Software Cost Estimation,2008,"Software cost estimation is an important area of research in software engineering. Various cost estimation model evaluation criteria (such as MMRE, MdMRE etc.) have been developed for comparing prediction accuracy among cost estimation models. All of these metrics capture the residual difference between the predicted value and the actual value in the dataset, but ignore the importance of the dataset quality. What is more, they implicitly assume the prediction model to be able to predict with up to 100% accuracy at its maximum for a given dataset. Given that these prediction models only provide an estimate based on observed historical data, absolute accuracy cannot be possibly achieved. It is therefore important to realize the theoretical maximum prediction accuracy (TMPA) for the given model with a given dataset. In this paper, we first discuss the practical importance of this notion, and propose a novel method for the determination of TMPA in the application of analogy-based software cost estimation. Specifically, we determine the TMPA of analogy using a unique dynamic K-NN approach to simulate and optimize the prediction system. The results of an empirical experiment show that our method is practical and important for researchers seeking to develop improved prediction models, because it offers an alternative for practical comparison between different prediction models.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724583,no
Experimental Study of Discriminant Method with Application to Fault-Prone Module Detection,2008,"Some techniques have been applied to improving software quality by classifying the software modules into fault-prone or non fault-prone categories. This can help developers focus on some high risk fault-prone modules. In this paper, a distribution-based Bayesian quadratic discriminant analysis (D-BQDA) technique is experimental investigated to identify software fault-prone modules. Experiments with software metrics data from two real projects indicate that this technique can classify software modules into a proper class with a lower misclassification rate and a higher efficiency.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724642,no
Simultaneously Removing Noise and Selecting Relevant Features for High Dimensional Noisy Data,2008,"The classification for the noisy training data in high dimension suffers from concurrent negative effects by noise and irrelevant/redundant features. Noise disrupts the training data and irrelevant/redundant features prevent the classifier from picking relevant features in building the model. Therefore they may reduce classification accuracy. This paper introduces a novel approach to improve the quality of training data sets with noisy dependent variable and high dimensionality by simultaneously removing noisy instances and selecting relevant features for classification. Our approach relies on two genetic algorithms, one for noise detection and the other for feature selection, and allows them to exchange their results periodically at certain generation intervals. Prototype selection is used to improve the performance along with the genetic algorithm in the noise detection method. This paper shows that our approach enhances the quality of noisy training data sets with high dimension and substantially increases the classification accuracy.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724968,no
Force Feature Spaces for Visualization and Classification,2008,"Distance-preserving dimension reduction techniques can fail to separate elements of different classes when the neighborhood structure does not carry sufficient class information. We introduce a new visual technique, K-epsilon diagrams, to analyze dataset topological structure and to assess whether intra-class and inter-class neighborhoods can be distinguished. We propose a force feature space data transform that emphasizes similarities between same-class points and enhances class separability. We show that the force feature space transform combined with distance-preserving dimension reduction produces better visualizations than dimension reduction alone. When used for classification, force feature spaces improve performance of K-nearest neighbor classifiers. Furthermore, the quality of force feature space transformations can be assessed using K-epsilon diagrams.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4725009,no
Evaluating Interpolation-Based Power Management,2008,"Power management for WSNs can take many forms, from adaptively tuning the power consumption of some of the components of a node to hibernating it completely. In the later case, the competence of the WSN must not be compromised. In general, the competence of a WSN is its ability to perform its function in an accurate and timely fashion. These two, related, Quality of Service (QoS) metrics are primarily affected by the density and latency of data from the environment, respectively. Without adequate density, interesting events may not be adequately observed or missed completely by the application, while stale data could result in event detection occurring too late. In opposition to this is the fact that the energy consumed by the network is related to the number of active nodes in the deployment. Therefore, given that the nodes have finite power resources, a trade-off exists between the longevity and QoS provided by the network and it is crucial that both aspects are considered when evaluating a power management protocol. In this paper, we present an evaluation of a novel node hibernation technique based on interpolated sensor readings according to these four metrics: energy consumption, density, message latency and the accuracy of an application utilising the data from the WSN. A comparison with a standard WSN that does not engage in power management is also presented, in order to show the overhead in the protocols operation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4725249,no
Orderly Random Testing for Both Hardware and Software,2008,"Based on random testing, this paper introduces a new concept of orderly random testing for both hardware and software systems. Random testing, having been employed for years, seems to be inefficient for its random selection of test patterns. Therefore, a new concept of pre-determined distance among test vectors is proposed in the paper to make it more effective in testing. The idea is based on the fact that the larger the distance between two adjacent test vectors in a test sequence, the more the faults will be detected by the test vectors. Procedure of constructing such a testing sequence is presented in detail. The new approach has shown its remarkable advantage of fitting in with both hardware and software testing. Experimental results and mathematical analysis are also given to evaluate the performances of the novel method.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4725292,no
Bayesian Inference Approach for Probabilistic Analogy Based Software Maintenance Effort Estimation,2008,"Software maintenance effort estimation is essential for the success of software maintenance process. In the past decades, many methods have been proposed for maintenance effort estimation. However, most existing estimation methods only produce point predictions. Due to the inherent uncertainties and complexities in the maintenance process, the accurate point estimates are often obtained with great difficulties. Therefore some prior studies have been focusing on probabilistic predictions. Analogy Based Estimation (ABE) is one popular point estimation technique. This method is widely accepted due to its conceptual simplicity and empirical competitiveness. However, there is still a lack of probabilistic framework for ABE model. In this study, we first propose a probabilistic framework of ABE (PABE). The predictive PABE is obtained by integrating over its parameter k number of nearest neighbors via Bayesian inference. In addition, PABE is validated on four maintenance datasets with comparisons against other established effort estimation techniques. The promising results show that PABE could largely improve the point estimations of ABE and achieve quality probabilistic predictions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4725294,no
A New Paradigm for Software Reliability Modeling Â– From NHPP to NHGP,2008,"Non-homogeneous gamma process (NHGP) models with typical reliability growth patterns are developed for software reliability assessment in order to overcome a weak point of the usual non-homogeneous Poisson process (NHPP) models. Though the analytical treatment of NHGPs as stochastic point processes is not so easy in general, they have an advantage to involve the NHPPs as well as a gamma renewal process as special cases, and are rather tractable on parameter estimation by means of the method of maximum likelihood. We perform the goodness-of fit test for several NHGP-based software reliability models (SRMs) and compare them with the existing NHPP-based ones. Throughout a numerical example with a real software fault data, it is shown that the NHGP-based SRMs can provide the better goodness-of-fit performances in earlier testing phases than the NHPP-based ones, but approach to them gradually as the testing time goes on. This implies that our new software reliability modeling framework with flexibility can describe better the software-fault detection phenomenon when the less information on software fault data is available.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4725300,no
Hyper-Erlang Software Reliability Model,2008,"This paper proposes a hyper-Erlang software reliability model (HErSRM) in the framework of non-homogeneous Poisson process (NHPP) modeling. The proposed HErSRM is a generalized model which contains some existing NHPP-based SRMs like Goel-Okumoto SRM and Delayed S-shaped SRM, and can represent a variety of software fault-detection patterns. Such characteristics are useful to solve the model selection problem arising in the practical use of NHPP-based SRMs. More precisely, we discuss the statistical inference of HErSRM based on the EM (expectation-maximization) algorithm. In numerical experiments, we show that the HErSRM outperforms conventional NHPP-based SRMs with respect to fitting ability.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4725301,no
PRASE: An Approach for Program Reliability Analysis with Soft Errors,2008,"Soft errors are emerging as a new challenge in computer applications. Current studies about soft errors mainly focus on the circuit and architecture level. Few works discuss the impact of soft errors on programs. This paper presents a novel approach named PRASE, which can analyze the reliability of a program with the effect of soft errors. Based on the simple probability theory and the corresponding assembly code of a program, we propose two models for analyzing the probabilities about error generation and error propagation. The analytical performance is increased significantly with the help of basic block analysis. The programÃ‚Â¿s reliability is determined according to its actual execution paths. We propose a factor named PVF (program vulnerability factor), which represents the characteristic of programÃ‚Â¿s vulnerability in the presence of soft errors. The experimental results show that the reliability of a program has a connection with its structure. Comparing with the traditional fault injection techniques, PRASE has the advantage of faster speed and lower price with more general results.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4725302,no
Training Security Assurance Teams Using Vulnerability Injection,2008,"Writing secure Web applications is a complex task. In fact, a vast majority of Web applications are likely to have security vulnerabilities that can be exploited using simple tools like a common Web browser. This represents a great danger as the attacks may have disastrous consequences to organizations, harming their assets and reputation. To mitigate these vulnerabilities, security code inspections and penetration tests must be conducted by well-trained teams during the development of the application. However, effective code inspections and testing takes time and cost a lot of money, even before any business revenue. Furthermore, software quality assurance teams typically lack the knowledge required to effectively detect security problems. In this paper we propose an approach to quickly and effectively train security assurance teams in the context of web application development. The approach combines a novel vulnerability injection technique with relevant guidance information about the most common security vulnerabilities to provide a realistic training scenario. Our experimental results show that a short training period is sufficient to clearly improve the ability of security assurance teams to detect vulnerabilities during both code inspections and penetration tests.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4725309,no
Panel: SOA and Quality Assurance,2008,This panel session discusses which qualities are of particular importance for SOA and it explores the possible ways to guarantee them. Presentations in the session will explore the use of different techniques and approaches in order to foster the adoption of quality modeling techniques by industrial software systems.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4725698,no
Software Defect Prediction Using Call Graph Based Ranking (CGBR) Framework,2008,"Recent research on static code attribute (SCA) based defect prediction suggests that a performance ceiling has been achieved and this barrier can be exceeded by increasing the information content in data. In this research we propose static call graph based ranking (CGBR) framework, which can be applied to any defect prediction model based on SCA. In this framework, we model both intra module properties and inter module relations. Our results show that defect predictors using CGBR framework can detect the same number of defective modules, while yielding significantly lower false alarm rates. On industrial public data, we also show that using CGBR framework can improve testing efforts by 23%.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4725722,no
IFPUG-COSMIC Statistical Conversion,2008,"One of the main issues faced within the Functional Size Measurement (FSM) community is the convertibility issue between FSM methods. A particular attention during last years was devoted to find a mathematical function for converting IFPUG functional size units to the newer COSMIC ones. Moving from the data sets and experiences described in previous studies, some attention points about cost and quality from the data gathering process emerge. This paper analyzes the data gathering process issue and proposes a solution for overcoming such difficulties. From an application of a repeteable and verifiable procedure, performed in a university course on Software Engineering with the support of an experienced measurer, two new data sets were derived. Finally an analysis of all datasets was done, presenting a possible interval for the conversion between IFPUG-COSMIC fsu.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4725750,no
AODE for Source Code Metrics for Improved Software Maintainability,2008,"Software metrics are collected at various phases of the whole software development process, in order to assist in monitoring and controlling the software quality. However, software quality control is complicated, because of the complex relationship between these metrics and the attributes of a software development process. To solve this problem, many excellent techniques have been introduced into software maintainability domain. In this paper, we propose a novel classification method--Aggregating One-Dependence Estimators (AODE) to support and enhance our understanding of software metrics and their relationship to software quality. Experiments show that performance of AODE is much better than eight traditional classification methods and it is a promising method for software quality prediction. Furthermore, we present a Symmetrical Uncertainty (SU) based feature selection method to reduce source code metrics taking part in classification, make these classifiers more efficient and keep their performances not undermined meanwhile. Our empirical study shows the promising capability of SU for selecting relevant metrics and preserving original performances of the classifiers.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4725932,no
An Improving Fault Detection Mechanism in Service-Oriented Applications BasedÂ Â on Queuing Theory,2008,"SOA has become more and more popular, but fault tolerance is not fully supported in most existing SOA frameworks and solutions provided by various major software companies. SOA implementations with large number of users, services, or traffic, maintaining the necessary performance levels of applications integrated using an ESB presents a substantial challenge, both to the architects who design the infrastructure as well as to IT professionals who are responsible for administration. In this paper, we improve the performance model for analyzing and detecting faults based on the queuing theory. The performance of services of SOA applications is measuring in two categories (individual services and composite services). We improve the model of the individuals services and add the composite services performance measuring.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4730494,no
Learning to rank with voted multiple hyperplanes for documents retrieval,2008,"The central problem for many applications in Information retrieval is ranking. Learning to rank has been considered as a promising approach for addressing the issue. In this paper, we focus on applying learning to rank to document retrieval, particularly the approach of using multiple hyperplanes to perform the task. Ranking SVM (RSVM) is a typical method of learning to rank. We point out that although RSVM is advantageous, it still has shortcomings. RSVM employs a single hyperplane in the feature space as the model for ranking, which is too simple to tackle complex ranking problems. In this paper, we look at an alternative approach to RSVM, which we call Â¿Â¿multiple vote rankerÂ¿Â¿ (MVR), and make comparisons between the two approaches. MVR employs several base rankers and uses the vote strategy for final ranking. We study the performance of the two methods with respect to several evaluation criteria, and the experimental results on the OHSUMED dataset show that MVR outperforms RSVM, both in terms of quality of results and in terms of efficiency.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4730996,no
Development of a New Optimization Algorithm Based on Artificial Immune System and Its Application,2008,"In resent years, research has taken an interest in design of approximation algorithms due to the requirement of these algorithms for solving many problems of science and engineering like system modeling, identification of plants, controller design, fault detection, computer security, prediction of data sets etc. The area of Artificial Immune System (AIS) is emerging as an active and attractive field involving models, techniques and applications of greater diversity. In this paper a new optimization algorithm based on AIS is developed. The proposed algorithm has been suitably applied to develop practical applications like design of a new model for efficient approximation of nonlinear functions and identification of nonlinear systems in noisy environments. Simulation study of few benchmark function approximation and system identification problems are carried out to show superior performance of the proposed model over the standard methods in terms of response matching, accuracy of identification and convergence speed achieved.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4731296,no
A Multipath Forwarding Algorithm Based-On Predictive Path Loss Rate for Real-Time Application,2008,"Delivery of real time streaming applications, such as voice over IP (VoIP), in packet switched networks is based on dividing the stream into packets and transferring each of the packets on an individual basis to the destination. To study the effect of packet dispersion on the quality of VoIP applications, we focus on the effect of the packet loss rate on the applications, and present a model in which packets of a certain session are dispersed over multiple paths based on the concept of predictive path loss rate (PPLR). The proposed PPLR strategy requires the dynamic prediction of loss levels over all available network paths. This predictive information is then used as an input to adjust the allocation proportion for next prediction period. Through simulations, we show that the proposed PPLR forwarding strategy provides significantly better performance than ECMP.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4731524,no
Research on Organizational-Level Software Process Improvement Model and Its Implementation,2008,"The development of software products is a complex activity with a large number of factors involved in defining success. Measurement is a necessary prerequisite, for software process improvement. However few guidelines exist for systematic planning of measurement programs within software projects and for multi-project selection. This paper discusses the major problems in software process measurement, presents an organizational-level software process improvement model (O-SPIM) to support software process improvement. Based on O-SPIM, software organizations can select suitable programs in those potential projects and establish adaptive measurement process and execute the measure just close-related the process goals, which focuses on their particular business environment. Subsequently, some methods of a comprehensive assessment of quality estimating to support higher level quantitative management are suggested. The implementation of O-SPIM and the related methods is introduced in the end. These works were integrated in a toolkit called SQMSP which was used in some medium-sized enterprises in China.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4731622,no
A LD-aCELP Speech Coding Algorithm Based on Modified SOFM Vector Quantizer,2008,"According to the character of codebook size and codeword dimension in low delay speech coding algorithm, a codebook design algorithm based on modified self-organizing feature map (SOFM) neural network is put forward. The input vectors and weight vectors are normalized. In order to reduce the computation complexity and improve the codebook performance, decompose the adaptive adjusting process of network weights into two stages of sequencing and convergence. The modified algorithm is used to generate vector quantization codebook in low delay speech coding algorithm experiment results show that the modified algorithm improves the performance of SOFM network significantly. Compared with the basic LBG algorithm, the synthesized speech using codebook with the modified SOFM neural network are greatly improved in the aspect of subjective and objective quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4731964,no
Test Case Prioritization for Multiple Processing Queues,2008,"Test case prioritization is an effective technique that helps to increase the rate of fault detection or code coverage in regression testing. However, all existing methods can only prioritize test cases to a single queue. Once there are two or more machines that participate in testing, all exiting techniques are not applicable any more. To extend the prioritization methods to parallel scenario, this paper defines the prioritization problem in such scenario and applies the task scheduling method to prioritization algorithms to help partitioning a test suite into multiple prioritized subsets. Besides, this paper also discusses the limitation of previous metrics and proposes a new measure of effectiveness of prioritization methods in a parallel scenario. Finally, a case study is performed to illustrate the algorithms and metrics presented in this article.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4732476,no
Research on Fuzzy-Grey Comprehensive Evaluation of Software Process Modeling Methods,2008,"Large numbers of practices show that software process modeling technique can be effectively used to describe and analyze software process and offer opportunity to discover process improvements. There are many kinds of software process modeling method, so it is significant to make reasonable evaluation of software process modeling methods which can help developers choose the most appropriate modeling method according to specific modeling environment and requirement for achieving the best modeling effect. On the basis of evaluation system of software process modeling methods, comprehensive evaluation method which combines fuzzy evaluation and grey theory is applied to evaluate some modeling methods. It can make full use of fuzziness and grayness of evaluation information by experts which makes the evaluation more objective and accurate. Further, it is proved that the method can estimate modeling methods soundly and have feasibility and engineering value to software development project practice by an example.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4732929,no
Full-Reference Quality Assessment for Video Summary,2008,"As video summarization techniques have attracted more and more attention for efficient multimedia data management, quality assessment of video summary is required. To address the lack of automatic evaluation techniques, this paper proposes a novel framework including several new algorithms to assess the quality of the video summary against a given reference. First, we partition the reference video summary and the candidate video summary into the sequences of summary unit (SU). Then, we utilize alignment based algorithm to match the SUs in the candidate summary with the SUs in the corresponding reference summary. Third, we propose a novel similarity based 4 C - assessment algorithm to evaluate the candidate video summary from the perspective of coverage, conciseness, coherence, and context, respectively. Finally, the individual assessment results are integrated according to userpsilas requirement by a learning based weight adaptation method. The proposed framework and techniques are experimented on a standard dataset of TRECVID 2007 and show the good performance in automatic video summary assessment.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4734018,no
APD-based measurement technique to estimate the impact of noise emission from electrical appliances on digital communication systems,2008,"This paper describes a technique to measure noise emissions from electrical appliances and study their impact towards digital communication systems by using the amplitude probability distribution (APD) methodology. The APD has been proposed within CISPR for measurement of electromagnetic noise emission. We present a measurement approach which utilizes a programmable digitizer with an analysis software to evaluate the noise APD pattern and probability density function (PDF). A unique noise APD pattern is obtained from each measurement of noise emission from different appliances. The noise PDF is useful for noise modeling and simulation, from which we can estimate the degradation on digital communication performance in terms of bit error probability (BEP). This technique provides a simple platform to examine the effect of other electrical appliances noise emission towards specific digital communication services.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4736019,no
Evaluating Quality of Software Systems by Design Patterns Detection,2008,"Software industry demands the development of products at a very fast rate with most effective solutions. For this reason most of the components are desired to be reused again and again. Use of design patterns can speed up the process of product development by providing the pre-tested footsteps to follow. Design patterns show their effectiveness from Design phase to maintenance. Design patterns can be analyzed for their quality which is the most necessary aspect of any software structure. This is a metric based study of the design patterns by proposing suitable metrics that are suitable for all the patterns. Higher is the quality of the used design pattern higher will be the quality of the product, with more flexibility towards change and maintenance. We can further extend the paper by determining most suitable design patterns for a problem to solve by providing highly reusable solutions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4736913,no
Application of Random Forest in Predicting Fault-Prone Classes,2008,"There are available metrics for predicting fault prone classes, which may help software organizations for planning and performing testing activities. This may be possible due to proper allocation of resources on fault prone parts of the design and code of the software. Hence, importance and usefulness of such metrics is understandable, but empirical validation of these metrics is always a great challenge. Random forest (RF) algorithm has been successfully applied for solving regression and classification problems in many applications. This paper evaluates the capability of RF algorithm in predicting fault prone software classes using open source software. The results indicate that the prediction performance of random forest is good. However, similar types of studies are required to be carried out in order to establish the acceptability of the RF model.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4736919,no
Class Ordering Tool Â– A Tool for Class Ordering in Integration Testing,2008,"The presence of cyclic dependency calls in integration testing is a major problem to determine an order of classes to be tested. We proposed an approach to solve this problem. Hence, this paper proposes a class ordering tool (CO Tool) for integration testing based on our approach for finding a test order using object-oriented slicing technique. Our approach breaks cycles by slicing classes for partial testing, whereas many researchers delete relationships to break cycles and create stub to represent interactions of testing between classes. Therefore, the benefit of our approach is to decrease the cost of implementing test stubs.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4737052,no
Analysis of software quality cost modelingâ€™s industrial applicability with focus on defect estimation,2008,The majority of software quality cost models is by design capable of describing costs retrospectively but relies on defect estimation in order to provide a cost forecast. We identify two major approaches to defect estimation and evaluate them in a large scale industrial software development project with special focus on applicability in quality cost models. Our studies show that neither static models based on code metrics nor dynamic software reliability growth models are suitable for an industrial application.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4737876,no
Optimization of feature weights and number of neighbors for Analogy based cost Estimation in software project management,2008,"Software cost estimation affects almost all activities of software project development such as: bidding, planning, and budgeting, thus it is very crucial to the success of software project management. In past decades, many methods have been proposed for cost estimation. Analogy based cost estimation (ABE) is among the most popular techniques due to its conceptual simplicity and empirical competitiveness. In order to improve ABE model, many previous studies have focused on optimizing the feature weights in the similarity function. However, according to some prior studies, the K parameter for the K-nearest neighbor is also essential to the performance of ABE. Nevertheless, few studies attempt to optimize the K number of neighbors and most of them are based on the trial-error scheme. In this study, we propose the genetic algorithm to simultaneously optimize the K parameter and the feature weights for ABE (OKFWSABE). The proposed OKFWABE method is validated on three real-world software engineering data sets. The experiment results show that our methods could significantly improve the prediction accuracy of conventional ABE and has the potential to become an effective method for software cost estimation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4738130,no
Simulation-based FDP & FCP analysis with queueing models,2008,"Continuous efforts have been devoted to software reliability modeling evolution over the past decades in order to adapt to the practical complex software testing environments. Many models have been proposed to describe the software fault related process. These software reliability growth models (SRGMs) have evolved from describing one fault detection process (FDP) into incorporating fault correction process (FCP) as well, in order to provide higher accuracy with more information. To provide mathematical tractability, models need to have closed form, with restrictive assumptions in a narrow sense. This in turn confines their capability for general applications. Alternatively, in this paper a general simulation based queueing modeling framework is proposed to describe FDP and FCP, with resource factors from practical software testing incorporated. Good simulation performance is observed with a numerical example. Furthermore, release time and debugger staffing issues are investigated with a revised cost model. The analysis is conducted through a simulation optimization approach.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4738137,no
Correlation analysis between maturity factors and performance indexes in software project,2008,"Software development inclined to production of scale, criterion and industrialization, with the development of information technology and software industry. Different kinds of Project Maturity Models for enhancing performances of software development were broadly exploited and used factually. Based on literature review and enterprise research this paper put forward 30 main factors which cover in three aspects of technology, organization and personnel. At the same time, the author classified the software development performance estimation indexes into five geniuses: quality, satisfaction, knowledge, control, and efficiency index. Recurring to large-scale questionnaire research, the author also applied SPSS statistic instrument consisted of gene analysis, correlation analysis and regression analysis to analyze the relationship between maturity factors and performance indexes. The results indicated that close connections between the project performance and different maturity factors affected the project performance to a certain extent.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4738181,no
Detecting Defects in Golden Surfaces of Flexible Printed Circuits Using Optimal Gabor Filters,2008,"This paper studies the application of advanced computer image processing techniques for solving the problem of automated defect detection for golden surfaces of flexible printed circuits (FPC). A special defect detection scheme based on semi-supervised mechanism is proposed, which consists of an optimal Gabor filter and a smoothing filter. The aim is to automatically discriminate between ""known"" non-defective background textures and ""unknown"" defective textures of golden surfaces of FPC. In developing the scheme, the parameters of the optimal Gabor filter are searched with the help of the genetic algorithm based on constrained minimization of a Fisher cost function. The performance of the proposed defect detection scheme is evaluated off-line by using a set of golden images acquired from CCD. The results exhibit accurate defect detection with low false alarms, thus showing the effectiveness and robustness of the proposed scheme.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4739587,no
Deformable Block Motion Estimation and Compensation in the Redundant Domain,2008,"Because the redundant discrete wavelet transform (RDWT) is shift invariant, the motion estimation algorithms in the redundant wavelet domain have good performance. And, deformable block matching algorithm (DBMA) not only can reduce 'block effect' which is brought by traditional block matching algorithm but also can have an effective prediction for the non-translational motion. So, this paper mainly focus.es on deformable block motion estimation and compensation in the redundant discrete wavelet transformation domain (RDWT-DBMA). The motion estimation is done in the biorthogonal 9/7 redundant wavelet domain, and four-nodes-based model is used in nodal-based deformable matching algorithm. Experimental results show that RDWT-DBMA has a better subjective and objective image quality than full search algorithm (FS) which has the best precision in block matching algorithm (BMA), especially for the sequence which has fast motions, and the average PSNR of the image can be improved 3.17dB.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4739679,no
Resource Planning Heuristics for Service-Oriented Workflows,2008,"Resource allocation and resource planning, especially in a SOA and grid environment, become crucial. Particularly, in an environment with a huge number of workflow consumers requesting a decentralized cross-organizational workflow, performance evaluation and execution-management of service-oriented workflows gain in importance. The need for an effective and efficient workflow management forces enterprises to use intelligent optimization models and heuristics to compose workflows out of several services under real-time conditions. This paper introduces the required architecture workflow performance extension - WPX.KOM for resource planning and workload prediction purposes. Furthermore, optimization approaches and a high-performance heuristic solving the addressed resource planning problem with low computational overhead are presented.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4740517,no
Error Correcting Output Coding-Based Conditional Random Fields for Web Page Prediction,2008,"Web page prefetching has been used efficiently to reduce the access latency problem of the Internet, its success mainly relies on the accuracy of Web page prediction. As powerful sequential learning models, conditional random fields (CRFs) have been used successfully to improve the Web page prediction accuracy when the total number of unique Web pages is small. However, because the training complexity of CRFs is quadratic to the number of labels, when applied to a Web site with a large number of unique pages, the training of CRFs may become very slow and even intractable. In this paper, we decrease the training time and computational resource requirements of CRFs training by integrating error correcting output coding (ECOC) method. Moreover, since the performance of ECOC-based methods crucially depends on the ECOC code matrix in use, we employ a coding method, search coding, to design the code matrix of good quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4740540,no
An Attack Modeling Based on Hierarchical Colored Petri Nets,2008,"Todaypsilas serious attacks are complex, multi-stage scenarios and can involve bypassing multiple security mechanisms and the use of numerous computer systems. A host which has been controlled by an attacker can become a stepping stone for further intrusion and destruction. Providing attack graphs is one of the most direct and effective way to analyze interactions among network components and sequences of vulnerabilities. However, the findings obtained from an attack graph highly depend on the quality of modeling. In this paper, attack modeling based on Petri nets is extended and an approach based on hierarchical Colored Petri nets is provided. We will use Colored Petri nets to describe attacks in two levels, those being generally and specifically. These treatments can facilitate the understanding of network vulnerabilities further, and enhance effective protection measures.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4741117,no
Estimate Test Execution Effort at an Early Stage: An Empirical Study,2008,"Software testing is becoming more and more important as it is a widely used activity to ensure software quality. Test execution becomes an activity in the critical path of a project. In this case, early estimation of test execution effort can benefit both tester managers and software projects. This paper reports an empirical study on early test execution effort estimation. In the study, we propose an approach which mainly consists of two parts: test case number prediction from use cases and test effort estimation based on the test suite execution vector model which combines test case number, test execution complexity and its tester together. For each part, we evaluate it with the data of real projects from a financial software company.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4741300,no
Adaptive partition size temporal error concealment for H.264,2008,"Existing temporal error concealment methods for H.264 often decide the partition size of the lost macroblock (MB) before recovering the motion information, without actual quality comparison between different partition modes. In this paper, we propose to select the best partition mode by minimizing the Weighted Double-Sided External Boundary Matching Error (WDS-EBME), which jointly measures the inter-MB boundary discontinuity, inter-partition boundary discontinuity and intrapartition block artifacts in the recovered MB. The proposed method estimates the best motion vectors for each of the candidate partition modes, calculates the overall WDS-EBME values for them, and selects the partition mode with the smallest overall WDS-EBME to recover the lost MB. We also propose a progressive concealment order for the 4times4 partition mode. Test results show that the adaptive partition size method always outperforms the fixed partition size methods. Both the adaptive and fixed partition size methods are much superior to the temporal error concealment (TEC) method in the H.264 reference software.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4746376,no
A cross-layer quality driven approach in Web service selection,2008,"In order to make Web services operate in a performance optimal status, it is necessary to make an effective decision on selecting the most suitable service provider among a set Web services that provide identical functions. We argue that the network performance between the service container and service consumer can pose a significant influence to the performance of Web service that the consumer actually receive, while current researches have limited emphasis on this issue. In this paper, we propose a cross-layer approach for Web service selection which takes the network performance issue into consideration during the service selection process. A discrete representation of cross-layer performance correlation is proposed. Based on which, a qualitative reasoning method is introduced to predict the performance at the service user side. The integration of the quality driven Web service selection method to service oriented architecture is also considered. Simulation is designed and experiment results suggest that the new approach significantly improves the accuracy of Web service selection and delivers a performance elevation for Web services.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4746795,no
Image quality assessment software of security screening system,2008,"Security screening systems based on imaging technology, like X-ray and gamma-ray based screening systems, are widely used in the field of aviation security. The image quality of the systems reflects detection performance directly. In the paper a set of image quality assessment software based-on human visual sensitivity (HVS) has been developed. The software integrated HVS method in the objective method and overcomes the shortage of both objective and subjective method of image processing. Experiments show that the assessment results are consistent with the perception of operators. With the help of the image quality assessment software, the image quality assessment of the security screening system can be finished justly and efficiently, which contributes the safety of aviation transportation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4751286,no
Multiphysic modeling and design of carbon nanotubes based variable capacitors for microwave applications,2008,"This paper describes the multiphysic methodology developed to design carbon nanotubes (CNT) based variable capacitor (varactor). Instead of using classical RF-MEMS design methodologies; we take into account the real shape of the CNT, its nanoscale dimensions and its real capacitance to ground. A capacitance-based numerical algorithm has then been developed in order to predict the pull-in voltage and the RF-capacitance of the CNT-based varactor. This software, which has been validated by measurements on various devices, has been used to design varactor device for which 20V of actuation voltage has been predicted. We finally extend the numerical modeling to describe the electromagnetical behavior of the devices. The RF performances has also been efficiently predicted and the varactor (in parallel configuration) exhibits predicted losses of 0.3 dB at 5 GHz and quality factor of 22at 5 GHz, which is relevant for high quality reconfigurable circuits requirements where as the expected sub-microsecond switching time range opens the door to real time tunability.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4751672,no
Broadband One-port Material Characterization Method of Porous and Fluidic Materials,2008,"Magnetic and electrical properties of materials are investigated by the use of a novel method inspired by the known distance-to-fault (DTF) measurement. The principles of the one-port method are detailed and it is verified by presenting measurement results. The complex relative permeability and permittivity of the samples are derived simultaneously by means of a network analyzer and control software. The coaxial sample holder designed for this measurement allows the investigation of liquid and porous materials. As a special feature, the method can be easily implemented into various applications, since it is based on using a scalar network analyzer. Precognition of the sample length is not necessary, which provides high level of flexibility when using the presented method.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4751680,no
Dynamically reconfigurable soft output MIMO detector,2008,"MIMO systems (with multiple transmit and receive antennas) are becoming increasingly popular, and many next-generation systems such as WiMAX, 3-GPP LTE and IEEE802.11 n wireless LANs rely on the increased throughput of MIMO systems with up to four antennas at receiver and transmitter. High throughput implementation of the detection unit for MIMO systems is a significant challenge. This challenge becomes still harder, because the above mentioned standards demand support for multiple modulation and coding schemes. This implies that the MIMO detector must be dynamically reconfigurable. Also, to achieve required bit error rate (BER) or frame error rate (FER) performance, the detector has to provide soft values to advanced forward error correction (FEC) schemes like turbo Codes. This paper presents an ASIC implementation of a novel MIMO detector architecture that is able to reconfigure on the fly and provides soft values as output. The design is implemented in 45 nm predictive technology library, and has a parallelism factor of four. The detector has many qualities of a systolic architecture and achieves a continuous throughput of 1 Gbps for QPSK, 500 Mbps for 16-QAM, and 187.5 Mbps for 64-QAM. The total area is estimated to be approximately 70 KGates equivalent, and power consumption is estimated to be 114 mW.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4751842,no
Reversi: Post-silicon validation system for modern microprocessors,2008,"Verification remains an integral and crucial phase of todaypsilas microprocessor design and manufacturing process. Unfortunately, with soaring design complexities and decreasing time-to-market windows, todaypsilas verification approaches are incapable of fully validating a microprocessor before its release to the public. Increasingly, post-silicon validation is deployed to detect complex functional bugs in addition to exposing electrical and manufacturing defects. This is due to the significantly higher execution performance offered by post-silicon methods, compared to pre-silicon approaches. Validation in the post-silicon domain is predominantly carried out by executing constrained-random test instruction sequences directly on a hardware prototype. However, to identify errors, the state obtained from executing tests directly in hardware must be compared to the one produced by an architectural simulation of the designpsilas golden model. Therefore, the speed of validation is severely limited by the necessity of a costly simulation step. In this work we address this bottleneck in the traditional flow and present a novel solution for post-silicon validation that exposes its native high performance. Our framework, called Reversi, generates random programs in such a way that their correct final state is known at generation time, eliminating the need for architectural simulations. Our experiments show that Reversi generates tests exposing more bugs faster, and can speed up post-silicon validation by 20x compared to traditional flows.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4751878,no
New approach to the modeling of Command and Control Information Systems,2008,"Probability of military activities successes in great degree depends on the qualities of their tactical planning processes. A very important part of tactical planning is the planning of a command and control information systempsilas (C2IS) communication infrastructure, respectively tactical communication networks. There are several known software tools that assist the process of tactical planning. However, they are often useless because different armies use different information and communication technologies. This is the main reason we started to develop a simulation system that helps in the process of planning tactical communication networks. Our solution is based on the well-known OPNET modeler simulation environment, which is also used for some other solutions in this area. In addition to the simulation and modeling methodologies we have also developed helper software tools. TPGEN is a tool which enables the user-friendly entering and editing of tactical network models. It performs mapping from C2IS and tactical communication network descriptions to an OPNET simulation modelpsilas parameters. Because the simulation results obtained by an OPNET modeler are user-unfriendly and need expert knowledge when analyzing them, we have developed an expert system for the automatic analysis of simulation results. One of outputs from this expert system is the user-readable evaluation of tactical networkspsila performances with guidance on how to improve the network. Another output is formatted to use in our tactical player. This tactical player is an application which helps when visualizing simulation and expert system results. It is also designed to control an OPNET history player in combination with 3DNV visualization of a virtual terrain. This developed solution, in user-friendly way, helps in the process of designing and optimizing tactical networks.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4753134,no
Improving software reliability and security with automated analysis,2008,"Static-analysis tools that identify defects and security vulnerabilities in source and executables have advanced significantly over the last few years. A brief description of how these tools work is given. Their strengths and weaknesses in terms of the kinds of flaws they can and cannot detect are discussed. Methods for quantifying the accuracy of the analysis are described, including sources of ambiguity for such metrics. Recommendations for deployment of tools in a production setting are given.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4753207,no
Dual mode radio: A new tranceiver architecture for UWB and 60 GHz-applications,2008,"The paper contains a new concept for installation of radio transmission links for digital signals with a mimimum of hardware and no software effort. As opposed to known architectures, the difference in information is evaluated between two free-space modes. A phase modulation is set on the differences between two transmission signals. Both transmitter and receiver work without crystal oscillator. Due to this procedure neither a oscillator or time recovery is required at the receiver. Each signal can for example be demodulated by a multiplier (mixer) in the receiver. Consequently, the receiver can be easily implemented. Owing to the fact that only the difference between two transmission signals influence transmission quality and no longer the absolute phase noise of the transmitter oscillators, the best transmission systems can be realized even with the worst phase noise of high frequency or microwave oscillators. This is the reason why this concept opens up the possibility of direct radio transmission of UWB and double digit GBit values in the microwave range. The architecture, the theory, and the very good measurement results of a 2.45 GHz-demonstrator in SMD-technology is presented.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4753828,no
ORTEGA: An Efficient and Flexible Online Fault Tolerance Architecture for Real-Time Control Systems,2008,"Fault tolerance is an important aspect in real-time computing. In real-time control systems, tasks could be faulty due to various reasons. Faulty tasks may compromise the performance and safety of the whole system and even cause disastrous consequences. In this paper, we describe On-demand real-time guard (ORTEGA), a new software fault tolerance architecture for real-time control systems. ORTEGA has high fault coverage and reliability. Compared with existing real-time fault tolerance architectures, such as Simplex, ORTEGA allows more efficient resource utilizations and enhances flexibility. These advantages are achieved through the on-demand detection and recovery of faulty tasks. ORTEGA is applicable to most industrial control applications where both efficient resource usage and high fault coverage are desired.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4753900,no
Influence of Routing Protocol on VoIP Quality Performance in Wireless Mesh Backbone,2008,"In this paper we focus on the following question: how routing protocols, working under different channel conditions, can influence the human perceived quality of VoIP calls in a wireless mesh backbone? In order to respond this question, we propose a study case where we analyze two routing approaches in a 802.11 wireless mesh backbone, namely, reactive AODV and proactive OLSR. We calculated the voice speech quality by making use of a reviewed version of ITU-T E-model proposed in previous work. Results obtained from highly credible stochastic simulation environment, also described in this paper, showed that AODV performs better in very hostile mediums while OLSR presents better results when more friendly mediums take place.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4756471,no
A Distributed Intrusion Detection System Based on Agents,2008,"Due to the rapid growth of the network application, new kinds of network attacks are emerging endlessly. So it is critical to protect the networks from attackers and the intrusion detection technology becomes popular. On the basis of analyzing the defect of a kind of modern distributed intrusion detection system this article proposes a distributed intrusion detection system model based on agents. This system adopts the way which combines static agent and mobile agent, host-based intrusion detection system (IDS) and network-based intrusion detection system. The function of each module in the system is described in detail. The system uses mobile agent for decentralized data collection, data analysis and response, and has certain dynamic learning capability. The self-adapt ability of the system is strong and can solve the main problems of the modern system. Finally, the preliminary implementation of the module in this system like agent is given in detail and the systempsilas performance evaluation is presented.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4756621,no
An Efficient Local Bandwidth Management System for Supporting Video Streaming,2008,"In order to guarantee continuous delivery of video streaming over best-effort (BE) forwarding network, some quality-of-service (QoS) strategies such as RSVP and DiffServ must be used to improve the transmission performance. However, these methods are too difficult to be employed in practical applications since their technical complexity. In this paper, we design and implement an efficient local bandwidth management system to tackle this problem in IPv6 environment. The system monitors local access network and provides assured forwarding (AF) service through controlling the video streaming requests based on available network bandwidth. To assess the benefit of this system, we perform tests to compare its performance with that of conventional BE service. Our test results indicate convincingly that AF offers substantially better performance than BE.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4756767,no
A new clustering approach based on graph partitioning for navigation patterns mining,2008,We present a study of the Web based user navigation patterns mining and propose a novel approach for clustering of user navigation patterns. The approach is based on the graph partitioning for modeling user navigation patterns. For the clustering of user navigation patterns we create an undirected graph based on connectivity between each pair of Web pages and we propose novel formula for assigning weights to edges in such a graph. The experimental results represent that the approach can improve the quality of clustering for user navigation pattern in Web usage mining systems. These results can be use for predicting userpsilas next request in the huge Web sites.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4761808,no
Cost-efficient Automated Visual Inspection system for small manufacturing industries based on SIFT,2008,"This paper presents a cost efficient automated visual inspection (AVI) system for small industriespsila quality control system. The complex hardware and software make current AVI systems too expensive to afford for small-size manufacturing industries. Proposed approach to AVI systems is based on an ordinary PC with a medium resolution camera without any other extra hardware. The scale invariant feature transform (SIFT) is used to acquire good accuracy and make it applicable for different situations with different sample sizes, positions, and illuminations. Proposed method can detect three different defect types as well as locating and measuring defect percentage for more specialized utilization. To evaluate the performance of this system different samples with different sizes, shapes, and complexities are used and the results show that proposed system is highly applicable to different applications and is invariant to noise, illumination changes, rotation, and transformation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4762145,no
A new adaptive hybrid neural network and fuzzy logic based fault classification approach for transmission lines protection,2008,"In this paper, an adaptive hybrid neural networks and fuzzy logic based algorithm is proposed to classify fault types in transmission lines. The proposed method is able to identify all ten shunt faults in transmission lines with high level of robustness against variable conditions such as measured amplitudes and fault resistance. In this approach, a two-end unsynchronized measurement of the signals is used. For real-time estimation of unknown synchronization angle and three phase phasors a two-layer adaptive linear neural (ADALINE) network is used. The estimated parameters are fed to a fuzzy logic system to classify fault types. This method is feasible to be used in digital distance relays which are able to be programmed, to share and discourse data with all protective and monitoring devices. The proposed method is evaluated by a number of simulations conducted in PSCAD/EMTDC and MATLAB software.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4762602,no
Impact of metrics based refactoring on the software quality: a case study,2008,"As the software system changes, the design of the software deteriorates hence reducing the quality of the system. This paper presents a case study in which an inventory application is considered and efforts are made to improve the quality of the system by refactoring. The code is an open source application namely ldquoinventor deluxe v 1.03rdquo, which was first, assessed using the tool Metrics 1.3.6 (an Eclipse plug-in). The code was then refactored and three more versions were built. At the end of creation of every version the code was assessed to find the improvement in the quality. The results obtained after measuring various metrics helped in tracing the spots in the code, which requires further improvement and hence can be refactored. Most of the refactoring was done manually with little tool support. Finally, a trend was found which shows, that average complexity and size of the code reduces with refactoring - based development, which helps to make the software more maintainable. Thus, although refactoring is time consuming and a labor-intensive work, it has a positive impact on the software quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4766459,no
StegoHunter: Passive audio steganalysis using Audio Quality Metrics and its realization through genetic search and X-means approach,2008,"Steganography is used to hide the occurrence of communication. This creates a potential problem when this technology is misused for planning criminal activities. Differentiating anomalous audio document (stego audio) from pure audio document (cover audio) is difficult and tedious. This paper investigates the use of a Genetic-X-means classifier, which distinguishes a pure audio document from the adulterated one. The basic idea is that, the various audio quality metrics (AQMs) calculated on cover audio signals and on stego-audio signals vis-a-vis their denoised versions, are statistically different. Our model employs these AQMs to steganalyse the audio data. Genetic paradigm is exploited to select the AQMs that are sensitive to various embedding techniques. The classifier between cover and stego-files is built using X-means clustering on the selected feature set. The presented method can not only detect the presence of hidden message but also identify the hiding domains. The experimental results show that the combination strategy (Genetic-X-means) can improve the classification precision even with lesser payload compared to the traditional ANN (Back Propagation Network).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4766490,no
Modeling Software Contention Using Colored Petri Nets,2008,"Commercial servers, such as database or application servers, often attempt to improve performance via multi-threading. Improper multi-threading architectures can incur contention, limiting performance improvements. Contention occurs primarily at two levels: (1) blocking on locks shared between threads at the software level and (2) contending for physical resources (such as the cpu or disk) at the hardware level. Given a set of hardware resources and an application design, there is an optimal number of threads that maximizes performance. This paper describes a novel technique we developed to select the optimal number of threads of a target-tracking application using a simulation-based colored Petri nets (CPNs) model. This paper makes two contributions to the performance analysis of multi-threaded applications. First, the paper presents an approach for calibrating a simulation model using training set data to reflect actual performance parameters accurately. Second, the model predictions are validated empirically against the actual application performance and the predicted data is used to compute the optimal configuration of threads in an application to achieve the desired performance. Our results show that predicting performance of application thread characteristics is possible and can be used to optimize performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4770577,no
A performance-correctness explicitly-decoupled architecture,2008,"Optimizing the common case has been an adage in decades of processor design practices. However, as the system complexity and optimization techniquespsila sophistication have increased substantially, maintaining correctness under all situations, however unlikely, is contributing to the necessity of extra conservatism in all layers of the system design. The mounting process, voltage, and temperature variation concerns further add to the conservatism in setting operating parameters. Excessive conservatism in turn hurt performance and efficiency in the common case. However, much of the systempsilas complexity comes from advanced performance features and may not compromise the whole systempsilas functionality and correctness even if some components are imperfect and introduce occasional errors. We propose to separate performance goals from the correctness goal using an explicitly-decoupled architecture. In this paper, we discuss one such incarnation where an independent core serves as an optimistic performance enhancement engine that helps accelerate the correctness-guaranteeing core by passing high-quality predictions and performing accurate prefetching. The lack of concern for correctness in the optimistic core allows us to optimize its execution in a more effective fashion than possible in optimizing a monolithic core with correctness requirements. We show that such a decoupled design allows significant optimization benefits and is much less sensitive to conservatism applied in the correctness domain.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4771800,no
Dynamic Resource Assignment for MC-CDMA with prioritization and QoS constraint,2008,"In this paper, we present a dynamic method for resource assignment by implementing a cross-layer design for Multi-Carrier Code Division Multiple Access (MC-CDMA) systems that adapts user allocation to channel fluctuations. Our approach consists of prioritizing users according to their class of service (CoS), assigning them to the best subchannels based on channel estimates, and allowing redundancy of assignment in case of strong degradation of otherspsila signals. This improves upon the performance of the existing methods by allowing adaptation to the system parameters. This technique has been implemented in software and validated experimentally, bringing about 6 dB gain for a bit error rate (BER) of 10-3 in a multipath fading environment with 16 users for 8 available codes, compared to the classic round robin (RR) assignment.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4773732,no
Implementation of a high level hands-on-training at an experimental PET scanner,2008,"The remarkable increase of medical imaging within the last decade demands the implementation of teaching programs for experts and students. Such courses must achieve both the realization of theoretical basics and an understanding of its implementation within real systems. This accounts for positron emission tomography (PET) as well. PET is, among other techniques like computer tomography (CT) or magnetic resonance imaging (MRI), one of the key technologies for biological and medical imaging. To teach and demonstrate the fundamentals of CT and the physical background of PET as well as to illustrate the signal processing of a coincidence measurement and the data processing of multi-parameter, list-mode data sets, the experimental PET scanner PET-TW05 has been developed. It is a simplified but still fully functional scanner consisting of two commercial BGO block-detectors. They are fixed in opposite to each other and can be moved along a linear axis and rotated around the field of view (FOV). The hardware layout and the software performance of the PET-TW05 scanner support the demonstration of important process steps of PET imaging like (i) system calibration, (ii) detector efficiency measurements, (iii) determination of time and spatial resolution of a PET scanner, (iv) list-mode data acquisition, (v) tomographic reconstruction by means of filtered backprojection, (vi) the effect of different filter kernels as well as of time and energy windows on the image quality, (vii) the influence of scatter on the image quality. Furthermore, the principle of time-of-flight PET can be experimentally demonstrated.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4774129,no
Computer evaluation of a novel multipinhole SPECT system for cardiac imaging,2008,"Multipinhole collimation for clinical myocardial perfusion imaging dates back to the 1970â€™s and 1980â€™s. However, the acceptance and use of pinhole collimator technique for application in single photon emission computed tomography (SPECT) was impeded by wide-spread availability of use for rotational dual-head cameras for general purpose imaging. Today virtually all SPECT imaging is performed with a parallel-hole collimator. In the past few years, experimental small animal SPECT systems have seen progress with pinhole collimation. There has also been an interest in designing dedicated cardiac SPECT systems for improved sensitivity to reduce imaging time. This has renewed clinical interest in multipinhole radionuclide cardiac imaging. It is known that pinhole collimation surpasses parallel-hole collimation in both sensitivity and resolution for small objects located close to the pinhole aperture. However, the short object-to-detector distance limits the size of the field of view (FOV) for the pinhole collimator, which causes part of the emission rays to miss the detector. The lost data are said to be truncated from the projection measurements. The objective of this work is to investigate the effect of truncation and limited angular sampling on image quality for a novel cardiac imaging system with stationary multipinhole collimation. Computer simulations show that the system produces high quality images of the myocardium comparable to that of a rotating multipinhole detector system. Improved quantitation for truncation of the interior problem is accomplished by calibrating the system matrix using a uniform phantom in the FOV of known activity.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4774167,no
Optimized multipinhole design for mouse imaging,2008,"To enhance high-sensitivity focused mouse imaging using multipinhole SPECT on a dual head camera, a fast analytical method was used to predict the contrast-to-noise ratio (CNR) in many points of a homogeneous cylinder for a large number of pinhole collimator designs with modest overlap. The design providing the best overall CNR, a configuration with 7 pinholes, was selected. Next, the pinhole pattern was made slightly irregular to reduce multiplexing artifacts. Two identical, but mirrored 7-pinhole plates were manufactured. In addition, the calibration procedure was refined to cope with small deviations of the camera from circular motion. First, the new plates were tested by reconstructing a simulated homogeneous cylinder measurement. Second, a Jaszczak phantom filled with 37 MBq <sup>99m</sup>Tc was imaged on a dual head gamma camera, equipped with the new pinhole collimators. The image quality before and after refined calibration was compared for both heads, reconstructed separately and together. Next, 20 short scans of the same phantom were performed with single and multipinhole collimation to investigate the noise improvement of the new design. Finally, two normal mice were scanned using the new multipinhole designs to illustrate the reachable image quality of abdomen and thyroid imaging. The simulation study indicated that the irregular patterns suppress most multiplexing artifacts. Using body support information strongly reduces the remaining multiplexing artifacts. Refined calibration improved the spatial resolution. Depending on the location in the phantom, the CNR increased with a factor of 1 to 2.5 using the new instead of a single pinhole design. The first proof of principle scans and reconstructions were successful, allowing the release of the new plates and software for preclinical studies in mice.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4774304,no
Alignment and monitoring of ATLAS Muon Spectrometer,2008,"The ATLAS Muon Spectrometer is designed to measure muons from 14 TeV p-p interactions over a wide transverse momentum (p<inf>T</inf> ) range with accurate momentum resolution, taking into account the high background environment, the inhomogeneous magnetic field, and the large size of the apparatus. At high p<inf>T</inf> one of the main contributions to the resolution is the uncertainty on the Spectrometer alignment. The design resolution requires accurate control of the position and deformation of muon chambers, induced by mechanical stress and thermal gradients. Corrections provided by an optical alignment system, as well as track-based alignment procedures, are integrated in the software leading to track reconstruction and momentum measurement. We present here this optical system and the alignment procedures, along with recent results on muon reconstruction quality improvements obtained after feeding the software with alignment corrections. These results are obtained on muons from cosmic rays, which are currently being used to finalize detector commissioning. To that purpose, and also for future running of ATLAS with LHC collisions, tools to monitor the status of the Muon Spectrometer and determine the quality of the data are being developed. The goal is to spot problems during data taking and flag the data accordingly. Careful monitoring is important at the beginning of an experiment where the environment is new and requires some experience to fully comprehend; such tools can help determine problems quickly and solve them. The different steps of offline monitoring are presented here with examples.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4774735,no
The ALICE Dimuon Spectrometer High Level Trigger,2008,"The ALICE Dimuon Spectrometer High Level Trigger (dHLT) is an on-line processing stage whose primary function is to select interesting events that contain distinct physics signals from heavy resonance decays such as J/Ïˆ and Ï’ particles, amidst unwanted background events. It forms part of the High Level Trigger of the ALICE experiment, whose goal is to reduce the large data rate of about 25 GB/s from the ALICE detectors by an order of magnitude, without loosing interesting physics events. The dHLT has been implemented as a software trigger within a high performance and fault tolerant data transportation framework, which is run on a large cluster of commodity compute nodes. To reach the required processing speeds, the system is built as a concurrent system with a hierarchy of processing steps. The main algorithms perform partial event reconstruction, starting with hit reconstruction on the level of the raw data received from the spectrometer. Then a tracking algorithm finds track candidates from the reconstructed hit points. Physical parameters such as momentum are extracted from the track candidates and finally a dHLT decision is made to readout the event based on certain trigger criteria. Various simulations and commissioning tests have shown that the dHLT can expect a background rejection factor of at least 5 compared to hardware triggering alone, with little impact on the signal detection efficiency.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4774751,no
Data quality monitor of the muon spectrometer tracking detectors of the ATLAS experiment at the Large Hadron Collider: First experience with cosmic rays,2008,"The Muon Spectrometer of the ATLAS experiment at the CERN Large Hadron Collider is completely installed and many data have been collected with cosmic rays in different trigger configurations. In the barrel part of the spectrometer, cosmic ray muons are triggered with Resistive Plate Chambers, RPC, and tracks are obtained joining segments reconstructed in three measurement stations equipped with arrays of high-pressure drift tubes, MDT. The data are used to validate the software tools for the data extraction, to assess the quality of the drift tubes response and to test the performance of the tracking programs. We present a first survey of the MDT data quality based on large samples of cosmic ray data selected by the second level processors for the calibration stream. This data stream was set up to provide high statistics needed for the continuous monitor and calibration of the drift tubes response. Track segments in each measurement station are used to define quality criteria and to assess the overall performance of the MDT detectors. Though these data were taken in not optimized conditions, when the gas temperature and pressure was not stabilized, the analysis of track segments shows that the MDT detector system works properly and indicates that the efficiency and space resolution are in line with the results obtained with previous tests with a high energy muon beam.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4774959,no
System tests of the LHCb RICH detectors in a charged particle beam,2008,"The RICH detectors of the LHCb experiment will provide efficient particle identification over the momentum range 1â€?00 GeVc. Results are presented from a beam test of the LHCb RICH system using final production pixel Hybrid Photon Detectors, the final readout electronics and an adapted version of LHCb RICH reconstruction software. Measurements of the photon yields and Cherenkov angle resolutions for both nitrogen and C<inf>4</inf>F<inf>10</inf> radiators agree well with full simulations. The quality of the data and the results obtained demonstrate that all aspects meet the stringent physics requirements of the experiment are now ready for first data.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4775045,no
"Surveillance of nuclear threats using multiple, autonomous detection units",2008,"A detection systems is presented that involves multiple autonomous detectors, which are merged to a net like structure. Each unit is equipped with a stabilized and linearized multichannel analyzer for the measurement of gamma spectra and optionally a He<sup>3</sup> neutron detector. The flexible hard- and software architecture allows the simple integration of a variable number of detectors, even with different volumes. With this technique, spectroscopical screening of large areas, e.g. airports or comparably sensitive objects, can be established by an individual combination of these detectors. The general outline of the nuclide identification algorithm is presented. In the context of medical isotopes, the importance of correct reference data that incorporates the scattering effects of human bodies is clarified. A special focus is drawn to the localization of nuclear sources. The model, corresponding simulations and its applications are depicted. Different tests and verification measures are described that refer to the nuclide identification and the source localization as well as to the spectral quality of the detectors. A finished real application is presented that combines the tested algorithms and has proven its reliability on different occasions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4775056,no
The ATLAS conditions database model for the Muon Spectrometer,2008,"The Muon System has extensively started to use the LCG conditions database project â€˜COOLâ€?as the basis for all its conditions data storage both at CERN and throughout the worldwide collaboration as decided by the ATLAS Collaboration. The management of the Muon COOL conditions database will be one of the most challenging applications for Muon System, both in terms of data volumes and rates, but also in terms of the variety of data stored. The Muon Conditions database is responsible for almost of all the â€˜non-eventâ€?data and detector quality flags storage needed for debugging of the detector operations and for performing reconstruction and analysis. The COOL database allows database applications to be written independently of the underlying database technology and ensures long-term compatibility with entire ATLAS Software. COOL implements an interval of validity database, i.e. objects stored or referenced in COOL have an associated start and end time between which they are valid, the data is stored in folders, which are themselves arranged in a hierarchical structure of folder sets. The structure is simple and mainly optimized to store and retrieve object(s) associated to a particular time. In this work, an overview of the entire Muon Conditions Database architecture is given, including the different sources of the data and the storage model used. In addiction the software interfaces used to access to the Conditions Data are described, more emphasis is given to the Offline Reconstruction framework ATHENA and the services developed to provide the Conditions data to the reconstruction.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4775106,no
Software quality prediction techniques: A comparative analysis,2008,"There are many software quality prediction techniques available in literature to predict software quality. However, literature lacks a comprehensive study to evaluate and compare various prediction methodologies so that quality professionals may select an appropriate predictor. To find a technique which performs better in general is an undecidable problem because behavior of a predictor also depends on many other specific factors like problem domain, nature of dataset, uncertainty in the available data etc. We have conducted an empirical survey of various software quality prediction techniques and compared their performance in terms of various evaluation metrics. In this paper, we have presented comparison of 30 techniques on two standard datasets.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4777508,no
A meta-measurement approach for software test processes,2008,"Existing test process assessment and improvement models intend to raise maturity of an organization with reference to testing activities. Such process assessments are based on ldquowhatrdquo testing activities are being carried out, and thus implicitly evaluate process quality. Other test process measurement techniques attempt to directly assess some partial quality attribute such as efficiency or effectiveness some test metrics. There is a need for a formalized method of test process quality evaluation that addresses both implicitly and partially of these current evaluations. This paper describes a conceptual framework to specify and explicitly evaluate test process quality aspects. The framework enables provision of evaluation results in the form of objective assessments, and problem-area identification to improve the software testing processes.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4777759,no
prediction of fault count data using genetic programming,2008,"Software reliability growth modeling helps in deciding project release time and managing project resources. A large number of such models have been presented in the past. Due to the existence of many models, the models' inherent complexity, and their accompanying assumptions; the selection of suitable models becomes a challenging task. This paper presents empirical results of using genetic programming (GP) for modeling software reliability growth based on weekly fault count data of three different industrial projects. The goodness of fit (adaptability) and predictive accuracy of the evolved model is measured using five different measures in an attempt to present a fair evaluation. The results show that the GP evolved model has statistically significant goodness of fit and predictive accuracy.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4777762,no
Global Sensitivity Analysis (GSA) Measures the Quality of Parameter Estimation. Case of Soil Parameter Estimation with a Crop Model,2008,"The behavior of crops can be accurately predicted when all the parameters of the crop model are well known, and assimilating data observed on crop status in the model is one way of estimating parameters. Nevertheless, the quality of the estimation depends on the sensitivity of model output variables to the parameters. In this paper, we quantify the link between the global sensitivity analysis (GSA) of the soil parameters of the mechanistic crop model STICS, and the ability to retrieve the true values of these parameters. The Global sensitivity indices were computed by a variance based method (Extended FAST) and the quality of parameter estimation (RRMSE) was computed with an importance sampling method based on Bayes theory (GLUE). Criteria based on GSA were built to link GSA indices with the quality of parameters estimation. The result shows that the higher the criteria, the better the quality of parameters estimation and GSA appeared to be useful to interpret and predict the performance of the estimation parameters process.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4779531,no
Online Optimization in Application Admission Control for Service Oriented Systems,2008,"In a service oriented environment an application is created through the composition of different service components. In this paper, we investigate the problem of application admission control in a service oriented environment. We propose an admission control system that makes admission decisions using an online optimization approach. The core part of the proposed system is an online optimization algorithm that solves a binary integer programming problem which we formulate in this paper. An online optimizer maximizes the system revenue given the system's available resources as well as the system's previous commitments. Another part of the proposed system carries out a feasibility evaluation that is intended to guarantee an agreed level of probability of success for each admitted application instance. We use simulations and performance comparisons to show that the proposed application admission control system can improve the system revenue while guaranteeing the required level of quality of service.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4780721,no
Timing Failures Detection in Web Services,2008,"Current business critical environments increasingly rely on SOA standards to execute business operations. These operations are frequently based on Web service compositions that use several Web services over the internet and have to fulfill specific timing constraints. In these environments, an operation that does not conclude in due time may have a high cost as it can easily turn into service abandonment with financial and prestige losses to the service provider. In fact, at certain points, carrying on with the execution of an operation may be useless as a timely response will be impossible to obtain. This paper proposes a time-aware programming model for Web services that provides transparent timing failure detection. The paper illustrates the proposed model using a set of services specified by the TPC-App performance benchmark.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4780733,no
A Stochastic Performance Model Supporting Time and Non-time QoS Matrices for Web Service Composition,2008,"In recently years, Web service composition becomes a new approach to overcome many difficult problems confronted by B2B e-commerce, inter-organization workflow management, enterprise application integration etc. Due to the uncertainty of the Internet and various Web services, the performance of the composed Web service can not be ensured. How to model and predict the performance of the composed Web service is a difficult problem in the Web service composition. A novel simulation model that can model and simulate time and non-time performance characters, called STPM<sup>+</sup>, is presented in this paper. Based on Petri net, the STPM<sup>+</sup> model can simulate and predict multiple performance characters, such as the cost, the reliability and the reputation of the composed Web service etc. To examine the validation of the STMP<sup>+</sup> model, a visual performance evaluation tool, called VisualWSCPE, has been implemented. Besides, some simulation experiments have been fulfilled based on VisualWSCPE. The experiment results demonstrate the feasibility and efficiency of the STPM<sup>+</sup> model.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4780808,no
Service Availability of Systems with Failure Prevention,2008,"In this paper we present a queueing model for service availability formulated as a Petri net. We define a metric for service availability and show how it can be estimated from the model. Assuming that a failure avoidance mechanism is present, we analyze the distribution of time-to-failure. Finally, we show in experiments how service availability depends on queue length, utilization and failure prevention and how it relates to steady-state system availability.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4780846,no
Time Sensitive Ranking with Application to Publication Search,2008,"Link-based ranking has contributed significantly to the success of Web search. PageRank and HITS are the best known link-based ranking algorithms. These algorithms do not consider an important dimension, the temporal dimension. They favor older pages because these pages have many in-links accumulated over time. Bringing new and quality pages to the users is important because most users want the latest information. Existing remedies to PageRank are mostly heuristic approaches. This paper investigates the temporal aspect of ranking with application to publication search, and proposes a principled method based on the stationary probability distribution of the Markov chain. The proposed techniques are evaluated empirically using a large collection of high energy particle physics publication. The results show that the proposed methods are highly effective.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4781197,no
Towards Process Rebuilding for Composite Web Services in Pervasive Computing,2008,"The emerging paradigm of pervasive computing and web services needs a flexible service discovery and composition infrastructure. A composite Web service is essential a process in a loosely-coupled service-oriented architecture. It is usually a black box for service requestors and only its interfaces can be seen externally. In some scenarios, to conduct performance debugging and analysis, a workflow representation of the underlying process is required. This paper describes a method to discover such underlying processes from execution logs. Based on a probabilistic assumption model, the algorithm can discover sequential, parallel, exclusive choice and iterative structures. Some examples are given to illustrate the algorithm.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4783614,no
Service Process Composition with QoS and Monitoring Agent Cost Parameters,2008,"Service-oriented architecture (SOA) provides a flexible paradigm to dynamically compose service processes from individual services. The flexibility, on the other hand, makes it necessary to monitor and manage service behaviors at runtime for performance assurance. One solution is to deploy software monitoring agents. In this paper, we present an approach to consider agent cost at process composition by selecting services with lower monitoring costs. We propose two algorithms to perform service selection with agent cost. IGA is able to achieve efficient service selection and bounded agent cost by modeling the problem as the weighted set covering (WSC) problem. We also study a heuristic algorithm to estimate the agent cost as part of the service utility function. The execution time of heuristic algorithm is shorter than IGA. On the other hand, the agent cost may not be consistently minimized using heuristic algorithms.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4785080,no
The Design and Fabrication of a Full Field Quantitative Mammographic Phantom,2008,"Breast cancer is among the leading causes of death in women worldwide. Screen-film mammography (SFM) is still the standard method used to detect early breast cancer thus leading to early treatment. Digital mammography (DM) has recently been designated as the imaging technology with the greatest potential for improving the diagnosis of breast cancer. For successful mammography, high quality images must be achieved and maintained, and reproducible quantitative quality control (QC) testing is thus required. Assessing images of known reference phantoms is one accepted method of doing QC testing. Quantitative QC techniques are useful for the long-term follow-up of mammographic quality. Following a comprehensive critical evaluation of available mammography phantoms, it was concluded that a more suitable phantom for DM could be designed. A new relatively inexpensive Applied Physics Group (APG) phantom was designed to be fast and easy to use, to provide the user with quantitative and qualitative measures of high and low contrast resolution over the full field of view and to demonstrate any geometric distortions. It was designed to cover the entire image receptor so as to assess the heel effect, and to be suitable for both SFM and DM. The APG phantom was designed and fabricated with embedded test objects and software routines were developed to provide a complete toolkit for SFM and DM QC. The test objects were investigated before embedding them.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4786058,no
Incorporating varying requirement priorities and costs in test case prioritization for new and regression testing,2008,"Test case prioritization schedules the test cases in an order that increases the effectiveness in achieving some performance goals. One of the most important performance goals is the rate of fault detection. Test cases should run in an order that increases the possibility of fault detection and also that detects the most severe faults at the earliest in its testing life cycle. Test case prioritization techniques have proved to be beneficial for improving regression testing activities. While code coverage based prioritization techniques are found to be taken by most scholars, test case prioritization based on requirements in a cost effective manner has not been taken for study so far. Hence, in this paper, we propose to put forth a model for system level test case prioritization (TCP) from software requirement specification to improve user satisfaction with quality software that can also be cost effective and to improve the rate of severe fault detection. The proposed model prioritizes the system test cases based on the six factors: customer priority, changes in requirement, implementation complexity, usability, application flow and fault impact. The proposed prioritization technique is experimented in three phases with student projects and two sets of industrial projects and the results show convincingly that the proposed prioritization technique improves the rate of severe fault detection.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4787662,no
A new robust distributed real time scheduling services for RT-CORBA applications,2008,"Distributed computing environment is flexible to control in complex embedded systems and their software components gain complexity when these systems are equipped with many microcontrollers and software object which covers diverse platforms and electronic control units connecting hundreds or thousands of analog and digital sensors and actuators, the simply known as DRE system. These DRE systems need new inter-object communication solution thus QoS-enabled middleware services and mechanisms have begun to emerge. It is also, widely known that scheduling services are used in real-time. We show how high performance scheduling services over the real time distributed environment can be achieved, when adopting the use of RT-CORBA specification which implements a dynamic scheduling policy to achieve end-to-end predictability and performance through novel dynamic scheduling service.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4787672,no
Data collection and analysis for the reliability prediction and estimation of a safety critical system using AIRS,2008,"Software is an essential part of many critical and non-critical applications, and virtually any industry is dependent on computers for their basic functioning. Techniques to measure and ensure reliability of hardware have seen rapid advances, leaving software as the bottleneck in achieving overall system reliability. Hence there rises a situation for the developers to develop high quality software. Having that in mind it is necessary to provide the reliability of the software to the developers before it is shipped. This can be achieved based on immune system. The immune system classifies the corrected data and error, also the error date are further classified into four different data sets. Then the error rate is measured from the data sets. This will produce high quality, reliable results over a wide variety of problems compared to a range of other approaches, without the need of expert fine-tuning.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4787675,no
Detecting spurious features using parity space,2008,"Detection of spurious features is instrumental in many computer vision applications. The standard approach is feature based, where extracted features are matched between the image frames. This approach requires only vision, but is computer intensive and not yet suitable for real-time applications. We propose an alternative based on algorithms from the statistical fault detection literature. It is based on image data and an inertial measurement unit (IMU). The principle of analytical redundancy is applied to batches of measurements from a sliding time window. The resulting algorithm is fast and scalable, and requires only feature positions as inputs from the computer vision system. It is also pointed out that the algorithm can be extended to also detect non-stationary features (moving targets for instance). The algorithm is applied to real data from an unmanned aerial vehicle in a navigation application.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4795545,no
Introducing Support for Release Planning of Quality Requirements â€?An Industrial Evaluation of the QUPER Model,2008,"In market-driven product development and release planning, it is important to market success to find the right balance among competing quality requirements. To address this issue, a conceptual model that incorporates quality as a dimension in addition to the cost and value dimensions used in prioritisation approaches for functional requirements has been developed. In this paper, we present an industrial evaluation of the model. The results indicate that the quality performance model provides helpful information about quality requirements in release planning. All subjects stated that the most difficult estimations may be more accurate by using the quality performance model.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4797424,no
Comparison of the acoustic response of attached and unattached BiSphereâ„?microbubbles,2008,"Two systems that independently allow the investigation of the response of individual unattached and attached microbubbles have previously been described. Both offered methods of studying the acoustic response of single microbubbles in well defined acoustic fields. The aim of the work described here was to investigate the responses of single attached microbubbles for a range of acoustic pressures and to compare these to the backscatter from unattached single microbubbles subjected to the same acoustic fields. Single attached BiSphere<sup>TM</sup> (Point Biomedical) microbubbles were attached to polyester with poly-L-lysine. Individual attached microbubbles were insonated at 1.6 MHz for acoustic pressures ranging from 300 to 1000 kPa using a Sonos5500 (Philips Medical Systems) research ultrasound scanner. Each microbubble was aligned to 6 cycle pulse, M-mode ultrasound beams, and unprocessed backscattered RF data captured using proprietary hardware and software. The backscatter from these microbubbles was compared to that of single unattached microbubbles subjected to the same acoustic parameters, microbubbles were insonated several times to determine possible differences in rate of decrease of backscatter between attached and unattached microbubbles. In total over 100 single attached microbubbles have been insonated. At 550kPa an acoustic signal was detected for 20 % of the attached microbubbles and at 1000 kPa for 63%. At acoustic pressures of 300kPa no signal was detected. Mean RMS fundamental pressure from attached and unattached microbubbles insonated at 800 kPa was 9.7 Pa and 8.7 Pa respectively. The ratio between the first two backscattered pulses decreased with increasing pressure. However, for unattached microbubbles the magnitude of the ratio was less than that of attached (at 550kPa mean ratio attached: 0.92 + 0.1, unattached: 0.28 + 0.2). There was no significant difference in the peak amplitude of the backscattered signal for unattached and attached micro- - bubbles. BiSphere<sup>TM</sup> microbubbles comprise an internal polymer shell with an albumin coating, resulting in a stiff shell. BiSphere<sup>TM</sup> microbubbles do not oscillate in the same manner as a softer shelled microbubble, but allow gas leakage which then performs free bubble oscillations. The results here agree with previous acoustic and optical microscopy measurements which show that a proportion of microbubbles will scatter and this number increases with acoustic pressure. The lack of difference in scatter between the unattached and attached microbubbles may be attributed to the free microbubble oscillation being in the vicinity of the stiff shell, which may provide the same motion damping to a wall. Second pulse exposure shows that the wall becomes important in the survival of the free gas. These high quality measurements can be further improved by incorporating microbubble sizing to increase the specificity of the comparisons between unattached and attached microbubbles.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4803287,no
Cardiac monitoring using transducers attached directly to the heart,2008,"Cardiac ultrasound systems deliver excellent information about the heart, but are constructed for intermittent imaging and interpretation by a skilled operator. This paper presents a dedicated ultrasound system to monitor cardiac function continuously during and after cardiac surgery. The system uses miniature 10 MHz transducers sutured directly to the heart surface. M-mode images give a visual interpretation of the contraction pattern, while tissue velocity curves give detailed quantitative information. The ultrasound measurements are supported by synchronous ECG and pressure recordings. The system has been tested on pigs, demonstrating M-mode and tissue velocity measurements of good quality. When occluding the LAD coronary artery, the system detected changes in contraction pattern that correspond with known markers of ischemia. The system uses dedicated analog electronics and a PC with digitizers and LabVIEW software, and may also be useful in other experimental ultrasound applications.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4803609,no
The PIRR Methodology to Estimate Resource Requirements for Distributed NM Applications in Mobile Networks,2008,"The current centralized Network Management approach in mobile networks may lead to problems with being able to scale up when new customer services and network nodes will be deployed in the network. The next generation mobile networks proposed by 3rd Generation Partnership Project (3GPP) envision a flat network architecture comprising some thousands of network nodes, each of which will need to be managed, in a timely and efficient fashion. This consideration has prompted a research activity to find alternative application architectures that could overcome the limits of the current centralized approaches.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4806887,no
Performance Analysis of SER in DVB-H by using Reed-Solomon Code with Erasure and Non Erasure Technique,2008,"Digital video broadcasting-handheld (DVB-H) is based on the earlier standard DVB-T, which is used for terrestrial TV broadcasting. This new standard brings features that make it possible to receive digital video broadcast of DVD quality video and sound in handheld devices, it also offers reliable high data rate reception for mobile & battery powered devices, The paper discusses and compares the key technology elements 4 K and 2 K modes, in-depth interleavers, time slicing and multi protocol encapsulation-forward error correction (MPE-FEC). In addition, extensive range of SER performance results on software based simulations is provided. The result suggest that by using an erasure decoding method with the Ree -Solomon code & cyclic redundancy check error detection as the link layer forward error correction, the strength of the signal is much higher while error detection & correction becomes doubles.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4810227,no
Application of kernel learning vector quantization to novelty detection,2008,"In this paper, we focus on kernel learning vector quantization (KLVQ) for handling novelty detection. The two key issues are addressed: the existing KLVQ methods are reviewed and revisited, while the reformulated KLVQ is applied to tackle novelty detection problems. Although the calculation of kernelising the learning vector quantization (LVQ) may add an extra computational cost, the proposed method exhibits better performance over the LVQ. The numerical study on one synthetic data set confirms the benefit in using the proposed KLVQ.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4811315,no
ARTOO,2008,"Intuition is often not a good guide to know which testing strategies will work best. There is no substitute for experimental analysis based on objective criteria: how many faults a strategy finds, and how fast. ""Random"" testing is an example of an idea that intuitively seems simplistic or even dumb, but when assessed through such criteria can yield better results than seemingly smarter strategies. The efficiency of random testing is improved if the generated inputs are evenly spread across the input domain. This is the idea of adaptive random testing (ART). ART was initially proposed for numerical inputs, on which a notion of distance is immediately available. To extend the ideas to the testing of object-oriented software, we have developed a notion of distance between objects and a new testing strategy called ARTOO, which selects as inputs objects that have the highest average distance to those already used as test inputs. ARTOO has been implemented as part of a tool for automated testing of object-oriented software. We present the ARTOO concepts, their implementation, and a set of experimental results of its application. Analysis of the results shows in particular that, compared to a directed random strategy, ARTOO reduces the number of tests generated until the first fault is found, in some cases by as much as two orders of magnitude. ARTOO also uncovers faults that the random strategy does not find in the time allotted, and its performance is more predictable.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4814118,no
The effect of the number of inspectors on the defect estimates produced by capture-recapture models,2008,"Inspections can be made more cost-effective by using capture-recapture methods to estimate post-inspection defects. Previous capture-recapture studies of inspections used relatively small data sets compared with those used in biology and wildlife research (the origin of the models). A common belief is that capture-recapture models underestimate the number of defects but their performance can be improved with data from more inspectors. This increase has not been evaluated in detail. This paper evaluates new estimators from biology not been previously applied to inspections. Using a data from seventy-three inspectors, we analyze the effect of the number of inspectors on the quality of estimates. Contrary to previous findings indicating that Jackknife is the best estimator, our results show that the SC estimators are better suited to software inspections. Our results also provide a detailed analysis of the number of inspectors necessary to obtain estimates within 5% to 20% of the actual.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4814144,no
Interval Quality: Relating Customer-Perceived Quality to Process Quality,2008,"We investigate relationships among software quality measures commonly used to assess the value of a technology, and several aspects of customer perceived quality measured by interval quality (IQ): a novel measure of the probability that a customer will observe a failure within a certain interval after software release. We integrate information from development and customer support systems to compare defect density measures and IQ for six releases of a major telecommunications system. We find a surprising negative relationship between the traditional defect density and IQ. The four years of use in several large telecommunication products demonstrates how a software organization can control customer perceived quality not just during development and verification, but also during deployment by changing the release rate strategy and by increasing the resources to correct field problems rapidly. Such adaptive behavior can compensate for the variations in defect density between major and minor releases.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4814186,no
Water Quality Measurement using Transmittance and 90Ã‚Â° Scattering Techniques through Optical Fiber Sensor,2008,This research is the continuance of the previous developed water quality fiber sensor by Omar et al (2007). The application of spectroscopy analysis is the major tasks established in the measurement of the capacity of total suspended solids (TSS) in water sample in the unit of mg/L. The interaction between the incident light and the turbid water sample will leads the light to be either transmitted through the water sample or absorbed and scattered by water molecules and TSS particles. The resultant light produced after the interaction will be collected by plastic optical fiber located at 180deg (transmittance measurement technique) and 90deg (90deg scattering measurement technique) from the incident light. The measurement is done through BLUE and RED Systems with peak response of emitter and detector at 470 nm and 635 nm respectively. Signal interpretation and data display is established through Basic Stamp 2 microcontroller. Transmittance measurement technique has a capability to sense the amount of solids suspended in water as low as 20 mg/L. 90deg measurement technique has a capability to sense the amount of solids suspended in water as low as 10 mg/L. Both measurement techniques recorded a very good linear correlation coefficient and low standard error.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4814227,no
A framework for interoperability analysis on the semantic web using architecture models,2008,IT decision making requires analysis of possible future scenarios. The quality of the decisions can be enhanced by the use of architecture models that increase the understanding of the components of the system scenario. It is desirable that the created models support the needed analysis effectively since creation of architecture models often is a demanding and time consuming task. This paper suggests a framework for assessing interoperability on the systems communicating over the semantic web as well as a metamodel suitable for this assessment. Extended influence diagrams are used in the framework to capture the relations between various interoperability factors and enable aggregation of these into a holistic interoperability measure. The paper is concluded with an example using the framework and metamodel to create models and perform interoperability analysis.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4815020,no
Improvement model for collaborative networked organizations,2008,Small and medium enterprises (SMEs) have huge improvement potential both in domain and in collaboration/interoperability capabilities. Before implementing respective improvement measures it's necessary to assess the performance in specific process areas which we divide in domain (e.g. tourism) and collaboration oriented ones. Both in enterprise collaboration (EC) and in enterprise interoperability (EI) the behavior of organizations regarding interoperability must be improved.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4815027,no
Inaccuracies of input data relevant for PV yield prediction,2008,"Accuracy of the PV yield prediction process, including meteorological data (direct and diffuse irradiance with its actual spectral composition and spatial distribution), material properties of encapsulation (refractive indices, absorption coefficients, thermal properties), parameters relevant for heat transfer, PV conversion parameters of the cell (temperature coefficients, spectral response, weak light performance, degradation) considerably depends on the quality of the input data applied (derived from literature, data sheets, norms, software tools, or own measurements). The contribution gives an overview of the processes involved, the relevant parameters, the accuracy achievable and the impact on yield prediction.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4922866,no
Identifying factors influencing reliability of professional systems,2008,"Modern product development strategies call for a more proactive approach to fight intense global competition in terms of technological innovation, shorter time to market, quality and reliability and accommodative price. From a reliability engineering perspective, development managers would like to estimate as early as possible how reliably the product is going to behave in the field, so they can then focus on system reliability improvement. To steer such a reliability driven development process, one of the important aspects in predicting the reliability behavior of a new product, is to know the factors that may influence its field performance. In this paper, two methods are proposed for identifying reliability factors and their significance in influencing the reliability of the product.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4925771,no
A Deterministic Methodology for Identifying Functionally Untestable Path-Delay Faults in Microprocessor Cores,2008,"Delay testing is crucial for most microprocessors. Software-based self-test (SBST) methodologies are appealing, but devising effective test programs addressing the true functionally testable paths and assessing their actual coverage are complex tasks. In this paper, we propose a deterministic methodology, based on the analysis of the processor instruction set architecture, for determining rules arbitrating the functional testability of path-delay faults in the data path and control unit of processor cores. Moreover, the performed analysis gives guidelines for generating test programs. A case study on a widely used 8-bit microprocessor is provided.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5070942,no
Research and Design of Intelligent Wireless Power Parameter Detection System,2008,"To assure electric power system and used in safe state, we urgent need to monitor, analyze, and measure various electric power parameter, which can provide rectifying and reforming scheme effectively, strength preventive measures, and assure electric power operation in safety, reliable, and economic. So, electric parameter monitoring technology emerges. Aiming to disadvantages of present electric parameter monitoring, we design a kind of wireless power electric parameter detection system based on single chip microprocessor (SCM) technology, wireless communication technology, and visual instrument (VI) technology, etc. System is composed of upper PC and lower PC, lower PC module acts SCM as core of processor, and uses voltage/currency transformer module, signal processor module to realize electric network signal conversion, which can be recognized by MSP430FI49 and completes signal A/D transfer by itself ADC12 module, as well as controls nRF905 wireless transceiver module to realize signal transmitting between lower PC and upper PC's wireless receiving module in real time. Wireless receiving module is connected with Upper PC installing of LabVIEW through serial interface RS-232, which power electric parameter detection system is carried out. This paper mainly introduces system's structure, part of hardware and software design in detail.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5076727,no
A Method of Multimedia Software Reliability Test Based on Software Partial Quality Measurement,2008,"According to the characteristics of multimedia software this paper presents an approach about how to use the software partial quality measurement and reliability prediction to adjust the test allocation of software reliability test which is based on the original operational profile. In this new approach, software reliability test is not only based on the operational profile but also guided by the prediction result of software partial quality measurement. The adjustable range is decided by the operation independence factor which can be gotten by neural network learning algorithm and is different from software to software.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5089223,no
Modeling and Analysis for Educational Software Quality Hierarchy Triangle,2008,"The area of educational software evaluation has been increasingly muddled because of a lack of consensus among software evaluators. This article presents the classification and evaluation granularity of educational software, proposes an educational software quality hierarchy triangle model based on some major software quality models, analyzes the signification and relativity of the factors. At last, it indicates the primary analysis for the model formalization.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5163782,no
A Quantitative Approach to eMM,2008,"eMM (e-learning maturity model) is used for evaluating the capability and maturity of an institution engaged in e-learning. Based on the original eMM, we suggest a quantitative approach , which can set up an eMM process meta-model as a frame of reference to process improvement, use process capability baseline to metric process capability, make quality estimation and project management, while use SPC to control itspsila continuous process improvement.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5163795,no
Evaluation of Test Criteria for Space Application Software Modeling in Statecharts,2008,"Several papers have addressed the problem of knowing which software test criteria are better than others with respect to parameters such as cost, efficiency and strength. This paper presents an empirical evaluation in terms of cost and efficiency for one test method for finite state machines, switch cover, and two test criteria of the statechart coverage criteria family, all-transitions and all-simple-paths, for a reactive system of a space application. Mutation analysis was used to evaluate efficiency in terms of killed mutants. The results show that the two criteria and the method presented the same efficiency but all-simple-paths presented a better cost because its test suite is smaller than the one generated by switch cover. Besides, test suite due to the all-simple-paths criterion killed the mutants faster than the other test suites meaning that it might be able to detect faults in the software more quickly than the other criteria.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5172617,no
Connector-Driven Gradual and Dynamic Software Assembly Evolution,2008,"Complex and long-lived software need to be upgraded at runtime. Replacing a software component with a newer version is the basic evolution operation that has to be supported. It is error-prone as it is difficult to guarantee the preservation of functionalities and quality. Few existing work on ADLs fully support a component replacement process from its description to its test and validation. The main idea of this work is to have software architecture evolution dynamically driven by connectors (the software glue between components). It proposes a connector model which autonomically handle the reconfiguration of connections in architectures in order to support the versioning of components in a gradual, transparent and testable manner. Hence, the system has the choice to commit the evolution after a successful test phase of the software or rollback to the previous state.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5172649,no
On Preprocessing Multi-channel Sensor Data for Online Process Monitoring,2008,This paper discusses online monitoring production processes based on multi-channel sensor data. Particularly the problem of transient and anomaly detection is addressed for which a processing framework consisting of a preprocessing module and a reasoning engine is outlined. While there is much theory available in the literature for the reasoning engine this is not the case for the preprocessing which massively depends on the physical interpretation and semantics of the data. The paper addresses these problems and proposes new normalization concepts based on regularization especially for making transients of multi-channel data comparable and adequate for further processing by a reasoning engine. A proof of concept is demonstrated by means of real data from an injection moulding process.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5172661,no
Mining Bug Repositories--A Quality Assessment,2008,"The process of evaluating, classifying, and assigning bugs to programmers is a difficult and time consuming task which greatly depends on the quality of the bug report itself. It has been shown that the quality of reports originating from bug trackers or ticketing systems can vary significantly. In this research, we apply information retrieval (IR) and natural language processing (NLP) techniques for mining bug repositories. We focus particularly on measuring the quality of the free form descriptions submitted as part of bug reports used by open source bug trackers. Properties of natural language influencing the report quality are automatically identified and applied as part of a classification task. The results from the automated quality assessment are used to populate and enrich our existing software engineering ontology to support a further analysis of the quality and maturity of bug trackers.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5172780,no
2-D Software Quality Model and Case Study in Software Flexibility Research,2008,"This work starts from the analysis of the traditional decomposed quality model and the relation quality model and discovers the deficiency of them presented the software quality. Then based on the measurement theory a 2-D software quality model is proposed, which synthesizes the traditional decomposed quality model and the relation quality model to comprehensive present software quality. In particular, the use of Bayesian network theory to construct the uncertainty casual relationships in the same level notions establishes a theoretical basis for the software quality assessment and prediction. Finally, the method of 2-D software quality model constructing and the initial result are presented by the research on the software flexibility in the study of cases.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5172787,no
An Explanation Model for Quality Improvement Effect of Peer Reviews,2008,"Through the analysis of Rayleigh model, an explanation model on the quality effect of peer reviews is constructed. The review activities are evaluated by the defect removal rate at each phase. We made Hypotheses on how these measurements are related to the product quality. These Hypotheses are verified through regression analysis of actual project data and concrete calculation formulae are obtained as a model. This explanation model can be used to (1) evaluate the effect of peer review, especially the upper steam review quantitatively, (2) make concrete review plan and to set objective values for review activities, (3) evaluate the effect of each peer review activity or method.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5172789,no
Non-effectively grounded system line selection device based on ARM,2008,"A hardware and software platform based on ARM (advanced RISC machines) is designed to meet the requisition of reliability and rapidity in line selection. ARM processor LPC2214 with low power loss acts as the controlling center of the platform. The software algorithm called integrated criterion combines several line detection methods to improve adaptability of line selection. The advantages of ARM, excellent real time interrupt response and high computing power, make the intelligent algorithm to be fast processed. The operation results show that this platform operates steadily and has high adaptability and line selection is implemented fast and reliable. This device is an ideal earth-fault detection platform for its better performance-price ratio.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5211799,no
An efficient parallel approach for identifying protein families in large-scale metagenomic data sets,2008,"Metagenomics is the study of environmental microbial communities using state-of-the-art genomic tools. Recent advancements in high-throughput technologies have enabled the accumulation of large volumes of metagenomic data that was until a couple of years back was deemed impractical for generation. A primary bottleneck, however, is in the lack of scalable algorithms and open source software for large-scale data processing. In this paper, we present the design and implementation of a novel parallel approach to identify protein families from large-scale metagenomic data. Given a set of peptide sequences we reduce the problem to one of detecting arbitrarily-sized dense subgraphs from bipartite graphs. Our approach efficiently parallelizes this task on a distributed memory machine through a combination of divide-and-conquer and combinatorial pattern matching heuristic techniques. We present performance and quality results of extensively testing our implementation on 160 K randomly sampled sequences from the CAMERA environmental sequence database using 512 nodes of a BlueGene/L supercomputer.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5214891,no
Nimrod/K: Towards massively parallel dynamic Grid workflows,2008,"A challenge for Grid computing is the difficulty in developing software that is parallel, distributed and highly dynamic. Whilst there have been many general purpose mechanisms developed over the years, Grid programming still remains a low level, error prone task. Scientific workflow engines can double as programming environments, and allow a user to compose dasiavirtualpsila Grid applications from pre-existing components. Whilst existing workflow engines can specify arbitrary parallel programs, (where components use message passing) they are typically not effective with large and variable parallelism. Here we discuss dynamic dataflow, originally developed for parallel tagged dataflow architectures (TDAs), and show that these can be used for implementing Grid workflows. TDAs spawn parallel threads dynamically without additional programming. We have added TDAs to Kepler, and show that the system can orchestrate workflows that have large amounts of variable parallelism. We demonstrate the system using case studies in chemistry and in cardiac modelling.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5215726,no
Influence of low order harmonic phase angles on tripping time of overcurrent relays,2008,"Experimental analyses are used to investigate the impact of low order harmonics on the operation and tripping times of overcurrent relays. Two analytical approaches based on relay characteristics provided by the manufacturer and simulations using PSCAD software package are used to estimate tripping times under non-sinusoidal operating conditions. Experiments were conducted such that the order, magnitude and phase angle of harmonics could be controlled by employing a computer-based single-phase harmonic source. Computed and measured tripping times are compared and suggestions on the application of overcurrent relays in harmonically polluted environments are provided.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5307345,no
A Quasi-experiment for Effort and Defect Estimation Using Least Square Linear Regression and Function Points,2008,"Software companies are currently investing large amounts of money in software process improvement initiatives in order to enhance their products' quality. These initiatives are based on software quality models, thus achieving products with guaranteed quality levels. In spite of the growing interest in the development of precise prediction models to estimate effort, cost, defects and other project's parameters, to develop a certain software product, a gap remains between the estimations generated and the corresponding data collected in the project's execution. This paper presents a quasi-experiment reporting the adoption of effort and defect estimation techniques in a large worldwide IT company. Our contributions are the lessons learned during (a) extraction and preparation of project historical data, (b) the use of estimation techniques on these data, and (c) the analysis of the results obtained. We believe such lessons can contribute to the improvement of the state-of-the-art in prediction models for software development.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5328367,no
Issues on Estimating Software Metrics in a Large Software Operation,2008,"Software engineering metrics prediction has been a challenge for researchers throughout the years. Several approaches for deriving satisfactory predictive models from empirical data have been proposed, although none has been massively accepted due to the difficulty of building a generic solution applicable to a considerable number of different software projects. The most common strategy on estimating software metrics is the linear regression statistical technique, for its ease of use and availability in several statistical packages. Linear regression has numerous shortcomings though, which motivated the exploration of many techniques, such as data mining and other machine learning approaches. This paper reports different strategies on software metrics estimation, presenting a case study executed within a large worldwide IT company. Our contributions are the lessons learned during the preparation and execution of the experiments, in order to aid the state of the art on prediction models of software development projects.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5328369,no
On the Relation between External Software Quality and Static Code Analysis,2008,"Only a few studies exist that try to investigate whether there is a significant correlation between external software quality and the data provided by static code analysis tools. A clarification on this issue could pave the way for more precise prediction models on the probability of defects based on the violation of programming rules. We therefore initiated a study where the defect data of selected versions of the open source development environment ldquoEclipse SDKrdquo is correlated with the data provided by the static code analysis tools PMD and FindBugs applied the source code of Eclipse. The results from this study are promising as especially some PMD rulesets show a good correlation with the defect data and could therefore serve as basis for measurement, control and prediction of software quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5328373,no
SeaWASP: A small waterplane area twin hull autonomous platform for shallow water mapping,2008,"Students with Santa Clara University (SCU) and the Monterey Bay Aquarium Research Institute (MBARI) are developing an innovative platform for shallow water bathymetry. Bathymetry data is used to analyze the geography, ecosystem, and health of marine habitats. However, current methods for shallow water measurements typically involve large, manned vessels. These vessels may pose a danger to themselves and the environment in shallow, semi-navigable waters. Small vessels, however, are prone to disturbance by the waves, tides, and currents of shallow water. The SCU / MBARI autonomous surface vessel (ASV) is designed to operate safely, stably in waters > 1 m and without significant manned support. Final deployment will be at NOAA's Kasitsna Bay Laboratory in Alaska. The ASV utilizes several key design components to provide stability, shallow draft, and long-duration unmanned operations. Bathymetry is measured with a multibeam sonar in concert with DVL and GPS sensors. Pitch, roll, and heave are minimized by a Small Waterplane Area Twin Hull (SWATH) design. The SWATH has a submerged hull, small water-plane area, and high mass to damping ratio, making it less prone to disturbance and ideal for accurate data collection. Precision sensing and actuation is controlled by onboard autonomous algorithms. Autonomous navigation increases the quality of the data collection and reduces the necessity for continuous manning. The vessel has been operated successfully in several open water test environments, including Elkhorn Slough, CA, Steven's Creek, CA, and Lake Tahoe, NV. It is currently is in the final stages of integration and test for its first major science mission at Orcas Island, San Juan Islands, WA, in August, 2008. The Orcas Island deployment will feature design upgrades implemented in Summer, 2008, including additional batteries for all-day power (minimum eight hours), active ballast, real-time data monitoring, updated autonomous control electronics and software, and data- editing using in-house bathymetry mapping software, MB-System. This paper will present the results of the Orcas Island mission and evaluate possible design changes for Alaska. Also, we will include a discussion of our shallow water bathymetry design considerations and a technical overview of the subsystems and previous test results. The ASV has been developed in partnership with Santa Clara University, the Monterey Bay Aquarium Research Institute, the University of Alaska Fairbanks, and NOAA's West Coat and Polar Regions Undersea Research Center.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5347598,no
"Availability and reliability estimation for a system undergoing minimal, perfect and failed rejuvenation",2008,"In this paper, a software rejuvenation model is presented in which two different rejuvenation actions are considered, perfect and minimal. The concept of a failed rejuvenation action which leads the system to failure is also introduced. The presented model is studied under a Continuous Time Markov Chain (CTMC) framework and a maximum likelihood estimator of the generator matrix is presented. Based on this, estimators for instantaneous availability and reliability function are also presented. Moreover, the behavior of the above estimators is studied under various rejuvenation policies. A numerical example based on simulation results is finally presented.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5355519,no
Optimization of economizer tubing system renewal decisions,2008,"The economizer is a critical component in coal fired power stations. An optimal renewal strategy is needed for minimizing the lifetime cost of this component. Here we present an effective optimization approach which considers economizer tubing failure probabilities, repair and renewal costs, potential production losses, and fluctuations in electricity market prices.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5439663,no
FDC-methods as trigger for predective maintenance for hot- and wet-process equipment,2008,"In high volume production a key performance-indicator is the highest possible uptime of process equipment. This may be achieved by moving from time-triggered â€œpreventinveâ€?to event-triggered â€œpredictiveâ€?maintenance, which has become a topic in the industry in the past years. This paper gives examples on events that can be used as triggers for maintenance actions. Performing FDC (Fault Detection and Classification) methods on trace-data and on event-logs collected from the equipment helped to identify these triggers. Furthermore, the FDC system may be used for recipe optimization and tool-matching.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5714927,no
A hardware-oriented variable block-size motion estimation method for H.264/AVC video coding,2008,"The variable block-size motion estimation (VBSME) is the most time-consuming component in H.264/AVC encoder. This paper presents a simple hardware-oriented VBSME method with an adaptive computation-aware motion estimation (ME) algorithm. To reduce the complexity, VBSME is driven by the 16Ã—16 mode and performed jointly for all modes. Moreover, this approach is integrated with a few other fast ME algorithms to demonstrate the efficiency of the adaptive ME. As a measure of efficiency, a cost function which combines the reconstruction error and the bit-rate is proposed. Obtained results show high efficiency of the adaptive strategy and a small negative influence of the proposed VBSME method on performance. It is also observed, that the increase of the number of search points (SPs) per macroblock (MB) over a certain threshold does not have to lead to the quality improvement.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5967612,no
Evolution and Search Based Metrics to Improve Defects Prediction,2009,"Testing activity is the most widely adopted practice to ensure software quality. Testing effort should be focused on defect prone and critical resources i.e., on resources highly coupled with other entities of the software application.In this paper, we used search based techniques to define software metrics accounting for the role a class plays in the class diagram and for its evolution over time. We applied Chidamber and Kemerer and the newly defined metrics to Rhino, a Java ECMA script interpreter, to predict version 1.6R5 defect prone classes. Preliminary results show that the new metrics favorably compare with traditional object oriented metrics.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5033176,yes
Predicting faults using the complexity of code changes,2009,"Predicting the incidence of faults in code has been commonly associated with measuring complexity. In this paper, we propose complexity metrics that are based on the code change process instead of on the code. We conjecture that a complex code change process negatively affects its product, i.e., the software system. We validate our hypothesis empirically through a case study using data derived from the change history for six large open source projects. Our case study shows that our change complexity metrics are better predictors of fault potential in comparison to other well-known historical predictors of faults, i.e., prior modifications and prior faults.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5070510,yes
Predicting defects in SAP Java code: An experience report,2009,"Which components of a large software system are the most defect-prone? In a study on a large SAP Java system, we evaluated and compared a number of defect predictors, based on code features such as complexity metrics, static error detectors, change frequency, or component imports, thus replicating a number of earlier case studies in an industrial context. We found the overall predictive power to be lower than expected; still, the resulting regression models successfully predicted 50-60% of the 20% most defect-prone components.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5070975,yes
Merits of using repository metrics in defect prediction for open source projects,2009,Many corporate code developers are the beta testers of open source software. They continue testing until they are sure that they have a stable version to build their code on. In this respect defect predictors play a critical role to identify defective parts of the software. Performance of a defect predictor is determined by correctly finding defective parts of the software without giving any false alarms. Having high false alarms means testers/developers would inspect bug free code unnecessarily. Therefore in this research we focused on decreasing the false alarm rates by using repository metrics. We conducted experiments on the data sets of Eclipse project. Our results showed that repository metrics decreased the false alarm rates on the average to 23% from 32% corresponding up to 907 less files to inspect.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5071357,yes
An investigation of the relationships between lines of code and defects,2009,"It is always desirable to understand the quality of a software system based on static code metrics. In this paper, we analyze the relationships between lines of code (LOC) and defects (including both pre-release and post-release defects). We confirm the ranking ability of LOC discovered by Fenton and Ohlsson. Furthermore, we find that the ranking ability of LOC can be formally described using Weibull functions. We can use defect density values calculated from a small percentage of largest modules to predict the number of total defects accurately. We also find that, given LOC we can predict the number of defective components reasonably well using typical classification techniques. We perform an extensive experiment using the public Eclipse dataset, and replicate the study using the NASA dataset. Our results confirm that simple static code attributes such as LOC can be useful predictors of software quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306304,yes
Ineffectiveness of Use of Software Science Metrics as Predictors of Defects in Object Oriented Software,2009,"Software science metrics (SSM) have been widely used as predictors of software defects. The usage of SSM is an effect of correlation of size and complexity metrics with number of defects. The SSM have been proposed keeping in view the procedural paradigm and structural nature of the programs. There has been a shift in software development paradigm from procedural to object oriented (OO) and SSM have been used as defect predictors of OO software as well. However, the effectiveness of SSM in OO software needs to be established. This paper investigates the effectiveness of use of SSM for: (a)classification of defect prone modules in OO software (b) prediction of number of defects. Various binary and numeric classification models have been applied on dataset kc1 with class level data to study the role of SSM. The results show that the removal of SSM from the set of independent variables does not significantly affect the classification of modules as defect prone and the prediction of number of defects. In most of the cases the accuracy and mean absolute error has improved when SSM were removed from the set of independent variables. The results thus highlight the ineffectiveness of use of SSM in defect prediction in OO software.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5319488,yes
On the Relationship Between Change Coupling and Software Defects,2009,"Change coupling is the implicit relationship between two or more software artifacts that have been observed to frequently change together during the evolution of a software system. Researchers have studied this dependency and have observed that it points to design issues such as architectural decay. It is still unknown whether change coupling correlates with a tangible effect of design issues, i.e., software defects.In this paper we analyze the relationship between change coupling and software defects on three large software systems. We investigate whether change coupling correlates with defects, and if the performance of bug prediction models based on software metrics can be improved with change coupling information.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5328803,yes
A Positive Detecting Code and Its Decoding Algorithm for DNA Library Screening,2009,"The study of gene functions requires high-quality DNA libraries. However, a large number of tests and screenings are necessary for compiling such libraries. We describe an algorithm for extracting as much information as possible from pooling experiments for library screening. Collections of clones are called pools, and a pooling experiment is a group test for detecting all positive clones. The probability of positiveness for each clone is estimated according to the outcomes of the pooling experiments. Clones with high chance of positiveness are subjected to confirmatory testing. In this paper, we introduce a new positive clone detecting algorithm, called the Bayesian network pool result decoder (BNPD). The performance of BNPD is compared, by simulation, with that of the Markov chain pool result decoder (MCPD) proposed by Knill et al. in 1996. Moreover, the combinatorial properties of pooling designs suitable for the proposed algorithm are discussed in conjunction with combinatorial designs and d-disjunct matrices. We also show the advantage of utilizing packing designs or BIB designs for the BNPD algorithm.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4384569,no
Deterministic Priority Channel Access Scheme for QoS Support in IEEE 802.11e Wireless LANs,2009,"The enhanced distributed channel access (EDCA) of IEEE 802.11e has been standardized to support quality of service (QoS) in wireless local area networks (LANs). The EDCA statistically supports the QoS by differentiating the probability of channel access among different priority traffic and does not provide the deterministically prioritized channel access for high-priority traffic, such as voice or real-time video. Therefore, lower priority traffic still affects the performance of higher priority traffic. In this paper, we propose a simple and effective scheme called deterministic priority channel access (DPCA) to improve the QoS performance of the EDCA mechanism. To provide guaranteed channel access to multimedia applications, the proposed scheme uses a busy tone to limit the transmissions of lower priority traffic when higher priority traffic has packets to send. Performance evaluation is conducted using both numerical analysis and simulation and shows that the proposed scheme significantly outperforms the EDCA in terms of throughput, delay, delay jitter, and packet drop ratio under a wide range of contention level.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4519209,no
Fault-Aware Runtime Strategies for High-Performance Computing,2009,"As the scale of parallel systems continues to grow, fault management of these systems is becoming a critical challenge. While existing research mainly focuses on developing or improving fault tolerance techniques, a number of key issues remain open. In this paper, we propose runtime strategies for spare node allocation and job rescheduling in response to failure prediction. These strategies, together with failure predictor and fault tolerance techniques, construct a runtime system called FARS (Fault-Aware Runtime System). In particular, we propose a 0-1 knapsack model and demonstrate its flexibility and effectiveness for reallocating running jobs to avoid failures. Experiments, by means of synthetic data and real traces from production systems, show that FARS has the potential to significantly improve system productivity (i.e., performance and reliability).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4569836,no
Beyond Output Voting: Detecting Compromised Replicas Using HMM-Based Behavioral Distance,2009,"Many host-based anomaly detection techniques have been proposed to detect code-injection attacks on servers. The vast majority, however, are susceptible to ""mimicry"" attacks in which the injected code masquerades as the original server software, including returning the correct service responses, while conducting its attack. ""Behavioral distance,"" by which two diverse replicas processing the same inputs are continually monitored to detect divergence in their low-level (system-call) behaviors and hence potentially the compromise of one of them, has been proposed for detecting mimicry attacks. In this paper, we present a novel approach to behavioral distance measurement using a new type of hidden Markov model, and present an architecture realizing this new approach. We evaluate the detection capability of this approach using synthetic workloads and recorded workloads of production Web and game servers, and show that it detects intrusions with substantially greater accuracy than a prior proposal on measuring behavioral distance. We also detail the design and implementation of a new architecture, which takes advantage of virtualization to measure behavioral distance. We apply our architecture to implement intrusion-tolerant Web and game servers, and through trace-driven simulations demonstrate that it experiences moderate performance costs even when thresholds are set to detect stealthy mimicry attacks.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4586395,no
What Types of Defects Are Really Discovered in Code Reviews?,2009,"Research on code reviews has often focused on defect counts instead of defect types, which offers an imperfect view of code review benefits. In this paper, we classified the defects of nine industrial (C/C++) and 23 student (Java) code reviews, detecting 388 and 371 defects, respectively. First, we discovered that 75 percent of defects found during the review do not affect the visible functionality of the software. Instead, these defects improved software evolvability by making it easier to understand and modify. Second, we created a defect classification consisting of functional and evolvability defects. The evolvability defect classification is based on the defect types found in this study, but, for the functional defects, we studied and compared existing functional defect classifications. The classification can be useful for assigning code review roles, creating checklists, assessing software evolvability, and building software engineering tools. We conclude that, in addition to functional defects, code reviews find many evolvability defects and, thus, offer additional benefits over execution-based quality assurance methods that cannot detect evolvability defects. We suggest that code reviews may be most valuable for software products with long life cycles as the value of discovering evolvability defects in them is greater than for short life cycle systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4604671,no
A Vector Approach for Image Quality Assessment and Some Metrological Considerations,2009,"The main purpose of this paper is to present a metrology-based view of the image quality assessment (IQA) field. Three main topics are developed. First, the state of the art in the field of IQA is presented, providing a classification of some of the most important objective and subjective IQA methods. Then, some aspects of the field are analyzed from a metrological point of view, also through a comparison with the software quality measurement area. In particular, a statistical approach to the evaluation of the uncertainty for IQA objective methods is presented, and the topic of measurement modeling for subjective IQA methods is analyzed. Finally, a vector approach for full-reference IQA is discussed, with applications to images corrupted by impulse and Gaussian noise. For these applications, the vector root mean squared error (VRMSE) and fuzzy VRMSE are introduced. These vector parameters provide a possible way to overcome the main limitations of the mean-squared-error-based IQA methods.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4627448,no
CoMoM: Efficient Class-Oriented Evaluation of Multiclass Performance Models,2009,"We introduce the class-oriented method of moments (CoMoM), a new exact algorithm to compute performance indexes in closed multiclass queuing networks. Closed models are important for performance evaluation of multitier applications, but when the number of service classes is large, they become too expensive to solve with exact methods such as mean value analysis (MVA). CoMoM addresses this limitation by a new recursion that scales efficiently with the number of classes. Compared to the MVA algorithm, which recursively computes mean queue lengths, CoMoM also carries on in the recursion information on higher-order moments of queue lengths. We show that this additional information greatly reduces the number of operations needed to solve the model and makes CoMoM the best-available algorithm for networks with several classes. We conclude the paper by generalizing CoMoM to the efficient computation of marginal queue-length probabilities, which finds application in the evaluation of state-dependent attributes such as quality-of-service metrics.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4641939,no
Vector Signal Analyzer Implemented as a Synthetic Instrument,2009,"Synthetic instruments (SIs) use the substantial signal processing assets of a field-programmable gate array (FPGA) to perform the multiple tasks of targeted digital signal processing (DSP)-based instruments. The topic of this paper is vector signal analysis from which time-dependent amplitude and phase is extracted from the input time signal. With access to the time-varying amplitude-phase profiles of the input signal, the vector signal analyzer can present many of the quality measures of a modulation process. These include estimates of undesired attributes such as modulator distortion, phase noise, clock jitter, <i>I</i> -<i>Q</i> imbalance, intersymbol interference, and others. This is where the SI is asked to become a smart software-defined radio (SDR), performing all the tasks of a DSP radio receiver and reporting small variations between the observed modulated signal parameters and those of an ideal modulated signal. Various quality measures (e.g., the size of errors) have value in quantifying and probing performance boundaries of communication systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4667670,no
PLR: A Software Approach to Transient Fault Tolerance for Multicore Architectures,2009,"Transient faults are emerging as a critical concern in the reliability of general-purpose microprocessors. As architectural trends point toward multicore designs, there is substantial interest in adapting such parallel hardware resources for transient fault tolerance. This paper presents process-level redundancy (PLR), a software technique for transient fault tolerance, which leverages multiple cores for low overhead. PLR creates a set of redundant processes per application process and systematically compares the processes to guarantee correct execution. Redundancy at the process level allows the operating system to freely schedule the processes across all available hardware resources. PLR uses a software-centric approach to transient fault tolerance, which shifts the focus from ensuring correct hardware execution to ensuring correct software execution. As a result, many benign faults that do not propagate to affect program correctness can be safely ignored. A real prototype is presented that is designed to be transparent to the application and can run on general-purpose single-threaded programs without modifications to the program, operating system, or underlying hardware. The system is evaluated for fault coverage and performance on a four-way SMP machine and provides improved performance over existing software transient fault tolerance techniques with a 16.9 percent overhead for fault detection on a set of optimized SPEC2000 binaries.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4668353,no
Compositional Dependability Evaluation for STATEMATE,2009,"Software and system dependability is getting ever more important in embedded system design. Current industrial practice of model-based analysis is supported by state-transition diagrammatic notations such as Statecharts. State-of-the-art modelling tools like STATEMATE support safety and failure-effect analysis at design time, but restricted to qualitative properties. This paper reports on a (plug-in) extension of STATEMATE enabling the evaluation of quantitative dependability properties at design time. The extension is compositional in the way the model is augmented with probabilistic timing information. This fact is exploited in the construction of the underlying mathematical model, a uniform continuous-time Markov decision process, on which we are able to check requirements of the form: ""The probability to hit a safety-critical system configuration within a mission time of 3 hours is at most 0.01."" We give a detailed explanation of the construction and evaluation steps making this possible, and report on a nontrivial case study of a high-speed train signalling system where the tool has been applied successfully.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4711060,no
Analysis of Pulsed THz Imaging Using Optical Character Recognition,2009,"Using a reflection-based pulsed THz imaging system built upon our ErAs:GaAs photoconductive switch and a gated receiver, we quantify image quality at different detection bands (centered at 100, 400, and 600 GHz). Zero-bias Schottky diode detectors mounted in various waveguide sizes are used to tune the operational frequency bands of the imaging system, while the rest of the imaging system remains unchanged. The image quality is quantified by applying an optical character recognition (OCR) algorithm on THz images of 8-by-10 mm copper letters on a fiberglass substrate. Using the OCR success rate as a metric, we see a fivefold improvement in image quality from a 400 GHz to a 600 GHz imaging system, while our 100 GHz images do not produce any correct OCR results. In a comparison experiment performed at 600 GHz, the image signal-to-noise ratio (SNR) is degraded by placing increasing numbers of denim sheets (5.4 dB decrease in signal per layer) into the beam path. We find that the OCR success rate is roughly constant from one sheet to four sheets of denim (33-25 dB SNR) and then drops off sharply starting at five denim sheets.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4711320,no
Mining Software History to Improve Software Maintenance Quality: A Case Study,2009,Errors in software updates can cause regressions failures in stable parts of the system. The Binary Change Tracer collects data on software projects and helps predict regressions in software projects.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721181,no
Analytics-Driven Dashboards Enable Leading Indicators for Requirements and Designs of Large-Scale Systems,2009,"Mining software repositories using analytics-driven dashboards provides a unifying mechanism for understanding, evaluating, and predicting the development, management, and economics of large-scale systems and processes. Dashboards enable measurement and interactive graphical displays of complex information and support flexible analytic capabilities for user customizability and extensibility. Dashboards commonly include system requirements and design metrics because they provide leading indicators for project size, growth, and volatility. This article focuses on dashboards that have been used on actual large-scale software projects as well as example empirical relationships revealed by the dashboards. The empirical results focus on leading indicators for requirements and designs of large-scale software systems based on insights from two sets of software projects containing 14 systems and 23 systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721182,no
Model Checking Timed and Stochastic Properties with CSL^{TA},2009,"Markov chains are a well-known stochastic process that provide a balance between being able to adequately model the system's behavior and being able to afford the cost of the model solution. The definition of stochastic temporal logics like continuous stochastic logic (CSL) and its variant asCSL, and of their model-checking algorithms, allows a unified approach to the verification of systems, allowing the mix of performance evaluation and probabilistic verification. In this paper we present the stochastic logic CSLTA, which is more expressive than CSL and asCSL, and in which properties can be specified using automata (more precisely, timed automata with a single clock). The extension with respect to expressiveness allows the specification of properties referring to the probability of a finite sequence of timed events. A typical example is the responsiveness property ""with probability at least 0.75, a message sent at time 0 by a system A will be received before time 5 by system B and the acknowledgment will be back at A before time 7"", a property that cannot be expressed in either CSL or asCSL. We also present a model-checking algorithm for CSLTA.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721440,no
Special Section on the 2007 IEEE AUTOTESTCON,2009,"In this Special Section, we highlight several of the key significant results that were presented at this conference by presenting nine papers, each of which highlights the theme of the conference and has been technically extended beyond the conference version. Thus, we are sharing new and updated results and advances in automatic testing. It is also exciting to note that these papers exhibit strong collaboration between academia and industry, thus leading to a strong balance between theory and practice in their contributions. The nine papers in this Special Section have been organized into three general categories that have strong interest within the automatic testing community. The first group of papers focuses on the advances in test and diagnosis technology. The second group of papers focuses on the result in the emerging field of prognostics and health management. And the final group of papers focuses on recent work in the area of synthetic instrumentation for automatic test.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4740173,no
An Evidential-Reasoning-Interval-Based Method for New Product Design Assessment,2009,"A key issue in successful new product development is how to determine the best product design among a lot of feasible alternatives. In this paper, the authors present a novel rigorous assessment methodology to improve the decision-making analysis in the complex multiple-attribute environment of new product design (NPD) assessment in early product design stage, where several performance measures, like product functions and features, manufacturability and cost, quality and reliability, maintainability and serviceability, etc., must be accounted for, but no concrete and reliable data are available, in which conventional approaches cannot be applied with confidence. The developed evidential reasoning (ER) interval methodology is able to deal with uncertain and incomplete data and information in forms of both qualitative and quantitative nature, data expressed in interval and range, judgment with probability functions, judgment in a comparative basis, unknown embedded, etc. An NPD assessment model, incorporated with the ER-based methodology, is then developed and a software system is built accordingly for validation. An industrial case study of electrical appliances is used to illustrate the application of the developed ER methodology and the product design assessment system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4757366,no
Design on the Fault Diagnostic System Based on Virtual Instrument Technique,2009,"Virtual instrument (VI) is one of the most prevalent technologies in the domain of testing, control and fault diagnosis systems, etc. In order to update entirely the means of the equipment testing for shipboard equipment, the fault diagnostic system for shipboard equipment is developed, based on VI technology, Delphi and database. The performance and constitutes of VI are introduced briefly. The modularization and universalization are proposed in its database-based design concept, realizing the design of software and hardware. The ODBC technique is applied for the interconnection of databases to ensure the generality and flexibility of the system. The aim of this design is to resolve the problems existing in the usage of testing equipments. The system mode the best of VI platform and grey diagnosis method, broke through conventional check diagnosis patterns for warships equipment, solved the problems of state prediction and trouble-mode recognition of warships equipment. It has been proved from the application that the system has merits both the testing speed and high accuracy.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4771937,no
Application of an Improved Particle Swarm Optimization for Fault Diagnosis,2009,"In this paper, the feasibility of using probabilistic causal-effect model is studied and we apply it in particle swarm optimization algorithm (PSO) to classify the faults of mine hoist. In order to enhance the PSO performance, we propose the probability function to nonlinearly map the data into a feature space in probabilistic causal-effect model, and with it, fault diagnosis is simplified into optimization problem from the original complex feature set. The proposed approach is applied to fault diagnosis, and our implementation has the advantages of being general, robust, and scalable. The raw datasets obtained from mine hoist system are preprocessed and used to generate networks fault diagnosis for the system. We studied the performance of the improved PSO algorithm and generated a Probabilistic Causal-effect network that can detect faults in the test data successfully. It can get >90% minimal diagnosis with cardinal number of fault symptom sets greater than 25.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4771990,no
Fault Detection for Fuzzy Systems With Intermittent Measurements,2009,"This paper investigates the problem of fault detection for Takagi-Sugeno (T-S) fuzzy systems with intermittent measurements. The communication links between the plant and the fault detection filter are assumed to be imperfect (i.e., data packet dropouts occur intermittently, which appear typically in a network environment), and a stochastic variable satisfying the Bernoulli random binary distribution is utilized to model the unreliable communication links. The aim is to design a fuzzy fault detection filter such that, for all data missing conditions, the residual system is stochastically stable and preserves a guaranteed performance. The problem is solved through a basis-dependent Lyapunov function method, which is less conservative than the quadratic approach. The results are also extended to T--S fuzzy systems with time-varying parameter uncertainties. All the results are formulated in the form of linear matrix inequalities, which can be readily solved via standard numerical software. Two examples are provided to illustrate the usefulness and applicability of the developed theoretical results.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4781785,no
Combined Reconstruction and Motion Correction in SPECT Imaging,2009,"Due to the long imaging times in SPECT, patient motion is inevitable and constitutes a serious problem for any reconstruction algorithm. The measured inconsistent projection data lead to reconstruction artifacts which can significantly affect the diagnostic accuracy of SPECT if not corrected. To address this problem a new approach for motion correction is introduced. It is purely based on the measured SPECT data and therefore belongs to the data-driven motion correction algorithm class. However, it does overcome some of the shortcomings of conventional methods. This is mainly due to the innovative idea to combine reconstruction and motion correction in one optimization problem. The scheme allows for the correction of abrupt and gradual patient motion. To demonstrate the performance of the proposed scheme extensive 3D tests with numerical phantoms for 3D rigid motion are presented. In addition, a test with real patient data is shown. Each test shows an impressive improvement of the quality of the reconstructed image. In this note, only rigid movements are considered. The extension to non-linear motion, as for example breathing or cardiac motion, is straightforward and will be investigated in a forthcoming paper.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4782143,no
Quality Characteristics of Collaborative Systems,2009,This paper describes the new concepts of collaborative systems quality evaluation. There are identified structures of collaborative systems. The paper defines the quality characteristics of collaborative systems. There are proposed a metric to estimate the quality level of collaborative systems. There are performed measurements of collaborative systems quality using a specially designed software.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4782509,no
Implementing a Software-Based 802.11 MAC on a Customized Platform,2009,"Wireless network (WLAN) platforms today are based on complex and inflexible hardware solutions to meet performance and deadline constraints for media access control (MAC). Contrary to that, physical layer solutions, such as software defined radio, become more and more flexible and support several competing standards. A flexible MAC counterpart is needed for system solutions that can keep up with the rising variety of WLAN protocols. We revisit the case for programmable MAC implementations looking at recent WLAN standards and show the feasibility of a software-based MAC. Our exploration uses IEEE 802.11a-e as design driver, which makes great demands on Quality-of-Service, security, and throughput. We apply our SystemC framework for efficiently assessing the quality of design points and reveal trade-offs between memory requirements, core performance, application-specific optimizations, and responsiveness of MAC implementations. In the end, two embedded cores at moderate speed (< 200 MHz) with lightweight support for message passing using small buffers are sufficient for sustaining worst-case scenarios in software.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4784695,no
"Noninvasive Fault Classification, Robustness and Recovery Time Measurement in Microprocessor-Type Architectures Subjected to Radiation-Induced Errors",2009,"In critical digital designs such as aerospace or safety equipment, radiation-induced upset events (single-event effects or SEEs) can produce adverse effects, and therefore, the ability to compare the sensitivity of various proposed solutions is desirable. As custom-hardened microprocessor solutions can be very costly, the reliability of various commercial off-the-shelf (COTS) processors can be evaluated to see if there is a commercially available microprocessor or microprocessor-type intellectual property (IP) with adequate robustness for the specific application. Most existing approaches for the measurement of this robustness of the microprocessor involve diverting the program flow and timing to introduce the bit flips via interrupts and embedded handlers added to the application program. In this paper, a tool based on an emulation platform using Xilinx field programmable gate arrays (FPGAs) is described, which provides an environment and methodology for the evaluation of the sensitivity of microprocessor architectures, using dynamic runtime fault injection. A case study is presented, where the robustness of MicroBlaze and Leon3 microprocessors executing a simple signal processing task written in C language is evaluated and compared. A hardened version of the program, where the key variables are protected, has also been tested, and its contributions to system robustness have also been evaluated. In addition, this paper presents a further improvement in the developed tool that allows not only the measurement of microprocessor robustness but, in addition, the study and classification of single-event upset (SEU) effects and the exact measurement of the recovery time (the time that the microprocessor takes to self repair and recover the fault-free state). The measurement of this recovery time is important for real-time critical applications, where criticality depends on both data correctness and timing. To demonstrate the proposed improvements, a new software program t- - hat implements two different software hardening techniques (one for Data and another for Control Flow) has been made, and a study of the recovery times in some significant fault-injection cases has been performed over the Leon3 processor.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4787115,no
CPS: A Cooperative-Probe Based Failure Detection Scheme for Application Layer Multicast,2009,"The failure of non-leaf nodes in application layer multicast (ALM) tree partitions the multicast tree, which may significantly degrade the quality of multicast service. Accurate and timely failure recovery from node failures in ALM tree, which is based on the premise of efficient failure detection scheme, is critical to minimize the disruption of service. In fact, failure detection has hardly been studied in the context of ALM. Firstly, this paper analyzes the failure detection issues in ALM and then proposes a model which is based on the relationship between parent and child nodes for it. Secondly, the cooperative-probe based failure detection scheme (CPS), in which the information about probe loss is shared among monitor nodes, is designed. Thirdly, the performance of CPS is analyzed. The Numerical results show that our proposed scheme can simultaneously reduce the detection time and the probability of false positive at the cost of little increased control overhead by comparing with the basic-probe based failure detection scheme (BPS). Finally, some possible directions for the future work are discussed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4795926,no
Usability Measurement Using a Fuzzy Simulation Approach,2009,"Usability is a three-dimensional quality of software product, encompassing efficiency, effectiveness and satisfaction. Satisfaction always relies on subjective judgment, but on some degree, the measurement of efficiency and effectiveness is able to be objectively calculated by some approaches, which can help designers to compare different prototype design candidates in the early stage of product development. To address the limitation of state-redundancy and inter-uncertainty of FSM, this paper introduces an existing approach to simplify a general FSM by reducing the redundant states, and proposes a fuzzy mathematical formula for calculating the probability of state-transition. Then, an algorithm for measuring efficiency and effectiveness data on simulated FSM is designed based on the fuzzy function. This measurement approach is very appropriate to be applied on the small-medium types of embedded software in practice.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4797361,no
A Method to Detect the Microshock Risk During a Surgical Procedure,2009,"During a surgical procedure, the patient is exposed to a risk that is inherent in the use of medical electric equipment. The situations involved in this study presume that, in open thorax surgery, 60-Hz currents with milliampere units can pass through the myocardium. A current as small as 50 or 70 muA crossing the heart can cause cardiac arrest. This risk exists due to the electrical supply in the building and medical electric equipment. Even while following established standards and technical recommendations, the equipment use electricity and can cause problems, even with a few milliamperes. This study simulates by software an electrical fault possibility of this type and shows the efficiency of the proposed method for detecting microshock risks. In addition to the simulation, a laboratory experiment is conducted with an electric circuit that is designed to produce leakage currents of known values that are detected by equipment named Protegemed. This equipment, as well as the simulation, also proves the efficiency of the proposed method. The developed method and the applied equipment for this method are covered in the Brazilian Invention Patent (PI number 9701995).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4797883,no
Eliminating microarchitectural dependency from Architectural Vulnerability,2009,"The architectural vulnerability factor (AVF) of a hardware structure is the probability that a fault in the structure will affect the output of a program. AVF captures both microarchitectural and architectural fault masking effects; therefore, AVF measurements cannot generate insight into the vulnerability of software independent of hardware. To evaluate the behavior of software in the presence of hardware faults, we must isolate the software-dependent (architecture-level masking) portion of AVF from the hardware-dependent (microarchitecture-level masking) portion, providing a quantitative basis to make reliability decisions about software independent of hardware. In this work, we demonstrate that the new program vulnerability factor (PVF) metric provides such a basis: PVF captures the architecture-level fault masking inherent in a program, allowing software designers to make quantitative statements about a program's tolerance to soft errors. PVF can also explain the AVF behavior of a program when executed on hardware; PVF captures the workload-driven changes in AVF for all structures. Finally, we demonstrate two practical uses for PVF: choosing algorithms and compiler optimizations to reduce a program's failure rate.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4798243,no
M-MAC: Mobility-Based Link Management Protocol for Mobile Sensor Networks,2009,"In wireless sensor networks with mobile sensors, frequent link failures caused by node mobility generate wasteful retransmissions, resulting in increased energy consumption and decreased network performance. In this paper we propose a new link management protocol called M-MAC that can dynamically measure and predict the link quality. Based on the projected link status information each node may drop, relay, or selectively forward a packet, avoiding unnecessary retransmissions. Our simulation results show that M-MAC can effectively reduce the per-node energy consumption by as much as 25.8% while improving the network performance compared to a traditional sensor network MAC protocol in the case of both low and high mobility scenarios.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4804601,no
Impact on Reader Performance for Lesion-Detection/ Localization Tasks of Anatomical Priors in SPECT Reconstruction,2009,"With increasing availability of multimodality imaging systems, high-resolution anatomical images can be used to guide the reconstruction of emission tomography studies. By measuring reader performance on a lesion detection task, this study investigates the improvement in image-quality due to use of prior anatomical knowledge, for example organ or lesion boundaries, during SPECT reconstruction. Simulated <sup>67</sup>Ga -citrate source and attenuation distributions were created from the mathematical cardiac-torso (MCAT) anthropomorphic digital phantom. The SIMIND Monte Carlo software was then used to generate SPECT projection data. The data were reconstructed using the De Pierro maximum a posteriori (MAP) algorithm and the rescaled-block-iterative (RBI) algorithm for comparison. We compared several degrees of prior knowledge about the anatomy: no knowledge about the anatomy; knowledge of organ boundaries; knowledge of organ and lesion boundaries; and knowledge of organ, lesion, and pseudo-lesion (non-emission uptake altering) boundaries. The MAP reconstructions used quadratic smoothing within anatomical regions, but not across any provided region boundaries. The reconstructed images were read by human observers searching for lesions in a localization receiver operating characteristic (LROC) study of the relative detection/localization accuracies of the reconstruction algorithms. Area under the LROC curve was computed for each algorithm as the comparison metric. We also had humans read images reconstructed using different prior strengths to determine the optimal trade-off between data consistency and the anatomical prior. Finally by mixing together images reconstructed with and without the prior, we tested to see if having an anatomical prior only some of the time changes the observer's detection/localization accuracy on lesions where no boundary prior is available. We found that anatomical priors including organ and lesion boundaries improve observer performance on- the lesion detection/localization task. Use of just organ boundaries did not provide a statistically significant improvement in performance however. We also found that optimal prior strength depends on the level of anatomical knowledge, with a broad plateau in which observer performance is near optimal. We found no evidence that having anatomical priors use lesion boundaries only when available changes the observer's performance when they are not available. We conclude that use of anatomical priors with organ and lesion boundaries improves reader performance on a lesion-detection/localization task, and that pseudo-lesion boundaries do not hurt reader performance. However, we did not find evidence that a prior using only organ boundaries helps observer performance. Therefore we suggest prior strength should be tuned to the organ-only case, since a prior will likely not be available for all lesions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4804739,no
An application of infrared sensors for electronic white stick,2009,"Presently, blind people use a white stick as a tool for directing them when they move or walk. Although, the white stick is useful, it cannot give a high guarantee that it can protect blind people away from all level of obstacles. Many researchers have been interested in developing electronic devices to protect blind people away from obstacles with a higher guarantee. This paper introduces an obstacles avoidance alternative by using an electronic stick that serves as a tool for blind people in walking. It employs an infrared sensor for detecting obstacles along the pathway. With all level of obstacles, the infrared stick enables to detect all type of materials available in the course such as concrete, wood, metal, glass, and human being. The result also shows that the stick detects obstacles in range of 80 cm which is the same as the length of white stick. The stick is designed to be small and light, so that blind people can carry it comfortably.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4806716,no
Mobility prediction for wireless network resource management,2009,"User mobility prediction has been studied for various applications under diverse scenarios to improve network performance. Examples include driving habits in cell stations, roaming habits of cell phone users, student movement in campuses, etc. Use of such information enables use to better manage and adapt resources to provide improved Quality Of Service (QoS). In particular, advance reservation of resources at future destinations can provide better service to a mobile user. However, there is a certain cost associated with each of these kinds of prediction schemes. We concentrate on a campus network to predict user movement and demonstrate a better movement prediction which can be used to design the network architecture differently. This user prediction scheme uses a second order Markov chain to capture user movement history to make predictions. This is highly suitable for a campus environment because of its simplicity and can also be extended to several other network architectures.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4806853,no
Automated Diagnosis of System Failures with Fa,2009,"While quick failure diagnosis and system recovery is critical, database and system administrators continue to struggle with this problem. The spectrum of possible causes of failure is huge: performance problems like resource contention, crashes due to hardware faults or software bugs, misconfiguration by system operators, and many others. The scale, complexity, and dynamics of modern systems make it laborious and time-consuming to track down the cause of failures manually. Conventional data-mining techniques like clustering and classification have a lot to offer to the hard problem of failure diagnosis. These techniques can be applied to the wealth of monitoring data that operational systems collect. However, some novel challenges need to be solved before these techniques can deliver an automated, efficient, and reasonably-accurate tool for diagnosing failures using monitoring data; a tool that is easy and intuitive to use. Fa is a new system for automated diagnosis of system failures that is designed to address the above challenges. When a system is running, Fa collects monitoring data periodically and stores it in a database.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4812557,no
Tracking High Quality Clusters over Uncertain Data Streams,2009,"Recently, data mining over uncertain data streams has attracted a lot of attentions because of the widely existed imprecise data generated from a variety of streaming applications. In this paper, we try to resolve the problem of clustering over uncertain data streams. Facing uncertain tuples with different probability distributions, the clustering algorithm should not only consider the tuple value but also emphasis on its uncertainty. To fulfill these dual purposes, a metric named tuple uncertainty will be integrated into the overall procedure of clustering. Firstly, we survey uncertain data model and propose our uncertainty measurement and corresponding properties. Secondly, based on such uncertainty quantification method, we provide a two phase stream clustering algorithm and elaborate implementation detail. Finally, performance experiments over a number of real and synthetic data sets demonstrate the effectiveness and efficiency of our method.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4812587,no
To Use or Not to Use? The Metrics to Measure Software Quality (Developers' View),2009,"The research related to software product metrics is becoming increasingly important and widespread. One of the most popular research topics in this area is the development of different software quality models that are based on product metrics. These quality models can be used to assist software project managers in making decisions e.g. if the most â€œcriticalâ€?parts of the code have been identified by using an error-proneness model, the testing resources can be concentrated on these parts of the code. As another example, the quality models based on complexity and coupling metrics may assist in estimating the cost of software changes (impact analysis). Many empirical studies have been conducted to validate these quality models. The main aim of these empirical investigations has been to identify which metrics are the best predictors to estimate the changes of software quality. The examination of the applicability of the quality models to different application domains and development platforms is also a very interesting research area.Besides the intensive research activities, software product metrics have also been integrated into the practical development processes of many software companies. This is partly due to the fact that generally a top-level management prefers to make its decisions relying on measurable data (â€œYou canâ€™t manage what you canâ€™t control, and you canâ€™t control what you donâ€™t measureâ€?â€?DeMarco). Our industrial experiences show that the people responsible for software quality and top-level project managers are also interested in applying quality monitoring tools based on product metrics. Obviously, the advantages of using such a tool should be clearly defined, but it should be noted as well that its use in the daily development process may require significant additional resources. On the other hand, according to our experiences, convincing developers about the usefulness - - of software product metrics can be very difficult. When analysing concrete metrics, the developersâ€?first reaction is usually looking for examples that justify the substantial deviation from the proposed metric value. Developers can hardly accept that there is a correlation between the metrics and the quality of the software developed by them. Consequently, we conducted a study where we asked the opinion of developers working in different fields and on different platforms about software product metrics. The experts participating in the study possessed different development skills (young developers and senior ones with many years of experience). The participants worked on C/C++, C#, Java and SQL platforms. Since some of the developers worked in open-source projects, open-source related questions were also included in the study. Besides the questions related to size, complexity, and coupling metrics, we asked the developersâ€?opinion about coding rule violations and copy-paste programming. We asked every developer to mark a so-called reference system (a system whose development process he had actively participated in). These reference systems have been analysed with the Columbus tool, and we calculated metrics, rule violations and clones. Besides the general questions, in the study we included questions that were related to the metrics derived from the given reference system. The answers were evaluated on the basis of different aspects. We examined to what extent the developersâ€?professional skills, the development platforms and the specialities of a given application domain can affect the answers.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4812732,no
Automatic Failure Diagnosis Support in Distributed Large-Scale Software Systems Based on Timing Behavior Anomaly Correlation,2009,"Manual failure diagnosis in large-scale software systems is time-consuming and error-prone. Automatic failure diagnosis support mechanisms can potentially narrow down, or even localize faults within a very short time which both helps to preserve system availability. A large class of automatic failure diagnosis approaches consists of two steps: 1) computation of component anomaly scores; 2) global correlation of the anomaly scores for fault localization. In this paper, we present an architecture-centric approach for the second step. In our approach, component anomaly scores are correlated based on architectural dependency graphs of the software system and a rule set to address error propagation. Moreover, the results are graphically visualized in order to support fault localization and to enhance maintainability. The visualization combines architectural diagrams automatically derived from monitoring data with failure diagnosis results. In a case study, the approach is applied to a distributed sample Web application which is subject to fault injection.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4812738,no
Evaluating Defect Prediction Models for a Large Evolving Software System,2009,"A plethora of defect prediction models has been proposed and empirically evaluated, often using standard classification performance measures. In this paper, we explore defect prediction models for a large, multi-release software system from the telecommunications domain. A history of roughly 3 years is analyzed to extract process and static code metrics that are used to build several defect prediction models with random forests. The performance of the resulting models is comparable to previously published work. Furthermore, we develop a new evaluation measure based on the comparison to an optimal model.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4812760,no
SQUALE Â– Software QUALity Enhancement,2009,"The Squale project was born from industrial effort to control software quality. Its goals are to refine and enhance Qualixo model, a software-metric based quality model already used by large companies in France (Air France-KLM, PSA Peugeot-Citroen) and to support the estimation of return on investment produced by software quality. Qualixo model is a software quality model based on the aggregation of software metrics into higher level indicators called practices, criterias and factors. The coordination of Squale is carried out by Qualixo.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4812772,no
Software Metrics Suites for Project Landscapes,2009,"Many software metrics have been proposed over the past decades. Selecting a small custom suite of metrics is desirable for quality assessment and defect prediction in industrial practice since developers cannot easily cope with dozens of metrics. In large software architectures, structurally similar projects are subject to similar defect conditions and can be analyzed by the same metrics suite. A large Java application developed at Continentale Insurance contains structurally similar subprojects for different insurance branches (health, life, accident, car, property). These branches are integrated in the IT-architecture in a technically uniform way. We investigated which subsets of metrics are predictive but uncorrelated with each other and compared the results for structurally similar projects.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4812783,no
Recovery scheme for hardening system on programmable chips,2009,"The checkpoint and rollback recovery techniques enable a system to survive failures by periodically saving a known good snapshot of the system's state, and rolling back to it in case a failure is detected. The approach is particularly interesting for developing critical systems on programmable chips that today offers multiple embedded processor cores, as well as configurable fabric that can be used to implement error detection and correction mechanisms. This paper presents an approach that aims at developing a safety- or mission-critical systems on programmable chip able to tolerate soft errors by exploiting processor duplication to implement error detection, as well as checkpoint and rollback recovery to correct errors in a cost-efficient manner. We developed a prototypical implementation of the proposed approach targeting the Leon processor core, and we collected preliminary results that outline the capability of the technique to tolerate soft errors affecting the processor's internal registers. This paper is the first step toward the definition of an automatic design flow for hardening processor cores (either hard of soft) embedded in programmable chips, like for example SRAM-based FPGAs.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4813816,no
Modeling Defect Enhanced Detection at 1550 nm in Integrated Silicon Waveguide Photodetectors,2009,"Recent attention has been attracted by photo-detectors integrated onto silicon-on-insulator (SOI) waveguides that exploit the enhanced sensitivity to subbandgap wavelengths resulting from absorption via point defects introduced by ion implantation. In this paper, we present the first model to describe the carrier generation process of such detectors, based upon modified Shockley-Read-Hall generation/recombination, and, thus, determine the influence of the device design on detection efficiency. We further describe how the model may be incorporated into commercial software, which then simulates the performance of previously reported devices by assuming a single midgap defect level (with properties commensurate with the single negatively charged divacancy). We describe the ability of the model to highlight the major limitations to responsivity, and thus suggest improvements which diminish the impact of such limitations.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4814835,no
A Flexible Software-Based Framework for Online Detection of Hardware Defects,2009,"This work proposes a new, software-based, defect detection and diagnosis technique. We introduce a novel set of instructions, called access-control extensions (ACE), that can access and control the microprocessor's internal state. Special firmware periodically suspends microprocessor execution and uses the ACE instructions to run directed tests on the hardware. When a hardware defect is present, these tests can diagnose and locate it, and then activate system repair through resource reconfiguration. The software nature of our framework makes it flexible: testing techniques can be modified/upgraded in the field to trade-off performance with reliability without requiring any change to the hardware. We describe and evaluate different execution models for using the ACE framework. We also describe how the proposed ACE framework can be extended and utilized to improve the quality of post-silicon debugging and manufacturing testing of modern processors. We evaluated our technique on a commercial chip-multiprocessor based on Sun's Niagara and found that it can provide very high coverage, with 99.22 percent of all silicon defects detected. Moreover, our results show that the average performance overhead of software-based testing is only 5.5 percent. Based on a detailed register transfer level (RTL) implementation of our technique, we find its area and power consumption overheads to be modest, with a 5.8 percent increase in total chip area and a 4 percent increase in the chip's overall power consumption.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4815209,no
Impact of Budget and Schedule Pressure on Software Development Cycle Time and Effort,2009,"As excessive budget and schedule compression becomes the norm in today's software industry, an understanding of its impact on software development performance is crucial for effective management strategies. Previous software engineering research has implied a nonlinear impact of schedule pressure on software development outcomes. Borrowing insights from organizational studies, we formalize the effects of budget and schedule pressure on software cycle time and effort as U-shaped functions. The research models were empirically tested with data from a 25 billion/year international technology firm, where estimation bias is consciously minimized and potential confounding variables are properly tracked. We found that controlling for software process, size, complexity, and conformance quality, budget pressure, a less researched construct, has significant U-shaped relationships with development cycle time and development effort. On the other hand, contrary to our prediction, schedule pressure did not display significant nonlinear impact on development outcomes. A further exploration of the sampled projects revealed that the involvement of clients in the software development might have ldquoerodedrdquo the potential benefits of schedule pressure. This study indicates the importance of budget pressure in software development. Meanwhile, it implies that achieving the potential positive effect of schedule pressure requires cooperation between clients and software development teams.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4815275,no
An Initial Characterization of Industrial Graphical User Interface Systems,2009,"To date we have developed and applied numerous model-based GUI testing techniques; however, we are unable to provide definitive improvement schemes to real-world GUI test planners, as our data was derived from open source applications, small compared to industrial systems. This paper presents a study of three industrial GUI-based software systems developed at ABB, including data on classified defects detected during late-phase testing and customer usage, test suites, and source code change metrics. The results show that (1) 50% of the defects found through the GUI are categorized as data access and handling, control flow and sequencing, correctness, and processing defects, (2) system crashes exposed defects 12-19% of the time, and (3) GUI and non-GUI components are constructed differently, in terms of source code metrics.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4815333,no
"A Flexible Framework for Quality Assurance of Software Artefacts with Applications to Java, UML, and TTCN-3 Test Specifications",2009,"Manual reviews and inspections of software artefacts are time consuming and thus, automated analysis tools have been developed to support the quality assurance of software artefacts. Usually, software analysis tools are implemented for analysing only one specific language as target and for performing only one class of analyses. Furthermore, most software analysis tools support only common programming languages, but not those domain-specific languages that are used in a test process. As a solution, a framework for software analysis is presented that is based on a flexible, yet high-level facade layer that mediates between analysis rules and the underlying target software artefact; the analysis rules are specified using high-level XQuery expressions. Hence, further rules can be quickly added and new types of software artefacts can be analysed without needing to adapt the existing analysis rules. The applicability of this approach is demonstrated by examples from using this framework to calculate metrics and detect bad smells in Java source code, in UML models, and in test specifications written using the testing and test control notations (TTCN-3).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4815342,no
Transforming and Selecting Functional Test Cases for Security Policy Testing,2009,"In this paper, we consider typical applications in which the business logic is separated from the access control logic, implemented in an independent component, called the Policy Decision Point (PDP). The execution of functions in the business logic should thus include calls to the PDP, which grants or denies the access to the protected resources/functionalities of the system, depending on the way the PDP has been configured. The task of testing the correctness of the implementation of the security policy is tedious and costly. In this paper, we propose a new approach to reuse and automatically transform existing functional test cases for specifically testing the security mechanisms. The method includes a three-step technique based on mutation applied to security policies (RBAC, XACML, OrBAC) and AOP for transforming automatically functional test cases into security policy test cases. The method is applied to Java programs and provides tools for performing the steps from the dynamic analyses of impacted test cases to their transformation. Three empirical case studies provide fruitful results and a first proof of concepts for this approach, e.g. by comparing its efficiency to an error-prone manual adaptation task.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4815349,no
Predicting Attack-prone Components,2009,"Limited resources preclude software engineers from finding and fixing all vulnerabilities in a software system. This limitation necessitates security risk management where security efforts are prioritized to the highest risk vulnerabilities that cause the most damage to the end user. We created a predictive model that identifies the software components that pose the highest security risk in order to prioritize security fortification efforts. The input variables to our model are available early in the software life cycle and include security-related static analysis tool warnings, code churn and size, and faults identified by manual inspections. These metrics are validated against vulnerabilities reported by testing and those found in the field. We evaluated our model on a large Cisco software system and found that 75.6% of the system's vulnerable components are in the top 18.6% of the components predicted to be vulnerable. The model's false positive rate is 47.4% of this top 18.6% or 9.1% of the total system components. We quantified the goodness of fit of our model to the Cisco data set using a receiver operating characteristic curve that shows 94.4% of the area is under the curve.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4815350,no
Evaluating the Effect of the Number of Naturally Occurring Faults on the Estimates Produced by Capture-Recapture Models,2009,"Project managers can use the capture-recapture models to estimate the number of faults in a software artifact. The capture-recapture estimates are calculated using the number of unique faults and the number of times each fault is found. The accuracy of the estimates is affected by the number of inspectors and the number of faults. Our earlier research investigated the effect that the number of inspectors had on the accuracy of the estimates. In this paper, we investigate the effect of the number of faults on the performance of the estimates using real requirement artifacts. These artifacts have an unknown amount of naturally occurring faults. The results show that while the estimators generally underestimate, they improve as the number of faults increases. The results also show that the capture-recapture estimators can be used to make correct re-inspection decisions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4815353,no
A Test-Driven Approach to Developing Pointcut Descriptors in AspectJ,2009,"Aspect-oriented programming (AOP) languages introduce new constructs that can lead to new types of faults, which must be targeted by testing techniques. In particular, AOP languages such as AspectJ use a pointcut descriptor (PCD) that provides a convenient way to declaratively specify a set of joinpoints in the program where the aspect should be woven. However, a major difficulty when testing that the PCD matches the intended set of joinpoints is the lack of precise specification for this set other than the PCD itself. In this paper, we propose a test-driven approach for the development and validation of the PCD. We developed a tool, AdviceTracer, which enriches the JUnit API with new types of assertions that can be used to specify the expected joinpoints. In order to validate our approach, we also developed a mutation tool that systematically injects faults into PCDs. Using these two tools, we perform experiments to validate that our approach can be applied for specifying expected joinpoints and for detecting faults in the PCD.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4815371,no
Development of an Embedded CPU-Based Instrument Control Unit for the SIR-2 Instrument Onboard the Chandrayaan-1 Mission to the Moon,2009,"This paper presents a computer architecture developed for the instrument control unit (ICU) of the Spectrometer Infrared 2 (SIR-2) instrument onboard the Chandrayaan-1 mission to the Moon. Characteristic features of this architecture are its high autonomy, its high reliability, and its high performance, which are obtained by the following methods: 1) adopting state-of-the-art digital-construction techniques using one single radiation-tolerant field-programmable gate array for implementing an embedded system with a 32-bit central processing unit, commercial intellectual-property cores, and custom-specified logic; 2) implementing two independent communication buses, one for instrument commanding and instrument health monitoring and another one for transferring scientific and housekeeping data to the spacecraft; 3) implementing simple and well-arranged hardware, firmware, and software; and 4) implementing in-flight software-reconfiguration capabilities available from ground command. The SIR-2 ICU performs data acquisition, data processing, and temperature regulation. Per-spectrum averaging and per-pixel oversampling are supported to reduce measurement noise. A temperature regulator for the instrument sensor unit is also implemented, with the purpose of reducing dark current noise from the detector. The embedded real-time software is implemented as a multirate cyclic executive with interrupts. Five different tasks are maintained, running with a 10-ms base cycle time. A safe mode is implemented in the boot-loader, allowing in-flight software patching through the MIL-STD-1553B bus. The advanced features of this architecture make it an excellent choice for the control unit of the scientific SIR-2 instrument, compared with architectures from previous heritage.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4838900,no
Finding All Small Error-Prone Substructures in LDPC Codes,2009,"It is proven in this work that it is NP-complete to exhaustively enumerate small error-prone substructures in arbitrary, finite-length low-density parity-check (LDPC) codes. Two error-prone patterns of interest include stopping sets for binary erasure channels (BECs) and trapping sets for general memoryless symmetric channels. Despite the provable hardness of the problem, this work provides an exhaustive enumeration algorithm that is computationally affordable when applied to codes of practical short lengths <i>n</i> ap 500. By exploiting the sparse connectivity of LDPC codes, the stopping sets of size <i>les</i> <i>13</i> and the trapping sets of size <i>les11</i> can be exhaustively enumerated. The central theorem behind the proposed algorithm is a new provably tight <i>upper</i> <i>bound</i> on the error rates of iterative decoding over BECs. Based on a tree-pruning technique, this upper bound can be iteratively sharpened until its asymptotic order equals that of the error floor. This feature distinguishes the proposed algorithm from existing non-exhaustive ones that correspond to finding <i>lower</i> <i>bounds</i> of the error floor. The upper bound also provides a worst case performance guarantee that is crucial to optimizing LDPC codes when the target error rate is beyond the reach of Monte Carlo simulation. Numerical experiments on both randomly and algebraically constructed LDPC codes demonstrate the efficiency of the search algorithm and its significant value for finite-length code optimization.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4839029,no
Tradeoff and Sensitivity Analysis of a Hybrid Model for Ranking Commercial Off-the-Shelf Products,2009,"Despite its popularity, The COTS-based development still faces some challenges, in particular the evaluation and selection process in which uncertainty plays a major role. A hybrid model, composed of the analytic hierarchy process (AHP) and Bayesian belief network (BBN), is proposed to evaluate and rank various COTS candidates while explicitly considering uncertainty. Several input parameters such as weights assigned to evaluation criteria, relative scores for various COTS candidates, and prior belief about the satisfaction of various attributes associated with the evaluation criteria need to be estimated. The estimation process of these input parameters is subject to uncertainty that limits the applicability of the modelpsilas results. In this paper, we apply sensitivity analysis to check the validity and robustness of the model. Further, we apply tradeoff analysis to explore the impact of relaxing one criterion in order to achieve an increase in another criterion that is considered as more desirable in a particular project context. A digital library system is used as a case study to illustrate how the proposed tradeoff and sensitivity analysis was performed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4839238,no
Hop-by-hop transport for satellite networks,2009,"This paper research the transport control scheme for satellite networks. The special characteristics make TCP protocols incapable of providing satisfying service for satellite networks. We analyze the performance of hop-by-hop and end-to-end transport under Bernoulli loss model. Then we propose a novel protocol based on hop-by-hop acknowledgment against the traditional end-to-end TCP for satellite networks, which uses ACK associated with NACK together, allows out-of-sequence packet transmission and has the capability of re-routing in arbitrary node. Both theoretical and simulation results showed that hop-by-hop communication scheme outperforms TCP under highly error prone, multi-hop and long delay conditions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4839393,no
Defining requirements for advanced PHM technologies for optimal reliability centered maintenance,2009,"Condition based maintenance plus (CBM+) can be described as optimal condition based maintenance (CBM) procedures defined by applying the principles and process of reliability centered maintenance. This approach offers a rigorous and disciplined method, based on the system FMECA, to determine the least cost maintenance policy and procedures that are consistent with acceptable levels of safety and readiness, applying available prognosis and health management tools. It is argued that the same process is the preferred method to define requirements for advanced PHM technologies based on RCM derived capability gaps, preferably accounting for synergies with concurrent continuous (maintenance) process improvement. There may be synergies in coupling this process with Continuous Process Improvement programs, such as NAVAIR's AIRSPEED. In discussing this proposed approach, several issues are addressed. The first is the question of interdependence between incommensurable safety, affordability and readiness objectives and metrics. The second is the problem of uncertainty in the FMECA failure modes and probabilities until the system and equipment has accumulated considerable service history, while still subject to the emergence or aggravation of failure modes by mission exposure, component deterioration, quality escapes and intentional configuration change. In practice it may be necessary to fall back on less rigorous (semi)qualitative methods to target innovation. In any case, more adaptable PHM architectures are needed to mitigate inevitable uncertainty in requirements.. Note: the terms equipment health management (also, more specifically, engine health management) [EHM] and prognostic health management (or prognosis and health management) [PHM] are used with little distinction in this paper, but in general PHM is restricted to methods generating estimates of remaining useful life.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4839682,no
The data quality estimation for the information web resources,2009,"The article is devoted to problems, which are related to the quality evaluation for the information Web resources. Basic tasks and perspective information technology of quality evaluation by the searching robot are selected.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4839865,no
A Novel Dual-Core Architecture for the Analysis of DNA Microarray Images,2009,"A deoxyribonucleic acid (DNA) microarray is a collection of microscopic DNA spots attached to a solid surface, such as a glass, plastic, or silicon chip forming an array. DNA microarray technologies are an essential part of modern biomedical research. DNA microarray allows compressing hundreds of thousands of different DNA nucleotide sequences in a little microscope glass and permits having all this information on a single image. The analysis of DNA microarray images allows the identification of gene expressions to draw biological conclusions for applications ranging from genetic profiling to diagnosis of cancer. Unfortunately, DNA microarray technology has a high variation of data quality. Therefore, to obtain reliable results, complex and extensive image analysis algorithms should be applied before the actual DNA microarray information can be used for biomedical purposes. In this paper, we present a novel hardware architecture that is specifically designed to analyze DNA microarray images. The architecture is based on a dual-core system that implements several units working in a single-instruction/multiple-data fashion. A field-programmable-gate-array (FPGA)-based prototypal implementation of the proposed architecture is presented. The effectiveness of the novel dual-core architecture is demonstrated by several analyses performed on original DNA microarray images, showing that the capability of detecting DNA spots increases by more than the 30% with respect to that of previously developed software techniques.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4840387,no
A Metric for Judicious Relaxation of Timing Constraints in Soft Real-Time Systems,2009,"For soft real-time systems, timing constraints are not as stringent as those in hard real-time systems: some constraint violations are permitted as long as the amount of violation is within a given limit. The allowed flexibility for soft real-time systems can be utilized to improve system's other quality-of-service (QoS) properties, such as energy consumption. One way to enforce constraint violation limit is to allow an expansion of timing constraint feasible region, but restrict the expansion in such a way that the relaxed constraint feasible region sufficiently resembles the original one. In this paper, we first introduce a new metric, constraint set similarity, to quantify the resemblance between two different timing constraint sets. Because directly calculating the exact value of the metric involves calculating the size of a polytope which is a #P-hard problem, we instead introduce an efficient method for estimating its bound. We further discuss how this metric can be exploited for evaluating trade-offs between timing constraint compromises and system's other QoS property gains. We use energy consumption reduction as an example to show the application of the proposed metric.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4840577,no
A reliable â€?double edged strategy for HLR mobility database failure detection and recovery in PCS networks,2009,"Since the main objective of PCS networks is to provide ldquoanytime-anywhererdquo cellular service, the existence of lost calls has become the major problem that hardly degrades the performance of such networks. A lost call arises when the location information of mobile users stored in Home Location Register (HLR) mobility database is corrupted or outdated. In order to tackle this situation, it is becoming necessary to tolerate HLR failure by implementing efficient recovery schemes. Traditionally, HLR mobility database is periodically checkpointed. After failure, HLR database is restored by reloading the backup. However, checkpointing suffers from major two hurdles, which are; (i) choosing the optimal checkpointing interval is still a challenge, and (ii) backup information may be obsolete. This paper proposes a reliable HLR failure detection and recovery strategy built upon a double edged architecture of HLR mobility database. The main contribution is to introduce novel registration and call delivery schemes for PCS networks. The proposed strategy is fault tolerant as it examines the availability of both the main HLR of the system as well the backup (BC) continuously, then take the suitable actions to keep the system working with relatively no lost calls. On the other hand, our architecture is double edged as both HLR and BC are employed in the system operation all the time. Experimental results have shown that the proposed failure detection and recovery strategy outperforms traditional checkpointing techniques as it introduces better performance with relatively no lost calls.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4907181,no
Analysis and enhancement of software dynamic defect models,2009,"Dynamic defect models are used to estimate the number of defects in a software project, predict the release date and required effort of maintenance, and measure the progress and quality of development. The literature suggests that defects projection over time follows a Rayleigh distribution. In this paper, data concerning defects are collected from several software projects and products. Data projection showed that the previous assumption of the Rayleigh distribution is not valid for current projects which are much more complex. Empirical data collected showed that defect distribution in even simpler software projects cannot be represented by the Rayleigh curves due to the adoption of several types of testing on different phases in the project lifecycle. The findings of this paper enhance the well known Puntam's defect model and propose new performance criteria to support the changes occurred during the project. Results of fitting and predicting the collected data show the superiority of the new enhanced defect model over the original defect model.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4907195,no
ESoftCheck: Removal of Non-vital Checks for Fault Tolerance,2009,"As semiconductor technology scales into the deep submicron regime the occurrence of transient or soft errors will increase. This will require new approaches to error detection. Software checking approaches are attractive because they require little hardware modification and can be easily adjusted to fit different reliability and performance requirements. Unfortunately, software checking adds a significant performance overhead. In this paper we present ESoftCheck, a set of compiler optimization techniques to determine which are the vital checks, that is, the minimum number of checks that are necessary to detect an error and roll back to a correct program state. ESoftCheck identifies the vital checks on platforms where registers are hardware-protected with parity or ECC, when there are redundant checks and when checks appear in loops. ESoftCheck also provides knobs to trade reliability for performance based on the support for recovery and the degree of trustiness of the operations. Our experimental results on a Pentium 4 show that ESoftCheck can obtain 27.1% performance improvement without losing fault coverage.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4907649,no
Fault-Tolerant Algorithm for Distributed Primary Detection in Cognitive Radio Networks,2009,"This paper attempts to identify the reliability of detection of licensed primary transmission based on cooperative sensing in cognitive radio networks. With a parallel fusion network model, the correlation issue of the received signals between the nodes in the worst case is derived. Leveraging the property of false sensing data due to malfunctioning or malicious software, the optimizing strategy, namely fault-tolerant algorithm for distributed detection (FTDD) is proposed, and quantitative analysis of false alarm reliability and detection probability under the scheme is presented. In particular, the tradeoff between licensed transmissions and user cooperation among nodes is discussed. Simulation experiments are also used to evaluate the fusion performance under practical settings. The model and analytic results provide useful tools for reliability analysis for other wireless decentralization-based applications (e.g., those involving robust spectrum sensing).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4908281,no
A New Multi-metric QoS Routing Protocol in Wireless Mesh Network,2009,"Real-time applications such as multimedia applications have high requirements on bandwidth, delay, jitter etc, which requires WMN (Wireless Mesh Networks) to support QoS. QoS routing is crucial to provide QoS. This paper focuses on QoS routing with bandwidth constraint in multi-radio multi-channel WMN, and proposes a new multi-metric and a QoS routing protocol MMQR. The routing metric has two advantages. First, it replaces the transmission rate of ETT with available bandwidth so that the nodes with light load are more likely to be selected. Second, it takes the channel diversity into account and assigns a weight to each link according to the channels of links within the range of three hops. MMQR addresses the problems that real QoS cannot be satisfied due to interference and congestion and the link doesn't work because of node failure.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4908306,no
Research and Design of Intelligent Wireless Electric Power Parameter Detection Algorithm Combining FFT with Wavelet Analysis,2009,"In modern society, power quality problem has become an important problem, which effects industrial production and product quality, etc. So, we need to monitor and analyze electric power parameter to assure electric power quality in prescribed limit. Aiming to disadvantages of present electric parameter detection of power harmonic, we design a kind of wireless power electric parameter detection system based on single chip microprocessor (SCM) technology, wireless communication technology, FFT, wavelet resolution analysis, and visual instrument (VI) technology, etc. System is composed of upper PC and lower PC. Lower PC uses transformer module, signal processor module to realize electric power signal conversion, which can be recognized by MSP430F149. Lower PC controls nRF905 wireless transceiver module to realize signal transmitting between lower PC and upper PC installing of VI software of LabVIEW. System software combines FFT and wavelet analysis method to realize electric power parameter of harmonic analysis, which solves shortcoming of single FFT analysis. Through simulation experiment, method of combining FFT with wavelet transform is verified to be accurate.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4908547,no
Allocation of extra components to k<inf>i</inf>-out-of-m<inf>i</inf> subsystems using the NPI method,2009,"The allocation of components to systems remains a challenge due to the components success and failure rate which is unpredictable to design engineers. Optimal algorithms often assume a restricted class for the allocation and yet still require a high-degree polynomial time complexity. Heuristic methods may be time-efficient but they do not guarantee optimality of the allocation. This paper introduces a new and efficient model of a system consisting of k<sub>i</sub>-out-of-m<sub>i</sub> subsystems for allocation of extra components. This model is more general than the traditional k-out-of-n one. This system which consists of subsystem i (i = 1, 2, ..., x) is working if at least k<sub>i</sub>(out of m<sub>i</sub>) components are working. All subsystems are independent and the components within subsystem i (i = 1, 2, ..., x) are exchangeable. Components exchangeable with those of each subsystem have been tested. For subsystem i, n<sub>i</sub> components have been tested for faults and none were discovered in s<sub>i</sub> of these n<sub>i</sub> components. We assume zero-failure testing, that is, we are assuming that none of the components tested is faulty so s<sub>i</sub> = n<sub>i</sub>, i = 1, 2, ..., x. We are using lower and upper probability that a system consisting of x independent k<sub>i</sub>-out-of-m<sub>i</sub> subsystems works. This allocation problem dealt with in this paper can be categorised as either to which subsystem the expected number of extra components should be allocated subject to achieving maximum reliability (Lower probability) of the system consisting subsystems, so s<sub>i</sub> = n<sub>i</sub>, i = 1, 2, ..., x. The resulting component allocation problems are too complicated to be solved by traditional approaches; therefore, the nonparametric predictive inference (NPI) method is used to solve them. These results show that NPI is a powerful tool for solving these kinds of problems which are helpful for design engineers to make optimal decisi- ons. The paper also includes suggestions for further research.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4909211,no
Automation of testing and qualification of inertial rate sensors,2009,"A very high assurance of performance is required in case of inertial rate sensors known as gyroscopes, before finalizing the product. This assurance is provided with a high quality automated test station (ATS). The major objectives of ATS are accurate measurement of gyroscope characteristic, the comprehensive characterization and performance evaluation, while reducing testing time cycle. To assist gyro testing an ATS is created to replace manual testing processes which are very time consuming and requires expensive test equipment. The present system automates most of the processes for a fraction of the cost. The system is based on Pentium IV computer with a 16-bit data acquisition card, serial ports, rate table with integrated thermal chamber and printer. All the instruments are controlled by the host computer through RS-232. The software ldquoGyro VIEWrdquo written in Visual C++ has capabilities of data acquisition, online estimation of gyroscope parameters, and run time analysis of graphs which makes it very flexible than existing softwares which support post processing. The algorithms are developed for run time processing that include mean, moving average filter, standard deviation and noise. Software allows carrying out different tests such as over thermal test, scale factor test (rotation test), misalignment test, and run-to-run test e.t.c.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4909220,no
FAST Failure Detection Service for Large Scale Distributed Systems,2009,"This paper addresses the problem of building a failure detection service for large scale distributed systems. We describe failure detection service, which merges some novel proposals and satisfies scalability, flexibility and adaptability properties. Afterwards, we present the architecture of such a service, show detailed information about its components and present the simulation results concerning performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4912938,no
Proactive Fault Tolerance Using Preemptive Migration,2009,"Proactive fault tolerance (FT) in high-performance computing is a concept that prevents compute node failures from impacting running parallel applications by preemptively migrating application parts away from nodes that are about to fail. This paper provides a foundation for proactive FT by defining its architecture and classifying implementation options. This paper further relates prior work to the presented architecture and classification, and discusses the challenges ahead for needed supporting technologies.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4912941,no
The Perk Station: Systems design for percutaneous intervention training suite,2009,"Image-guided percutaneous needle-based surgery has become part of routine clinical practice in performing procedures such as biopsies, injections and therapeutic implants. A novice physician typically performs needle interventions under the supervision of a senior physician; a slow and inherently subjective training process that lacks objective, quantitative assessment of the surgical skill and performance. Current evaluations of needle-based surgery are also rather simplistic: usually only needle tip accuracy and procedure time are recorded, the latter being used as an indicator of economical feasibility. Shortening the learning curve and increasing procedural consistency are critically important factors in assuring high-quality medical care for all segments of society. This paper describes the design and development of a laboratory validation system for measuring operator performance under different assistance techniques for needle-based surgical guidance systems - The perk station. The initial focus of the perk station is to assess and compare three different techniques: the image overlay, bi-plane laser guide, and conventional freehand. The integrated system comprises of a flat display with semi-transparent mirror (image overlay), bi-plane laser guide, a magnetic tracking system, a tracked needle, a phantom, and a stand-alone laptop computer running the planning and guidance software. The prototype Perk Station has been successfully developed, the associated needle insertion phantoms have been built, and the graphic surgical interface has been implemented.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4913256,no
FMEA is not enough,2009,"A failure modes and effects analysis is a methodology for analysis of potential failure modes within a system which includes the classification of the failure modes severity and impact on the system. Most organizations stop at the completion of the FMEA and assume that the work is done and a solid design will now be created. It is not always the case. There is a significant amount of work that needs to go into a solid design before and after an FMEA from the fault management (FM) perspective. Also, there are challenges to prove that the content of the FMEA is correct and it will add value to the overall product. The key is to connect the dots that make up a fault management infrastructure. This paper will discuss these points and provide the blue print of their relationships and how to take advantage of each element and effectively connect them to observe their power. This approach provides tangible results that customers can understand and relate to. In addition, it provides clear, straight forward and meaningful results that can not be disputed and can even be tested in the customer environment. After completion of an FMEA, it is important to verify the information provided by the hardware, software and the rest of the FMEA team. Finally, this paper discusses the concept of Fault Insertion Testing or FIT, relation of FIT to FMEA and how FIT enables design engineers and all interested parties to observe if the product is performing as expected. There are different methods for performing FIT. Each method will be discussed at a high level. One of the key differences in the approach being suggested in this paper is the final step of the fault management process which is to tie the FMEA and fault insertion testing together. These two activities complement each other and significantly increase their value by being performed right after one another. The FMEA provides the necessary information such as fault insertion points and the required parameters to execute fault ins- ertion testing.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4914698,no
Study on the Prediction Model and Software of Environment Contaminants in Coal-Burning Power Plants,2009,"The model and software for systematically estimating the concentration of the main contaminants (dust, SO<sub>2</sub>, NO<sub>x</sub>, CO<sub>2</sub>, F, As, Hg, Cd, Pd etc) in exhaust gas and waste waters from coal-burning power plants were studied. The prediction of contaminants in exhaust gas was based on conventional calculating models. The key factors are ascertained the material conservation theory and the on-line data of coal quality are used to build the formula for calculating the quantity and the concentration of the gas contaminants. The prediction of contaminants in water was based on the status quo of ash-flushing systems in national power plants through studying the ash-water in two different systems separately. The power plant monitor and prediction software using VB and access database were developed based on the above prediction model. The prediction result from the software shows that the opposite error value about the prediction and measure was all less than 10%, this proved the estimate was authentic.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4918848,no
Transforming traditional iris recognition systems to work on non-ideal situations,2009,"Non-ideal iris images can significantly affect the accuracy of iris recognition systems for two reasons: 1) they cannot be properly preprocessed by the system; and/or 2) they have poor image quality. However, many traditional iris recognition systems have been deployed in law enforcement, military, or many other important locations. It will be expensive to replace all these systems. It will be desirable if the traditional systems can be transformed to perform in non-ideal situations without an expensive update. In this paper, we propose a method that can help traditional iris recognition systems to work on the non-ideal situation using a video image approach. The proposed method will quickly identify and eliminate the bad quality images from iris videos for further processing. The segmentation accuracy is critical in recognition and would be challenging for traditional systems. The segmentation evaluation is designed to evaluate if the segmentation is valid. The information distance based quality measure is used to evaluate if the image has enough quality for recognition. The segmentation evaluation score and quality score are combined to predict the recognition performance. The research results show that the proposed methods can work effectively and objectively. The combination of segmentation and quality scores is highly correlated with the recognition accuracy and can be used to improve the performance of iris recognition systems in a non-ideal situation. The deployment of such a system would not cost much since the core parts of the traditional systems are not changed and we only need to add software modules. It will be very practical to transform the traditional system using the proposed method.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4925679,no
Decision trees as information source for attribute selection,2009,"Attribute selection (AS) is known to help improve the results of algorithmic learning processes by selecting fewer, but predictive, input attributes. This study introduces a new ranking filter AS method, the tree node selection (TNS) method. The idea of TNS is to determine significant but fewer attributes by searching through the pre-generated decision tree as information source in the manner of a pruning process. To test the performance of TNS, 33 benchmark datasets (UCI) with various numbers of instances, attributes and classes were investigated along with five known AS methods, and the results were tested with the C4.5 (unpruned) and naive Bayes classifiers. The performance, in terms of classification accuracy improvement, reduction in the number of attributes and the size of the generated decision tree are assessed by various statistical analyses for multiple comparisons. TNS was found to provide the most consistent performance for C4.5 and naive Bayes classifiers, and generated unpruned C4.5 trees with selected fewer attributes were generally found to achieve similar quality to pruned C4.5 trees without any attribute selection.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4938636,no
A Tool for the Application of Software Metrics to UML Class Diagram,2009,"How to improve software quality are the important directions in software engineering research field. The complexity has a close relationship with the developing cost, time spending and the number of detects which a program may exist. OOA and OOD have been widely used, so the requirement of measuring software complexity written in object-oriented language is emerging. UML class diagrams describe the static view of a system in terms of classes and relationships among the classes. In order to objectively assess UML class diagrams, this paper presents a suite of metrics based on UML class diagram that is adapted to Java to assess the complexity of UML class diagrams in various aspects, and verifies them with a suite of evaluation rules suggested by Weyuker.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4958751,no
Metric-based Evolution Analysis and Evaluation of Software Core Assets Library,2009,"The core assets library is the most important component of any software product line; it evolved responding to organization businesses, origination object, technology renovation and time. From the metric of the core assets library, this paper gives several measurements and their definitions which correlate with the evolution of a core assets library. It is combing with data processing, putting forward an evaluation model with the evolution of a core assets library,using analytic hierarchy process to estimate the core assets librarypsilas evolution level, illuminating with an example at last.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4959431,no
One-pass multi-layer rate-distortion optimization for quality scalable video coding,2009,"In this paper, a one-pass multi-layer rate-distortion optimization algorithm is proposed for quality scalable video coding. To improve the overall coding efficiency, the MB mode in the base layer is selected not only based on its rate-distortion performance relative to this layer but also according to its impact on the enhancement layer. Moreover, the optimization module for residues is also improved to benefit inter-layer prediction. Simulations show that the proposed algorithm outperforms the most recent SVC reference software. For eight test sequences, a gain of 0.35 dB on average and 0.75 dB at maximum is achieved at a cost of less than 8% increase of the total coding time.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4959664,no
A fast CABAC rate estimator for H.264/AVC mode decision,2009,"H.264/AVC coders use the rate-distortion (R-D) cost function to decide the coding mode for each coding unit for better R-D tradeoff. To evaluate the R-D cost function, both the bit rate and the video quality degradation of the candidate mode must be calculated. In this paper, a fast context-adaptive binary arithmetic coding (CABAC) rate estimation scheme is proposed to accelerate the rate calculation. The speed of the proposed rate estimator depends only on the number of contexts used in the coding unit. Experimental results show that the proposed rate estimator reduces about 20% of the computational complexity of the R-D optimized mode decision when it is implemented as software. The entire encoder implemented as software is then accelerated by 16% with negligible degradation in the R-D performance. If implemented as hardware, the proposed scheme is expected to accelerate the rate estimation for a macroblock by 5 to 18 times faster than the conventional CABAC operation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4959737,no
Video quality monitoring of streamed videos,2009,"This paper describes a video quality analysis system for inservice monitoring of streamed videos, particularly over mobile/wireless networks. The algorithm adopts the no-reference method, and enables real-time measurement of video quality at any point in the content production and delivery chain using any given video. The technologies developed include no-reference methods for measuring picture freeze, picture loss, and blockiness. The developed system (where the software has not been optimized for speed) is able to process video of CIF size (352 times 288 pixels) at more than 30 fps on a Pentium-IV 3 GHz computer. The experimental results show that the proposed video quality analysis system gives good accuracy for picture freeze, picture loss, and blocking detections.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4959793,no
Resource usage prediction for groups of dynamic image-processing tasks using Markov modeling,2009,"With the introduction of dynamic image processing, such as in image analysis, the computational complexity has become data dependent and memory usage irregular. Therefore, the possibility of runtime estimation of resource usage would be highly attractive and would enable quality-of-service (QoS) control for dynamic image-processing applications with shared resources. A possible solution to this problem is to characterize the application execution using model descriptions of the resource usage. In this paper, we attempt to predict resource usage for groups of dynamic image-processing tasks based on Markov-chain modeling. As a typical application, we explore a medical imaging application to enhance a wire mesh tube (stent) under X-ray fluoroscopy imaging during angioplasty. Simulations show that Markov modeling can be successfully applied to describe the resource usage function even if the flow graph dynamically switches between groups of tasks. For the evaluated sequences, an average prediction accuracy of 97% is reached with sporadic excursions of the prediction error up to 20-30%.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4959987,no
A study of pronunciation verification in a speech therapy application,2009,Techniques are presented for detecting phoneme level mispronunciations in utterances obtained from a population of impaired children speakers. The intended application of these approaches is to use the resulting confidence measures to provide feedback to patients concerning the quality of pronunciations in utterances arising within interactive speech therapy sessions. The pronunciation verification scenario involves presenting utterances of known words to a phonetic decoder and generating confusion networks from the resulting phone lattices. Confidence measures are derived from the posterior probabilities obtained from the confusion networks. Phoneme level mispronunciation detection performance was significantly improved with respect to a baseline system by optimizing acoustic models and pronunciation models in the phonetic decoder and applying a nonlinear mapping to the confusion network posteriors.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4960657,no
PDF Research on IEEE802.11 DCF in Wireless Distributed Measurement System,2009,"This paper analyzes the basic and RTS/CTS and fragment MAC access mechanism of IEEE802.11 DCF (distribution coordination function), and puts forward the model of wireless distributed measurement system (WDMS). It analyzes the elements that affect system real-time communication. It sets up the simulation network scenario of real-time communication through software, then simulates to research the MAC PDF (probability distribution function) performance in wireless distributed measurement system, which gives theory argument and reference data for further system design.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4960853,no
Staffing Level and Cost Analyses for Software Debugging Activities Through Rate-Based Simulation Approaches,2009,"Research in the field of software reliability, dedicated to the analysis of software failure processes, is quite diverse. In recent years, several attractive rate-based simulation approaches have been proposed. Thus far, it appears that most existing simulation approaches do not take into account the number of available debuggers (or developers). In practice, the number of debuggers will be carefully controlled. If all debuggers are busy, they may not address newly detected faults for some time. Furthermore, practical experience shows that fault-removal time is not negligible, and the number of removed faults generally lags behind the total number of detected faults, because fault detection activities continue as faults are being removed. Given these facts, we apply the queueing theory to describe and explain possible debugging behavior during software development. Two simulation procedures are developed based on G/G/infin, and G/G/m queueing models, respectively. The proposed methods will be illustrated using real software failure data. The analysis conducted through the proposed framework can help project managers assess the appropriate staffing level for the debugging team from the standpoint of performance, and cost-effectiveness.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4967914,no
Evolutionary Sampling and Software Quality Modeling of High-Assurance Systems,2009,"Software quality modeling for high-assurance systems, such as safety-critical systems, is adversely affected by the skewed distribution of fault-prone program modules. This sparsity of defect occurrence within the software system impedes training and performance of software quality estimation models. Data sampling approaches presented in data mining and machine learning literature can be used to address the imbalance problem. We present a novel genetic algorithm-based data sampling method, named evolutionary sampling, as a solution to improving software quality modeling for high-assurance systems. The proposed solution is compared with multiple existing data sampling techniques, including random undersampling, one-sided selection, Wilson's editing, random oversampling, cluster-based oversampling, synthetic minority oversampling technique (SMOTE), and borderline-SMOTE. This paper involves case studies of two real-world software systems and builds C4.5- and RIPPER-based software quality models both before and after applying a given data sampling technique. It is empirically shown that evolutionary sampling improves performance of software quality models for high-assurance systems and is significantly better than most existing data sampling techniques.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4967988,no
LA1 testBed: Evaluation testbed to assess the impact of network impairments on video quality,2009,"Currently, a complete system for analyzing the effect of packet loss on a viewer's perception is not available. Given the popularity of digital video and the growing interest in live video streams where channel coding errors cannot be corrected, such a system would give great insight into the problem of video corruption through transmission errors and how they are perceived by the user. In this paper we introduce such a system, where digital video can be corrupted according to established loss patterns and the effect is measured automatically. The corrupted video is then used as input for user tests. Their results are analyzed and compared with the automatically generated. Within this paper we present the complete testing system that makes use of existing software as well as introducing new modules and extensions. With the current configuration the system can test packet loss in H.264 coded video streams and produce a statistical analysis detailing the results. The system is fully modular allowing for future developments such as other types of statistical analysis, different video measurements and new video codecs.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4976208,no
Collaborative defense as a pervasive service Architectural insights and validation methodologies of a trial deployment,2009,"Network defense is an elusive art. The arsenal to defend our devices from attack is constantly lagging behind the latest methods used by attackers to break into them and subsequently into our networks. To counteract this trend, we developed a distributed, scalable approach that harnesses the power of collaborative end-host detectors or sensors. Simulation results reveal order of magnitude improvements over stand-alone detectors in the accuracy of detection (fewer false alarms) and in the quality of detection (the ability to capture stealthy anomalies that would otherwise go undetected). Although these results arise out of a proof of concept in the arena of botnet detection in an enterprise network, they have broader applicability to the area of network self-manageability of pervasive computing devices. To test the efficacy of these ideas further, Intel Corporation partnered with British Telecommunications plc to launch a trial deployment. In this paper, we report on results and insights gleaned from the development of a testbed infrastructure and phased experiments; (1) the design of a re-usable measurement-inference architecture into which 3rd party sensor developers can integrate a wide variety of ldquoanomaly detectionrdquo algorithms to derive the same correlation-related performance benefits; (2) the development of a series of validation methodologies necessitated by the lack of mature tools and approaches to attest to the security of distributed networked systems; (3) the critical role of learning and adaptation algorithms to calibrate a fully-distributed architecture of varied devices in varied contexts, and (4) the utility of large-scale data collections to assess what's normal behavior for Enterprise end-host background traffic as well as malware command-and-control protocols. Finally, we propose collaborative defense as a blueprint for emergent collaborative systems and its measurement-everywhere approach as the adaptive underpinnings needed for pervasive- services.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4976261,no
Formal Correctness of a Passive Testing Approach for Timed Systems,2009,"In this paper we extend our previous work on passive testing of timed systems to establish a formal criterion to determine correctness of an implementation under test. In our framework, an invariant expresses the fact that if the implementation under test performs a given sequence of actions, then it must exhibit a behavior in a lapse of time reflected in the invariant. In a previous paper we gave an algorithm to establish the correctness of an invariant with respect to a specification. In this paper we continue the work by providing an algorithm to check the correctness of a log, recorded form the implementation under test, with respect to an invariant. We show the soundness of our method by relating it to an implementation relation. In addition to the theoretical framework we have developed a tool, called PASTE, that facilitates the automation of our passive testing approach.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4976372,no
Evolving the Quality of a Model Based Test Suite,2009,"Redundant test cases in newly generated test suites often remain undetected until execution and waste scarce project resources. In model-based testing, the testing process starts early on in the developmental phases and enables early fault detection. The redundancy in the test suites generated from models can be detected earlier as well and removed prior to its execution. The article presents a novel model-based test suite optimization technique involving UML activity diagrams by formulating the test suite optimization problem as an Equality Knapsack Problem. The aim here is the development of a test suite optimization framework that could optimize the model-based test suites by removing the redundant test cases. An evolution-based algorithm is incorporated into the framework and is compared with the performances of two other algorithms. An empirical study is conducted with four synthetic and industrial scale Activity Diagram models and results are presented.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4976379,no
Assertion-Driven Development: Assessing the Quality of Contracts Using Meta-Mutations,2009,"Agile development methods have gained momentum in the last few years and, as a consequence, test-driven development has become more prevalent in practice. However, test cases are not sufficient for producing dependable software and we rather advocate approaches that emphasize the use of assertions or contracts over that of test cases. Yet, writing self-checks in code has been shown to be difficult and is itself prone to errors. A standard technique to specify runtime properties is design-by contract(DbC). But how can one test if the contracts themselves are sensible and sufficient? We propose a measure to quantify the goodness of contracts (or assertions in a broader sense). We introduce meta-mutations at the source code level to simulate common programmer errors that the self-checks are supposed to detect. We then use random mutation testing to determine a lower and upper bound on the detectable mutations and compare these bounds with the number of mutants detected by the contracts. Contracts are considered ldquogoodrdquo if they detect a certain percentage of the detectable mutations.We have evaluated our tools on Java classes with contracts specified using the Java Modeling Language (JML). We have additionally tested the contract quality of 19 implementations, written independently by students, based on the same specification.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4976385,no
AjMutator: A Tool for the Mutation Analysis of AspectJ Pointcut Descriptors,2009,"Aspect-oriented programming introduces new challenges for software testing. In particular the pointcut descriptor (PCD) requires particular attention from testers. The PCD describes the set of join points where the advices are woven.In this paper we present a tool, AjMutator, for the mutation analysis of PCDs. AjMutator implements several mutation operators that introduce faults in the PCDs to generate a set of mutants. AjMutator classifies the mutants according to the set of join points they match compared to the set of join points matched by the initial PCD. An interesting result is that this automatic classification can identify equivalent mutants for a particular class of PCDs. AjMutator can also run a set of test cases on the mutants to give a mutation score. We have applied AjMutator on two systems to show that this tool is suitable for the mutation analysis of PCDs on large AspectJ systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4976387,no
Work in Progress: Building a Distributed Generic Stress Tool for Server Performance and Behavior Analysis,2009,"One of the primary tools for performance analysis of multi-tier systems are standardized benchmarks. They are used to evaluate system behavior under different circumstances to assess whether a system can handle real workloads in a production environment. Such benchmarks are also helpful to resolve situations when a system has an unacceptable performance or even crashes. System administrators and developers use these tools for reproducing and analyzing circumstances which provoke the errors or performance degradation. However, standardized benchmarks are usually constrained to simulating a set of pre-fixed workload distributions. We present a benchmarking framework which overcomes this limitation by generating real workloads from pre-recorded system traces. This distributed tool allows more realistic testing scenarios, and thus exposes the behavior and limits of a tested system with more details. Further advantage of our framework is its flexibility. For example, it can be used to extend standardized benchmarks like TPC-W thus allowing them to incorporate workload distributions derived from real workloads.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4976626,no
Comparing design of experiments and evolutionary approaches to multi-objective optimisation of sensornet protocols,2009,"The lifespan, and hence utility, of sensornets is limited by the energy resources of individual motes. Network designers seek to maximise energy efficiency while maintaining an acceptable network quality of service. However, the interactions between multiple tunable protocol parameters and multiple sensornet performance metrics are generally complex and unknown. In this paper we address this multi-dimensional optimisation problem by two distinct approaches. Firstly, we apply a Design Of Experiments approach to obtain a generalised linear interaction model, and from this derive an estimated near-optimal solution. Secondly, we apply the Two-Archive evolutionary algorithm to improve solution quality for a specific problem instance. We demonstrate that, whereas the first approach yields a more generally applicable solution, the second approach yields a broader range of viable solutions at potentially lower experimental cost.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4983074,no
Designing for Feel: Contrasts between Human and Automated Parametric Capture of Knob Physics,2009,"We examine a crucial aspect of a tool intended to support designing for feel: the ability of an objective physical-model identification method to capture perceptually relevant parameters, relative to human identification performance. The feel of manual controls, such as knobs, sliders, and buttons, becomes critical when these controls are used in certain settings. Appropriate feel enables designers to create consistent control behaviors that lead to improved usability and safety. For example, a heavy knob with stiff detents for a power plant boiler setting may afford better feedback and safer operations, whereas subtle detents in an automobile radio volume knob may afford improved ergonomics and driver attention to the road. To assess the quality of our identification method, we compared previously reported automated model captures for five real mechanical reference knobs with captures by novice and expert human participants who were asked to adjust four parameters of a rendered knob model to match the feel of each reference knob. Participants indicated their satisfaction with the matches their renderings produced. We observed similar relative inertia, friction, detent strength, and detent spacing parameterizations by human experts and our automatic estimation methods. Qualitative results provided insight on users' strategies and confidence. While experts (but not novices) were better able to ascertain an underlying model in the presence of unmodeled dynamics, the objective algorithm outperformed all humans when an appropriate physical model was used. Our studies demonstrate that automated model identification can capture knob dynamics as perceived by a human, and they also establish limits to that ability; they comprise a step towards pragmatic design guidelines for embedded physical interfaces in which methodological expedience is informed by human perceptual requirements.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5010437,no
De-interlacing using priority processing for guaranteed real time performance with limited processing resources,2009,Multi-media-systems (MMS) increasingly use software solutions for signal processing instead of dedicated hardware. Scalable video algorithms (SVA) using novel priority processing can guarantee real-time performance on programmable platforms even with limited resources. In this paper we propose a scalable de-interlacing algorithm with priority processing. The de-interlacer adapts to the available hardware resources and shows a wide range of scalability with good output quality. The output quality has been compared with other proposed de-interlacers.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5012320,no
How Can Optimization Models Support the Maintenance of Component-Based Software?,2009,"The maintenance phase of software systems is ever more increasing its incidence, in terms of effort, to the whole software lifecycle. Therefore the introduction of automated techniques that can help software maintainers to take decision on the basis of quantitative evaluation would be a suitable phenomenon.Search-based techniques offer today a very promising view on the automation of searching processes in the software engineering domain. Component-based software is a very interesting paradigm to apply such type of techniques, for example for component selection. In this paper we introduce optimization techniques to manage the problem of failures at maintenance time. In particular,we introduce two approaches that provide maintenance actions to be taken in order to overcome system failures in case of monitored and non-monitored software systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5033187,no
"Embedded Software: Facts, Figures, and Future",2009,"Due to the complex system context of embedded-software applications, defects can cause life-threatening situations, delays can create huge costs, and insufficient productivity can impact entire economies. Providing better estimates, setting objectives, and identifying critical hot spots in embedded-software engineering requires adequate benchmarking data.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5054871,no
OneClick: A Framework for Measuring Network Quality of Experience,2009,"As the service requirements of network applications shift from high throughput to high media quality, interactivity, and responsiveness, the definition of QoE (Quality of Experience) has become multidimensional. Although it may not be difficult to measure individual dimensions of the QoE, how to capture users' overall perceptions when they are using network applications remains an open question. In this paper, we propose a framework called OneClick to capture users' perceptions when they are using network applications. The framework only requires a subject to click a dedicated key whenever he/she feels dissatisfied with the quality of the application in use. OneClick is particularly effective because it is intuitive, lightweight, efficient, time-aware, and application-independent. We use two objective quality assessment methods, PESQ and VQM, to validate OneClick's ability to evaluate the quality of audio and video clips. To demonstrate the proposed framework's efficiency and effectiveness in assessing user experiences, we implement it on two applications, one for instant messaging applications, and the other for first- person shooter games. A Flash implementation of the proposed framework is also presented.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5061978,no
Quantifying the Importance of Vantage Points Distribution in Internet Topology Measurements,2009,"The topology of the Internet has been extensively studied in recent years, driving a need for increasingly complex measurement infrastructures. These measurements have produced detailed topologies with steadily increasing temporal resolution, but concerns exist about the ability of active measurement to measure the true Internet topology. Difficulties in ensuring the accuracy of every individual measurement when millions of measurements are made daily, and concerns about the bias that might result from measurement along the tree of routes from each vantage point to the wider reaches of the Internet must be addressed. However, early discussions of these concerns were based mostly on synthetic data, oversimplified models or data with limited or biased observer distributions. In this paper, we show the importance that extensive sampling from a broad distribution of vantage points has on the resulting topology and bias. We present two methods for designing and analyzing the topology coverage by vantage points: one, when system-wide knowledge exists, provides a near-optimal assignment of measurements to vantage points; while the second one is suitable for an oblivious system and is purely probabilistic. The majority of the paper is devoted to a first look at the importance of the distribution's quality. We show that diversity in the locations and types of vantage points is required for obtaining an unbiased topology. We analyze the effect that broad distribution has over the convergence of various autonomous systems topology characteristics. We show that although diverse and broad distribution is not required for all inspected properties, it is required for some. Finally, some recent bias claims that were made against active traceroute sampling are revisited, and we empirically show that diverse and broad distribution can question their conclusions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5061988,no
Keynote: Event Driven Software Quality,2009,"Summary form only given. Event-driven programming has found pervasive acceptance, from high-performance servers to embedded systems, as an efficient method for interacting with a complex world. The fastest research Web servers are event- driven, as is the most common operating system for sensor nodes. An event-driven program handles concurrent logical tasks using a cooperative, application-level scheduler. The application developer separates each logical task into event handlers; the scheduler runs multiple handlers in an interleaved fashion. Unfortunately, the loose coupling of the event handlers obscures the program's control flow and makes dependencies hard to express and detect, leading to subtle bugs. As a result, event-driven programs can be difficult to understand, making them hard to debug, maintain, extend, and validate. This talk presents recent approaches to event-driven software quality based on static analysis and testing, along with some open problems. We will discuss progress on how to avoid buffer overflow in TCP servers, stack overflow and missed deadlines in microcontrollers, and rapid battery drain in sensor networks. Our work is part of the Event Driven Software Quality project at UCLA, which is aimed at building the next generation of language and tool support for event-driven programming.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5066621,no
Proposal of Stateful Relilability Counter in Small-World Cellular Neural Networks,2009,"Cellular neural networks (CNN) is a neural network model linked to only neighborhoods. CNN is suited for image processing such as noise reduction and edge detection. Small world cellular neural networks (SWCNN) is a CNN extended by adding a small world link, which is global short-cut. SWCNN has better performance than CNN. One of weak points of SWCNN is fault tolerance. We proposed multiple SWCNN layers in order to improve fault tolerance of SWCNN. However, it is not sufficient because only stop failure is considered. In this paper, we propose stateful reliability counter for triple modular redundancy (stateful RC-TMR) method in order to improve tolerance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5066782,no
Performance Evaluation of Link Quality Extension in Multihop Wireless Mobile Ad-hoc Networks,2009,"Recently, mobile ad-hoc networks (MANET) are continuing to attract the attention for their potential use in several fields. Most of the work has been done in simulation, because a simulator can give a quick and inexpensive understanding of protocols and algorithms. However, experimentation in the real world are very important to verify the simulation results and to revise the models implemented in the simulator. In this paper, we present the implementation and analysis of our testbed considering the link quality window size (LQWS) parameter for optimized link state routing (OLSR) protocol. We investigate the effect of mobility in the throughput of a MANET. The mobile nodes move toward the destination at a regular speed. When the mobile nodes arrive at the corner, they stop for about three seconds. In our experiments, we consider two cases: only one node is moving (mobile node)and two nodes (intermediate nodes) are moving at the same time. We assess the performance of our testbed in terms of throughput, round trip time, jitter and packet loss. From our experiments, we found that throughput of TCP was improved by reducing LQWS.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5066803,no
Bandwidth Scalable Wideband Codec Using Hybrid Matching Pursuit Harmonic/CELP Scheme,2009,"A novel hybrid harmonic/CELP scheme for bandwidth scalable wideband codec is proposed that utilizes a band-split technique, where the low-band (0-4 kHz) is critically subsampled and coded using 11.8 kbps G.729E. The high-band signal is divided into stationary mode (SM) and non-stationary mode (NSM) components based on its unique characteristics. In the SM portion, the high-band signal is compressed using a multi-stage coding combined sinusoidal model based on the matching pursuit (MP) algorithm and CELP with the circular codebook. In the NSM portion, the high-band signals are coded by CELP with both pulse and circular codebooks. For efficient bit allocation and enhanced performance, the pitch of the high-band codec is estimated using the quantized pitch parameter in low-band codec. In an informal listening test, the subjective speech quality was rated as comparable to that obtainable with 48 kbps G.722 and 12.85 kbps G.722.2.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5066851,no
Automatic modulation classification for cognitive radios using cyclic feature detection,2009,"Cognitive radios have become a key research area in communications over the past few years. Automatic modulation classification (AMC) is an important component that improves the overall performance of the cognitive radio. Most modulated signals exhibit the property of cyclostationarity that can be exploited for the purpose of classification. In this paper, AMCs that are based on exploiting the cyclostationarity property of the modulated signals are discussed. Inherent advantages of using cyclostationarity based AMC are also addressed. When the cognitive radio is in a network, distributed sensing methods have the potential to increase the spectral sensing reliability, and decrease the probability of interference to existing radio systems. The use of cyclostationarity based methods for distributed signal detection and classification are presented. Examples are given to illustrate the concepts. The Matlab codes for some of the algorithms described in the paper are available for free download at http://filebox.vt.edu/user/bramkum.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5067400,no
Experiments on the test case length in specification based test case generation,2009,"Many different techniques have been proposed to address the problem of automated test case generation, varying in a range of properties and resulting in very different test cases. In this paper we investigate the effects of the test case length on resulting test suites: Intuitively, longer test cases should serve to find more difficult faults but will reduce the number of test cases necessary to achieve the test objectives. On the other hand longer test cases have disadvantages such as higher computational costs and they are more difficult to interpret manually. Consequently, should one aim to generate many short test cases or fewer but longer test cases? We present the results of a set of experiments performed in a scenario of specification based testing for reactive systems. As expected, a long test case can achieve higher coverage and fault detecting capability than a short one, while giving preference to longer test cases in general can help reduce the size of test suites but can also have the opposite effect, for example, if minimization is applied.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5069037,no
Calculating BPEL test coverage through instrumentation,2009,"Assessing the quality of tests for BPEL processes is a difficult task in projects following SOA principles. Since insufficient testing can lead to unforeseen defects that can be extremely costly in complex and mission critical environments, this problem needs to be addressed. By using formally defined test metrics that can be evaluated automatically by using an extension to the BPELUnit testing framework, testers are able to assess whether their white box tests cover all important areas of a BPEL process. This leads to better tests and thus to better BPEL processes because testers can improve their test cases by knowing which important areas of the BPEL process have not been tested yet.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5069049,no
Testing for trustworthiness in scientific software,2009,"Two factors contribute to the difficulty of testing scientific software. One is the lack of testing oracles - a means of comparing software output to expected and correct results. The second is the large number of tests required when following any standard testing technique described in the software engineering literature. Due to the lack of oracles, scientists use judgment based on experience to assess trustworthiness, rather than correctness, of their software. This is an approach well established for assessing scientific models. However, the problem of assessing software is more complex, exacerbated by the problem of code faults. This highlights the need for effective and efficient testing for code faults in scientific software. Our current research suggests that a small number of well chosen tests may reveal a high percentage of code faults in scientific software and allow scientists to increase their trust.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5069163,no
VLSI oriented fast motion estimation algorithm based on macroblock and motion feature analysis,2009,"The latest H.264/AVC standard can provide us superior coding performance. However, the new technique also brings about complexity problem, especially in motion estimation (ME) part. In hardware, the pipeline stage division of H.264 based ME engine degrades many software oriented complexity reduction schemes. In our paper, we propose one VLSI friendly fast ME algorithm. Firstly, pixel difference based adaptive subsampling achieves complexity reduction for homogeneous macroblock (MB). Secondly, a multiple reference frame elimination scheme is introduced to early terminate ME process for static MB. Thirdly, based on the motion feature analysis, the search range is adjusted to remove redundant search points. Experimental results show that, compared with hardware friendly full search algorithm, our proposed algorithm can reduce 71.09% to 95.26%ME time with negligible video quality degradation. Moreover, our fast algorithm can be combined with existing fast motion estimation algorithms such as UMHexagon search for further reduction in complexity and it is friendly to hardware implementation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5069209,no
Bi-direction Motion Vector retrieval based error concealment scheme for H.264/AVC,2009,"As the newest video coding standard, H.264/AVC adopts the high-efficiently predictive coding and variable length entropy coding to achieve high compression efficiency. On the other side, transmission errors become the major problem faced by video broadcasting service providers. Error concealment (EC) here is adopted to handle slices with huge conjunctive corrupted areas inside. Considering error propagation from corrupted slice to succeeding ones is the key factor affecting the video quality, this paper proposes a novel temporal EC scheme including the bi-direction motion vector (MV) retrieval method and an adaptive EC ordering basing on it. Background and motional steady shift part of slice will be given top and second priority, respectively. Combined with our proposed improved boundary matching algorithm (IBMA) which provides more accurate distortion function, experiments results show that our proposal achieves better performance under different error rate channel, compared with EC algorithm adopted in H.264 reference software.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5069214,no
Software reliability prediction using multi-objective genetic algorithm,2009,"Software reliability models are very useful to estimate the probability of the software fail along the time. Several different models have been proposed to predict the software reliability growth (SRGM); however, none of them has proven to perform well considering different project characteristics. The ability to predict the number of faults in the software during development and testing processes. In this paper, we explore Genetic Algorithms (GA) as an alternative approach to derive these models. GA is a powerful machine learning technique and optimization techniques to estimate the parameters of well known reliably growth models. Moreover, machine learning algorithms, proposed the solution overcome the uncertainties in the modeling by combining multiple models using multiple objective function to achieve the best generalization performance where. The objectives are conflicting and no design exists which can be considered best with respect to all objectives. In this paper, experiments were conducted to confirm these hypotheses. Then evaluating the predictive capability of the ensemble of models optimized using multi-objective GA has been calculated. Finally, the results were compared with traditional models.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5069339,no
A platform for software engineering research,2009,"Research in the fields of software quality, maintainability and evolution requires the analysis of large quantities of data, which often originate from open source software projects. Collecting and preprocessing data, calculating metrics, and synthesizing composite results from a large corpus of project artifacts is a tedious and error prone task lacking direct scientific value. The Alitheia Core tool is an extensible platform for software quality analysis that is designed specifically to facilitate software engineering research on large and diverse data sources, by integrating data collection and preprocessing phases with an array of analysis services, and presenting the researcher with an easy to use extension mechanism. Alitheia Core aims to be the basis of an ecosystem of shared tools and research data that will enable researchers to focus on their research questions at hand, rather than spend time on re-implementing analysis tools. In this paper, we present the Alitheia Core platform in detail and demonstrate its usefulness in mining software repositories by guiding the reader through the steps required to execute a simple experiment.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5069478,no
Does calling structure information improve the accuracy of fault prediction?,2009,"Previous studies have shown that software code attributes, such as lines of source code, and history information, such as the number of code changes and the number of faults in prior releases of software, are useful for predicting where faults will occur. In this study of an industrial software system, we investigate the effectiveness of adding information about calling structure to fault prediction models. The addition of calling structure information to a model based solely on non-calling structure code attributes provided noticeable improvement in prediction accuracy, but only marginally improved the best model based on history and non-calling structure code attributes. The best model based on history and non-calling structure code attributes outperformed the best model based on calling and non-calling structure code attributes.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5069481,no
Mining the coherence of GNOME bug reports with statistical topic models,2009,"We adapt latent Dirichlet allocation to the problem of mining bug reports in order to define a new information-theoretic measure of coherence. We then apply our technique to a snapshot of the GNOME Bugzilla database consisting of 431,863 bug reports for multiple software projects. In addition to providing an unsupervised means for modeling report content, our results indicate substantial promise in applying statistical text mining algorithms for estimating bug report quality. Complete results are available from our supplementary materials Web site at http://sourcerer.ics.uci.edu/msr2009/gnome_coherence.html.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5069486,no
Colony image classification on fuzzy mathematics,2009,"Due to colony image quality variation very much, an ordinary colony delineation algorithm is difficult to segment all kinds of colony images, therefore, image classification is necessary before image segmentation. The developed special colony image classification method in this study is to use definition of judgment set, determination of fuzzy judgment matrix, and defining weight set based on colony image characteristics, which are: (1) colony density; (2) colony area percentage (the ratio between colony area and whole area of the image); (3) colony area variance; and (4) grey contrast between colony and nutrient fluid. Experiments prove that the studied method make the classification reasonable, it can be used for colony image recognition and image pre-segmentation, and can also be expanded into the other similar applications.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5069943,no
Accurate Interprocedural Null-Dereference Analysis for Java,2009,"Null dereference is a commonly occurring defect in Java programs, and many static-analysis tools identify such defects. However, most of the existing tools perform a limited interprocedural analysis. In this paper, we present an interprocedural path-sensitive and context-sensitive analysis for identifying null dereferences. Starting at a dereference statement, our approach performs a backward demand-driven analysis to identify precisely paths along which null values may flow to the dereference. The demand-driven analysis avoids an exhaustive program exploration, which lets it scale to large programs. We present the results of empirical studies conducted using large open-source and commercial products. Our results show that: (1) our approach detects fewer false positives, and significantly more interprocedural true positives, than other commonly used tools; (2) the analysis scales to large subjects; and (3) the identified defects are often deleted in subsequent releases, which indicates that the reported defects are important.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5070515,no
MINTS: A general framework and tool for supporting test-suite minimization,2009,"Test-suite minimization techniques aim to eliminate redundant test cases from a test-suite based on some criteria, such as coverage or fault-detection capability. Most existing test-suite minimization techniques have two main limitations: they perform minimization based on a single criterion and produce suboptimal solutions. In this paper, we propose a test-suite minimization framework that overcomes these limitations by allowing testers to (1) easily encode a wide spectrum of test-suite minimization problems, (2) handle problems that involve any number of criteria, and (3) compute optimal solutions by leveraging modern integer linear programming solvers. We implemented our framework in a tool, called MINTS, that is freely-available and can be interfaced with a number of different state-of-the-art solvers. Our empirical evaluation shows that MINTS can be used to instantiate a number of different test-suite minimization problems and efficiently find an optimal solution for such problems using different solvers.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5070541,no
Alitheia Core: An extensible software quality monitoring platform,2009,"Research in the fields of software quality and maintainability requires the analysis of large quantities of data, which often originate from open source software projects. Pre-processing data, calculating metrics, and synthesizing composite results from a large corpus of project artefacts is a tedious and error prone task lacking direct scientific value. The Alitheia Core tool is an extensible platform for software quality analysis that is designed specifically to facilitate software engineering research on large and diverse data sources, by integrating data collection and preprocessing phases with an array of analysis services, and presenting the researcher with an easy to use extension mechanism. The system has been used to process several projects successfully, forming the basis of an emerging ecosystem of quality analysis tools.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5070560,no
Clustering and Metrics Thresholds Based Software Fault Prediction of Unlabeled Program Modules,2009,"Predicting the fault-proneness of program modules when the fault labels for modules are unavailable is a practical problem frequently encountered in the software industry. Because fault data belonging to previous software version is not available, supervised learning approaches can not be applied, leading to the need for new methods, tools, or techniques. In this study, we propose a clustering and metrics thresholds based software fault prediction approach for this challenging problem and explore it on three datasets, collected from a Turkish white-goods manufacturer developing embedded controller software. Experiments reveal that unsupervised software fault prediction can be automated and reasonable results can be produced with techniques based on metrics thresholds and clustering. The results of this study demonstrate the effectiveness of metrics thresholds and show that the standalone application of metrics thresholds (one-stage) is currently easier than the clustering and metrics thresholds based (two-stage) approach because the selection of cluster number is performed heuristically in this clustering based method.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5070617,no
Automated substring hole analysis,2009,"Code coverage is a common measure for quantitatively assessing the quality of software testing. Code coverage indicates the fraction of code that is actually executed by tests in a test suite. While code coverage has been around since the 60's there has been little work on how to effectively analyze code coverage data measured in system tests. Raw data of this magnitude, containing millions of data records, is often impossible for a human user to comprehend and analyze. Even drill-down capabilities that enable looking at different granularities starting with directories and going through files to lines of source code are not enough. Substring hole analysis is a novel method for viewing the coverage of huge data sets. We have implemented a tool that enables automatic substring hole analysis. We used this tool to analyze coverage data of several large and complex IBM software systems. The tool identified coverage holes that suggested interesting scenarios that were untested.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5070982,no
Configuration aware prioritization techniques in regression testing,2009,"Configurable software lets users customize applications in many ways, and is becoming increasingly prevalent. Regression testing is an important but expensive way to build confidence that software changes introduce no new faults as software evolves, resulting in many attempts to improve its performance given limited resources. Whereas problems such as test selection and prioritization at the test case level have been extensively researched in the regression testing literature, they have rarely been considered for configurations, though there is evidence that we should not ignore the effects of configurations on regression testing. This research intends to provide a framework for configuration aware prioritization techniques, evaluated through empirical studies.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5071025,no
Reducing search space of auto-tuners using parallel patterns,2009,"Auto-tuning is indispensable to achieve best performance of parallel applications, as manual tuning is extremely labor intensive and error-prone. Search-based auto-tuners offer a systematic way to find performance optimums, and existing approaches provide promising results. However, they suffer from large search spaces. In this paper we propose the idea to reduce the search space using parameterized parallel patterns. We introduce an approach to exploit context information from Master/Worker and Pipeline patterns before applying common search algorithms. The approach enables a more efficient search and is suitable for parallel applications in general. In addition, we present an implementation concept and a corresponding prototype for pattern-based tuning. The approach and the prototype have been successfully evaluated in two large case studies. Due to the significantly reduced search space a common hill climbing algorithm and a random sampling strategy require on average 54% less tuning iterations, while even achieving a better accuracy in most cases.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5071379,no
Discovering determinants of high volatility software,2009,"This topic paper presents a line of research that we are proposing that incorporates, in a very explicit and intentional way, human and organizational aspects in the prediction of troublesome (defect-prone or change-prone or volatile, depending on the environment) software modules. Much previous research in this area tries to identify these modules by looking at their structural characteristics, so that increased effort can be concentrated on those modules in order to reduce future maintenance costs. The outcome of this work will be a set of models that describe the relationships between software change characteristics (in particular human, organizational, and process characteristics), changes in the structural properties of software modules (e.g. complexity), and the future volatility of those modules. The impact of such models is two-fold. First, maintainers will have an improved technique for estimation of maintenance effort based on recent change characteristics. Also, the models will help identify management practices that are associated with high future volatility.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5071416,no
Software aging assessment through a specialization of the SQuaRE quality model,2009,"In the last years the software application portfolio has become a key asset for almost all companies. During their lives, applications undergo lots of changes to add new functionalities or to refactor older ones; these changes tend to reduce the quality of the applications themselves, causing the phenomenon known as software aging. Monitoring of software aging is very important for companies, but up to now there are no standard approaches to perform this task. In addition many of the suggested models assess software aging basing on few software features, whereas this phenomenon affects all of the software aspects. In 2005ISO/IEC released the SQuaRE quality model which covers several elements of software quality assessment, but some issues make SQuaRE usage quite difficult. The purpose of this paper is to suggest an approach to software aging monitoring that considers the software product in its wholeness and to provide a specialization of the SQuaRE quality model which allows to perform this task.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5071554,no
Existing model metrics and relations to model quality,2009,"This paper presents quality goals for models and provides a state-of-the-art analysis regarding model metrics. While model-based software development often requires assessing the quality of models at different abstraction and precision levels and developed for multiple purposes, existing work on model metrics do not reflect this need. Model size metrics are descriptive and may be used for comparing models but their relation to model quality is not well-defined. Code metrics are proposed to be applied on models for evaluating design quality while metrics related to other quality goals are few. Models often consist of a significant amount of elements, which allows a large amount of metrics to be defined on them. However, identifying useful model metrics, linking them to model quality goals, providing some baseline for interpretation of data, and combining metrics with other evaluation models such as inspections requires more theoretical and empirical work.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5071555,no
An investigation of using Neuro-Fuzzy with software size estimation,2009,"Neuro-fuzzy refers to a hybrid intelligent system using both neural network and fuzzy logic. In this study, neuro-fuzzy is applied to a backfiring and categorical data size estimation model. Evaluation was conducted to determine whether a neuro-fuzzy approach improves software size estimations. It was found that a neuro-fuzzy approach provides minimal improvement over the traditional backfiring sizing technique.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5071557,no
Assessing Quality of Derived Non Atomic Data by Considering Conflict Resolution Function,2009,We present a data quality manager (DQM) prototype providing information regarding the elements of derived non-atomic data values. Users are able to make effective decisions by trusting data according to the description of the conflict resolution function that was utilized for fusing data along with the quality properties of data ancestor. The assessment and ranking of non-atomic data is possible by the specification of quality properties and priorities from users at any level of experience.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5071816,no
SPEWS: A Framework for the Performance Analysis of Web Services Orchestrated with BPEL4WS,2009,"This paper addresses quality of service aspects of Web services (WS) orchestrations created using the business process execution language for Web services (BPEL4WS). BPEL4WS is a promising language describing the WS orchestrations in form of business processes, but it lacks of a sound formal semantic, which hinders the formal analysis and verification of business processes specified in it. Formal methods, like Petri nets (PN), may provide a means to analyse BPEL4WS processes, evaluating its performance, detecting weaknesses and errors in the process model already at design-time. A framework for transformation of BPEL4WS into generalized stochastic Petri nets (GSPN) is proposed to analyse the performance and throughput of WS, based on the execution of orchestrated processes.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5072545,no
A State Transition Model for Anti-Interference on the Embedded System,2009,"Microprogrammed control unit (MCU) and embedded systems are widely used in all kinds of industrial, communication devices and household appliances. However, for the embedded system, even with well hardware protection, the stability are often impacted by the complicated electro magnetic interference (EMI) and the others interference. To get a better system stability ratio, this paper introduces a potent state transition model for the anti-interference intention instead of the empiristic methods. The states of the embedded system are classified into three types: I/O, computing, and error. By the state transition diagram, we deduced a set of anti-interference probability model, which guide the software measures for the anti-interference intention and reduce the unstable probability of the embedded system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5072677,no
Adaptable Link Quality Estimation for Multi Data Rate Communication Networks,2009,"QoS-sensitive applications transmitted over wireless links require precise knowledge of the wireless environment. However, the dynamic nature of wireless channel, together with its different configurations and types, makes so called link quality estimation (LQE) a difficult task. This paper looks into the design of an accurate and fast LQE method for a multi data rate environment. We investigate the impact of various conditions on the LQE accuracy. In result, two different link quality estimation sources, i.e., based on hello packet delivery ratio and signal strength, are measured and their performances are compared. We find that these two methods are not always accurate. As an improvement we propose an adaptive LQE method that chooses different LQE indicators depending on the wireless environment. The performance of the proposed method is verified via an extensive measurement campaign.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5073358,no
Are real-world power systems really safe?,2009,"With the advent of new power system analysis software, a more detailed arc flash (AF) analysis can be performed under various load conditions. These new tools can also evaluate equipment damage, design systems with lower AF, and predict electrical fire locations based on high AF levels. This article demonstrates how AF levels change with available utility mega volt amperes (MVA), additions in connected load, and selection of system components. This article summarizes a detailed analysis of several power systems to illustrate the possible misuses of the ""Risk Category Classification Tables"" in the Standard for Electrical Safety Requirements for Employee Workplaces, 2004 (NFPA 70E), while pointing toward future improvements of such standards.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5073963,no
Level-2 Calorimeter Trigger Upgrade at CDF,2009,"The CDF Run II level 2 calorimeter trigger is implemented in hardware and is based on a simple algorithm that was used in Run I. This system has worked well for Run II at low luminosity. As the Tevatron instantaneous luminosity increases, the limitation due to this simple algorithm starts to become clear. As a result, some of the most important jet and MET (missing ET) related triggers have large growth terms in cross section at higher luminosity. In this paper, we present an upgrade of the L2CAL system which makes the full calorimeter trigger tower information directly available to the level 2 decision CPU. This upgrade is based on the Pulsar, a general purpose VME board developed at CDF and already used for upgrading both the level 2 global decision crate and the level 2 silicon vertex tracking. The upgrade system allows more sophisticated algorithms to be implemented in software and both level 2 jets and MET can be made nearly equivalent to offline quality, thus significantly improving the performance and flexibility of the jet and MET related triggers. This is a natural expansion of the already-upgraded level 2 trigger system, and is a big step forward to improve the CDF triggering capability at level 2. This paper describes the design, the hardware and software implementation and the performance of the upgrade system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5076030,no
Improved Spatial Resolution in PET Scanners Using Sampling Techniques,2009,"Increased focus towards improved detector spatial resolution in PET has led to the use of smaller crystals in some form of light sharing detector design. In this work we evaluate two sampling techniques that can be applied during calibrations for pixelated detector designs in order to improve the reconstructed spatial resolution. The inter-crystal positioning technique utilizes sub-sampling in the crystal flood map to better sample the Compton scatter events in the detector. The Compton scatter rejection technique, on the other hand, rejects those events that are located further from individual crystal centers in the flood map. We performed Monte Carlo simulations followed by measurements on two whole-body scanners for point source data. The simulations and measurements were performed for scanners using scintillators with Z<sub>eff</sub> ranging from 46.9 to 63 for LaBr<sub>3</sub> and LYSO, respectively. Our results show that near the center of the scanner, inter-crystal positioning technique leads to a gain of about 0.5-mm in reconstructed spatial resolution (FWHM) for both scanner designs. In a small animal LYSO scanner the resolution improves from 1.9-mm to 1.6-mm with the inter-crystal technique. The Compton scatter rejection technique shows higher gains in spatial resolution but at the cost of reduction in scanner sensitivity. The inter-crystal positioning technique represents a modest acquisition software modification for an improvement in spatial resolution, but at a cost of potentially longer data correction and reconstruction times. The Compton scatter rejection technique, while also requiring a modest acquisition software change with no increased data correction and reconstruction times, will be useful in applications where the scanner sensitivity is very high and larger improvements in spatial resolution are desirable.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5076098,no
QoS Enhancements and Performance Analysis for Delay Sensitive Applications,2009,This paper presents a comprehensive system modeling and analysis approach for both predicting and controlling queuing delay at an expected value under multi-class traffic in a single buffer. This approach could effectively enhance QoS delivery for delay sensitive applications. Six major contributions are given in the paper: (1) a discrete-time analytical model is developed for capturing multi-class traffic with binomial distribution; (2) a control strategy with dynamic queue thresholds is used in simulation experiments to control the delay at a specified value within the buffer; (3) the feasibility of the system is validated by comparing theoretical analysis with simulation scenarios; (4) the arrival rate can be adjusted for each forthcoming time window during the simulation with multi-packet sources; (5) statistical evaluation is performed to show both efficiency and accuracy of the analytical and simulation results; (6) a graphical user interface is developed that can provide flexible configuration for the simulation and validate input values.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5076218,no
Packet Scheduling Algorithm with QoS Provision in HSDPA,2009,"The 3rd generation WCDMA standard has been enhanced to offer significantly increased performance for packet data. But coming application like multimedia on desiring data rates will spur the UMTS can support. To support for such high data rates, high speed downlink access (HSDPA), labeled as a 3.5G wireless system , has been published in UMTS Release05. Under the HSDPA system, have to support high speed transmission and promises a peak data rate of up to 10 Mb/s. To achieve such high speed rate, system provides the channel quality indicator (CQI) information to detect the air interface condition. Then, there are many channel condition related scheduling schemes have been proposed to attend achievement of high system performance and guarantee the quality-of-service (QoS) requirements. However, there is no solution of packet scheduling algorithm can consider differences of data-types priority management under the hybrid automatic repeat (H-ARQ) scenario. In this paper, we propose the weight combination packet scheduling algorithm to target to enhance the system efficiency and balanced QoS requirements. The proposed schemes is simulated with OPNET simulator and compared with the Max CIR and PF algorithms in fast fading channel. Simulation result shows that the proposed algorithm can both effectively increase the cell throughput and meet userpsilas satisfaction base on QoS requirements.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5076256,no
Mining Frequent Flows Based on Adaptive Threshold with a Sliding Window over Online Packet Stream,2009,"Traffic measurement is an important component of network applications including usage-based charging, anomaly detection and traffic engineering. With high-speed linksiquestthe main problem with traffic measurement is its lack of scalability. Aiming at circumvent this deficiency, we develop a novel and scalable sketch to mine frequent flows over online packet stream. Dividing the sliding window into buckets, the sketch can not only be easily-implemented, but also remove obsolete data to identify recent usage trends. Besides, an unbiased estimator is introduced based on a pruning function to preserve large flows. In particular, we illustrate a mechanism of configuring adaptive thresholds which are bound to the actual data without artificial behavior. The adaptive threshold can be regulated to target the mean number of the reserved flows in order to protect memory resources. Experiments are also conducted based on real network traces. Results demonstrate that the proposed method can achieve adaptability and controllability of resource consumption without sacrificing accuracy.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5076841,no
Delay and Throughput Trade-Off in WiMAX Mesh Networks,2009,"We investigate WiMAX mesh networks in terms of delay and throughput trade-off. For a given topology, using our proposed analytical model we study how slot allocation policy and forwarding probability affects per-node and network performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5076857,no
Packet Loss Based Real Time Network Condition Estimation,2009,"Packet loss behaviors have been studied for many years. Traditional packet loss evaluation criteria are not sufficient to describe characteristics of losses: the loss rate is too simple while traditional loss episodes sometimes are not stable due to the randomness of loss events. In this paper, we propose an extended loss episode approach, merged loss episode based on the traditional definition, to estimate the network condition. Also we presented the corresponding algorithm for calculation. Simulation results show that compared with traditional loss episodes, the merged version could be more stable and effective to measure network performance on loss issues.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5076900,no
Analysis of Real-time Multimedia Transmission over PR-SCTP with Failover Detection Delay and Reliability Level Differential,2009,The growing availability of different wireless access technologies provides the opportunity for real-time distribution of multimedia content using multi-homing technology. Investigating the behaviors and quality of such applications under heavy network load is necessary. This paper studies the effect of path failure detection threshold and reliability level on Stream Control Transmission Protocol Partial Reliability Extension (PR-SCTP) performance in symmetric and asymmetric path conditions respectively with different path loss rates. The platform Evalvid-SCTP implemented in the University of Delawarepsilas SCTP ns-2 module performs the emulation experiment.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5076959,no
Predicting Quality of Object-Oriented Systems through a Quality Model Based on Design Metrics and Data Mining Techniques,2009,"Most of the existing object-oriented design metrics and data mining techniques capture similar dimensions in the data sets, thus reflecting the fact that many of the metrics are based on similar hypotheses, properties, and principles. Accurate quality models can be built to predict the quality of object-oriented systems by using a subset of the existing object-oriented design metrics and data mining techniques. We propose a software quality model, namely QUAMO (QUAlity MOdel) which is based on divide-and-conquer strategy to measure the quality of object-oriented systems through a set of object-oriented design metrics and data mining techniques. The primary objective of the model is to make similar studies on software quality more comparable and repeatable. The proposed model is augmented from five quality models, namely McCall Model, Boehm Model, FURPS/FURPS+ (i.e. functionality, usability, reliability, performance, and supportability), ISO 9126, and Dromey Model. We empirically evaluated the proposed model on several versions of JUnit releases. We also used linear regression to formulate a prediction equation. The technique is useful to help us interpret the results and to facilitate comparisons of results from future similar studies.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5077034,no
D<inf>n</inf>-based architecture assessment of Java Open Source software systems,2009,"Since their introduction in 1994 the Martin's metrics became popular in assessing object-oriented software architectures. While one of the Martin metrics, normalised distance from the main sequence D<sub>n</sub>, has been originally designed with assessing individual packages, it has also been applied to assess quality of entire software architectures. The approach itself, however, has never been studied. In this paper we take the first step to formalising the D<sub>n</sub>-based architecture assessment of Java open source software. We present two aggregate measures: average normalised distance from the main sequence Dmacr<sub>n</sub>, and parameter of the fitted statistical model lambda. Applying these measures to a carefully selected collection of benchmarks we obtain a set of reference values that can be used to assess quality of a system architecture. Furthermore, we show that applying the same measures to different versions of the same system provides valuable insights in system architecture evolution.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090043,no
Classifying software visualization tools using the Bloom's taxonomy of cognitive domain,2009,"There are a lot of software visualization tools existing for various purposes. Therefore, how to choose the right visualization tool for a specific task becomes an important issue. Although there are a few taxonomies for classifying visualization tools, each has its own defects. This paper proposes a new classification schema based on the widely used Bloom's cognitive taxonomy. We summarize a set of questions for each level of the Bloom taxonomy using the well defined verbs, to identify if the visualization tool belongs to the individual cognitive level, which in turn can determine the purposes of the tool. We also conduct case studies on four tools to evaluate this new approach. The result demonstrates that the new classification schema can provide good guidance for people to choose a useful visualization tool.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090082,no
Program phase and runtime distribution-aware online DVFS for combined Vdd/Vbb scaling,2009,"Complex software programs are mostly characterized by phase behavior and runtime distributions. Due to the dynamism of the two characteristics, it is not efficient to make workload predictions during design-time. In our work, we present a novel online DVFS method that exploits both phase behavior and runtime distribution during runtime in combined Vdd/Vbb scaling. The presented method performs a bi-modal analysis of runtime distribution, and then a runtime distribution-aware workload prediction based on the analysis. In order to minimize the runtime overhead of the sophisticated workload prediction method, it performs table lookups to the pre-characterized data during runtime without compromising the quality of energy reduction. It also offers a new concept of program phase suitable for DVFS. Experiments show the effectiveness of the presented method in the case of H.264 decoder with two sets of long-term scenarios consisting of total 4655 frames. It offers 6.6% ~ 33.5% reduction in energy consumption compared with existing offline and online solutions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090699,no
pTest: An adaptive testing tool for concurrent software on embedded multicore processors,2009,"More and more processor manufacturers have launched embedded multicore processors for consumer electronics products because such processors provide high performance and low power consumption to meet the requirements of mobile computing and multimedia applications. To effectively utilize computing power of multicore processors, software designers interest in using concurrent processing for such architecture. The master-slave model is one of the popular programming models for concurrent processing. Even if it is a simple model, the potential concurrency faults and unreliable slave systems still lead to anomalies of entire system. In this paper, we present an adaptive testing tool called pTest to stress test a slave system and to detect the synchronization anomalies of concurrent software in the master-slave systems on embedded multicore processors. We use a probabilistic finite-state automaton(PFA) to model the test patterns for stress testing and shows how a PFA can be applied to pTest in practice.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090812,no
Generation of compact test sets with high defect coverage,2009,"Multi-detect (N-detect) testing suffers from the drawback that its test length grows linearly with N. We present a new method to generate compact test sets that provide high defect coverage. The proposed technique makes judicious use of a new pattern-quality metric based on the concept of output deviations. We select the most effective patterns from a large N-detect pattern repository, and guarantee a small test set as well as complete stuck-at coverage. Simulation results for benchmark circuits show that with a compact, 1-detect stuck-at test set, the proposed method provides considerably higher transition-fault coverage and coverage ramp-up compared to another recently-published method. Moreover, in all cases, the proposed method either outperforms or is as effective as the competing approach in terms of bridging-fault coverage and the surrogate BCE+ metric. In many cases, higher transition-fault coverage is obtained than much larger N-detect test sets for several values of N. Finally, our results provide the insight that, instead of using N-detect testing with as large N as possible, it is more efficient to combine the output deviations metric with multi-detect testing to get high-quality, compact test sets.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090833,no
Software Sensor-Based STATCOM Control Under Unbalanced Conditions,2009,"A new control strategy for static compensators (STATCOMs) operating under unbalanced voltages and currents is presented in this paper. The proposed strategy adopts a state observer (software sensor) to estimate ac voltages at the STATCOM connection point. This way, physical voltage sensors are not needed and the hardware gets simplified, among other advantages. Using the superposition principle, a controller is designed to independently control both positive and negative sequence currents, eliminating the risk of overcurrents during unbalanced conditions and improving the power quality at the STATCOM connection bus. Two operating modes are proposed for the computation of the reference currents, depending on whether the objective is to compensate unbalanced load currents or regulate bus voltages. The proposed observer allows positive and negative sequences to be estimated in a fraction of the fundamental cycle, avoiding the delay often introduced by filter-based methods. Overall, the STATCOM performance is improved under unbalanced conditions, according to simulation results presented in the paper.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5109843,no
Risk-Aware SLA Brokering Using WS-Agreement,2009,"Service level agreements (SLAs) are facilitators for widening the commercial uptake of grid technology. They provide explicit statements of expectation and obligation between service consumers and providers. However, without the ability to assess the probability that an SLA might fail, commercial uptake will be restricted, since neither party will be willing to agree. Therefore, risk assessment mechanisms are critical to increase confidence in grid technology usage within the commercial sector. This paper presents an SLA brokering mechanism including risk assessment techniques which evaluate the probability of SLA failure. WS-agreement and risk metrics are used to facilitate SLA creation between service consumers and providers within a typical grid resource usage scenario.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5136662,no
Quantifying software reliability and readiness,2009,"As the industry moves to more mature software processes (e.g., CMMI) there is increased need to adopt more rigorous, sophisticated (i.e., quantitative) metrics. While quantitative product readiness criteria are often used for business cases and related areas, software readiness is often assessed more subjectively & qualitatively. Quite often there is no explicit linkage to original performance and reliability requirements for the software. The criteria are primarily process-oriented (versus product oriented) and/or subjective. Such an approach to deciding software readiness increases the risk of poor field performance and unhappy customers. Unfortunately, creating meaningful and useful quantitative in-process metrics for software development has been notoriously difficult. This paper describes novel and quantitative software readiness criteria to support objective and effective decision-making at product shipment. The method organizes and streamlines existing quality and reliability data into a simple metric and visualizations that are applicable across products and releases. The methodology amalgamates two schools of thoughts in quantitative terms: product and process parameters that have been adequately represented to formalize the software readiness index. Parameters from all aspects of software development life cycle (e.g., requirements, project management & resources, development & testing, audits & assessments, stability and reliability, and technical documentation) that could impact the readiness index are considered.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5137352,no
Accuracy improvement of multi-stage change-point detection scheme by weighting alerts based on false-positive rate,2009,"One promising approach for large-scale simultaneous events (e.g., DDoS attacks and worm epidemics) is to use a multi-stage change-point detection scheme. The scheme adopts two-stage detection. In the first stage, local detectors (LDs), which are deployed on each monitored subnet, detects a change point in a monitored metric such as outgoing traffic rate. If an LD detects a change-point, it sends an alert to global detector (GD). In the second stage, GD checks whether the proportion of LDs that send alerts simultaneously is greater than or equal to a threshold value. If so, it judges that large-scale simultaneous events are occurring. In previous studies for the multi-stage change-point detection scheme, it is assumed that weight of each alert is identical. Under this assumption, false-positive rate of the scheme tends to be high when some LDs sends false-positive alerts frequently. In this paper, we weight alerts based on false-positive rate of each LD in order to decrease false-positive rate of the multi-stage change-point detection scheme. In our scheme, GD infers false-positive rate of each LD and gives lower weight to LDs with higher false-positive rate. Simulation results show that our proposed scheme can achieve lower false-positive rate than the scheme without alert weighting under the constraint that detection rate must be 1.0.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5137356,no
Static Detection of Un-Trusted Variables in PHP Web Applications,2009,"Web applications support more and more our daily activities, it's important to improve their reliability and security. The content which users input to Web applications' server-side is named un-trusted content. Un-trusted content has a significant impact on the reliability and security of Web applications, so detecting the un-trusted variables in server-side program is important for improving the quality of Web applications. The previous methods have poor performance on weak typed and none typed server-side programs. To address this issue, this paper proposed a new technique for detecting un-trusted variables in PHP web applications (PHP is a weak typed server- side language). The technique is based upon a two phases static analysis algorithm. In the first phase, we extract modules from the Web application. Then un-trusted variables are detected from modules in the second phase. An implementation of the proposed techniques DUVP was also presented in the paper and it's successfully applied to detect un-trusted variables in large-scale PHP web application.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5138078,no
Study on the Quality Prediction Model of Software Development,2009,"Based on the fundamental quality data given by the software development centre and in accordance of the principle analysis, the project quality standard had undertaken analysis and predication. Three stages were respectively as follows: project estimation, project budget and project propagation. The three stages had co-related the differential value of workload, changes of project scope and quality standards between the influences behind of quality standards. The result shown that, the model reflected tremendous effect in its defect elimination rate and rate of problem occurrences [defects density] during the prediction process in the software development.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5138097,no
Power quality analysis of traction supply systems with high speed train,2009,"The characteristic of harmonic currents of high speed train is significantly different from DC-drived locomotives. The rectifier of CRH5 electric multiple units (EMUs) in China is researched in this paper.The principle of four-quadrant converter and its predictive current control strategy are studied. The formula of fundamental and harmonic currents of the parallel two-level four-quadrant converter is given under different power and grid voltage based on double Fourier transform method. Therefore the distribution characteristics of harmonic currents are analyzed. The simulation software based on Matlab/Simulink is developed for harmonic analysis of CRH5 EMUs. With different traction power, braking power and traction grid voltage, the harmonic currents of EMUs injecting to the traction grid are studied and compared.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5138600,no
An efficient parallel approach to Random Sample Matching (pRANSAM),2009,"This paper introduces a parallelized variant of the Random Sample Matching (RANSAM) approach, which is a very time and memory efficient enhancement of the common Random Sample Consensus (RANSAC). RANSAM exploits the theory of the birthday attack whose mathematical background is known from cryptography. The RANSAM technique can be applied to various fields of application such as mobile robotics, computer vision, and medical robotics. Since standard computers feature multi-core processors nowadays, a considerable speedup can be obtained by distributing selected subtasks of RANSAM among the available cores. First of all this paper addresses the parallelization of the RANSAM approach. Several important characteristics are derived from a probabilistic point of view. Moreover, we apply a fuzzy criterion to compute the matching quality, which is an important step towards real-time capability. The algorithm has been implemented for Windows and for the QNX RTOS. In an experimental section the performance of both implementations is compared and our theoretical results are validated.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5152282,no
Research of micro integrated navigation system based on MIMU,2009,"According to the characteristics of applicable environment and MIMU itself, a new Kalman filter was designed. The shallow integrated mode was adopted for improving fault-tolerant and reliability, and considering the low-precision MIMU, mathematics model of integrated navigation system was reasonably simplified. An adoptive Kalman filter was designed through judging average value and variance of the residual, and the weights of the filter were adoptively adjusted along with external environment. Thus it can avoid divergence and enhance system robustness. The simulation test show attitude angle error, velocity error and position error were well estimated; and then vehicle test illustrated the design was feasible.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5156581,no
A Proposal for Stable Semantic Metrics Based on Evolving Ontologies,2009,"In this paper, we propose a set of semantic cohesion metrics for ontology measurement, which can be used to assess the quality of evolving ontologies in the context of dynamic and changing Web. We argue that these metrics are stable and can be used to measure ontology semantics rather than ontology structures. Measuring ontology quality with semantic inconsistencies caused by ontology evolutions, is mainly considered in this paper. The proposed semantic cohesion metrics are theoretically and empirically validated.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5158958,no
Realization of Wavelet Soft Threshold De-noising Technology Based on Visual Instrument,2009,"Electric power network injects with amount of harmonic current because of widespread use of nonlinear load, which does great harm to the using electricity consumption. In order to prevent harmonic current from influencing safety of systempsilas operation, we should know how much the distorted wave contains harmonic and take corresponding measure to make suppression or compensation of it. But due to a lot of noise affect existing, so detection result is inaccuracy, by using multi-resolution wavelet method, we get more accurate network voltage and currency, which can carry on next harmonic detection, etc. By simulation software of MATLAB combing with LabVIEW, wavelet de-noising has better function in filtering high frequency and noise signal, etc than traditional low-passing filter of Butterworth. Through harmonic detection simulation, result is exact through <i>THD</i>% calculation, which difference between standard value and measurement value is very small in <i>THD</i>% measurement error of 0.01%. Wavelet soft threshold de-noising technology can be applied into other monitor, such as three-phase unbalance factor monitor, frequency tracking monitor, fundamental wave monitor, etc.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5159136,no
Aerosol Lidar Intercomparison in the Framework of SPALINETâ€”The Spanish Lidar Network: Methodology and Results,2009,"A group of eight Spanish lidars was formed in order to extend the European Aerosol Research Lidar Network-Advanced Sustainable Observation System (EARLINET-ASOS) project. This study presents intercomparisons at the hardware and software levels. Results of the system intercomparisons are based on range-square-corrected signals in cases where the lidars viewed the same atmospheres. Comparisons were also made for aerosol backscatter coefficients at 1064 nm (2 systems) and 532 nm (all systems), and for extinction coefficients at 532 nm (2 systems). In total, three field campaigns were carried out between 2006 and 2007. Comparisons were limited to the highest layer found before the free troposphere, i.e., either the atmospheric boundary layer or the aerosol layer just above it. Some groups did not pass the quality assurance criterion on the first attempt. Following modification and improvement to these systems, all systems met the quality criterion. The backscatter algorithm intercomparison consisted of processing lidar signal profiles simulated for two types of atmospheric conditions. Three stages with increasing knowledge of the input parameters were considered. The results showed that all algorithms work well when all inputs are known. They also showed the necessity to perform, when possible, additional measurements to attain better estimation of the lidar ratio, which is the most critical unknown in the elastic lidar inversion.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5159490,no
A cross-input adaptive framework for GPU program optimizations,2009,"Recent years have seen a trend in using graphic processing units (GPU) as accelerators for general-purpose computing. The inexpensive, single-chip, massively parallel architecture of GPU has evidentially brought factors of speedup to many numerical applications. However, the development of a high-quality GPU application is challenging, due to the large optimization space and complex unpredictable effects of optimizations on GPU program performance. Recently, several studies have attempted to use empirical search to help the optimization. Although those studies have shown promising results, one important factor-program inputs-in the optimization has remained unexplored. In this work, we initiate the exploration in this new dimension. By conducting a series of measurement, we find that the ability to adapt to program inputs is important for some applications to achieve their best performance on GPU. In light of the findings, we develop an input-adaptive optimization framework, namely G-ADAPT, to address the influence by constructing cross-input predictive models for automatically predicting the (near-)optimal configurations for an arbitrary input to a GPU program. The results demonstrate the promise of the framework in serving as a tool to alleviate the productivity bottleneck in GPU programming.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5160988,no
Crash fault detection in celerating environments,2009,"Failure detectors are a service that provides (approximate) information about process crashes in a distributed system. The well-known ldquoeventually perfectrdquo failure detector, diamP, has been implemented in partially synchronous systems with unknown upper bounds on message delay and relative process speeds. However, previous implementations have overlooked an important subtlety with respect to measuring the passage of time in ldquoceleratingrdquo environments, in which absolute process speeds can continually increase or decrease while maintaining bounds on relative process speeds. Existing implementations either use action clocks, which fail in accelerating environments, or use real-time clocks, which fail in decelerating environments. We propose the use of bichronal clocks, which are a composition of action clocks and real-time clocks. Our solution can be readily adopted to make existing implementations of diamP robust to process celeration, which can result from hardware upgrades, server overloads, denial-of-service attacks, and other system volatilities.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5161050,no
Evaluating the performance and intrusiveness of virtual machines for desktop grid computing,2009,"We experimentally evaluate the performance overhead of the virtual environments VMware Player, QEMU, VirtualPC and VirtualBox on a dual-core machine. Firstly, we assess the performance of a Linux guest OS running on a virtual machine by separately benchmarking the CPU, file I/O and the network bandwidth. These values are compared to the performance achieved when applications are run on a Linux OS directly over the physical machine. Secondly, we measure the impact that a virtual machine running a volunteer @home project worker causes on a host OS. Results show that performance attainable on virtual machines depends simultaneously on the virtual machine software and on the application type, with CPU-bound applications much less impacted than IO-bound ones. Additionally, the performance impact on the host OS caused by a virtual machine using all the virtual CPU, ranges from 10% to 35%, depending on the virtual environment.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5161134,no
Predicting cache needs and cache sensitivity for applications in cloud computing on CMP servers with configurable caches,2009,"QoS criteria in cloud computing require guarantees about application runtimes, even if CMP servers are shared among multiple parallel or serial applications. Performance of computation-intensive application depends significantly on memory performance and especially cache performance. Recent trends are toward configurable caches that can dynamically partition the cache among cores. Then, proper cache partitioning should consider the applications' different cache needs and their sensitivity towards insufficient cache space. We present a simple, yet effective and therefore practically feasible black-box model that describes application performance in dependence on allocated cache size and only needs three descriptive parameters. Learning these parameters can therefore be done with very few sample points. We demonstrate with the SPEC benchmarks that the model adequately describes application behavior and that curve fitting can accomplish very high accuracy, with mean relative error of 2.8% and maximum relative error of 17%.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5161233,no
Generalized Tardiness Quantile Metric: Distributed DVS for Soft Real-Time Web Clusters,2009,"Performing QoS (Quality of Service) control in large computing systems requires an on line metric that is representative of the real state of the system. The Tardiness Quantile Metric (TQM) introduced earlier allows control of QoS by measuring efficiently how close to the specified QoS the system is, assuming specific distributions. In this paper we generalize this idea and propose the Generalized Tardiness Quantile Metric (GTQM). By using an online convergent sequential process, defined from a Markov chain, we derive quantile estimations that do not depend on the shape of the workload probability distribution. We then use GTQM to keep QoS controlled in a fine grain manner, saving energy in soft real-time Web clusters. To evaluate the new metric, we show practical results in a real Web cluster running Linux, Apache, and MySQL, with our QoS control and for both a deterministic workload and an e-commerce workload. The results show that the GTQM method has excellent workload prediction capabilities, which immediately translates in more accurate QoS control, allowing for slower speeds and larger energy savings than the state-of-the-art in soft real-time Web cluster systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5161518,no
Machine Vision for Automated Inspection of Surgical Instruments,2009,"In order to improve the measuring speed and accuracy of the surgical instruments inspection, a computer-aided measuring and analysis method was highly needed. The purpose of this study was provide surgical instruments manufactures with a surgical and microsurgical instruments inspection system, so as to ensure that surgical instruments came from assembly lines were manufactured correctly. Software and hardware components were designed and developed into a machine vision inspection system. Different surgical instruments such as scissors, forceps, clamps, microsurgical instruments were inspected; the system accuracy could be 0.001 mm. The result showed that the detection system had significant effects. The detection system could measure the vast amount of measurements of surgical instruments required for mass-produced devices.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5163022,no
FTTH Network Management Software Tool: SANTAD ver 2.0,2009,"This paper focused on developing a fiber-to-the-home (FTTH) network management software tool named Smart Access Network Testing, Analyzing and Database (SANTAD) ver 2.0 based on Visual Basic for transmission surveillance and fiber fault identification. SANTAD will be installed with optical line terminal (OLT) at central office (CO) to allow the network service providers and field engineers to detect a fiber fault, locate the failure location, and monitor the network performance downwardly from CO towards customer residential locations. SANTAD is able to display the status of each optical network unit (ONU) connected line on a single computer screen with capability to configure the attenuation and detect the failure simultaneously. The failure information will be delivered to the field engineers for promptly actions, meanwhile the failure line will be diverted to protection line to ensure the traffic flow continuously. The database system enable the history of network scanning process be analyzed and studied by the engineer.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5163850,no
Autonomous SPYÂ©: Intelligent Web proxy caching detection using neurocomputing and Particle Swarm Optimization,2009,"The demand for Internet content rose dramatically. Servers became more and more powerful and the bandwidth of end user connections and backbones grew constantly. Nevertheless users often experience poor performance when they access Web sites or download files. Reasons for such problems are often performance problems which occur directly on the servers (e.g. poor performance of server-side applications or during flash crowds), problems concerning the network infrastructure (e.g. long geographical distances, network overloads, etc.) and a surprising fact is that many people tend to access the same piece of information repeatedly. Web caching has been recognized as the effective schemes to alleviate the service bottleneck, minimize the user access latency and reduce the network traffic. Consequently, in this paper we discuss an alternative way to tackle these problems with an implementation of autonomous SPY tool. This tool is capable to self directed either to cache or not to cache the objects in a document based on the behavior of users' activities (number of object hits, script size of objects, and time to receive objects) in Internet based electronic services (e-services) for enhancing Web access.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5164846,no
Application of Service Oriented Architecture to emulation of onboard processing satellite systems,2009,"With increased service demands on remote sensing and communication satellites researchers are considering equipping the next generation of satellite constellations with onboard computer systems capable of general purpose computing in space. This project focuses on the design of an emulator capable of using a local area network to emulate both the passing of data between satellites and to ground-based users and the execution of data processing applications. Because satellite constellations perform a wide variety of services, the emulator design incorporates Service Oriented Architecture (SOA) to allow the flexibility of running a variety of different software applications and to easily support new, as-yet designed applications. This emulator uses the Hypercast package of Java network interfaces and a modular programming structure for passing information and files between simulated satellite and user objects. The data routing is controlled through the use of the commercial software Satellite Toolkit (STK), which calculates all communication capability and quality information for any satellite constellation. The system is capable of passing any necessary data allowing for the design of a universal interface for how applications receive information and return results. Currently, this capability is tested using image capture/passing and image classification software both of which have proved successful applications on a Service Oriented Architecture system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5166188,no
Notice of Violation of IEEE Publication Principles<BR>Evaluation of GP Model for Software Reliability,2009,"Notice of Violation of IEEE Publication Principles<BR><BR>""Evaluation of GP Model for Software Reliability,""<BR>by S. Paramasivam, and M. Kumaran,<BR>in the 2009 International Conference on Signal Processing Systems, May 2009<BR><BR>After careful and considered review of the content and authorship of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>This paper contains significant portions of original text from the paper cited below. The original text was copied without attribution (including appropriate references to the original author(s) and/or paper title) and without permission.<BR><BR>Due to the nature of this violation, reasonable effort should be made to remove all past references to this paper, and future references should be made to the following article:<BR><BR>""A Comparative Evaluation of Using Genetic Programming for Predicting Fault Count Data,""<BR>by W. Afzal, R. Torkar,<BR>in the Third International Conference on Software Engineering Advances, 2008. ICSEA '08, pp.407-414, October 2008<BR><BR>There has been a number of software reliability growth models (SRGMs) proposed in literature. Due to several reasons, such as violation of modelspsila assumptions and complexity of models, the practitioners face difficulties in knowing which models to apply in practice. This paper presents a comparative evaluation of traditional models and use of genetic programming (GP) for modeling software reliability growth based on weekly fault count data of three different industrial projects. The motivation of using a GP approach is its ability to evolve a model based entirely on prior data without the need of making underlying assumptions. The results show the strengths of using GP for predicting fault count.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5166890,no
Estimation of Defect Proneness Using Design Complexity Measurements in Object-Oriented Software,2009,"Software engineering is continuously facing the challenges of growing complexity of software packages and increased level of data on defects and drawbacks from software production process. This makes a clarion call for inventions and methods which can enable a more reusable, reliable, easily maintainable and high quality software systems with deeper control on software generation process. Quality and productivity are indeed the two most important parameters for controlling any industrial process. Implementation of a successful control system requires some means of measurement. Software metrics play an important role in the management aspects of the software development process such as better planning, assessment of improvements, resource allocation and reduction of unpredictability. The process involving early detection of potential problems, productivity evaluation and evaluating external quality factors such as reusability, maintainability, defect proneness and complexity are of utmost importance. Here we discuss the application of CK metrics and estimation model to predict the external quality parameters for optimizing the design process and production process for desired levels of quality. Estimation of defect-proneness in object-oriented system at design level is developed using a novel methodology where models of relationship between CK metrics and defect-proneness index is achieved. A multifunctional estimation approach captures the correlation between CK metrics and defect proneness level of software modules.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5166892,no
Graph-Based Task Replication for Workflow Applications,2009,"The Grid is an heterogeneous and dynamic environment which enables distributed computation. This makes it a technology prone to failures. Some related work uses replication to overcome failures in a set of independent tasks, and in workflow applications, but they do not consider possible resource limitations when scheduling the replicas. In this paper, we focus on the use of task replication techniques for workflow applications, trying to achieve not only tolerance to the possible failures in an execution, but also to speed up the computation without demanding the user to implement an application-level checkpoint, which may be a difficult task depending on the application. Moreover, we also study what to do when there are not enough resources for replicating all running tasks. We establish different priorities of replication depending on the graph of the workflow application, giving more priority to tasks with a higher output degree. We have implemented our proposed policy in the GRID superscalar system, and we have run the fastDNAml as an experiment to prove our objectives are reached. Finally, we have identified and studied a problem which may arise due to the use of replication in workflow applications: the replication wait time.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5166972,no
Evaluating Provider Reliability in Grid Resource Brokering,2009,"If Grid computing is to experience widespread commercial adoption, then incorporating risk assessment and management techniques is essential,both during negotiation between service provider and service requester and during run-time. This paper focuses on the role of a resource broker in this context. Specifically, an approach to evaluating the reliability of risk information received from resource providers is presented, using historical data to provide a statistical estimate of the average integrity of their risk assessments, with respect to systematic overestimation or underestimation of the probability of failure. Simulation results are presented, indicating the effectiveness of this approach.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5166974,no
Efective multi-frame motion estimation method for H.264,2009,"In order to achieve a high coding gain, the H.264 video coding standard allows one or more reference frames. However, this greatly increases the computational burden in proportion to the number of the searched reference frames. Therefore, we propose a method to reduce the complexity by selecting the proper reference frames. The proposed algorithm uses the optimal reference frame information in both the P16 times 16 mode and the adjacent blocks, thus decreasing any unnecessary searching process in the inter modes except for P16 times 16. Experimental results show that the proposed method considerably saves coding time with negligible quality and bit-rate difference. It can also be adopted with any of the existing motion estimation algorithms to obtain additional performance improvement.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5167474,no
Indoor monitoring of respiratory distress triggering factors using a wireless sensing network and a smart phone,2009,"A wireless sensing network Bluetooth enabled was designed and implemented for continuous monitoring of indoor humidity and temperature conditions as well as to detect pollutant gases and vapors. The novelty of the work is related to the development of an embedded software using Java2ME technology for a smart phone that materializes a user friendly HMI. Two mobile software modules assure sensor nodes data reading through Bluetooth connection, primary data processing, data storage and alarm generation according with imposed thresholds for air quality parameters. Additional .NET developed software for a Notebook PC platform permits to remotely configure the mobile application and to receive the data logged in the mobile phone. Using the implemented distributed measurement system, including the smart phone, an intelligent assessment of air conditions for risk factor reduction of asthma or chronic obstructive pulmonary disease is carried out. Several experimental results are also included.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5168491,no
Detecting Software Faults in Distrubted Systems,2009,"We are concerned with the problem of detecting faults in distributed software, rapidly and accurately. We assume that the software is characterized by events or attributes, which determine operational modes; some of these modes may be identified as failures. We assume that these events are known and that their probabilistic structure, in their chronological evolution, is also known, for a finite set of different operational modes. We propose and analyze a sequential algorithm that detects changes in operational modes rapidly and reliably. Further more, a threshold operational parameter of the algorithm controls effectively the induced speed versus correct detection versus false detection tradeoff.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5170330,no
Real-Time Designing Software for the Sewing Path Design on the Broidery Industry,2009,The software with the user graphic interface has been developed for the sewing path design in the embroidery industry. This new version of software is an advanced version of the last one and is incorporated many advanced new functions. The main purpose of the software is to create an easy and painless environment for the designer to create their own artistic sewing pattern with suitable sewing path. The resulting sawing path will be automatically transformed into a file with the absolutely or relatively coordinate value of each stitch point on the sewing path. The coordinate file will feed into the embroidery machine for the recreation of the design pattern on the surface of the product. Commercial graphic software does not provide the suitable function to transform each stitch point into their respective coordinate values. This drawback is resulted in the transformation process to be done by hand and by means of measuring each stitch point by designer - one point by one point. Most of the designed sewing pattern will involve more than hundreds of stitch points. It is a tedious and error-prone process for the designer and will deprive most of the design time away from the designer. This software will provide an immediately help for the designer to do the painful work of the process of the coordinate transformation. This will leave the designer to focus only on the artistic pattern design.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5170372,no
Using the Number of Faults to Improve Fault-Proneness Prediction of the Probability Models,2009,"The existing fault-proneness prediction methods are based on unsampling and the training dataset does not contain the information on the number of faults of each module and the fault distributions among these modules. In this paper, we propose an oversampling method using the number of faults to improve fault-proneness prediction. Our method uses the information on the number of faults in the training dataset to support better prediction of fault-proneness. Our test illustrates that the difference between the predictions of oversampling and unsampling is statistically significant and our method can improve the prediction of two probability models, i.e. logistic regression and naive Bayes with kernel estimators.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5170411,no
Reliability Analysis in the Early Development of Real-Time Reactive Systems,2009,"The increasing trend toward complex software systems has highlighted the need to incorporate quality requirements earlier in the development process. Reliability is one of the important quality indicators of such systems. This paper proposes a reliability analysis approach to measure reliability in the early development of real-time reactive systems (RTRS). The goal is to provide decision support and detect the first signs of low or decreasing reliability as the system design evolves. The analysis is conducted in a formal development environment for RTRS, formalized mathematically and illustrated using a train-gate-controller case study.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5170427,no
Structuring Cognitive Information for Software Complexity Measurement,2009,"Cognitive complexity of software has emerged as an interesting research area recently with the growing studies in cognitive informatics disciplines. Cognitive complexity measurement attempts to quantify the software from the perspective of the difficulty for human brain to process and comprehend the software, in order to enable more precise prediction of the critical information about testability, reliability, and maintainability, as well as the effort spent in the software project. Cognitive informatics theories suggest that cognitive complexity of software depends on fundamental factors such as inputs, outputs, loops/branches structure, and number of operators and operands. Analysis in this paper shows the significant flaw of current cognitive complexity measures that they quantify the factors without considering the dependencies among them. We therefore propose a new method to solve this problem by structuring the factors. The proposed measure was evaluated comparatively to existing metrics, and also proven by satisfying all nine Weyuker's properties.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5170431,no
Testing of High Resolution ADCs Using Lower Resolution DACs via Iterative Transfer Function Estimation,2009,"Linearity testing of high resolution analog-to-digital Converters (ADCs) requires test instrumentation that has high precision digital-to-analog conversion (DAC) capability. Further, a large number of samples need to be collected for linearity testing of high resolution ADCs (18-24 bit) to guarantee test quality. In this paper a novel fast linearity testing approach is proposed for testing high resolution ADCs using a low precision DAC and a potentiometer. A polynomial fit of the transfer function of the ADC is generated using measurements made at intermediate code points. The test setup and analysis procedure makes no assumption about the linearity of the lower precision DAC or the potentiometer used to generate the ADC test stimulus. A least squares based polynomial fitting approach is used to characterize the transfer function of the ADC. The computed transfer function is then used to estimate the Integral Non-Linearity (INL) and the Differential Non-Linearity(DNL) of the system accurately. Software simulations and hardware experiments are performed to validate the proposed methodology.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5170452,no
PID-Based Feature Weight Learning and Its Application in Intrusion Detection,2009,"In case-based reasoning (CBR), cases are generally represented by features. Different features have different importance, which are often described by weights. So how to adaptively learning weights of different features is a very key issue in CBR, which impact directly the quality and performance of case extraction. Currently, in most practical CBR systems, the feature weights are given by domain experts subjectively. In this paper, we propose a PID operator-based feature weight learning method based on the fundamental theory of the control system. PID-based feature weighting method is a self-adaptive method, which utilizing the similar neural network architecture to construct the case base. Through designing 3 kinds of adjusting operators: Proportional, integral and derivative operator (PID), and each operator with different properties: reactive, prudent and sensitive, we can adjust the feature weight from different point of views, such as the current adjust results, the history results or the last two results. In order to evaluate the effectiveness of the method, the experiment of network anomaly detection is conducted and the experimental results show that all 3 operators are effective which can converge the intrusion detection system into a stable state in relative small iterations.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5170599,no
Ensemble Similarity Measures for Clustering Terms,2009,"Clustering semantically related terms is crucial for many applications such as document categorization, and word sense disambiguation. However, automatically identifying semantically similar terms is challenging. We present a novel approach for automatically determining the degree of relatedness between terms to facilitate their subsequent clustering. Using the analogy of ensemble classifiers in machine learning, we combine multiple techniques like contextual similarity and semantic relatedness to boost the accuracy of our computations. A new method, based on Yarowskypsilas word sense disambiguation approach, to generate high-quality topic signatures for contextual similarity computations, is presented. A technique to measure semantic relatedness between multi-word terms, based on the work of Hirst and St. Onge is also proposed. Experimental evaluation reveals that our method outperforms similar related works. We also investigate the effects of assigning different importance levels to the different similarity measures based on the corpus characteristics.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5171010,no
Quadruple bandwidth true time delay printed microwave lens beam former for ultra wideband multifunctional phased array applications,2009,"A 9 X 13 printed microwave lens beam former was investigated numerically using MoM, and good performance was predicted across the quadruple band 6 GHz - 25 GHz. TTD behavior across entire band was achieved by array factor comparisons, with good beam quality demonstrations. Future work on this lens will involve the power efficiency study and prototype measurement validations.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5172353,no
Performance analysis of workflow instances,2009,"In workflow systems, time performance of workflow instances is key to business management and scheduling. Firstly, the concept of active transition was proposed and its performance equivalent model was analyzed under resources-limited circumstances. Then, the concept of active pattern was proposed and time performances of four basic active patterns were deduced. Finally, an example was given to demonstrate how to use above results to estimate time performance of workflow instances. The proposed method can be used to estimate time performance of workflow instances at any time point and build efficient performance analysis algorithm.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5175003,no
Multiobjective Evolutionary Optimization Algorithm for Cognitive Radio Networks,2009,"Under Cognitive radio (CR), the Quality of Service (QoS) suffers from many dimensions or metrics of communication quality for improving spectrum utilization. To investigate this issue, this paper develops a methodology based on the multiobjective optimization model with genetic algorithms (GAs). The influence of evolving a radio defined by a chromosome is identified. The Multiobjective Cognitive Radio (MOCR) algorithm from genetically manipulating the chromosomes is proposed. Using adaptive component as an example, the bounds for the maximum benefit is predicted by a proposed model that considers Pareto front. To find a set of parameters that optimize the radio for userpsilas current needs, several solutions are presented. Simulation results show that MOCR is able to find a comparatively better spread of compromise solutions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5175095,no
Toward a Framework for Evaluating Heterogeneous Architecture Styles,2009,"Evaluating architectures and choosing the correct one is a critical issue in software engineering domain, in accordance with extremely extension of architecture-driven designs. In the first years of defining architecture styles, some special quality attributes were introduced as their basic attributes. After a moment, by utilizing them in practice, some results were obtained confirming some of attributes; some others meanwhile were not witnessed. As software architecture construction process is dependent on and addressed by both usage conditions and quality attributes, in this paper a framework has been proposed to provide an environment and a platform that can cover evaluation of architecture styles with a technique that not only exploits both qualitative and quantitative information but also considering users' needs is possible precisely and with high quality. Moreover, we define a classification and notation in order to describe heterogeneous architectures. It provides us with the ability of evaluating heterogeneous architecture styles of a software system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5175985,no
Using Virtualization to Improve Software Rejuvenation,2009,"In this paper, we present an approach for software rejuvenation based on automated self-healing techniques that can be easily applied to off-the-shelf application servers. Software aging and transient failures are detected through continuous monitoring of system data and performability metrics of the application server. If some anomalous behavior is identified, the system triggers an automatic rejuvenation action. This self-healing scheme is meant to disrupt the running service for a minimal amount of time, achieving zero downtime in most cases. In our scheme, we exploit the usage of virtualization to optimize the self-recovery actions. The techniques described in this paper have been tested with a set of open-source Linux tools and the XEN virtualization middleware. We conducted an experimental study with two application benchmarks (Tomcat/Axis and TPC-W). Our results demonstrate that virtualization can be extremely helpful for failover and software rejuvenation in the occurrence of transient failures and software aging.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5184821,no
Probabilistic fault diagnosis for IT services in noisy and dynamic environments,2009,"The modern society has come to rely heavily on IT services. To improve the quality of IT services it is important to quickly and accurately detect and diagnose their faults which are usually detected as disruption of a set of dependent logical services affected by the failed IT resources. The task, depending on observed symptoms and knowledge about IT services, is always disturbed by noises and dynamic changing in the managed environments. We present a tool for analysis of IT services faults which, given a set of failed end-to-end services, discovers the underlying resources of faulty state. We demonstrate empirically that it applies in noisy and dynamic changing environments with bounded errors and high efficiency. We compare our algorithm with two prior approaches, Shrink and Max coverage, in two well-known types of network topologies. Experimental results show that our algorithm improves the overall performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5188804,no
Heteroscedastic models to track relationships between management metrics,2009,"Modern software systems expose management metrics to help track their health. Recently, it was demonstrated that correlations among these metrics allow faults to be detected and their causes localized. In particular, linear regression models have been used to capture metric correlations. We show that for many pairs of correlated metrics in software systems, such as those based on Java Enterprise Edition (JavaEE), the variance of the predicted variable is not constant. This behaviour violates the assumptions of linear regression, and we show that these models may produce inaccurate results. In this paper, leveraging insight from the system behaviour, we employ an efficient variant of linear regression to capture the non-constant variance. We show that this variant captures metric correlations, while taking the changing residual variance into consideration. We explore potential causes underlying this behaviour, and we construct and validate our models using a realistic multi-tier enterprise application. Using a set of 50 fault-injection experiments, we show that we can detect all faults without any false alarm.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5188838,no
Monitoring probabilistic SLAs in Web service orchestrations,2009,"Web services are software applications that are published over the Web, and can be searched and invoked by other programs. New Web services can be formed by composing elementary services, such composite services are called Web service orchestrations. Quality of service (QoS) issues for Web service orchestrations deeply differ from corresponding QoS issues in network management. In an open world of Web services, service level agreements (SLAs) play an important role. They are contracts defining the obligations and rights between the provider of a Web service and a client with respect to the services' function and quality. In a previous work we have advocated using soft contracts of probabilistic nature, for the QoS part of contracts. Soft contracts have no hard bounds on QoS parameters, but rather probability distributions for them. An essential component of SLA management is the continuous monitoring of the performance of called Web services, to check for violation of the agreed SLA. In this paper we propose a statistical technique for QoS contract run time monitoring. Our technique is compatible with the use of soft probabilistic contracts.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5188851,no
Queuing model based end-to-end performance evaluation for MPLS Virtual Private Networks,2009,"Monitoring the end-to-end Quality-of-Service (QoS) is an important work for service providers' Operation Support System (OSS), because it is the fundamental requirement for QoS provisioning. However, it is in fact a challenging work, and there are few efficient approaches to address it. In this paper, for multi-protocol label switching (MPLS) virtual private networks (VPNs), we propose a queuing model based end-to-end performance evaluation scheme for OSS to monitor the end-to-end delay, which is one of the important QoS metrics. By means of a queuing model, we deduce the relationship between the end-to-end delay and the information available in the Management Information Base (MIB) of routers, and then we present the evaluation scheme which avoids the costly per-packet measurement. The complexity of the proposed scheme is much lower than existing schemes. Extensive simulation results show that the proposed scheme can efficiently evaluate the end-to-end performance metrics (the estimation error is nearly 10%).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5188852,no
An approach to measurement based Quality of Service control for communications networks,2009,"This paper presents a purely empirical approach to estimating the effective bandwidth of aggregated traffic flows independent of traffic model assumptions. The approach is shown to be robust when used in a variety of traffic scenarios such as both elastic and streaming traffic flows of varying degrees of aggregation. The method then forms the basis of two quality of service related traffic performance optimisation strategies. The paper presents a cost efficient approach to supplying suitably accurate demand matrix input for QoS related network planning and a QoS provisioning, revenue maximising admission control algorithm for an IPTV services network. This paper summarises these approaches and discusses the major benefits of an appropriately accurate effective bandwidth estimation algorithm.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5188890,no
Reasoning on Scientific Workflows,2009,"Scientific workflows describe the scientific process from experimental design, data capture, integration, processing, and analysis that leads to scientific discovery. Laboratory Information Management Systems (LIMS) coordinate the management of wet lab tasks, samples, and instruments and allow reasoning on business-like parameters such as ordering (e.g., invoicing) and organization (automation and optimization) whereas workflow systems support the design of workflows in-silico for their execution. We present an approach that supports reasoning on scientific workflows that mix wet and digital tasks. Indeed, experiments are often first designed and simulated with digital resources in order to predict the quality of the result or to identify the parameters suitable for the expected outcome. ProtocolDB allows the design of scientific workflows that may combine wet and digital tasks and provides the framework for prediction and reasoning on performance, quality, and cost.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5190673,no
Queuing Theoretic and Evolutionary Deployment Optimization with Probabilistic SLAs for Service Oriented Clouds,2009,"This paper focuses on service deployment optimization in cloud computing environments. In a cloud, each service in an application is deployed as one or more service instances. Different service instances operate at different quality of service (QoS) levels. In order to satisfy given service level agreements (SLAs) as end-to-end QoS requirements of an application, the application is required to optimize its deployment configuration of service instances. E<sup>3</sup>/Q is a multiobjective genetic algorithm to solve this problem. By leveraging queuing theory, E<sup>3</sup>/Q estimates the performance of an application and allows for defining SLAs in a probabilistic manner. Simulation results demonstrate that E<sup>3</sup>/Q efficiently obtains deployment configurations that satisfy given SLAs.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5190680,no
Gossip-Based Workload Prediction and Process Model for Composite Workflow Service,2009,"In this paper, we propose to predict the workloads of the service components within the composite workflow based on the communication of the queue condition of service nodes. With this information, we actively discard the requests that has high probability to be dropped in the later stages of the workflow. The benefit of our approach is the efficient saving of limited system resource as well as the SLA-satisfied system performance. We present mechanisms for four basic workflow patterns and evaluate our mechanism through simulation. The experiment results show that our mechanism can help to successfully serve more requests than the normal mechanism. In addition, our mechanism can maintain more stable response time than the normal one.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5190689,no
A defect prediction model for software based on service oriented architecture using EXPERT COCOMO,2009,"Software that adopts service oriented architecture has specific features on project scope, lifecycle and technical methodology, therefore demands for specific defect management process to ensure quality. By leveraging recent best practices in projects that adopt service oriented architecture, this paper presents a defect prediction model for software based on service oriented architecture, and discusses the defect management process based on the presented model.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5191800,no
The research on a new fault wave recording device in generator-transformer units,2009,"This paper presents a new kind of distributed fault recorder including the design of the system structure, the hardware and software design of the recorder. The recorder adopts NI CompaceRIO series programmable automation controller (PAC) in the hardware while virtual instrument technology in the software. The network communication based on TCP/IP between the client and the server is adopted in the power plant. Moreover, an improved frequency tracking algorithm is presented in the monitoring of the electric quantity to improve the detection precision and the processing speed. The detection and operation results show that it has improved the performance greatly and realized authenticity, integrity and reliability and so on.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5192126,no
A novel QoS modeling approach for soft real-time systems with performance guarantees,2009,"This paper introduces a systematic approach for modeling QoS requirements of soft real-time systems with stochastic responsiveness guarantees. While deadline miss ratio and its proposed extensions have been considered for evaluating firm real-time systems, this work brings out its limitations for assessing the performance of emerging computer services operating over communication infrastructures with non-deterministic dynamics. This work explains how delay frequencies and delay lengths can be both represented into a single quantitative meaningful measure for performance evaluation of soft real-time constrained computer applications. It also explores the presented approach in the design of scheduling strategies which can ground novel business models for QoS-enabled service-oriented systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5192494,no
Usability Evaluation Driven by Cooperative Software Description Framework,2009,"The usability problems of CSCW system mainly come from two aspects of limitation. On the higher level, it lacks sufficient simulation of the use-context influenced by social, political and organizational features; on the lower level, it lacks suitable measurement and description of basic collaborative behavior happened in common cooperative activities. However, the traditional task-based user interface models and discount usability inspection methods can not address the above problems. Therefore, an overall framework for describing CSCW system was proposed in this paper from the perspective of usability evaluation. This framework covers all aspects in understanding the use-context, so that it is very helpful to better instruct low-cost usability walkthrough technique in the early stage of software development cycle to detect usability problems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5193714,no
Optimizing a highly fault tolerant software RAID for many core systems,2009,We present a parallel software driver for a RAID architecture to detect and correct corrupted disk blocks in addition to tolerate disk failures. The necessary computations demand parallel execution to avoid the processor being the bottleneck for a RAID with high bandwidth. The driver employs the processing power of multicore and manycore systems. We report on the performance of a prototype implementation on a quadcore processor that indicates linear speedup and promises good scalability on larger machines. We use reordering of I/O orders to ensure balance between CPU and disk load.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5194712,no
Built-in aging monitoring for safety-critical applications,2009,"Complex electronic systems for safety or mission-critical applications (automotive, space) must operate for many years in harsh environments. Reliability issues are worsening with device scaling down, while performance and quality requirements are increasing. One of the key reliability issues is to monitor long-term performance degradation due to aging in such harsh environments. For safe operation, or for preventive maintenance, it is desirable that such monitoring may be performed on chip. On-line built-in aging sensors (activated from time to time) can be an adequate solution for this problem. The purpose of this paper is to present a novel methodology for electronic systems aging monitoring, and to introduce a new architecture for an aging sensor. Aging monitoring is carried out by observing the degrading timing response of the digital system. The proposed solution takes into account power supply voltage and temperature variations and allows several levels of failure prediction. Simulation results are presented, that ascertain the usefulness of the proposed methodology.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5195976,no
"Quality Indicators on Global Software Development Projects: Does ""Getting to Know You"" Really Matter?",2009,"In Spring 2008, five student teams were put into competition to develop software for a Cambodian client. Each extended team comprised students distributed across a minimum of three locations, drawn from the US, India, Thailand and Cambodia. This paper describes a couple of exercises conducted with students to examine their basic awareness of the countries of their collaborators and competitors, and to assess their knowledge of their own extended team members during the course of the project. The results from these exercises are examined in conjunction with the high-level communication patterns exhibited by the participating teams and provisional findings are drawn with respect to quality, as measured through a final product selection process. Initial implications for practice are discussed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5196914,no
A Method of Software Reliability Test Based on Relative Reliability in Object-Oriented Programming,2009,"This paper presents an approach about how to use the relative reliability test and operation pathspsila reliability prediction to adjust the test allocation of software reliability test in object-oriented programming which is based on the original operational profile. In this new method, software reliability test is not only based on the operational profiles but also guided by the relative reliability prediction results of operation paths. The adjustable range is decided by the original operation running rate and the operation independence factor which can be gotten by neural network learning algorithm and differentiated from software to software.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5197092,no
Interpreting a Successful Testing Process: Risk and Actual Coverage,2009,"Testing is inherently incomplete; no test suite will ever be able to test all possible usage scenarios of a system. It is therefore vital to assess the implication of a system passing a test suite. This paper quantifies that implication by means of two distinct, but related, measures: the risk quantifies the confidence in a system after it passes a test suite, i.e., the number of faults still expected to be present (weighted by their severity); the actual coverage quantifies the extent to which faults have been shown absent, i.e., the fraction of possible faults that has been covered. We provide evaluation algorithms that calculate these metrics for a given test suite, as well as optimisation algorithms that yield the best test suite for a given optimisation criterion.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5198509,no
A Review of Uncertainty Methods Employed in Water Quality Modeling,2009,"The aquatic environment is a complicated system with uncertainty. The study of uncertainty in water quality modeling can improve peoplepsilas understanding of the nature of the aquatic environment and it is important to the development of water quality modeling. The typology and sources of uncertainty is represented to improve the understanding and characterizing of the uncertainty in water quality modeling process. And according to the different sources, a comprehensive review on different methods of uncertainty study is made. The integration of different uncertainty methods, mathematical theories and algorithms is an important trend of water quality modeling research. The research on uncertainty is an essential part of water quality modeling. It is suggested that uncertainty should be investigated systemically within the entire process of water quality modeling.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5199827,no
"Congestion location detection: Methodology, algorithm, and performance",2009,"We address the following question in this study: Can a network application detect not only the occurrence, but also the location of congestion? Answering this question will not only help the diagnostic of network failure and monitor server's QoS, but also help developers to engineer transport protocols with more desirable congestion avoidance behavior. The paper answers this question through new analytic results on the two underlying technical difficulties: 1) synchronization effects of loss and delay in TCP, and 2) distributed hypothesis testing using only local loss and delay data. We present a practical congestion location detection (CLD) algorithm that effectively allows an end host to distributively detect whether congestion happens in the local access link or in more remote links. We validate the effectiveness of CLD algorithm with extensive experiments.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5201404,no
Admission control for roadside unit access in Intelligent Transportation Systems,2009,"Roadside units can provide a variety of potential services for passing-by vehicles in future intelligent transportation systems. Since each vehicle has a limited time period when passing by a roadside unit, it is important to predict whether a service can be finished in time. In this paper, we focus on admission control problems, which are important especially when a roadside unit is in or close to overloaded conditions. Traditional admission control schemes mainly concern long-term flows, such as VOIP and multimedia service. They are not applicable to highly mobile vehicular environments. Our analysis finds that it is not necessarily accurate to use deadline to evaluate the risk whether a flow can be finished in time. Instead, we introduce a potential metric to more accurately predict the total data size that can be transmitted to/from a vehicle. Based on this new concept, we formulate the admission control task as a linear programming problem and then propose a lexicographically maxmin algorithm to solve the problem. Simulation results demonstrate that our scheme can efficiently make admission decisions for coming transmission requests and effectively avoid system overload.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5201409,no
Experiments results and large scale measurement data for web services performance assessment,2009,"Service provisioning is a challenging research area for the design and implementation of autonomic service- oriented software systems. It includes automated QoS management for such systems and their applications. Monitoring and Measurement are two key features of QoS management. They are addressed in this paper as elements of a main step in provisioning of self-healing web services. In a previous work, we defined and implemented a generic architecture applicable for different services within different business activities. Our approach is based on meta-level communications defined as extensions of the SOAP envelope of the exchanged messages, and implemented within handlers provided by existing web service containers. Using the web services technology, we implemented a complete prototype of a service-oriented Conference Management System (CMS). We experienced our monitoring and measurement architecture using the implemented application and assessed successfully the scalability of our approach under the French grid5000. In this paper, experimental results are analyzed and concluding remarks are given.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5202399,no
Oil Contamination Monitoring Based on Dielectric Constant Measurement,2009,"Fault information of machinery equipments is often contained in the process of lubricating oil wearing off. Meanwhile, the dielectric constant of the oil would change accordingly during the process. The principle of online oil contamination monitoring based on dielectric constant measurement is proposed. The measure system is also developed, which includes capacitance sensor, small capacitance detecting circuit and software of monitoring and analysis. Experiments are carried out on lubricating oil polluted by different contamination. The results show that change of lubricating oil's relative dielectric constant can be detected effectively and properly distinguished using the developed measure system, which can be used to determine the proper oil-replacing period, and to perform fault prediction of the machinery equipments.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5202959,no
Research on the Gear Fault Diagnosis Using Order Envelope Spectrum,2009,"The speed-up and speed-down of the gearbox are non-stationary process and the vibration signal can not be processed by traditional processing method. In order to process the non-stationary vibration signals such as speed-up or speed-down signals effectively, the order envelope analysis technique is presented. This new method combines order tracking technique with envelope spectrum analysis. Firstly, the vibration signal is sampled at constant time increments and then uses software to resample the data at constant angle increments. Therefore, the time domain non-stationary signal is changed into angle domain stationary signal. In the end, the resampled signals are processed by envelope spectrum analysis. The experimental results show that order envelope spectrum analysis can effectively diagnosis the faults of the gear crack.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5203072,no
Vibration Controller of Marine Electromagnetic Vibrator,2009,"Marine electromagnetic vibrator is a non-explosive seismic source, for high resolution detection of marine seismic exploration, which used sublevel sweep mode of marine electromagnetic vibrator. The vibration controller provides sublevel Chirp signal by marine geology condition. The Chirp signal was sent to exciter through power amplifier. Exciter coupled with marine water by shaking base plate, the elastic wave was sent to marine .The vibration controller is the source of marine vibrator, so the quality of signal influences the whole performance index of seismic explosive system. This thesis discusses hardware and software of marine electromagnetic vibrator controller.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5203122,no
A method for selecting and ranking quality metrics for optimization of biometric recognition systems,2009,"In the field of biometrics evaluation of quality of biometric samples has a number of important applications. The main applications include (1) to reject poor quality images during acquisition, (2) to use as enhancement metric, and (3) to apply as a weighting factor in fusion schemes. Since a biometric-based recognition system relies on measures of performance such as matching scores and recognition probability of error, it becomes intuitive that the metrics evaluating biometric sample quality have to be linked to the recognition performance of the system. The goal of this work is to design a method for evaluating and ranking various quality metrics applied to biometric images or signals based on their ability to predict recognition performance of a biometric recognition system. The proposed method involves: (1) Preprocessing algorithm operating on pairs of quality scores and generating relative scores, (2) Adaptive multivariate mapping relating quality scores and measures of recognition performance and (3) Ranking algorithm that selects the best combinations of quality measures. The performance of the method is demonstrated on face and iris biometric data.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5204309,no
Global and local quality measures for NIR iris video,2009,"In the field of iris-based recognition, evaluation of quality of images has a number of important applications. These include image acquisition, enhancement, and data fusion. Iris image quality metrics designed for these applications are used as figures of merit to quantify degradations or improvements in iris images due to various image processing operations. This paper elaborates on the factors and introduces new global and local factors that can be used to evaluate iris video and image quality. The main contributions of the paper are as follows. (1) A fast global quality evaluation procedure for selecting the best frames from a video or an image sequence is introduced. (2) A number of new local quality measures for the iris biometrics are introduced. The performance of the individual quality measures is carefully analyzed. Since performance of iris recognition systems is evaluated in terms of the distributions of matching scores and recognition probability of error, from a good iris image quality metric it is also expected that its performance is linked to the recognition performance of the biometric recognition system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5204310,no
Characterization of CdTe Detectors for Quantitative X-ray Spectroscopy,2009,"Silicon diodes have traditionally been the detectors of choice for quantitative X-ray spectroscopy. Their response has been very well characterized and existing software algorithms process the spectra for accurate, quantitative analysis. But Si diodes have limited sensitivity at energies above 30 keV, while recent regulations require measurement of heavy metals such as lead and mercury, with K X-ray emissions well above 30 keV. Measuring the K lines is advantageous to reduce the interference between the L lines of these elements with each other and with the K lines of light elements. CdTe has much higher stopping power than Si, making it attractive for measuring higher energies, but the quality and reproducibility of its spectra have limited its use in the energy range of characteristic X-rays, <100 keV. CdTe detectors have now been optimized for energies <100 keV, yielding electronic noise of 500 eV for a 5 mm times 5 mm times 0.75 mm device. This provides adequate energy resolution for distinguishing characteristic X-ray peaks >30 keV. The response function of these CdTe detectors differs in important ways from that of Si detectors, requiring changes to the X-ray analytical software used to process the spectra. This paper will characterize the response of the latest generation of high resolution CdTe detectors at the energies of characteristic X-rays. It will present effects important for X-ray analysis, including the peak shape arising from hole tailing, escape peaks, spectral background, linearity, and stability. Finally, this paper will present spectral processing algorithms optimized for CdTe, including peak shape and escape peak algorithms.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5204583,no
Measurement of worst-case power delivery noise and construction of worst case current for graphics core simulation,2009,"Worst case graphics core power delivery noise is a major indicator of graphics chip performance. The design of good graphics core power delivery network (PDN) is technically difficult because it is not easy to predict a worst case current stimulus during pre-silicon design stage. Many times, the worst case power delivery noise is observed when graphics benchmark software is run during post-silicon validation. At times like this, it is too late to rectify the power delivery noise issue unless many extra capacitor placeholders are placed during early design stage. To intelligently optimize the graphics core power delivery network design and determining the right amount of decoupling capacitors, this paper suggests an approach that setup a working platform to capture the worst case power delivery noise; and later re-construct the worst case power delivery current using Thevenin's Theorem. The measurement is based on actual gaming application instead of engineering a special stimulus that is generated thru millions of logic test-vectors. This approach is practical, direct and quick, and does not need huge computing resources; or technically skilled logic designers to design algorithms to build the stimulus.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5206296,no
Black-box and gray-box components as elements for performance prediction in telecommunications system,2009,"In order to enable quality impact prediction for newly added functionality to the existing telecommunications system, we introduce new modeling elements to the common development process. In the paper we describe a black-box and grey-box instrumentation and modeling method, used for performance prediction of a distributed system. To verify basic concept of the method, a prototype system was made. The prototype consists of the existing proprietary telephone exchange system and new, open-source components that make an extension of the system. The paper presents conceptual architecture of the prototype and analyzes basic performance impact for introduction of open-source software into existing system. Instrumentation and modeling parameters are discussed, as well as measurements environment. Initial comparison of predicted and experimentally measured values validates current work on the method.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5206365,no
Workflow Model Performance Analysis Concerning Instance Dwelling Times Distribution,2009,Instances dwelling times (IDT) which consist of waiting times and handle times in a workflow model is a key performance analysis goal. In a workflow model the instances which act as customers and the resources which act as servers form a queuing network. Multidimensional workflow net (MWF-net) includes multiple timing workflow nets (TWF-nets) and the organization and resource information. This paper uses queuing theory and MWF-net to discuss mean value and probability distribution density function (PDDF) of IDT. An example is used to show that the proposed method can be effectively utilized in practice.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5207874,no
A Low Latency Handoff Scheme Based on the Location and Movement Pattern,2009,"Providing a seamless handoff and quality of service (QoS) guarantees is one of the key issues in real-time services. Several IPv6 mobility management schemes have been proposed and can provide uninterrupted service. However, these schemes either have synchronization issues or introduce signaling overhead and high packet loss rate. This paper presents a scheme that reduces the handoff latency based on the location and movement pattern of a Mobile Node (MN). An MN can detect its movement actively and forecast the handoff, which alleviates the communication cost between the MN and its Access Router (AR). Furthermore, by setting the dynamic interval in an appropriate range, the MNpsilas burden can also be alleviated. Finally, by performance evaluation using both theoretical analysis and computer simulations, we show that the proposed scheme can lower the handoff latency efficiently.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5207911,no
"Fuzzy Evaluation on Software Maintainability Based on Membership Degree Transformation New Algorithm M(1,2,3)",2009,"Software maintainability means the ease with which a software system or component can be modified to correct faults, improve performance or other attributes, or adapt to a changed environment. For different software maintenance organizations or personnel, there are different concerns on software maintainability. If any one of the factors affecting the maintainability has not been completely resolved, or been unqualified, it will be affected. Therefore, in the evaluation process of software maintainability, there are a lot of uncertainty and ambiguity, so it is reasonable and scientific to apply fuzzy comprehensive evaluation method for software maintainability fuzzy evaluation. The core of fuzzy evaluation is membership degree transformation. But the transformation methods should be questioned, because redundant data in index membership degree is also used to compute object membership degree, which is not useful for object classification. The new algorithm is: using data mining technology based on entropy to mine knowledge information about object classification hidden in every index, affirm the relationship of object classification and index membership, eliminate the redundant data in index membership for object classification by defining distinguishable weight and extract valid values to compute object membership. The new algorithm of membership degree transformation includes three calculation steps which can be summarized as ldquoeffective, comparison and compositionrdquo, which is denoted as M(1, 2, 3). The paper applied the new algorithm in the fuzzy evaluation of software maintainability.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5208930,no
A Method for Selecting ERP System Based on Fuzzy Set Theory and Analytical Hierarchy Process,2009,"Enterprise resource planning (ERP) system can significantly improve future competitiveness and performance of a company, but it is also a critical investment and has a high failure probability. Thus, selecting an appropriate ERP system is very important. A method for selecting ERP system is proposed based on fuzzy set theory and analytical hierarchy process (AHP). The evaluation criteria system, including vendor brand, quality, price and service, is constructed, a multi-criteria decision-making model is formulated and the solution process is introduced in detail, that is, AHP is used to identify the weights of the criteria and fuzzy set theory is used to deal with the fuzzification of the criteria. An application example of ERP system selection demonstrates the feasibility of the proposed method.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5208964,no
Low Cost Mechanism for QoS-Aware Re-Planning of Composite Service,2009,"To meet the business and user requirements, it is crucial to manage the quality of service (QoS) of composite Web service. Due to the dynamics nature of the Web services, the runtime behavior of Web services is likely to differ from the initial estimated one. Therefore, a re-planning to adapt the composite service to the QoS violated services, ensuring the global constraints and preferences will be needed. In this paper, we propose a low cost mechanism for re-planning. Such mechanism is supported by an offline re-planning execution environment and a performance prediction based low cost policy.The experimentations show better performance of the mechanism.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5209662,no
Comparative Assessment of Fingerprint Sample Quality Measures Based on Minutiae-Based Matching Performance,2009,"This fingerprint sample quality is one of major factors influencing the matching performance of fingerprint recognition systems. The error rates of fingerprint recognition systems can be decreased significantly by removing poor quality fingerprints. The purpose of this paper is to assess the effectiveness of individual sample quality measures on the performance of minutiae-based fingerprint recognition algorithms. Initially, the authors examined the various factors that influenced the matching performance of the minutiae-based fingerprint recognition algorithms. Then, the existing measures for fingerprint sample quality were studied and the more effective quality measures were selected and compared with two image quality software packages, (NFIQ from NIST, and QualityCheck from Aware Inc.) in terms of matching performance of a commercial fingerprint matcher (Verifinger 5.0 from Neurotechnologija). The experimental results over various fingerprint verification competition (FVC) datasets show that even a single sample quality measure can enhance the matching performance effectively.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5209773,no
Prediction Models for BPMN Usability and Maintainability,2009,"The measurement of a business process in the early stages of the lifecycle, such as the design and modelling stages, could reduce costs and effort in future maintenance tasks. In this paper we present a set of measures for assessing the structural complexity of business processes models at a conceptual level. The aim is to obtain useful information about process maintenance and to estimate the quality of the process model in the early stages. Empirical validation of the measures was carried out along with a linear regression analysis aimed at estimating process model quality in terms of modifiability and understandability.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5210770,no
Evaluation of Prioritization in Performance Models of DTP Systems,2009,"Modern IT systems serve many different business processes on a shared infrastructure in parallel. The automatic request execution on the numerous interconnected components, hosted on heterogeneous hardware resources, is coordinated in distributed transaction processing (DTP) systems. While pre-defined quality-of-service metrics must be met, IT providers have to deal with a highly dynamic environment concerning workload structure and overall demand when provisioning their systems. Adaptive prioritization is a way to react to short-term demand variances. Performance models can be applied to predict the impacts of prioritization strategies on the overall performance of the system. In this paper we describe the workload characteristics and particularities of two real-world DTP systems and evaluate the effects of prioritization concerning supported overall load and resulting end-to-end performance measures.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5210822,no
Integrity-Checking Framework: An In-situ Testing and Validation Framework for Wireless Sensor and Actuator Networks,2009,"Wireless sensor and actuator network applications require several levels of testing during their development. Although software algorithms can be tested through simulations and syntax checking, it is difficult to predict or test for problems that may occur once the wireless sensor and actuator has been deployed. The requirement for testing is not however limited to the development phase. During the lifecycle of the system, faults, upgrades, retasking, etc. lead to further needs for system validation. In this paper we review the state-of-the-art techniques for testing wireless sensor and actuator applications and propose the integrity-checking framework. The framework provides in-situ full lifecycle testing and validation of wireless sensor and actuator applications by performing an ldquointegrity checkrdquo, during which the sensor inputs and actuator responses are emulated within the physical wireless sensor and actuator. This enables application-level testing by feeding controlled information to the sensor inputs, while data processing, communication, aggregation and decision making continue as normal across the physical wireless sensor and actuator.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5210860,no
Quantitative Assessment for Organisational Security & Dependability,2009,"There are numerous metrics proposed to assess security and dependability of technical systems (e.g., number of defects per thousand lines of code). Unfortunately, most of these metrics are too low-level, and lack on capturing high-level system abstractions required for organisation analysis. The analysis essentially enables the organisation to detect and eliminate possible threats by system re-organisations or re-configurations. In other words, it is necessary to assess security and dependability of organisational structures next to implementations and architectures of systems. This paper focuses on metrics suitable for assessing security and dependability aspects of a socio-technical system and supporting decision making in designing processes. We also highlight how these metrics can help in making the system more effective in providing security and dependability by applying socio-technical solutions (i.e., organisation design patterns).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5211092,no
Reliable Transactional Web Service Composition Using Refinement Method,2009,"Web services composition is a good way to construct complex Web software. However, Web services composition is prone to fail due to the unstable Web services execution. Thus, it is necessary to deal with the abstract hierarchical modeling of multi-partner business process. Therefore, this paper proposes a refinement method for failure compensation process of transaction mechanism, constructs failure service composition compensation model with the help of paired Petri net and builds a services composition compensation model. It discusses the refinement model and aggregated QoS estimation of the five common aggregation compensation constructs. It takes the classical traveling reservation business process as an example to show the influence on composite business process brought by the aggregated QoS metrics in different failure points, and analyzes the influence of reputation in different failure rate.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5211261,no
Fault Detection of Satellite Network Based on MNMP,2009,"The fault detection of satellite network is required to collect data of managed objects via network management protocol. Therefore, it is a premise of fault management of satellite network to choose a network management protocol designed according to the feature of satellite network. Due to the uniqueness of satellite network, the current network management protocol and standard, such as simple network management protocol (SNMP) and CMIP, are both directly unsuitable for the applying of fault detection of satellite network. By analysis of multiplex network management protocol (MNMP), this paper focuses on the discussion of the MNMP application in the fault detection of satellite network. Besides, in order to investigate the performance of fault detection based on MNMP, this paper also gives a performance test on several means of data acquisition of MNMP, and compares as well as analyze it with SNMP.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5211461,no
An empirical investigation of filter attribute selection techniques for software quality classification,2009,"Attribute selection is an important activity in data preprocessing for software quality modeling and other data mining problems. The software quality models have been used to improve the fault detection process. Finding faulty components in a software system during early stages of software development process can lead to a more reliable final product and can reduce development and maintenance costs. It has been shown in some studies that prediction accuracy of the models improves when irrelevant and redundant features are removed from the original data set. In this study, we investigated four filter attribute selection techniques, automatic hybrid search (AHS), rough sets (RS), Kolmogorov-Smirnov (KS) and probabilistic search (PS) and conducted the experiments by using them on a very large telecommunications software system. In order to evaluate their classification performance on the smaller subsets of attributes selected using different approaches, we built several classification models using five different classifiers. The empirical results demonstrated that by applying an attribution selection approach we can build classification models with an accuracy comparable to that built with a complete set of attributes. Moreover, the smaller subset of attributes has less than 15 percent of the complete set of attributes. Therefore, the metrics collection, model calibration, model validation, and model evaluation times of future software development efforts of similar systems can be significantly reduced. In addition, we demonstrated that our recently proposed attribute selection technique, KS, outperformed the other three attribute selection techniques.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5211564,no
Poster abstract: Exploiting the LQI variance for rapid channel quality assessment,2009,"Communicating over a reliable radio channel is vital for an efficient resource usage in sensor networks: a bad radio channel can lead to poor application performance and higher energy consumption. Previous research has shown that the LQI mean value is a good estimator of the link quality. Nevertheless, due to its high variance, many packets are needed to obtain a reliable estimation. Based on experimental results, we show instead that the LQI variance is not a limitation. We show that the variance of the LQI can be used as a metric for a rapid channel quality assessment. Our initial results indicate that identifying good channels using the LQI variance requires an order of magnitude fewer packets than when using the mean LQI.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5211911,no
Humanoid Intelligent Control System of Plastic Corrugated Board Synchronous Shearing,2009,"The synchronous shearing control of the plastic corrugated board is one of the important production processes. This process has a significant impact on the product quality. Through the analysis of its synchronization process, combined with technological requirements, this study puts forward a design method of humanoid intelligent control system. Adopting microprocessor system C8051F310, this study illustrates the design methods of the controller from both hardware and software. The practical use shows that the controller meets the high precision, steady performance, precise shearing, and the requirements of verticality and flatness. Currently, it has been successfully applied to a corrugated board plant of a plastic factory and has obtained high profits for enterprises.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5223042,no
Risky Module Estimation in Safety-Critical Software,2009,"Software used in safety-critical system must have high dependability. Software testing and V&V (Verification and Validation) activities are very important for assuring high software quality. If we can predict the risky modules in safety-critical software, testing activities and regulation activities can be applied to them more intensively. In this paper, we classify the estimated risk classes which can be used for deep testing and V&V. We predict the risk class for each module using support vector machines. We can consider that the modules classified to risk class 5 or 4 are more risky than others relatively. For all classification error rates, we expect that the results can be useful and practical for software testing, V&V, and activities for regulatory reviews. In the future works, to improve the practicality, we will have to investigate other machine learning algorithms and datasets.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5223201,no
Prototype Implementation of a Goal-Based Software Health Management Service,2009,"The FAILSAFE project is developing concepts and prototype implementations for software health management in mission-critical real-time embedded systems. The project unites features of the industry standard ARINC 653 Avionics Application Software Standard Interface and JPL's Mission Data System (MDS) technology. The ARINC 653 standard establishes requirements for the services provided by partitioned real-time operating systems. The MDS technology provides a state analysis method, canonical architecture, and software framework that facilitates the design and implementation of software-intensive complex systems. We use the MDS technology to provide the health management function for an ARINC 653 application implementation. In particular, we focus on showing how this combination enables reasoning about and recovering from application software problems. Our prototype application software mimics the space shuttle orbiter's abort control sequencer software task, which provides safety-related functions to manage vehicle performance during launch aborts. We turned this task into a goal-based function that, when working in concert with the software health manager, aims to work around software and hardware problems in order to maximize abort performance results. In order to make it a compelling demonstration for current aerospace initiatives, we additionally imposed on our prototype a number of requirements derived from NASA's Constellation Program. Lastly, the ARINC 653 standard imposes a number of requirements on the system integrator for developing the requisite error handler process. Under ARINC 653, the health monitoring (HM) service is invoked by an application calling the application error service or by the operating system or hardware detecting a fault. It is these HM and error process details that we implement with the MDS technology, showing how a state-analytic approach is appropriate for identifying fault determination details, and showing how the framework supp- orts acting upon state estimation and control features in order to achieve safety-related goals. We describe herein the requirements, design, and implementation of our software health manager and the software under control. We provide details of the analysis and design for the phase II prototype, and describe future directions for the remainder of phase II and the new topics we plan to address in phase III.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5226840,no
Simulating and Detecting Radiation-Induced Errors for Onboard Machine Learning,2009,"Spacecraft processors and memory are subjected to high radiation doses and therefore employ radiation-hardened components. However, these components are orders of magnitude more expensive than typical desktop components, and they lag years behind in terms of speed and size. We have integrated algorithm-based fault tolerance (ABFT) methods into onboard data analysis algorithms to detect radiation-induced errors, which ultimately may permit the use of spacecraft memory that need not be fully hardened, reducing cost and increasing capability at the same time. We have also developed a lightweight software radiation simulator, BITFLIPS, that permits evaluation of error detection strategies in a controlled fashion, including the specification of the radiation rate and selective exposure of individual data structures. Using BITFLIPS, we evaluated our error detection methods when using a support vector machine to analyze data collected by the Mars Odyssey spacecraft. We observed good performance from both an existing ABFT method for matrix multiplication and a novel ABFT method for exponentiation. These techniques bring us a step closer to ""rad-hard"" machine learning algorithms.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5226841,no
Synthesizing hardware from sketches,2009,"This paper proposes to adapt sketching, a software synthesis technique, to hardware development. In sketching, the designer develops an incomplete hardware description, providing the ""insight"" into the design. The synthesizer completes the design to match an executable specification. This style of synthesis liberates the designer from tedious and error-prone details-such as timing delays, wiring in combinational circuits and initialization of lookup tables-while allowing him to control low-level aspects of the design. The main benefit will be a reduction of the time-to-market without impairing system performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5227085,no
Predicting Performance of Multi-Agent systems during feasibility study,2009,"Agent oriented software engineering (AOSE) is a software paradigm that has grasped the attention of researchers/developers for the last few years. As a result, many different methods have been introduced to enable researchers/developers to develop multi agent systems. However Performance, a non- functional attribute have not been given that much importance for producing quality software. Performance issues must be considered throughout software project development. Predicting performance early in the life cycle during feasibility study is not considered for predicting performance. In this paper, we consider the data collected (technical and environmental factors) during feasibility study of Multi-Agent software development to predict performance. We derive an algorithm to predict the performance metrics and simulate the results using a case study on scheduling the use of runways on an airport.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5228069,no
Estimation of software projects effort based on function point,2009,"With the development of software industry, software estimation and measurement is catching much more concern. Scale increasing in applications and a variety of programming languages using at the same time, manual measurement based on the LOC (line of code) cannot meet the estimating requirements. The emergence of function point resolves these difficult issues. In order to obtain the software effort, we propose an estimating method for software effort based on function point. It helps to estimate software effort more accurately without considering the languages or developing environment you choose. Firstly, use actual project records to obtain the linear relation between function point and software effort. Then determine the parameters of linear equations by maximum likelihood estimating method. Finally, you can get the effort of the project according to this equation with the function point given. After obtaining the software effort, project manager can arrange the project progress, control the cost and ensure the quality more accurately.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5228317,no
An Approach to Measuring Software Development Risk Based on Information Entropy,2009,"Software development risk always influence the success of software project, even determine a enterprise surviving or perishment. Consequently, there are some significant meaning for software company and software engineering field to measure effectively the risk. Whereas, the measurement is very difficult because software is a sort of logic product. There are many methods to measure the risk at the present time, but lack of quantificational measurement methods, and these measurements are all localization and they cannot consider well the risk factors and the effect. In this paper, we bring forward a quantificational approach to measuring the software development risk based on information entropy. It involves both the probabilities of the risk factors and the bloss, it is a effective synthesis method to measuring software development risk in practice.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5230960,no
Towards Improved Assessment of Bone Fracture Risk,2009,"Summary form only given. The mechanical competence of a bone depends on its density, its geometry and its internal trabecular microarchitecture. The gold standard to determine bone competence is an experimental, mechanical test. Direct mechanical testing is a straight-forward procedure, but is limited by its destructiveness. For the clinician, the prediction of bone quality for individual patients is, so far, more or less restricted to the quantitative analysis of bone density alone. Finite element (FE) analysis of bone can be used as a tool to non-destructively assess bone competence. FE analysis is a computational technique; it is the most widely used method in engineering for structural analysis. With FE analysis it is possible to perform a 'virtual experiment', i.e. the simulation of a mechanical test in great detail and with high precision. What is needed for that are, first, in vivo imaging capabilities to assess bone structure with adequate resolution, and second, appropriate software to solve the image-based FE models. Both requirements have seen a tremendous development over the last years. The last decade has seen the commercial introduction and proliferation of non-destructive microstructural imaging systems such as desktop micro- computed tomography (muCT), which allow easy and relatively inexpensive access to the 3D microarchitecture of bone. Furthermore, the introduction of new computational techniques has allowed to solve the increasingly large FE models, that represent bone in more and more detail. With the recent advent of microstructural in vivo patient imaging systems, these methodologies have reached a level that it is now becoming possible to accurately assess bone strength in humans. Although most applications are still in an experimental setting, it has been clearly demonstrated that it is possible to use these techniques in a clinical setting. The high level of automation, the continuing increase in computational power, and above all the im- proved predictive capacity over predictions based on bone mass, make clear that there is great potential in the clinical arena for in vivo FE analyses Ideally, the development of in vivo imaging systems with microstructural resolution better than 50 mum would allow measurement of patients at different time points and at different anatomical sites. Unfortunately, such systems are not yet available, but the resolution at peripheral sites has reached a level (80 mum) that allows elucidation of individual microstructural bone elements. Whether a resolution of 50 mum in vivo will be reached using conventional CT technology remains to be seen as the required doses may be too high. With respect to these dose considerations, MRI may have considerable potential for future clinical applications to overcome some of the limitations with X-ray CT. With the advent of new clinical MRI systems with higher field strengths, and the introduction of fast parallel- imaging acquisition techniques, higher resolutions in MRI will be possible with comparable image quality and without the adverse effects of ionizing radiation. With these patient scanners, it will be possible to monitor changes in the microarchitectural aspects of bone quality in vivo. In combination with FE analysis it will also allow to predict the mechanical competence of whole bones in the course of age- and disease-related bone loss and osteoporosis. We expect these findings to improve our understanding of the influence of densitometric, morphological but also loading factors in the etiology of spontaneous fractures of the hip, the spine, and the radius. Eventually, this improved understanding may lead to more successful approaches in the prevention of age- and disease-related fractures.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5231296,no
Experimental Analysis of Different Metrics (Object-Oriented and Structural) of Software,2009,"In this paper first investigate the relationships between existing object-oriented metrics (coupling, cohesion) and procedure-oriented metrics (Line of code, Cyclomatic complexity and knot metric) measure the probability of error detection in system classes during testing and is to propose an investigation and analysis strategy to make these kinds of studies more reusable and comparable, a problem which is persistent in the quality measurement. The metrics are first defined and then explained using practical applications finally, a review of the empirical study concerning chosen and coupling metrics and subset of these measures that provide sufficient information is given and metrics providing overlapping information are expelled from the set. The paper defines a new set of operational measures for the conceptual coupling of classes, which are theoretically valid and empirically studied. In this paper, we shows that these metrics capture new dimensions in coupling measurement, compared to existing structural metrics.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5231908,no
Filtering Spam in Social Tagging System with Dynamic Behavior Analysis,2009,"Spam in social tagging systems introduced by some malicious participants has become a serious problem for its global popularizing. Some studies which can be deduced to static user data analysis have been presented to combat tag spam, but either they do not give an exact evaluation or the algorithms' performances are not good enough. In this paper, we proposed a novel method based on analysis of dynamic user behavior data for the notion that users' behaviors in social tagging system can reflect the quality of tags more accurately. Through modeling the different categories of participants' behaviors, we extract tag-associated actions which can be used to estimate whether tag is spam, and then present our algorithm that can filter the tag spam in the results of social search. The experiment results show that our method indeed outperforms the existing methods based on static data and effectively defends against the tag spam in various spam attacks.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5231927,no
Prioritization of Scenarios Based on UML Activity Diagrams,2009,"Increased size and complexity of software requires better methods for different activities in the software development lifecycle. Quality assurance of software is primarily done by means of testing, an activity that faces constraints of both time and resources. Hence, there is need to test effectively within the constraints in order to maximize throughput i.e. rate of fault detection, coverage, etc. Test case prioritization involves techniques aimed at finding the best prioritized test suite. In this paper, we propose a prioritization technique based on UML activity diagrams. The constructs of an activity diagram are used to prioritize scenarios. Preliminary results obtained on a case-study indicate that the technique is effective in extracting the critical scenarios from the activity diagram.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5231936,no
Quality of Service Composition and Adaptability of Software Architectures,2009,"Quality of service adaptability refers to the ability of components/services to adapt in run-time the quality exhibited. A composition study from a quality point of view would investigate how these adaptable elements could be combined to meet systempsilas quality requirements. Enclosing quality properties with architectural models has been typically used to improve system understanding. Nevertheless these properties along with some supplementary information about quality adaptation would allow us to carry out a composition study during the design phase and even to predict some features of the adaptability behavior of the system. Existing modeling languages and tools lack enough mechanisms to cope with adaptability, e.g. to describe system elements that may offer/require several quality levels. This paper shows how we reuse existing modeling languages and tools, combine them and create new ones to tackle the problem of quality of service adaptability and composition. The final goal of this work is to evaluate architectural models to predict systempsilas QoS behavior before it is implemented.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5231986,no
Online Self-Healing Support for Embedded Systems,2009,"In this paper, online system-level self-healing support is presented for embedded systems. Different from off-line log analysis methods used by conventional intrusion detection systems, our research focuses on analyzing runtime kernel data structures hence perform self-diagnosis and self-healing. Inside the infrastructure, self-diagnosis and self-healing solutions have been implemented based on several selected critical kernel data structures. They can fully represent current system status and are also closely related with system resources. At runtime once any system inconsistency has been detected, predefined recovery functions are invoked. Our prototype is developed based on a lightweight virtual machine monitor, above on which the monitored Linux kernel, runtime detection and recovery services run simultaneously. The proposed infrastructure requires few modifications to current Linux kernel source code, thus it can be easily adopted into existing embedded systems. It is also fully software-based without introducing any specific hardware, therefore it is cost-efficient. The evaluation experiment results indicate that our prototype system can correctly detect inconsistent kernel data structures caused by security attacks with acceptable penalty to system performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5232025,no
Improving Software-Quality Predictions With Data Sampling and Boosting,2009,"Software-quality data sets tend to fall victim to the <i>class-imbalance</i> problem that plagues so many other application domains. The majority of faults in a software system, particularly high-assurance systems, usually lie in a very small percentage of the software modules. This imbalance between the number of fault-prone (fp) and non-fp (nfp) modules can have a severely negative impact on a data-mining technique's ability to differentiate between the two. This paper addresses the class-imbalance problem as it pertains to the domain of software-quality prediction. We present a comprehensive empirical study examining two different methodologies, data sampling and boosting, for improving the performance of decision-tree models designed to identify fp software modules. This paper applies five data-sampling techniques and boosting to 15 software-quality data sets of different sizes and levels of imbalance. Nearly 50 000 models were built for the experiments contained in this paper. Our results show that while data-sampling techniques are very effective in improving the performance of such models, boosting almost always outperforms even the best data-sampling techniques. This significant result, which, to our knowledge, has not been previously reported, has important consequences for practitioners developing software-quality classification models.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5233804,no
Fault diagnosis and failure prognosis for engineering systems: A global perspective,2009,"Engineering systems, such as aircraft, industrial processes, manufacturing systems, transportation systems, electrical and electronic systems, etc., are becoming more complex and are subjected to failure modes that impact adversely their reliability, availability, safety and maintainability. Such critical assets are required to be available when needed, and maintained on the basis of their current condition rather than on the basis of scheduled or breakdown maintenance practices. Moreover, on-line, real-time fault diagnosis and prognosis can assist the operator to avoid catastrophic events. Recent advances in Condition-Based Maintenance and Prognostics and Health Management (CBM/PHM) have prompted the development of new and innovative algorithms for fault, or incipient failure, diagnosis and failure prognosis aimed at improving the performance of critical systems. This paper introduces an integrated systems-based framework (architecture) for diagnosis and prognosis that is generic and applicable to a variety of engineering systems. The enabling technologies are based on suitable health monitoring hardware and software, data processing methods that focus on extracting features or condition indicators from raw data via data mining and sensor fusion tools, accurate diagnostic and prognostic algorithms that borrow from Bayesian estimation theory, and specifically particle filtering, fatigue or degradation modeling, and real-time measurements to declare a fault with prescribed confidence and given false alarm rate while predicting accurately and precisely the remaining useful life of the failing component/system. Potential benefits to industry include reduced maintenance costs, improved equipment uptime and safety. The approach is illustrated with examples from the aircraft and industrial domains.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5234094,no
Fault detection of Air Intake Systems of SI gasoline engines using mean value and within cycle models,2009,"This paper addresses the detection of faults in Air Intake Systems (AIS) of SI gasoline engines based on realtime measurements. It presents comparison of two classes of models for fault detection, namely those using a Mean Value Engine Model (MVEM) involving variables averaged over cycles and Within Cycle Crank-angle-based Model (WCCM) involving instantaneous values of variables changing with crank angle. Numerical simulation results of intake manifold leak and mass air flow sensor gain faults, obtained using the industry standard software called AMESim<sup>TM</sup>, have been used to demonstrate the fault detection capabilities of individual approaches. Based on these results it is clear that the method using WCCM has a higher fault detection sensitivity compared to one that uses MVEM, albeit at the expense of increased computational and modeling complexity.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5234095,no
Dealing with Driver Failures in the Storage Stack,2009,"This work augments MINIX 3's failure-resilience mechanisms with novel disk-driver recovery strategies and guaranteed file-system data integrity. We propose a flexible filter-driver framework that operates transparently to both the file system and the disk driver and enforces different protection strategies. The filter uses checksumming and mirroring in order to achieve end-to-end integrity and provide hard guarantees for detection of silent data corruption and recovery of lost data. In addition, the filter uses semantic information about the driver's working in order to verify correct operation and proactively replace the driver if an anomaly is detected. We evaluated our design through a series of experiments on a prototype implementation: application-level benchmarks show modest performance overhead of 0-28% and software-implemented fault-injection (SWIFI) testing demonstrates the filter's ability to detect and transparently recover from both data-integrity problems and driver-protocol violations.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5234311,no
Mapping Web-Based Applications Failures to Faults,2009,"In a world where software system is a daily need, system dependability has been a focus of interest. Nowadays, Web-based systems are more and more used and there is a lack of works about software fault representativeness for this platform. This work presents a field data study to analyze software faults found in real Java Web-based software during system test phase, performed by an independent software verification & validation team. The preliminary results allow us to conclude that previous classification partially fits Java Web-based software systems and must be extended to allow specific Web resources like JSP pages.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5234315,no
Study of the relationship of bug consistency with respect to performance of spectra metrics,2009,"In practice, manual debugging to locate bugs is a daunting and time-consuming task. By using software fault localization, we can reduce this time substantially. The technique of software fault localization can be performed using execution profiles of the software under several test inputs. Such profiles, known as program spectra, consist of the coverage of correct and incorrect executions statement from a given test suite. We have performed a systematic evaluation of several metrics that make use of measurement obtained from program spectra on Siemens test suite. In this paper, we discuss how the effectiveness of various metrics degrade in determining buggy statements as the bug consistency (error detection accuracy, q<sub>e</sub>) of a statement approaches zero. Bug consistency of a statement refers to the ratio of the number of failed tests executing the statement over the total number of tests executing the statement. We proposed effect(M) as to measure the effectiveness of these metrics as q<sub>e</sub> value varies. We also demonstrate that the q<sub>e</sub> (previously not considered as a metric), is just as effective as some of the metrics proposed. We also formally prove that q<sub>e</sub> is identical to the metric that Tarantula system uses for bug localization.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5234512,no
Prioritizing test cases for resource constraint environments using historical test case performance data,2009,"Regression testing has been widely used to assure the acquirement of appropriate quality through several versions of a software program. Regression testing, however, is too expensive in that, it requires many test case executions and a large number of test cases. To provide the missing flexibility, researchers introduced prioritization techniques. The aim in this paper has been to prioritize test cases during software regression test. To achieve this, a new equation is presented. The proposed equation considers historical effectiveness of the test cases in fault detection, each test case's execution history in regression test and finally the last priority assigned to the test case. The results of applying the proposed equation to compute the priority of regression test cases for two benchmarks, known as Siemens suite and Space program, demonstrate the relatively faster fault detection in resource and time constrained environments.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5234968,no
Modeling and reconstruction of the statistical data from a system electronic application for administrative management in IPN through the filter of Kalman,2009,"On the Management of Human Capital at the National Polytechnic Institute are developing information systems that seek improvements in administrative procedures, ensuring the acquisition complete of date, reliable and opportune, using technology available at the WEB. This aims to provide a quality service to more than 24,800 employees. In this case we propose the application forms via the Internet, because these employees carry data and digital documents. However, to provide the best service and sizing the server, it is necessary to analyze the accesses to the database and rebuild the system; for this is necessary to realize measurements using a free software tool and to analyze it as a system with parameter variations in time represented by an ARMA model using the Kalman filter as parameter estimator with good results.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5235956,no
Vehicle suspensions performance testing system based on virtual instrument,2009,"On the base of the vibration mechanical model and the motion equation of automobile suspension built. Analyze and bring forward the performance evaluation index of the suspension, and put forward the new testing method of the suspension performance. Using the virtual instrument technology, we develop the automobile suspension performance testing system. It adopts the PC-DAQ scheme. On the base of the original excitation suspension performance testing table we equip the table with some necessary sensors, the personal computer and the virtual software. Then the automobile suspension performance testing system is built. It can measure the absorptivity, the vibration frequency, the phase difference and the corresponding vibration waveform and so on. It also provides the foundation for the integrated estimation of the suspension performance and fault diagnosis. By testing cars in practice, the academic model is proved accurate and feasible and the testing system is exact and credible.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5245046,no
Design of embedded laser beam measurement system,2009,"In view of disadvantages lying in traditional laser beam quality measurement, an evaluation system was developed which can be used to detect laser beam quality automatically, M<sup>2</sup> factor was used to evaluate the laser beam quality which was suggested by ISO. This system can be used in the laboratory and industry field, which collected image data of the laser speckle by image sampling system, stored image data and transmitted them to the embedded processor ARM11 under the control of FPGA, which is the abbreviation of field programmable gate array and DSP which is the abbreviation of digital signal processor, then run the image processing application software, executed laser parameter calculating algorithm, imitated laser beam transmitting hyperbolic curve in space, calculated the value of M<sup>2</sup>. Results showed that this system had advantages such as small size, light scale, low cost, simple operation, easy use, high measurement precision and so on, which had better application and popularize value.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5246349,no
Comparing apples and oranges: Subjective quality assessment of streamed video with different types of distortion,2009,"Video quality assessment is essential for the performance analysis of visual communication applications. Objective metrics can be used to estimate the relative quality differences, but they typically give reliable results only if the compared videos contain similar type of quality distortion. However, video compression typically produces different kinds of visual artifacts than transmission errors. In this paper, we propose a novel subjective quality assessment method that is suitable for comparing different types of quality distortions. The proposed method has been used to evaluate how well PSNR estimates the relative subjective quality levels for content with different types of quality distortions. Our conclusion is that PSNR is not a reliable metric for assessing the co-impact of compression artifacts and transmission errors on the subjective quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5246963,no
Towards a computational quality model for IP-based audio,2009,"This paper proposes a methodology for the development of a computational quality model for IP-based audio. The project mainly focuses on two questions: 1) choice of a quality measurement scale in the computational model and mapping of this scale with existing audio quality measurement metrics, and 2) quantification of encoding and packet loss impairments (only two quality degradation parameters are considered; others will be discussed in future work). The objective PEAQ algorithm in the SEAQ software implementation is used for the parameter quantification and the model accuracy estimation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5246968,no
Modelling of partial discharge activity in different spherical cavity sizes and locations within a dielectric insulation material,2009,"The pattern of partial discharge (PD) occurrence at a defect site within a solid dielectric material is influenced by the conditions of the defect site. This is because the defect conditions, mainly its size and location determine the electric field distributions at the defect site which influence the patterns of PD occurrence. A model for a spherical cavity within a homogeneous dielectric material has been developed by using Finite Element Analysis (FEA) software. The model is used to study the influence of different conditions of the cavity on the electric field distribution in the cavity and the PD activity. In addition, experimental measurements of PD in spherical cavities of different size within a dielectric material have been undertaken. The obtained results show that PD activity depends on the size of the cavity within the dielectric material.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5252384,no
Adaptive Control Framework for Software Components: Case-Based Reasoning Approach,2009,"The proposed architecture for an adaptive software system is based on a multi-threaded programming model. It includes two basic logic components: a database for selected case gathering and a decision making sub-system. Using CBR-methods, a control procedure for the decision-making sub-system is worked out, which uses a database for selected case gathering. A respective CBR algorithm determines the number of threads needed for guaranteed predicted performance (measured in ms), and reliability (defined in %).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5254069,no
Towards a Unified Behavioral Model for Component-Based and Service-Oriented Systems,2009,"There is no clear distinction between service-oriented systems (SOS) and component-based systems (CBS). However, there are several characteristics that could let one consider SOS as a step further from CBS. In this paper, we discuss the general features of CBS and SOS, while accounting for behavioral modeling in the language called REMES. First, we present REMES in the context of CBS modeling, and then we show how it can become suitable for SOS. We also discuss the relation between our model and the current state of the art.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5254076,no
Test Selection Prioritization Strategy,2009,"A wide divergence is observed in projects between test activities planned in the test plan and the actual tests that can be executed. Estimates for test execution computed during the planning are inaccurate without test design. The actual time and resources available are usually less than planned. Assuming that time and resources cannot be changed, a dynamic selection of tests for execution that maximizes quality is required.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5254084,no
Predicting Change Impact in Object-Oriented Applications with Bayesian Networks,2009,"This study has to be considered as another step towards the proposal of assessment/predictive models in software quality. We consider in this work, that a probabilistic model using Bayesian nets constitutes an interesting alternative to non-probabilistic models suggested in the literature. Thus, we propose in this paper a probabilistic approach using Bayesian networks to analyze and predict change impact in object-oriented systems. An impact model is built and probabilities are assigned to network nodes. Data obtained from a real system are exploited to empirically study causality hypotheses between some software internal attributes and change impact. Several scenarios are executed on the network, and the obtained results confirm that coupling is a good indicator of change impact.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5254257,no
Applicability of Software Reliability Growth Modeling in the Quality Assurance Phase of a Large Business Software Vendor,2009,Software reliability growth modeling aims to use data on experienced failures for prediction of future quality levels. We analyze the applicability of this approach in the context of business software that is still in the quality assurance phase and has not been released to the customer yet. We find that the approach is not applicable in our research setup that is characterized by an agile process organization. We identify qualitative reasons why the models' assumptions are violated in our industrial case study and conduct a quantitative analysis whether data availability challenges can be addressed by mapping issue tracking data as the only available data source to the required system under test execution time. The derived metrics support managers in their decision whether their processes are suited for software reliability growth modeling.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5254261,no
Improvements for 802.11-Based Location Fingerprinting Systems,2009,"Position estimation with 802.11 and location fingerprinting has been a topic in research for quite some time already. But there are still some unaddressed issues that are the reason why such systems are not widely used. First, the positioning accuracy still leaves space for improvements. Second, users generally have no information about the quality of the estimated position. Especially in cases where the positioning error is large, the user's trust in the system suffers if he is not notified. Third, most systems that rely on a location fingerprinting approach need a time and effort consuming setup phase in which the training data has to be collected and processed. In this paper, we give an overview of existing possibilities to improve location fingerprinting systems under each of these three aspects. Several alternative solutions for each problem are presented. Finally, a discussion gives an overall picture of the usability of the solutions when considering the system as a whole.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5254284,no
Research and Application on Automatic Network Topology Discovery in ITSM System,2009,"Automatic network topology discovery is important for network management and network analysis in ITSM system. Issues like network environment configuration, performance testing and fault detecting all require accurate information of the network topology. According to the requirement of the ITSM system, in this paper we propose an efficient algorithm that is based on SNMP for discovering Layer 3 and Layer 2 topology of the network. The proposed algorithm only requires SNMP to be enabled on routers and managed switches. For Layer 3 topology discovery, this paper proposes a shared subnet based approach to analyze connections between routers. In addition, the algorithm uses a STP based method to discover Layer 2 topology, which does not require the completeness of switchpsilas AFT information. The algorithm has been implemented and tested in several enterprise-level networks, and the results demonstrate that it discovers the network topology accurately and efficiently.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5254478,no
Research of a Service Monitoring System Based on SIP in Hybrid Network,2009,"In IPv4 and IPv6 network, there are especially shortages of Application Performance Monitor system, which can help system owners to monitor and diagnostic system performance and provide warning message. In the paper, we have designed and implemented a SIP service monitoring scheme (SSMS) that includes a Service Detection System (SDS) and Real-time Alert System (RAS). The Service Detection System simulates the way of applied service access to proceed the quality testing in order to increase the credibility, while the real-time alert system utilizes the way of VoIP to inform the service provider and to confirm the alarm message.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5254504,no
A Service Self-Optimization Algorithm based on Autonomic Computing,2009,"Under the intrusion or abnormal attack, how to autonomously supply undergraded service to users is the ultimate goal of network security technology. Firstly, combined with martingale difference principle, a service self optimization algorithm based on autonomic computing-S<sup>2</sup>O<sub>AC</sub> is proposed. Secondly, according to the prior self optimizing knowledge and parameter information of inner environment, S<sup>2</sup>O<sub>AC</sub> searches the convergence trend of self optimizing function and executes the dynamic self optimization, aiming at minimum the optimization mode rate and maximum the service performance. Thirdly, set of the best optimization mode is updated and prediction model is renewed, which will implement the static self optimization and improve the accuracy of self optimization prediction. At last, the simulation results validate the efficiency and superiority of S<sup>2</sup>O<sub>AC</sub>.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5255010,no
On the integration of protein contact map predictions,2009,"Protein structure prediction is a key topic in computational structural proteomics. The hypothesis that protein biological functions are implied by their three-dimensional structure makes the protein tertiary structure prediction a relevant problem to be solved. Predicting the tertiary structure of a protein by using its residue sequence is called the protein folding problem. Recently, novel approaches to the solution of this problem have been found and many of them use contact maps as a guide during the prediction process. Contact map structures are bidimensional objects which represent some of the structural information of a protein. Many approaches and bioinformatics tools for contact map prediction have been presented during the past years, having different performances for different protein families. In this work we present a novel approach based on the integration of contact map predictions in order to improve the quality of the predicted contact map with a consensus-based algorithm.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5255418,no
Next-Generation Power Information System,2009,"Power monitoring systems (power quality monitors, digital fault recorders, digital relays, advanced controllers, etc.) continue to get more powerful and provide a growing array of benefits to overall power system operation and performance evaluation. Permanent monitoring systems are used to track ongoing performance, to watch for conditions that could require attention, and to provide information for utility and customer personnel when there is a problem to investigate. An important development area for these monitoring systems is the implementation of intelligent systems that can automatically evaluate disturbances and conditions to make conclusions about the cause of a problem or even predict problems before they occur. This paper describes the development of a Next Generation Power Information System that will provide an open platform for implementing advanced monitoring system applications that involve integration with many different data sources. The work builds on many years of experience with software for management and analysis of large power quality monitoring systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5255806,no
Quantifying the Potential of Program Analysis Peripherals,2009,"Tools such as multi-threaded data race detectors, memory bounds checkers, dynamic type analyzers, data flight recorders, and various performance profilers are becoming increasingly vital aids to software developers. Rather than performing all the instrumentation and analysis on the main processor, we exploit the fact that increasingly high-throughput board level interconnect is available on many systems, a fact we use to off-load analysis to an off-chip accelerator. We characterize the potential of such a system to both accelerate existing software development tools and enable a new class of heavyweight tools. There are many non-trivial technical issues in taking such an approach that may not appear in simulation, and to flush them out we have developed a prototype system that maps a DMA based analysis engine, sitting on a PCI-mounted FPGA, into the Valgrind instrumentation framework. With our novel instrumentation methods, we demonstrate that program analysis speedups of 29% to 440% could be achieved today with strictly off-the-shelf components on some of the state-of-the-art tools, and we carefully quantify the bottlenecks to illuminate several new opportunities for further architectural innovation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5260550,no
A Domain Specific Language in Dependability Analysis,2009,"Domain specific languages gain increasing popularity as they substantially leverage software development by bridging the gap between technical and business area. After a domain framework is produced, experts gain an effective vehicle for assessing quality and performance of a system in the business-specific context. We consider the domain to be dependability of multi-agent system (MAS), for which a key requirement is an efficient verification of a topology model of a power system. As a result, we come up with a reliability evaluation solution offering a significant rise in the level of abstraction towards MAS utilized for purposes of a power system topology verification. By means of the mentioned solution safety engineers are enabled to perform analysis while the design is still incomplete. A new DSL is developed in XText in order to specify a structure of the system together with dependability extensions, which are further translated into dynamic fault trees using model to model transformations. The Eclipse Ecore becomes a common denominator, in which both metamodelspsila abstract syntax trees are defined. Finally, an expert is offered with two ways of defining a model: through abstract and textual concrete syntax, both of which are checked for consistency using object constraint language.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5261002,no
A Model Based Framework for Specifying and Executing Fault Injection Experiments,2009,"Dependability is a fundamental property of computer systems operating in critical environment. The measurement of dependability (and thus the assessment of the solutions applied to improve dependability) typically relies on controlled fault injection experiments that are able to reveal the behavior of the system in case of faults (to test error handling and fault tolerance) or extreme input conditions (to assess robustness of system components). In our paper we present an Eclipse-based fault injection framework that provides a model-based approach and a graphical user interface to specify both the fault injection experiments and the run-time monitoring of the results. It automatically implements the modifications that are required for fault injection and monitoring using the Javassist technology, this way it supports the dependability assessment and robustness testing of software components written in Java.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5261017,no
A Comparison of Structural Testing Strategies Based on Subdomain Testing and Random Testing,2009,"Both partition testing and random testing methods are commonly followed practice towards selection of test cases. For partition testing, the programpsilas input domain is divided into subsets, called subdomains, and one or more representatives from each subdomain are selected to test the program. In random testing test cases are selected from the entire programpsilas input domain randomly. The main aim of the paper is to compare the fault-detecting ability of partition testing and random testing methods. The results of comparing the effectiveness of partition testing and random testing may be surprising to many people. Even when partition testing is better than random testing at finding faults, the difference in effectiveness is marginal. Using some effectiveness metrics for testing and some partitioning schemes this paper investigates formal conditions for partition testing to be better than random testing and vice versa.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5261018,no
Scrum and CMMI Going from Good to Great,2009,"Projects combining agile methods with CMMI combine adaptability with predictability to better serve large customer needs. The introduction of Scrum at Systematic, a CMMI level 5 company, doubled productivity and cut defects by 40% compared to waterfall projects in 2006 by focusing on early testing and time to fix builds. Systematic institutionalized Scrum across all projects and used data driven tools like story process efficiency to surface product backlog impediments. This allowed them to systematically develop a strategy for a second doubling in productivity. Two teams have achieved a sustainable quadrupling of productivity compared to waterfall projects. We discuss here the strategy to bring the entire company to that level. Our experiences shows that Scrum and CMMI together bring a more powerful combination of adaptability and predictability than either one alone and suggest how other companies can combine them to achieve Toyota level performance - 4 times the productivity and 12 times the quality of waterfall teams.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5261061,no
Study on application of three-dimensional laser scanning technology in forestry resources inventory,2009,"Terrestrial laser scanners, as efficient tools, have opened a wide range of application fields within a rather short period of time. Beyond interactive measurement in 3D point clouds, techniques for the automatic detection of objects and the determination of geometric parameters form a high priority research issue. The quality of 3D point clouds generated by laser scanners and the automation potential make terrestrial laser scanning also an interesting tool for forest inventory and management. The paper will first review current laser scanner systems from a technological point of view and discuss different scanner technologies and system parameters regarding their suitability for forestry applications. In the second part of the paper, results of a pilot study on the applicability of terrestrial laser scanners in forest inventory tasks will be presented. The study concentrates on the automatic detection of trees and the subsequent determination of tree height and diameter at breast height. Reliability and precision of techniques for automatic point cloud processing were analysed based on scans of a test region in Harbin Experimental Forest Farm. In the pilot study, which represents an early stage of software development, more than 95% of the trees in a test region could be detected correctly. Tree heights could be determined with a precision of 80 cm, and breast height diameters could be determined with a precision of less than 1.5 cm.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5262597,no
Efficient fault-prone software modules selection based on complexity metrics,2009,"In order to improve the software reliability early, this paper proposes an efficient algorithm to select fault-prone software module. Based on software module's complexity metrics, the algorithm uses modified cascaded-correlation algorithm as neural network classifier to select the fault-prone software module. Finally, by analyzing the algorithm's application in the project MAP, the paper shows the advantage of the algorithm.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5267972,no
The research on fault equivalent analysis method in testability experiment validation,2009,"With the existing fault injection techniques, many faults that can fully expose testability design defects can not be injected. To solve this problem, a method of fault equivalent analysis is proposed. By this means, some characteristics are extracted from the faults those unable to be injected, and ldquoyield analysisrdquo or ldquoyielded analysisrdquo is performed. Then the minimal cut sets of atom faults is obtained and selected, which are finally equivalent to the atom faults sequence. Applications show that the method not only solves the problem that many faults are not able to be injected, but also ensures the effect of testability experiment validation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5269965,no
Software task processing with dependent modules and some measures,2009,"This paper proposes a new model by combining an infinite-server queueing model for multi-task processing software system with a perfect debugging model based on Markov process with two types of faults suggested by Lee, Nam and Park. We apply this model for module and integration testing in the testing process. Also, we compute the expected number of tasks whose processes can be completed and the task completion probability are investigated under the proposed model. We interpret the meaning of the interaction between modules in a software composed of dependent modules.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5270081,no
Software FMEA approach based on failure modes database,2009,"A classification method of software failure modes based on software IPO process is presented. And then two database called general failure modes database (GFMD) and special failure modes database (SFMD) are proposed based on this classification method. Furthermore, a new approach of software FMEA which is based on GFMD and SFMD is presented. This approach which makes the analysis process of FMEA more operable and the failure modes obtained from analysis more comprehensive improves the efficiency of Software FMEA. Meanwhile GFMD and SFMD also offer a platform for the analyzers to accumulate and share their experience. The case study shows that this approach mentioned in this paper is effective in the practice.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5270088,no
An approach of software quality prediction based on relationship analysis and prediction model,2009,"By predicting the quality of the software that will be formed in the early stage of development, faults brought in at the phase of design will be found out early in order not leave them in the software product. Furthermore, it will be easy for designers to adopt appropriate plans based on specific expectations of the target software. However, the traditional prediction models have following shortages: 1) the relationship between attributes and metrics effectively cannot be expressed; 2) lack of the ability to process data both qualitatively and quantitatively; 3) not appropriate to the case with uncompleted information. In this paper, a model built based on and fuzzy neural network is proved to be good at quality prediction of object-oriented software.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5270097,no
Three types of fault coverage in multi-state systems,2009,"Fault-tolerance is an essential architectural attribute for achieving high reliability in many critical applications of digital systems. Automatic fault and error handling mechanisms play a crucial role in implementing fault tolerance because an uncovered (undetected) fault may lead to a system or a subsystem failure even when adequate redundancy exists. Examples of this effect can be found in computing systems, electrical power distribution networks, pipelines carrying dangerous materials etc. Because an uncovered fault may lead to overall system failure, an excessive level of redundancy may even reduce the system reliability. We consider three types of coverage models: 1. element level coverage where the fault coverage probability of an element does not depend on the states of other elements; 2. the multi-fault coverage where the effectiveness of recovery mechanisms depends on the coexistence of multiple faults in a group of elements that collectively participate in detecting and recovering the faults in that group; 3. the performance dependent coverage where the effectiveness of recovery mechanisms in a group depends on the entire performance level of this group. The paper presents a modification of the generalized reliability block diagram (RBD) method for evaluating reliability and performance indices of complex multi-state series-parallel systems with all these types of fault coverage. The suggested method based on a universal generating function technique allows the system performance distribution to be obtained using a straightforward recursive procedure.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5270224,no
An end-to-end approach for the automatic derivation of application-aware error detectors,2009,"Critical Variable Recomputation (CVR) based error detection provides high coverage for data critical to an application while reducing the performance overhead associated with detecting benign errors. However, when implemented exclusively in software, the performance penalty associated with CVR based detection is unsuitably high. This paper addresses this limitation by providing a hybrid hardware/software tool chain which allows for the design of efficient error detectors while minimizing additional hardware. Detection mechanisms are automatically derived during compilation and mapped onto hardware where they are executed in parallel with the original task at runtime. When tested using an FPGA platform, results show that our approach incurs an area overhead of 53% while increasing execution time by 27% on average.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5270291,no
Automatic fault detection and diagnosis in complex software systems by information-theoretic monitoring,2009,"Management metrics of complex software systems exhibit stable correlations which can enable fault detection and diagnosis. Current approaches use specific analytic forms, typically linear, for modeling correlations. In this paper we use normalized mutual information as a similarity measure to identify clusters of correlated metrics, without knowing the specific form. We show how we can apply the Wilcoxon rank-sum test to identify anomalous behaviour. We present two diagnosis algorithms to locate faulty components: RatioScore, based on the Jaccard coefficient, and SigScore, which incorporates knowledge of component dependencies. We evaluate our mechanisms in the context of a complex enterprise application. Through fault injection experiments, we show that we can detect 17 out of 22 faults without any false positives. We diagnose the faulty component in the top five anomaly scores 7 times out of 17 using SigScore, which is 40% better than when system structure is ignored.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5270324,no
Improving students' hardware and software skills by providing unrestricted access to state of the art design tools and hardware systems,2009,"The technology and CAD tools employed by industry to design digital hardware evolve quickly and continuously. Well prepared engineers, who are able to produce actual designs and adapt themselves to the global world, are in demand. Educational programs must keep pace with technologies in common use in order to produce graduates who are competitive in the marketplace. Studies conducted at two different universities, Rose Hulman Institute of Technology and Washington State University measure changes in student performance when all students have unlimited access to state of the art design tools and hardware systems. Data are collected from surveys, exams, and course assignments. Quantitative data are analyzed by comparison to historical data gathered from student groups that did not have unlimited access to hardware systems, and qualitative data are used to determine the subjective quality of each student's experience. Outcomes include: assessing whether the overall learning process is improved; whether students have a better knowledge of modern technologies and design methods; whether their comprehension of founding concepts has improved or faltered.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5270827,no
Relative evaluation of partition algorithms for complex networks,2009,"Complex networks partitioning consists in identifying denser groups of nodes. This popular research topic has applications in many fields such as biology, social sciences and physics. This led to many different partition algorithms, most of them based on Newman's modularity measure, which estimates the quality of a partition. Until now, these algorithms were tested only on a few real networks or unrealistic artificial ones. In this work, we use the more realistic generative model developed by Lancichinetti et al. to compare seven algorithms: Edge-betweenness, Eigenvector, Fast Greedy, Label Propagation, Markov Clustering, Spinglass and Walktrap. We used normalized mutual information (NMI) to assess their performances. Our results show Spinglass and Walktrap are above the others in terms of quality, while Markov Clustering and Edge-Betweenness also achieve good performance. Additionally, we compared NMI and modularity and observed they are not necessarily related: some algorithms produce better partitions while getting lower modularity.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5272078,no
Seam Reconstruct: Dynamic scene stitching with Large exposure difference,2009,"Panoramic stitching of static and dynamic scenes is a very important and challenging research area. For static scenes, number of approaches has been proposed so far. The result produced by these approaches provides great similarity between resultant panorama and input images with almost zero seam visibility. However for dynamic scenes, we have found that existing approaches are unable to overcome the challenges introduced due to simultaneous presence of (A) Large exposure difference and (B) Moving Objects. In this paper we propose a Seam-Reconstruction technique which overcomes these limitations. Seam-Reconstruction technique is a two step approach. The first step resolves the position of moving objects and prevents ghosting due to parallax by building a reference panorama with the help of optimal seam evaluation technique while the second step removes the remaining differences along the seam with the help of Poisson's equation. Experimental result depicts that we were able to stitch dynamic scene containing large exposure difference maintaining the quality as well as photometric and geometric similarity.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5273843,no
Matching schemas of heterogeneous relational databases,2009,"Schema matching is a basic problem in many database application domains, such as data integration. The problem of schema matching can be formulated as follows, ldquogiven two schemas, S<sub>i</sub> and S<sub>j</sub>, find the most plausible correspondences between the elements of S<sub>i</sub> and S<sub>j</sub>, exploiting all available information, such as the schemas, instance data, and auxiliary sourcesrdquo. Given the rapidly increasing number of data sources to integrate and due to database heterogeneities, manually identifying schema matches is a tedious, time consuming, error-prone, and therefore expensive process. As systems become able to handle more complex databases and applications, their schemas become large, further increasing the number of matches to be performed. Thus, automating this process, which attempts to achieve faster and less labor-intensive, has been one of the main tasks in data integration. However, it is not possible to determine fully automatically the different correspondences between schemas, primarily because of the differing and often not explicated or documented semantics of the schemas. Several solutions in solving the issues of schema matching have been proposed. Nevertheless, these solutions are still limited, as they do not explore most of the available information related to schemas and thus affect the result of integration. This paper presents an approach for matching schemas of heterogeneous relational databases that utilizes most of the information related to schemas, which indirectly explores the implicit semantics of the schemas, that further improves the results of the integration.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5273926,no
Assessing easiness with Froglingo,2009,"Expressive power has been a well-established as a dimension of measuring the quality of a computer language. Easiness is another dimension. It is the main stream of development in programming language and database management. However, there has not been a method to precisely measure the easiness of a computer language. This article discusses easiness. Provided that a data model be easier than a programming language in representing the semantics of the given data model, this article concludes that Froglingo is the easiest in database application development and maintenance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5273928,no
Design and data processing of a real-time power quality monitoring instrument,2009,"Power quality (PQ) monitoring is an important issue to electric utilities and many industrial power customers. This paper presents a new distributed monitoring instrument based on Digital Signal Processing (DSP) and virtual measuring technology. The instrument is composed of EOC module, Feeder Control Unit (FCU) and Supervision Unit (SU). EOC module is used to implement the data acquisition and high speed data transmission of busbar voltage; FCU performs data acquisition and processing of feeders; SU based on virtual measuring technology is used to further analyze and process power quality parameters, achieves data storage and management. Wavelet-transformation which is implemented into SU detects transient power quality disturbance, while digital filtering, windowing, the software fixed-frequency sampling method, linear interpolation and Fast Fourier Transformation (FFT) are realized by DSP. Therefore, the monitoring instrument not only implements real-time, comprehensive high-precision and management for all power quality parameters, but also helps confirm source of the disturbance, improve the quality of power supply and increase the stability performance of power system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5274063,no
Research and design of IUR76-II test system for infrared flame detectors,2009,"IUR76-II performance test system for infrared flame detectors is based on the National Standard of P.R.C, which is named as `performance requirements and test methods for point infrared flame detectors' (GB15631-1995). The architecture of new test system includes of infrared point source, optical track, modulator, optical filter device, 4-dimension holder, optical simulators, data collector, and computer. The common test is consisted of response threshold measurement, response time measurement, direction measurement, surrounding optical interference test, and power on test. In the new test system, SiC infrared source is used to replace the fire of the combusting gasified gasoline, which is used in national standard. The stability of SiC infrared source is smaller than 0.1%, which is much better than 20% (stability of the combusting gasified gasoline). The optical filter device is equipped with stepper motor. There are four bands infrared waves could pass through optical filter device, which are 4.2 mum ~4.7 mum wide band, 4.32 mum narrow band, 4.45 mum narrow band, 4.58 mum narrow band. The data collector and computer software is used in new test system to analyze measurement results and obtain quality reports for DUT. Based on the experience procedure and results, the new test system is proved more safe, convenient, and precision than the system proposed by national standard. This new test system for infrared flame detectors is applied in testing the point infrared flame detectors equipped in industrial or civilian buildings.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5274466,no
The design of online system for multi-rang measuring sediment concentration,2009,"According to actual sediment testing needs, an online system is designed to measure sediment concentration. In this system, two testing ways will be selected at the different sediment range. When the water contain low sediment, the system will chose photoelectric-based detection methods to measure. It has a low measurement range, but has a high accuracy. When the water contain high sediment, the system will chose capacitive differential pressure detection methods to measure. On the contrary, it has a lower precision, but has higher measurement range. This paper particularly introduces photoelectric testing technology, principle of the capacitive differential pressure sensor, PLC hardware design, sensor inclination correction and monitoring software design. The system can overcome the effects of different environments, and work steadily. This system is more suitable to measure sediment concentration in the reservoir dredging, water quality management and slurry treatment etc.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5274553,no
Research on harmonic current distribution of beijing's rail traffic power supply system,2009,"This paper will discuss the monitoring and data analysis of nonlinear load power quality by analyzing electrical railway, which is a typical nonlinear load in Beijing. Firstly, this paper will introduce a method of designing the software and hardware of a kind of highly accurate and large-capacity energy quality data recorder, which is capable of accurately recording voltage, electric current wave, etc., data on site for a long standing and can restore the original data with upper computer in lab. Secondly, this paper will analyze the data collected by the energy quality data recorder from Beijing Xiazhuang transformer substation, which is responsible for supplying electricity to Daqin railway and is a typical nonlinear electrical railway load. On the one hand this paper presents the complete data of fundamental current and harmonic current in a typical working day (as shown in graphs) and analyzes the correlation between fundamental current and main harmonic current. On the other hand, this paper also analyzes the amplitude of main harmonic current distribution with statistical methods and establishes an autoregressive (AR) model to illustrate the distribution of main harmonic current. From the above analysis we can get the key data for evaluating the impact generated by this transformer substation on the grid's electrical energy quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5274615,no
Design of a remote-monitoring system of MOA based on GPRS,2009,"By measuring resistive current of the Metal-oxide Arresters(MOA) on-line monitoring technology, we can understand the performance condition of MOA at any time without unnecessary power-cut-off overhaul. Thus we can detect the abnormal phenomena and hidden accidents of MOA in time, take measures in advance to prevent the accident from getting worse and to avoid the economic loss resulted from the accident. To conquer the defect of this transmission method, a new method of monitoring of metal-oxide arresters in long distance was presented. The monitoring system consists of on-spot collection module, GPRS transmission module, long-distance monitoring center. The design of hardware and software of TMS320F2812 microprocessor according to the data collection and processing module was introduced. Data report can be formed in this system. The state of MOA can be inspected conveniently and accurately. Electric Power system can run in the reliable state.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5274776,no
Research on location of single-phase earth fault based on pulse injection method in distribution network,2009,"A method of fault location based on pulse signal injection has been promoted in this paper, which is not only independent of the following factors, such as system operating mode, topology, neutral grounding and random fault, but also the site of signal injection, the width and period of pulse are flexible and adjustable. In this paper the design scheme of software and hardware for signal source is proposed, high-pressure pulse generator is carried out by adopting C8051F310 MCU, and signal detector is designed based on the principle of electromagnetic induction.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5274777,no
Real-time power quality monitoring and analysis platform of rural power network based on virtual instrument,2009,"A new design proposal of real-time power quality monitoring and analysis platform based on virtual instruments technology is proposed that aims at the key issues that affecting the quality of rural power network. From steady-state to transient-state, this article discusses the typical algorithms of harmonic analysis, voltage deviation and unbalance in three-phase, voltage swells and dips, voltage fluctuation, transient oscillation and the theory of power quality disturbances detection based on Hilbert phase-shifting. Completing the software design of power quality monitoring and analysis platform based on NI (National Instruments)-LabVIEW, which including visual design algorithm of the monitoring and analysis, graphical display of the calculation, analysis and display of the results, auxiliary we also use the FLUKE standard power source. The actual results show that the test results of our platform are accurate, the interfaces are friendly and the performances are steady and practical.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5274786,no
Research and design of IUR76-I test system for ultraviolet flame detectors,2009,"IUR7 6-1 performance test system for ultraviolet (UV) flame detectors is based on the National Standard of P.R.C, which is named as 'performance requirements and test methods for point ultraviolet flame detectors' (GB12791-199Gamma). The architecture of new test system includes of ultraviolet point source, optical track, modulator, optical filter device, 4-dimension holder, optical simulators, data collector, and computer. The common test is consisted of response threshold measurement, response characteristic curve measurement, direction measurement, surrounding optical interference test, and power on test. In the new test system, deuterium lamp is used as the ultraviolet point source to replace the fire of the combusting gasified gasoline, which is used in national standard. The stability of deuterium lamp is smaller than 0.05%, which is much better than 20% (stability of the combusting gasified gasoline). The optical filter device is equipped with stepper motor. There are four bands ultraviolet waves could pass through optical filter device, which are 200 nm~280 nm wide band, 210 nm narrow band, 254 nm narrow band, 270 nm narrow band. The data collector and computer software is used in new test system to analyze measurement results and obtain quality reports for DUT based on the experience procedure and results, the new test system is proved more convenient, safe, and precision than the system proposed by national standard. This new test system for ultraviolet flame detectors is applied in testing the point ultraviolet flame detectors equipped in industrial or civilian buildings.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5274814,no
Phase based-voltage dip classification using a new classification Algorithm,2009,"This paper is concerned with the development and implementation of an improved algorithm for dip classification. It can determine the type of dip based on the difference in phase angle between the measured voltages using Zinari's Algorithm. The improved algorithm overcomes the shortcomings of existing conventional dip classification methods which are based solely on the duration and magnitude of the voltage deviation. A software program was developed using Matlab/Simulink and the performance of the algorithm was checked over the full range of phase shift and dip magnitude. Moreover, the algorithm was implemented in a laboratory set up, and various dip types were simulated and then classified using the improved algorithm.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5275197,no
Assessment of flicker impact of fluctuating loads prior to connection,2009,"With the acceptance of IEEE Std. 1453, many utility companies in North America are facing problems in applying the recommended concepts. These problems center around difficulties in predicting flicker produced by specific customers or loads before they are connected. The discussions in this paper are intended to serve as an overview of some of the methods that are commonly used, mostly outside North America, to perform pre-connection flicker assessments of fluctuating loads.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5275496,no
"Investigation of residential customer safety-line, neutral and earth integrity",2009,"Reverse polarity and neutral failures can produce potentially dangerous voltage levels within electrical consumer's premises. While earthing at the consumers premises is normally good during the installation, it may degrade over time. Existing conventional electromechanical energy meters do not detect such conditions at the consumer premises. Hence, an accurate detection of conditions such as reverse polarity, earthing and neutral failure and degradation is essential for safe and reliable operation of a household electrical system. It is highly desirable that a protection system is designed such that it should detect such conditions accurately and it should not be oversensitive, as this could lead to an unnecessarily unacceptable high level of ldquonuisancerdquo operation. In addition, such a solution should have to be reliable, economical and easily adoptable into existing premises without any major modification to the installation. This paper is intended to derive various necessary indices to detect neutral and earthing failure or degradation and reverse polarity conditions at the electrical consumer's premises. The simulation is carried out with the MATLAB<sup>reg</sup> - SIMULINK<sup>reg</sup> software with SimPowerSystemstrade toolbox. These indices can be integrated into a smart meter or similar device to accurately detect earthing and neutral failure or degradation and reverse polarity conditions at consumer premises.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5275507,no
An Adaptive mimic filter â€?Based algorithm for the detections of CT saturations,2009,"An adaptive mimic filter-based algorithm for detecting the saturation of current transformers (CTs) has been presented and implemented in this paper. First, an adaptive method was developed for obtaining the line impedance of a digital mimic filter. The variations of the obtained line impedance were then used to detect the CT saturations. By using the proposed algorithm, the saturation period of a current waveform can be accurately detected. This paper finally utilized the MATLAB/SIMULINK software and a DSP-based environment to verify the proposed algorithm. Test results show the proposed algorithm can accurately detect the CT saturations.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5275886,no
Geometrical approach on masked gross errors for power systems state estimation,2009,"In this paper, a geometrical based-index, called undetectability index (UI), that quantifies the inability of the traditional normalized residue test to detect single gross errors is proposed. It is shown that the error in measurements with high UI is not reflected in their residues. This masking effect is due to the ldquoproximityrdquo of a measurement to the range of the Jacobian matrix associated with the power system measurement set. A critical measurement is the limit case of measurement with high UI, that is, it belongs to the range of the Jacobian matrix, has an infinite UI index, its error is totally masked and cannot be detected in the normalized residue test at all. The set of measurements with high UI contains the critical measurements and, in general, the leverage points, however there exist measurements with high UI that are neither critical nor leverage points and whose errors are masked by the normalized residue test. In other words, the proposed index presents a more comprehensive picture of the problem of single gross error detection in power system state estimation than critical measurements and leverage points. The index calculation is very simple and is performed using routines already available in the existing state estimation software. Two small examples are presented to show the way the index works to assess the quality of measurement sets in terms of single gross error detection. The IEEE-14 bus system is used to show the efficiency of the proposed index to identify measurements whose errors are masked by the estimation processing.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5275941,no
Hierarchical fault detection and diagnosis for unmanned ground vehicles,2009,"This paper presents a fault detection and diagnosis (FDD) method for unmanned ground vehicles (UGVs) operating in multi agent systems. The hierarchical FDD method consisting of three layered software agents is proposed: Decentralized FDD (DFDD), centralized FDD (CFDD), and supervisory FDD (SFDD). Whereas the DFDD is based on modular characteristics of sensors, actuators, and controllers connected or embedded to a single DSP, the CFDD is to analyze the performance of vehicle control system and/or compare information between different DSPs or connected modules. The SFDD is designed to monitor the performance of UGV and compare it with local goal transmitted from a ground station via wireless communications. Then, all software agents for DFDD, CFDD, and SFDD interact with each other to detect a fault and diagnose its characteristics. Finally, the proposed method will be validated experimentally via hardware-in-the-loop simulations.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5276202,no
Enterprise architecture dependent application evaluations,2009,"Chief information officers (CIO) are faced with the increasing complexity of application landscapes. The task of specifying future architectures depends on an exact assessment of the current state. When CIOs have to evaluate the whole landscape or certain applications of it, they are facing an enormous challenge, since there is no common methodology for that purpose. Within this paper we address this task and present an approach for enterprise architecture dependent application evaluations. We outline why it is important not only to assess single applications by classical software metrics in order to get an indication of their quality, but also to regard the overall context of an enterprise application. To round up this contribution we present a brief example from a project with one of our industrial partners.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5276733,no
Robust Color Image Enhancement of Digitized Books,2009,"We present a spatially variant framework for correcting uneven illumination and color cast, problems commonly associated with digitized books. The core of our method is a color image segmentation algorithm based on watershed transform and noise thresholding. We propose a threshold estimation method using the histogram of the gradient magnitude. The critical parameters of the segmentation are further constrained by statistical analysis of multiple book pages conducted in a bootstrap pass. We demonstrate the consistent performance of the proposed method on books with wide-ranging content and image quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5277590,no
Chlorophyll measurement from Landsat TM imagery,2009,"Water quality is an important factor for human health and quality of life. This has been recognized many years ago. Remote sensing can be used for various purposes. Environmental monitoring through the method of traditional ship sampling is time consuming and requires a high survey cost. This study uses an empirical model, based on actual water quality of chlorophyll measurements from the Penang Strait, Malaysia to predict chlorophyll based on optical properties of satellite digital imagery. The feasibility of using remote sensing technique for estimating the concentration of chlorophyll using Landsat satellite imagery in Penang Island, Malaysia was investigated in this study. The objective of this study is to evaluate the feasibility of using Landsat TM image to provide useful data for the chlorophyll mapping studies. The chlorophyll measurements were collected simultaneously with the satellite image acquisition through a field work. The in-situ locations were determined using a handheld Global Positioning Systems (GPS). The surface reflectance values were retrieved using ATCOR2 in the PCI Geomatica 10.1.3 image processing software. And then the digital numbers for each band corresponding to the sea-truth locations were extracted and then converted into radiance values and reflectance values. The reflectance values were used for calibration of the water quality algorithm. The efficiency of the proposed algorithm was investigated based on the observations of correlation coefficient (R) and root-mean-square deviations (RMS) with the sea-truth data. Finally the chlorophyll map was color-coded and geometrically corrected for visual in terpretation. This study shows that the Landsat satellite imagery has the potential to supply useful data for chlorophyll studies by using the developed algorithm. This study indicates that the chlorophyll mapping can be carried out using remote sensing technique by using Landsat imagery and the previously developed algorithm over Penang,- Malaysia.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5278182,no
The data management system for the VENUS and NEPTUNE cabled observatories,2009,"The VENUS (http://venus.uvic.ca/) and NEPTUNE Canada (http://neptunecanada.ca/) cabled ocean observatories have been envisioned from their inception as underwater extensions of the Internet. Having sensors connected to a network opens up tremendous opportunities, from realtime access to sensor measurements to the interactive control of remote assets. Moreover, with a software system in control, data from all sensors can be managed, archived and made available to the wider community. Finally, a data management system can also offer features that are unique to centrally managed instrumentation: thanks to communication standards, autonomous event detection and reaction becomes a very powerful possibility. This talk will summarize the salient features of DMAS (Data Management and Archiving System) that have been implemented over the past four years as well as the planned features to be delivered soon. DMAS consists of two main components: the Data Acquisition Framework (DAF) takes care of the interaction with instruments in terms of control, monitoring as well as data acquisition and storage. The framework also contains operation control tools. The user interaction features include data search and retrieval, data distribution. Current developments in the Web 2.0 area will provide a complete research environment where users will have the ability to work and interact on-line with colleagues, process and visualize data, establish observation schedules and pre-program autonomous, event detection and reaction. We call this environment ""Oceans 2.0"". The architecture of DMAS is focused on limiting the amount of data loss and maximizing up time. It is a service oriented architecture, making use of some of the more advanced tools available. The code is written in Java and makes use of an enterprise service bus and of the publish-and-subscribe paradigm. The paper also describes the management approaches that were adopted to build DMAS. Good code quality, the continuous support of- VENUS and the need for flexibility in dealing with rapidly changing requirements were the drivers behind the adoption of in-house development, the Agile development methodology and the creation of three teams. The development team deals with requirement collection, analysis, design, coding and unit testing; the QA/QC team performs software tests and regression in a production-like environment. The systems/operations team focuses on the hardware and system software preparation and support, receives and installs new code releases after QA approval and monitors systems operations. We are vying to perform new code releases twice a month. The method has worked well during the four years of the system development phase and has allowed for a constant support of VENUS. DMAS will be completed well under budget.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5278187,no
Automated Refactoring Suggestions Using the Results of Code Analysis Tools,2009,"Static analysis tools are used for the detection of errors and other problems on . The detected problems related to the internal structure of a software can be removed by source code transformations called refactorings. To automate such source code transformations, refactoring tools are available. In modern integrated development environments, there is a gap between the static analysis tools and the refactoring tools. This paper presents an automated approach for the improvement of the internal quality of software by using the results of code analysis tools to call a refactoring tool to remove detected problems. The approach is generic, thus allowing the combination of arbitrary tools. As a proof of concept, this approach is implemented as a plug-in for the integrated development environment Eclipse.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5279916,no
Quality of Code Can Be Planned and Automatically Controlled,2009,"Quality of code is an important and critical health indicator of any software development project. However, due to the complexity and ambiguousness of calculating this indicator it is rarely used in commercial contracts. As programmers are much more motivated with respect to the delivery of functionality than quality of code beneath it,they often produce low quality code, which leads to post-delivery and maintenance problems. The proposed mechanism eliminates this lack of attention to quality of code. The results achieved after the implementation of the mechanism are more motivated programmers, higher project sponsor confidence and a predicted quality of code.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5279944,no
Static Estimation of Test Coverage,2009,"Test coverage is an important indicator for unit test quality. Tools such as Clover compute coverage by first instrumenting the code with logging functionality, and then logging which parts are executed during unit test runs. Since computation of test coverage is a dynamic analysis, it presupposes a working installation of the software. In the context of software quality assessment by an independent third party, a working installation is often not available. The evaluator may not have access to the required libraries or hardware platform. The installation procedure may not be automated or documented. In this paper, we propose a technique for estimating test coverage at method level through static analysis only. The technique uses slicing of static call graphs to estimate the dynamic test coverage. We explain the technique and its implementation. We validate the results of the static estimation by statistical comparison to values obtained through dynamic analysis using Clover. We found high correlation between static coverage estimation and real coverage at system level but closer analysis on package and class level reveals opportunities for further improvement.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5279992,no
ZX: A Novel Service Selection Solution Based on Selector-Side QoS Measuring,2009,"This paper proposes a service selection solution named ZX, which is based on the selector-side QoS measuring results. In ZX, we identify the QoS attributes which could be measured in selector's side, and provide possible solutions to gather the QoS information, and we introduce a new service selecting method, which concerns normal-the-better QoS attributes, by implementing the nearest neighbor algorithm.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5280026,no
Optimized Multipinhole Design for Mouse Imaging,2009,"The aim of this study was to enhance high-sensitivity imaging of a limited field of view in mice using multipinhole collimators on a dual head clinical gamma camera. A fast analytical method was used to predict the contrast-to-noise ratio (CNR) in many points of a homogeneous cylinder for a large number of pinhole collimator designs with modest overlap. The design providing the best overall CNR, a configuration with 7 pinholes, was selected. Next, the pinhole pattern was made slightly irregular to reduce multiplexing artifacts. Two identical, but mirrored 7-pinhole plates were manufactured. In addition, the calibration procedure was refined to cope with small deviations of the camera from circular motion. First, the new plates were tested by reconstructing a simulated homogeneous cylinder measurement. Second, a Jaszczak phantom filled with 37 MBq <sup>99m</sup>Tc was imaged on a dual head gamma camera, equipped with the new pinhole collimators. The image quality before and after refined calibration was compared for both heads, reconstructed separately and together. Next, 20 short scans of the same phantom were performed with single and multipinhole collimation to investigate the noise improvement of the new design. Finally, two normal mice were scanned using the new multipinhole designs to illustrate the reachable image quality of abdomen and thyroid imaging. The simulation study indicated that the irregular patterns suppress most multiplexing artifacts. Using body support information strongly reduces the remaining multiplexing artifacts. Refined calibration improved the spatial resolution. Depending on the location in the phantom, the CNR increased with a factor of 1 to 2.5 using the new instead of a single pinhole design. The first proof of principle scans and reconstructions were successful, allowing the release of the new plates and software for preclinical studies in mice.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5280507,no
Optimal Radial Basis Function Neural Network power transformer differential protection,2009,"This paper presents a new algorithm for protection of power transformer by using optimal radial basis function neural network (ORBFNN). ORBFNN based technique is applied by amalgamating the conventional differential protection scheme of power transformer and internal faults are precisely discriminated from inrush condition. The proposed method neither depend on any threshold nor the presence of harmonic contain in differential current. The RBFNN is designed by using particle swarm optimization (PSO) technique. The proposed RBFNN model has faster learning and detecting capability than the conventional neural networks. A comparison in the performance of the proposed ORBFNN and more commonly reported feed forward back propagation neural network (FFBPNN), in literature, is made. The simulations of different faults, over-excitation, and switching conditions on three different power transformers are performed by using PSCAD/EMTDC software and presented algorithm is evaluated by using MATLAB. The test results show that the new algorithm is quick and accurate.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5282183,no
Application of Discrete Wavelet Transform for differential protection of power transformers,2009,"This paper presents a novel formulation for differential protection of three-phase transformers. The discrete wavelet transform (DWT) is employed to extract transitory features of transformer three-phase differential currents to detect internal faulty conditions. The performance of the proposed algorithm is evaluated through simulation of faulty and non-faulty test cases on a power transformer using ATP/EMTP software. The optimal mother wavelet selection includes performance analysis of different mother wavelets and resolution number of levels. In order to test the formulations performance, the proposed method was implemented on MatLabreg environment. Simulated comparative test results with a percentage differential protection with harmonic restraint formulation shows that the proposed technique improves the discrimination performance. Simulated test cases of magnetizing inrush and close by external faults are also presented in order to test the performance of the proposed method in extreme conditions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5282195,no
Security Protocol Testing Using Attack Trees,2009,In this paper we present an attack injection approach for security protocol testing aiming at vulnerability detection. We use attack tree model to describe known attacks and derive injection test scenarios to test the security properties of the protocol under evaluation. The test scenarios are converted to a specific fault injector script after performing some transformations. The attacker is emulated using a fault injector. This model based approach facilitates there usability and maintainability of the generated injection attacks as well as the generation of fault injectors scripts. The approach is applied to an existing mobile security protocol. We performed experiments with truncation and DoS attacks; results show good precision and efficiency in the injection method.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5283774,no
A Resource Management System for Fault Tolerance in Grid Computing,2009,"In grid computing, resource management and fault tolerance services are important issues. The availability of the selected resources for job execution is a primary factor that determines the computing performance. The failure occurrence of resources in the grid computing is higher than in a tradition parallel computing. Since the failure of resources affects job execution fatally, fault tolerance service is essential in computational grids. And grid services are often expected to meet some minimum levels of quality of service (QoS) for desirable operation. However Globus toolkit does not provide fault tolerance service that supports fault detection service and management service and satisfies QoS requirement. Thus this paper proposes fault tolerance service to satisfy QoS requirement in computational grids. In order to provide fault tolerance service and satisfy QoS requirements, we expand the definition of failure, such as process failure, processor failure, and network failure. And we propose resource scheduling service, fault detection service and fault management service and show implement and experiment results.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5283868,no
Quantitative Modeling of Communication Cost for Global Service Delivery,2009,"IT service providers are increasingly utilizing globally distributed resources to drive down costs, reduce risk through diversification and gain access to a larger talent pool. However, fostering effective collaboration among geographically distributed resources is a difficult challenge. In this paper, we present our initial attempt to quantify the increased overhead in leveraging distributed resources as one of the project costs. We associate this overhead cost measurement with metrics that measure communication quality, such as reduction in productivity and communication delay. These metrics can in turn be computed as functions of underlying project parameters. To achieve this goal, we first build a project communication model (PCM) to categorize different types of collaborative communication. We then represent communication efficiency and changes in resource availability in terms of information theoretic concepts such as reduced channel capacity, information encoding efficiency and channel availability. This analysis is used to help determine the cost associated with team formation and task distribution during the project planning phase.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5283925,no
Monitoring of Complex Applications Execution in Distributed Dependable Systems,2009,"The execution of applications in dependable system requires a high level of instrumentation for automatic control. We present in this paper a monitoring solution for complex application execution. The monitoring solution is dynamic, offering real-time information about systems and applications. The complex applications are described using workflows. We show that the management process for application execution is improved using monitoring information. The environment is represented by distributed dependable systems that offer a flexible support for complex application execution. Our experimental results highlight the performance of the proposed monitoring tool, the MonALISA framework.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5284346,no
Taguchi Smaller-the-Best Software Quality Metrics,2009,"Taguchi-based software metrics (numerical software measurements) define software quality in terms of ""loss imparted to society"" after a software product is delivered to the end user. Previous work has focused on nominal-the-best and larger-the-best loss functions for estimating the loss to society. This paper focuses on the smaller-the-best loss function for estimating the loss to society. The smaller-the-best loss function is for a property whose value should be as small as possible. It is shown that the smaller-the-best loss function can be obtained from the nominal-the-best loss function by substituting zero for t, the target value in the nominal-the-best loss function. This derivation is illustrated and a case study utilizing the derivation is presented. Taguchi-based metrics are in line with six-sigma statistical quality control techniques that have been used with software products for several years. Experiences with Taguchi-based metrics have shown good correlation with existing popularly known software quality metrics.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5286604,no
Machine Vision Based Image Analysis for the Estimation of Pear External Quality,2009,"The research on real time fruit quality detection with machine vision is an attractive and prospective subject for improving marketing competition and post harvesting value-added processing technology of fruit products. However, the farm products with different varieties and different quality have caused tremendous losses in economy due to lacking the post-harvest inspecting standards and measures in China. In view of the existing situations of fruit quality detection and the broad application prospect of machine vision in quality evaluation of agricultural products in China, the methods to detect the external quality of pear by machine vision were researched in this work. It aims at solving the problems, such as fast processing the large amount of image information, processing capability and increasing precision of detection, etc. The research is supported by the software of Lab Windows/CVI of NI Company. The system can be used for fruit grading by the external qualities of size, shape, color and surface defects. Some fundamental theories of machine vision based on virtual instrumentation were investigated and developed in this work. It is testified that machine vision is an alternative to unreliable manual sorting of fruits.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5287572,no
Reliability Modeling and Analysis of Safety-Critical Manufacture System,2009,"There are working, fail-safe and fail-dangerous states in safety-critical manufacture systems. This paper presents three typical safety-critical manufacture system architecture models: series, parallel and series-parallel system, whose components lifetime distributions are general forms. Also the reliability related indices, such as the probabilities that the system in these states and the mean times for the system fail-safe and fail-dangerous, are derived respectively. And the relationships among the obtained indices of the three system architecture models are analyzed. Finally some numerical examples are employed to elaborate the results obtained in this paper. The derived indices formulas are new results and without component lifetime distribution assumptions, which have significant meaning for evaluating the manufacture system reliability and improving the manufacture system safety design.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5287742,no
Distributed control plane architecture of next generation IP routers,2009,"In this paper, we present our research aiming at building a petabit router model for next generation networks. Considering the increasing traffic requirements on the Internet, current gigabit and terabit speed routers will soon not be able to meet user demand. One of the promising trends of router evolution is to build next generation routers with enhanced memory capacity and computing resources, distributed across a very high speed switching fabric. The main limitation of the current routing and signaling software modules, traditionally designed in a centralized manner, is that they do not scale in order to fully exploit such an advanced distributed hardware architecture. This paper discusses an implementation for an control plane for next generation routers integrating several protocol dedicated distributed architectures, aiming at increasing the scalability and resiliency. The proposed architecture distributes the processing functions on router cards, i.e., on both control and line cards. Therefore, it reduces the bottlenecks and improves both the overall performance and the resiliency in the presence of faults. Scalability is estimated with respect to the CPU utilization and memory requirements.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5289143,no
An efficient hardware-software approach to network fault tolerance with InfiniBand,2009,"In the last decade or so, clusters have observed a tremendous rise in popularity due to excellent price to performance ratio. A variety of Interconnects have been proposed during this period, with InfiniBand leading the way due to its high performance and open standard. Increasing size of the InfiniBand clusters has reduced the mean time between failures of various components of these clusters tremendously. In this paper, we specifically focus on the network component failure and propose a hybrid hardware-software approach to handling network faults. The hybrid approach leverages the user-transparent network fault detection and recovery using Automatic Path Migration (APM), and the software approach is used in the wake of APM failure. Using Global Arrays as the programming model, we implement this approach with Aggregate Remote Memory Copy Interface (ARMCI), the runtime system of Global Arrays. We evaluate our approach using various benchmarks (siosi7, pentane, h2o7 and siosi3) with NWChem, a very popular ab initio quantum chemistry application. Using the proposed approach, the applications run to completion without restart on emulated network faults and acceptable overhead for benchmarks executing for a longer period of time.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5289168,no
Reliability-aware scalability models for high performance computing,2009,"Scalability models are powerful analytical tools for evaluating and predicting the performance of parallel applications. Unfortunately, existing scalability models do not quantify failure impact and therefore cannot accurately account for application performance in the presence of failures. In this study, we extend two well-known models, namely Amdahl's law and Gustafson's law, by considering the impact of failures and the effect of fault tolerance techniques on applications. The derived reliability-aware models can be used to predict application scalability in failure-present environments and evaluate fault tolerance techniques. Trace-based simulations via real failure logs demonstrate that the newly developed models provide a better understanding of application performance and scalability in the presence of failures.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5289177,no
MITHRA: Multiple data independent tasks on a heterogeneous resource architecture,2009,"With the advent of high-performance COTS clusters, there is a need for a simple, scalable and fault-tolerant parallel programming and execution paradigm. In this paper, we show that the popular MapReduce programming model can be utilized to solve many interesting scientific simulation problems with much higher performance than regular cluster computers by leveraging GPGPU accelerators in cluster nodes. We use the Massive Unordered Distributed (MUD) formalism and establish a one-to-one correspondence between it and general Monte Carlo simulation methods. Our architecture, MITHRA, leverages NVIDIA CUDA technology along with Apache Hadoop to produce scalable performance gains using the MapReduce programming model. The evaluation of our proposed architecture using the Black Scholes option pricing model shows that a MITHRA cluster of 4 GPUs can outperform a regular cluster of 62 nodes, achieving a speedup of about 254 times in our testbed, while providing scalable near linear performance with additional nodes.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5289201,no
Benchmarking Quality-Dependent and Cost-Sensitive Score-Level Multimodal Biometric Fusion Algorithms,2009,"Automatically verifying the identity of a person by means of biometrics (e.g., face and fingerprint) is an important application in our day-to-day activities such as accessing banking services and security control in airports. To increase the system reliability, several biometric devices are often used. Such a combined system is known as a multimodal biometric system. This paper reports a benchmarking study carried out within the framework of the BioSecure DS2 (Access Control) evaluation campaign organized by the University of Surrey, involving face, fingerprint, and iris biometrics for person authentication, targeting the application of physical access control in a medium-size establishment with some 500 persons. While multimodal biometrics is a well-investigated subject in the literature, there exists no benchmark for a fusion algorithm comparison. Working towards this goal, we designed two sets of experiments: quality-dependent and cost-sensitive evaluation. The quality-dependent evaluation aims at assessing how well fusion algorithms can perform under changing quality of raw biometric images principally due to change of devices. The cost-sensitive evaluation, on the other hand, investigates how well a fusion algorithm can perform given restricted computation and in the presence of software and hardware failures, resulting in errors such as failure-to-acquire and failure-to-match. Since multiple capturing devices are available, a fusion algorithm should be able to handle this nonideal but nevertheless realistic scenario. In both evaluations, each fusion algorithm is provided with scores from each biometric comparison subsystem as well as the quality measures of both the template and the query data. The response to the call of the evaluation campaign proved very encouraging, with the submission of 22 fusion systems. To the best of our knowledge, this campaign is the first attempt to benchmark quality-based multimodal fusion algorithms. In the presence of changing - - image quality which may be due to a change of acquisition devices and/or device capturing configurations, we observe that the top performing fusion algorithms are those that exploit automatically derived quality measurements. Our evaluation also suggests that while using all the available biometric sensors can definitely increase the fusion performance, this comes at the expense of increased cost in terms of acquisition time, computation time, the physical cost of hardware, and its maintenance cost. As demonstrated in our experiments, a promising solution which minimizes the composite cost is sequential fusion, where a fusion algorithm sequentially uses match scores until a desired confidence is reached, or until all the match scores are exhausted, before outputting the final combined score.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290123,no
A Concept of System Usability Assessment: System Attentiveness as the Measure of Quality,2009,"The goal of this paper is to present novel metrics for system usability assessment and quality assessment (SQA). Proposed metrics should provide means of capturing overall system interference with regular daily routines and habits of system users, referred to as ""attentive interference"". We argue that assuring the system is attentive proves essential when trying to mitigate risks related to system rejection by the intended users.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290979,no
Performance of FMIPv6-based cross-layer handover for supporting mobile VoIP in WiMAX networks,2009,"This paper presents validation and evaluation of mobile VoIP support over WiMAX networks using the FMIPv6-based cross layer handover scheme. A software module has been implemented for the FMIPv6-based handoff scheme. The handoff delay components are formulated. To evaluate its support of mobile VoIP, we carefully assess the handoff delay, the total delay, and the R factor which is a representation of voice user satisfactory degree. Simulation results show that the cross-layering handoff scheme, as compared with the non-cross-layer scheme, successfully decreases layer-3 handoff delay by almost 50%, and is therefore thriving to support mobile VoIP services. We believe this is the first performance evaluation work for the FMIPv6-based cross-layer scheme, and hence an important work for the WiMAX research community.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5291368,no
Exploiting scientific workflows for large-scale gene expression data analysis,2009,"Microarrays are state technologies of the art for the measurement of expression of thousands of genes in a single experiment. The treatment of these data are typically performed with a wide range of tools, but the understanding of complex biological system by means of gene expression usually requires integrating different types of data from multiple sources and different services and tools. Many efforts are being developed on the new area of scientific workflows in order to create a technology that links both data and tools to create workflows that can easily be used by researchers. Currently technologies in this area aren't mature yet, making arduous the use of these technologies by the researcher. In this paper we present an architecture that helps the researchers to make large-scale gene expression data analysis with cutting edge technologies. The main underlying idea is to automate and rearrange the activities involved in gene expression data analysis, in order to freeing the user of superfluous technological details and tedious and error-prone tasks.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5291850,no
The effect of granularity level on software defect prediction,2009,"Application of defect predictors in software development helps the managers to allocate their resources such as time and effort more efficiently and cost effectively to test certain sections of the code. In this research, we have used naive Bayes classifier (NBC) to construct our defect prediction framework. Our proposed framework uses the hierarchical structure information about the source code of the software product, to perform defect prediction at a functional method level and source file level. We have applied our model on SoftLAB and Eclipse datasets. We have measured the performance of our proposed model and applied cost benefit analysis. Our results reveal that source file level defect prediction improves the verification effort, while decreasing the defect prediction performance in all datasets.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5291866,no
Negotiation based advance reservation priority grid scheduler with a penal clause for execution failures,2009,"The utility computing in a grid demands much more adaptability and dynamism when certain levels of commitments are to be complied with. Users with distinct priorities are categorized on the basis of the types of organizations or applications they belong to and can submit multiple jobs with varying specific needs. The interests of consumers and the resource service providers must be equally watched upon, with focus on commercials too. At the same time, the quality of the service must be ascertained to a committed level, thus enforcing an agreement. The authors propose an algorithm for a negotiation based scheduler that dynamically analyses and assesses the incoming jobs in terms of priorities and requirements, reserves them to resources after negotiations and match-making between the resource providers and the users. The jobs thus reserved are allocated resources for future. The performance evaluation of the scheduler for various parameters was done through simulations. The results were found to be optimal after incorporating advance reservation with dynamic priority control over job selection, involving the impact of situations confronting resource failures introducing economic policies and penalties.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5291869,no
An outlier detection algorithm based on object-oriented metrics thresholds,2009,"Detection of outliers in software measurement datasets is a critical issue that affects the performance of software fault prediction models built based on these datasets. Two necessary components of fault prediction models, software metrics and fault data, are collected from the software projects developed with object-oriented programming paradigm. We proposed an outlier detection algorithm based on these kinds of metrics thresholds. We used Random Forests machine learning classifier on two software measurement datasets collected from jEdit open-source text editor project and experiments revealed that our outlier detection approach improves the performance of fault predictors based on Random Forests classifier.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5291882,no
Hand tracking and trajectory analysis for physical rehabilitation,2009,"In this work we present a framework for physical rehabilitation, which is based on hand tracking. One particular requirement in physical rehabilitation is the capability of the patient to correctly reproduce a specific path, following an example provided by the medical staff. Currently, these assignments are typically performed manually, and a nurse or doctor, who supervises the correctness of the movement, constantly assists the patient throughout the whole rehabilitation process. With the proposed system, our aim is to provide medical institutions and patients with a low-cost and portable instrument to automatically assess the rehabilitation improvements. To evaluate the performance of the exercise, and to determine the distance between the trial and the reference path, we adopted the dynamic time warping (DTW) and the longest common sub-sequence (LCSS) as discriminating metrics. Trajectories and numerical values are then stored to track the history of the patient and appraise the improvements of the rehabilitation process over time. Thanks to the tests conducted with real patients, it has been possible to evaluate the quality of the proposed tool, in terms of both graphical interface and functionalities.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5293284,no
Preserving Cohesive Structures for Tool-Based Modularity Reengineering,2009,"The quality of software systems heavily depends on their structure, which affects maintainability and readability. However, the ability of humans to cope with the complexity of large software systems is limited. To support reengineering large software systems, software clustering techniques that maximize module cohesion and minimize inter-modular coupling have been developed. The main drawback of these approaches is that they might pull apart elements that were thoughtfully placed together. This paper describes how strongly connected component analysis, dominance analysis, and intra-modular similarity clustering can be applied to identify and to preserve cohesive structures in order to improve the result of reengineering. The use of the proposed method allows a significant reduction of the number of component movements. As a result, the probability of false component movements is reduced. The proposed approach is illustrated by statistics and examples from 18 open source Java projects.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5298407,no
Filtering System Metrics for Minimal Correlation-Based Self-Monitoring,2009,"Self-adaptive and self-organizing systems must be self-monitoring. Recent research has shown that self-monitoring can be enabled by using correlations between monitoring variables (metrics). However, computer systems often make a very large number of metrics available for collection. Collecting them all not only reduces system performance, but also creates other overheads related to communication, storage, and processing. In order to control the overhead, it is necessary to limit collection to a subset of the available metrics. Manual selection of metrics requires a good understanding of system internals, which can be difficult given the size and complexity of modern computer systems. In this paper, assuming no knowledge of metric semantics or importance and no advance availability of fault data, we investigate automated methods for selecting a subset of available metrics in the context of correlation-based monitoring. Our goal is to collect fewer metrics while maintaining the ability to detect errors. We propose several metric selection methods that require no information beside correlations. We compare these methods on the basis of fault coverage. We show that our minimum spanning tree-based selection performs best, detecting on average 66% of faults detectable by full monitoring (i.e., using all considered metrics) with only 30% of the metrics.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5298441,no
UML Specification and Correction of Object-Oriented Anti-patterns,2009,"Nowadays, the detection and correction of software defects has become a very hard task for software engineers. Most importantly, the lack of standard specifications of these software defects along with the lack of tools for their detection, correction and verification forces developers to perform manual modifications; resulting not only in mistakes, but also in costs of time and resources. The work presented here is a study of the specification and correction of a particular type of software defect: Object-Oriented anti-patterns. More specifically, we define a UML based specification of anti-patterns and establish design transformations for their correction. Through this work, we expect to open up the possibility to automate the detection and correction of these kinds of software defects.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5298450,no
Scenario-Based Genetic Synthesis of Software Architecture,2009,"Software architecture design can be regarded as finding an optimal combination of known general solutions and architectural knowledge with respect to given requirements. Based on previous work on synthesizing software architecture using genetic algorithms, we propose a refined fitness function for assessing software architecture in genetic synthesis, taking into account the specific anticipated needs of the software system under design. Inspired by real life architecture evaluation methods, the refined fitness function employs scenarios, specific situations possibly occurring during the lifetime of the system and requiring certain modifiability properties of the system. Empirical studies based on two example systems suggest that using this kind of fitness function significantly improves the quality of the resulting architecture.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5298846,no
MUD receiver capacity optimization for wireless ad hoc networks,2009,"In general the performance of ad hoc networks is limited by mobility, half-duplex operation, and possible collisions. One way to overcome these bottlenecks is to apply a multiuser detection (MUD) reception that can significantly increase the network throughput and improve quality of service. Recent technological advances allow implementation of a multi-user detection receiver in one software-defined radio chip and thus making it feasible to consider this technology for ad hoc networks. Nevertheless, the MUD receiver power consumption and complexity grow exponentially with its capacity defined as the maximum number of CDMA signals, originated at different nodes, which can be received simultaneously. Therefore the receiver capacity should be optimized to provide reasonable trade-off between the network performance and the MUD receiver cost and power consumption. We address this issue by presenting an approximate analysis of the network throughput as a function of MUD receiver capacity and other network parameters such as node density and offered traffic. The numerical analysis illustrates the gains in network throughput and performance with increasing receiver capacity. Based on this analysis we propose a framework for MUD receiver capacity optimization based on performance and cost utility functions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5300256,no
PRR-PRR Dynamic Relocation,2009,"Partial bitstream relocation (PBR) on FPGAs has been gaining attention in recent years as a potentially promising technique to scale parallelism of accelerator architectures at run time, enhance fault tolerance, etc. PBR techniques to date have focused on reading inactive bitstreams stored in memory, on-chip or off-chip, whose contents are generated for a specific partial reconfiguration region (PRR) and modified on demand for configuration into a PRR at a different location. As an alternative, we propose a PRR-PRR relocation technique to generate source and destination addresses, read the bitstream from an active PRR (source) in a non-intrusive manner, and write it to destination PRR. We describe two options of realizing this on Xilinx Virtex 4 FPGAs: (a) hardware-based accelerated relocation circuit (ARC) and (b) a software solution executed on Microblaze. A comparative performance analysis to highlight the speed-up obtained using ARC is presented. For real test cases, performance of our implementations are compared to estimated performances of two state of the art methods.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5300796,no
Study on Coordinated Distributed Scheduling in WiMAX Mesh Network,2009,"The IEEE802.16 (WiMAX) standard specifies the air interface of broad width access systems. Mesh topology is supported as an optional structure. In the mesh network, coordinated distributed scheduling is an important scheduling mode. Such mode permits traffic to be routed through other subscriber stations (SS) and occur directly between SSs. Xmt holdoff exponent is an import parameter in the mode. How to set it makes a great impact on the whole scheduling performance. In this paper, a dynamic setting scheme based on classification is proposed and compares in performance with the static scheme with fixed values. Simulation results show the proposed scheme can increase throughout and can provide different quality of service (QoS) levels for multimedia services.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5301219,no
Research of Software Defect Prediction Model Based on Gray Theory,2009,"The software testing process is an important stage in the software life cycle to improve software quality. An amount of software data and information can be obtained. Based on analyzing the source and the type of software, this paper describes several use of the defects data and explains how to estimate the density of the software by using the software defects data collected in the practical work, and to use GM model to predict and assess the software reliability.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5301677,no
Sub-Path Congestion Control in CMT,2009,"Using the application of block data transfer, we investigate the performance of concurrent multipath transfer using SCTP multihoming (CMT) with congestion control policy under the scenario of a sender is constrained by the receive buffer (rbuf). We find that existing policy has some defects in aspect of bandwidth-aware source scheduling. Based on this, Based on this We proposed a sub-path congestion-control policy for SCTP (SPCC-SCTP) with bandwidth-aware source scheduling by dividing an association into sub paths based on shared bottleneck detection to overcome existing flaws of standard SCTP in supporting the multi-homing feature in the case of concurrent multipath transfer (CMT). The performance of the SPCC-SCTP is assessed through ns-2 experiments in a simplified Diff-Serv network. Simulation results demonstrated the effectiveness of our proposed mechanism and invite further research.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5302613,no
Changes and bugs â€?Mining and predicting development activities,2009,"Software development results in a huge amount of data: changes to source code are recorded in version archives, bugs are reported to issue tracking systems, and communications are archived in e-mails and newsgroups. We present techniques for mining version archives and bug databases to understand and support software development. First, we introduce the concept of co-addition of method calls, which we use to identify patterns that describe how methods should be called. We use dynamic analysis to validate these patterns and identify violations. The co-addition of method calls can also detect cross-cutting changes, which are an indicator for concerns that could have been realized as aspects in aspect-oriented programming. Second, we present techniques to build models that can successfully predict the most defect-prone parts of large-scale industrial software, in our experiments Windows Server 2003. This helps managers to allocate resources for quality assurance to those parts of a system that are expected to have most defects. The proposed measures on dependency graphs outperformed traditional complexity metrics. In addition, we found empirical evidence for a domino effect, i.e., depending on defect-prone binaries increases the chances of having defects.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306296,no
Introducing a test suite similarity metric for event sequence-based test cases,2009,"Most of today's event driven software (EDS) systems are tested using test cases that are carefully constructed as sequences of events; they test the execution of an event in the context of its preceding events. Because sizes of these test suites can be extremely large, researchers have developed techniques, such as reduction and minimization, to obtain test suites that are ldquosimilarrdquo to the original test suite, but smaller. Existing similarity metrics mostly use code coverage; they do not consider the contextual relationships between events. Consequently, reduction based on such metrics may eliminate desirable test cases. In this paper, we present a new parameterized metric, CONTeSSi(n) which uses the context of n preceding events in test cases to develop a new context-aware notion of test suite similarity for EDS. This metric is defined and evaluated by comparing four test suites for each of four open source applications. Our results show that CONT eSSi(n) is a better indicator of the similarity of EDS test suites than existing metrics.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306305,no
Analysis of pervasive multiple-component defects in a large software system,2009,"Certain software defects require corrective changes repeatedly in a few components of the system. One type of such defects spans multiple components of the system, and we call such defects pervasive multiple-component defects (PMCDs). In this paper, we describe an empirical study of six releases of a large legacy software system (of approx. size 20 million physical lines of code) to analyze PMCDs with respect to: (1) the complexity of fixing such defects and (2) the persistence of defect-prone components across phases and releases. The overall hypothesis in this study is that PMCDs inflict a greater negative impact than do other defects on defect-correction efficacy. Our findings show that the average number of changes required for fixing PMCDs is 20-30 times as much as the average for all defects. Also, over 80% of PMCD-contained defect-prone components still remain defect-prone in successive phases or releases. These findings support the overall hypothesis strongly. We compare our results, where possible, to those of other researchers and discuss the implications on maintenance processes and tools.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306307,no
Modeling class cohesion as mixtures of latent topics,2009,"The paper proposes a new measure for the cohesion of classes in object-oriented software systems. It is based on the analysis of latent topics embedded in comments and identifiers in source code. The measure, named as maximal weighted entropy, utilizes the latent Dirichlet allocation technique and information entropy measures to quantitatively evaluate the cohesion of classes in software. This paper presents the principles and the technology that stand behind the proposed measure. Two case studies on a large open source software system are presented. They compare the new measure with an extensive set of existing metrics and use them to construct models that predict software faults. The case studies indicate that the novel measure captures different aspects of class cohesion compared to the existing cohesion measures and improves fault prediction for most metrics, which are combined with maximal weighted entropy.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306318,no
Assessing the impact of framework changes using component ranking,2009,"Most of today's software applications are built on top of libraries or frameworks. Just as applications evolve, libraries and frameworks also evolve. Upgrading is straightforward when the framework changes preserve the API and behavior of the offered services. However, in most cases, major changes are introduced with the new framework release, which can have a significant impact on the application. Hence, a common question a framework user might ask is, ldquoIs it worth upgrading to the new framework version?rdquo In this paper, we study the evolution of an application and its underlying framework to understand the information we can get through a multi-version use relation analysis. We use component rank changes to measure this impact. Component rank measurement is a way of quantifying the importance of a component by its usage. As framework components are used by applications, the rankings of the components are changed. We use component ranking to identify the core components in each framework version. We also confirm that upgrading to the new framework version has an impact to a component rank of the entire system and the framework, and this impact not only involves components which use the framework directly, but also other indirectly-related components. Finally, we also confirm that there is a difference in the growth of use relations between application and framework.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306323,no
On predicting the time taken to correct bug reports in open source projects,2009,"Existing studies on the maintenance of open source projects focus primarily on the analyses of the overall maintenance of the projects and less on specific categories like the corrective maintenance. This paper presents results from an empirical study of bug reports from an open source project, identifies user participation in the corrective maintenance process through bug reports, and constructs a model to predict the corrective maintenance effort for the project in terms of the time taken to correct faults. Our study focuses on 72482 bug reports from over nine releases of Ubuntu, a popular Linux distribution. We present three main results: (1) 95% of the bug reports are corrected by people participating in groups of size ranging from 1 to 8 people, (2) there is a strong linear relationship (about 92%) between the number of people participating in a bug report and the time taken to correct it, (3) a linear model can be used to predict the time taken to correct bug reports.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306337,no
The squale model â€?A practice-based industrial quality model,2009,"ISO 9126 promotes a three-level model of quality (factors, criteria, and metrics) which allows one to assess quality at the top level of factors and criteria. However, it is difficult to use this model as a tool to increase software quality. In the Squale model, we add practices as an intermediate level between metrics and criteria. Practices abstract away from raw information (metrics, tool reports, audits) and provide technical guidelines to be respected. Moreover, practice marks are adjusted using formulae to suit company development habits or exigences: for example bad marks are stressed to point to places which need more attention. The Squale model has been developed and validated over the last couple of years in an industrial setting with Air France-KLM and PSA Peugeot-Citroen.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306381,no
Data transformation and attribute subset selection: Do they help make differences in software failure prediction?,2009,"Data transformation and attribute subset selection have been adopted in improving software defect/failure prediction methods. However, little consensus was achieved on their effectiveness. This paper reports a comparative study on these two kinds of techniques combined with four classifier and datasets from two projects. The results indicate that data transformation displays un obvious influence on improving the performance, while attribute subset selection methods show distinguishably inconsistent output. Besides, consistency across releases and discrepancy between the open-source and in-house maintenance projects in the evaluation of these methods are discussed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306382,no
Fundamental performance assessment of 2-D myocardial elastography in a phased-array configuration,2009,"Two-dimensional myocardial elastography, an RF-based, speckle-tracking technique, uses 1-D cross-correlation and recorrelation methods in a 2-D search, and can estimate and image the 2-D transmural motion and deformation of the myocardium so as to characterize the cardiac function. Based on a 3-D finite-element (FE) canine left-ventricular model, a theoretical framework was previously developed by our group to evaluate the estimation quality of 2-D myocardial elastography using a linear array. In this paper, an ultrasound simulation program, Field II, was used to generate the RF signals of a model of the heart in a phased-array configuration and under 3-D motion conditions; thus simulating a standard echocardiography exam. The estimation method of 2-D myocardial elastography was adapted for use with such a configuration. All elastographic displacements and strains were found to be in good agreement with the FE solutions, as indicated by the mean absolute error (MAE) between the two. The classified first and second principal strains approximated the radial and circumferential strains, respectively, in the phased-array configuration. The results at different sonographic signal-to-noise ratios (SNR<sub>s</sub>) showed that the MAEs of the axial, lateral, radial, and circumferential strains remained relatively constant when the SNR<sub>s</sub> was equal to or higher than 20 dB. The MAEs of the strain estimation were not significantly affected when the acoustic attenuation was included in the simulations. A significantly reduced number of scatterers could be used to speed up the simulation, without sacrificing the estimation quality.The proposed framework can further be used to assess the estimation quality, explore the theoretical limitation and investigate the effects of various parameters in 2-D myocardial elastography under more realistic conditions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306777,no
Image feature extraction for mobile processors,2009,"High-quality cameras are a standard feature of mobile platforms, but the computational capabilities of mobile processors limit the applications capable of exploiting them. Emerging mobile application domains, for example mobile augmented reality (MAR), rely heavily on techniques from computer vision, requiring sophisticated analyses of images followed by higher-level processing. An important class of image analyses is the detection of sparse localized interest points. The scale invariant feature transform (SIFT), the most popular such analysis, is computationally representative of many other feature extractors. Using a novel code-generation framework, we demonstrate that a small set of optimizations produce high-performance SIFT implementations for three very different architectures: a laptop CPU (Core 2 Duo), a low-power CPU (Intel Atom), and a low-power GPU (GMA X3100). We improve the runtime of SIFT by more than 5X on our low-power architectures, enabling a low-power mobile device to extract SIFT features up to 63% as fast as the laptop CPU.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306789,no
Software component quality prediction in the legacy product development environment using Weibull and other mathematical distributions,2009,"Software component quality has a major influence in software development project performances such as lead-time, time to market and cost. It also affects the other projects within the organization, the people assigned into the projects and the organization in general. Software development organization must have indication and prediction about software component quality and project performances in general. One of the software component quality prediction techniques is statistical and probabilistic technique. The paper deals with software quality prediction techniques, different applied models in different development projects and faults existing indicators. Three case studies are presented and evaluated for prediction of software quality in very large development projects within the AXE platform in the Ericsson Nikola Tesla R&D.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306825,no
Evaluating rate-estimation for a mobility and QoS-aware network architecture,2009,"In a nearby future wireless networks will run applications with special QoS requirements. FHMIP is an effective scheme to reduce Mobile IPv6 handover disruption but it does not deal with any other specific QoS requirement. Therefore new traffic management schemes are needed in order to provide QoS guarantees to real-time applications and this implies network mobility optimizations and congestion control support. Traffic management schemes should deal with QoS requirements during handover and should use some resource management strategy in order to achieve this. In this article a new resource management scheme for DiffServ QoS model is proposed, to be used by access routers as an extension to FHMIP micromobility protocol. In order to prevent QoS deterioration, access routers pre-evaluate the impact of accepting all traffic from a mobile node, previous to the handover. This pre-evaluation and post decision on whether or not to accept any, or all, of this new traffic is based on a measurement based admission control procedure. This mobility and QoS-aware network architecture, integrating a simple signaling protocol, a traffic descriptor, and exhibiting adaptive behavior has been implemented and tested using ns-2. All measurements and decisions are based on DiffServ class-of-service aggregations, thus avoiding large flow state information maintenance. Rate estimators are essential mechanisms to the efficiency of this QoS-aware overall architecture. Therefore, in order to be able to choose the rate estimator that better fits this global architecture, two rate estimators - Time Sliding Window (TSW) and Exponential Moving Average (EMA) - have been studied and evaluated by means of ns-2 simulations in QoS-aware wireless mobility scenarios.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306834,no
RSSI-based Cross Layer Link Quality Management for Layer 3 wireless mesh networks,2009,"We propose a framework of link quality management for wireless mesh networks. In this method, packet error rate for each rate of each link is estimated based on RSSI measured for received hello packets using noise parameter to select the optimum rate, such that the link metric becomes minimum. We present a method to estimate the noise parameter and show that the proposed method is promising using an actual testbed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306886,no
Characterization of the Effects of CW and Pulse CW Interference on the GPS Signal Quality,2009,"In the Global Positioning System (GPS), code division multiple access (CDMA) signals are used. Because of the spectral characteristics of the CDMA signal, each particular type of interference (signals to be rejected) has a different effect on the quality of the received GPS satellite signals. In this paper, the effects of three types of interference are studied on the carrier-to-noise ratio (C/N<sub>o</sub>) of the received GPS signal as an indicator of the quality of that signal; continuous wave (CW), pulse CW, and swept CW. For CW interference, it is analytically shown that the C/N<sub>o</sub> of the signal can be calculated using a closed formula after the correlator in the receiver. This result is supported by calculating the C/N<sub>o</sub> using the I and Q data from a software GPS receiver. For pulsed CW, a similar analysis is performed to characterize the effect of parameters such as pulse repetition period (PRP) and also duty cycle on the received signal quality. It is specifically shown that for equal interference power levels, in the cases where the PRP is far less than the pseudorandom noise code period, the signal degradation increases with increasing the duty cycle whereas it doesn't change when the two periods are equal or the PRP is far bigger than the code period.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5310308,no
Pre-determining comparative tests and utilizing signal levels to perform accurate diagnostics,2009,Standard diagnostic schemes don't do enough analysis to focus in on the actual cause of a test failure. Often measurements will be border-line on test sequences prior to an actual test failure. These border-line measurements can be used to aid in the determination of an actual fault. An actual fault and the associated test that should detect that fault can be deceiving. The specific test in question can pass but be right on the border-line. The failure might not show up in testing until a later test is performed. This later test assumes all the prior tests passed and therefore the circuitry associated with these prior tests is good. This is not necessarily the case if some of the output measurements prior to the actual failing test were right on the border-line. What can we do? Take advantage of the order in which the faults are simulated<sup>1</sup>. We should structure our TPSs such that a review of preliminary tests should be evaluated before the R/R component list is presented. The review of the preliminary test can be rather straight forward. We can look at signals that are within 8-10% of the lower or upper limit. We can then use an inter-related test scheme to evaluate the test(s) that can be associated with the actual failing test. This paper will use an example TPS and show how a test scheme and an evaluation scheme can be used to determine the PCOF. The paper will show actual measurements and how these measurements can be evaluated to determine the actual cause of a failure.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5314091,no
"A comparison of software cost, duration, and quality for waterfall vs. iterative and incremental development: A systematic review",2009,"The objective of this study is to present a body of evidence that will assist software project managers to make informed choices about software development approaches for their projects. In particular, two broadly defined competing approaches, the traditional ldquowaterfallrdquo approach and iterative and incremental development (IID), are compared with regards to development cost and duration, and resulting product quality. The method used for this comparison is a systematic literature review. The small set of studies we located did not demonstrate any identifiable cost, duration, or quality trends, although there was some evidence suggesting the superiority of IID (in particular XP). The results of this review indicate that further empirical studies, both quantitative and qualitative, on this topic need to be undertaken. In order to effectively compare study results, the research community needs to reach a consensus on a set of comparable parameters that best assess cost, duration, and quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5314228,no
An experiment to observe the impact of UML diagrams on the effectiveness of software requirements inspections,2009,"Software inspections aim to find defects early in the development process and studies have found them to be effective. However, there is almost no data available regarding the impact of UML diagram utilization in software requirements specification documents on inspection effectiveness. This paper addresses this issue by investigating whether inclusion of UML diagrams impacts the effectiveness of requirements inspection. We conducted an experiment in an academic environment with 35 subjects to empirically investigate the impact of UML diagram inclusion on requirements inspections' effectiveness and the number of reported defects. The results show that including UML diagrams in requirements specification document significantly impacts the number of reported defects, and there is no significant impact on the effectiveness of individual inspections.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5314229,no
Improving CVSS-based vulnerability prioritization and response with context information,2009,"The growing number of software security vulnerabilities is an ever-increasing challenge for organizations. As security managers in the industry have to operate within limited budgets they also have to prioritize their vulnerability responses. The Common Vulnerability Scoring System (CVSS) aids in such prioritization by providing a metric for the severity of vulnerabilities. In its most prominent application, as the severity metric in the U.S. National Vulnerability Database (NVD), CVSS scores omit information pertaining the potential exploit victims' context. Researchers and managers in the industry have long understood that the severity of vulnerabilities varies greatly among different organizational contexts. Therefore the CVSS scores provided by the NVD alone are of limited use for vulnerability prioritization in practice. Security managers could address this limitation by adding the missing context information themselves to improve the quality of their CVSS-based vulnerability prioritization. It is unclear for them, however, whether the potential improvements are worth the additional effort. We present a method that enables practitioners to estimate these improvements. Our method is of particular use to practitioners who do not have the resources to gather large amounts of empirical data, because it allows them to simulate the improvement potential using only publicly available data in the NVD and distribution models from the literature. We applied the method on a sample set of 720 vulnerability announcements from the NVD and found that adding context information significantly improved the prioritization and selection of vulnerability response process. Our findings contribute to the discourse on returns on security investment, measurement of security processes and quantitative security management.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5314230,no
The impact of limited search procedures for systematic literature reviews â€?A participant-observer case study,2009,"This study aims to compare the use of targeted manual searches with broad automated searches, and to assess the importance of grey literature and breadth of search on the outcomes of SLRs. We used a participant-observer multi-case embedded case study. Our two cases were a tertiary study of systematic literature reviews published between January 2004 and June 2007 based on a manual search of selected journals and conferences and a replication of that study based on a broad automated search. Broad searches find more papers than restricted searches, but the papers may be of poor quality. Researchers undertaking SLRs may be justified in using targeted manual searches if they intend to omit low quality papers; if publication bias is not an issue; or if they are assessing research trends in research methodologies.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5314238,no
Tool supported detection and judgment of nonconformance in process execution,2009,"In the past decades the software engineering community has proposed a large collection of software development life cycles, models, and processes. The goal of a major set of these processes is to assure that the product is finished within time and budget, and that a predefined set of functional and nonfunctional requirements (e.g. quality goals) are satisfied at delivery time. Based upon the assumption that there is a real relationship between the process applied and the characteristics of the product developed from that process, we developed a tool supported approach that uses process nonconformance detection to identify potential risks in achieving the required process characteristics. In this paper we present the approach and a feasibility study that demonstrates its use on a large-scale software development project in the aerospace domain. We demonstrate that our approach, in addition to meeting the criteria above, can be applied to a real system of reasonable size; can represent a useful and adequate set of rules of relevance in such an environment; and can detect relevant examples of process nonconformance that provide useful insight to the project manager.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5315983,no
The curse of copy&paste â€?Cloning in requirements specifications,2009,"Cloning in source code is a well known quality defect that negatively affects software maintenance. In contrast, little is known about cloning in requirements specifications. We present a study on cloning in 11 real-world requirements specifications comprising 2,500 pages. For specification clone detection, an existing code clone detection tool is adapted and its precision analyzed. The study shows that a considerable amount of cloning exists, although the large variation between specifications suggests that some authors manage to avoid cloning. Examples of frequent types of clones are given and the negative consequences of cloning, particularly the obliteration of commonalities and variations, are discussed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5315992,no
An empirical quality model for web service ontologies to support mobile devices,2009,"As Web services and the semantic Web become more important, enabling technologies such as Web service ontologies will grow larger. The ability of mobile devices, such as cell phones and PDAs, to download and reason across them will be severely limited. Given that an agent on a mobile device only needs a subset of what is described in a Web service ontology, an ontology sub-graph can be created. In this paper, we develop a empirical software engineering approach to build a prediction model to measure the quality in terms of correct query handling of ontology sub-graphs relative to the original ontology - mean average recall of the sub-graph compared to the original ontology is used as the quality standard. Our metrics allow speedy selection of a sub-graph for use by a mobile device.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5315993,no
Gauging acceptance of software metrics: Comparing perspectives of managers and developers,2009,"Metrics efforts are often impeded by factors such as poor data quality and developer resistance. To better understand and thus to address the developer perspective in a metrics program we undertook a case study at a large multi-national corporation. We identified six projects, and conducted surveys of both project managers and developers. These surveys were based on the metrics acceptance model (MAM) which is a framework (i.e. a model of relationships between factors, operationalized by a survey instrument) for gauging developer opinions toward software metrics. We noticed some interesting differences between developers' and managers' perceptions of metrics. While managers were on the same page as developers when it came to factors such as ease of use of the metrics tool, they over-estimated developers' confidence to report accurate measures. Managers under-estimated developers' beliefs about the usefulness of metrics and about their fear of adverse consequences. These findings suggest that the MAM could provide useful insights to project managers to train and motivate their developers. We also found that the MAM can be an effective diagnostic tool both at an organizational and project level to identify potential impediments in metrics programs.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5315999,no
Towards logistic regression models for predicting fault-prone code across software projects,2009,"In this paper, we discuss the challenge of making logistic regression models able to predict fault-prone object-oriented classes across software projects. Several studies have obtained successful results in using design-complexity metrics for such a purpose. However, our data exploration indicates that the distribution of these metrics varies from project to project, making the task of predicting across projects difficult to achieve. As a first attempt to solve this problem, we employed simple log transformations for making design-complexity measures more comparable among projects. We found these transformations useful in projects which data is not as spread as the data used for building the prediction model.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5316002,no
Simulation of the defect removal process with queuing theory,2009,"In this paper, we simulate the defects removal process using finite independent queues with different capacity and loading, which represent the limitation of developers and the ability differences of developers. The re-assignment strategy used in defects removal represents the cooperation between relevant developers. Experimental results based on real data show that the simulated approach can provide very useful and important information which can help project manager to estimate the duration of the whole defects removal process, the utilization of each developers and the defects remain at a specific time.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5316005,no
Reducing false alarms in software defect prediction by decision threshold optimization,2009,"Software defect data has an imbalanced and highly skewed class distribution. The misclassification costs of two classes are not equal nor are known. It is critical to find the optimum bound, i.e. threshold, which would best separate defective and defect-free classes in software data. We have applied decision threshold optimization on Naiumlve Bayes classifier in order to find the optimum threshold for software defect data. ROC analyses show that decision threshold optimization significantly decreases false alarms (on the average by 11%) without changing probability of detection rates.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5316006,no
Scope error detection and handling concerning software estimation models,2009,"Over the last 25+ years, the software community has been searching for the best models for estimating variables of interest (e.g., cost, defects, and fault proneness). However, little research has been done to improve the reliability of the estimates. Over the last decades, scope error and error analysis have been substantially ignored by the community. This work attempts to fill this gap in the research and enhance a common understanding within the community. Results provided in this study can eventually be used to support human judgment-based techniques and be an addition to the portfolio. The novelty of this work is that, we provide a way of detecting and handling the scope error arising from estimation models. The answer whether or not scope error will occur is a pre-condition to safe use of an estimation model. We also provide a handy procedure for dealing with outliers as to whether or not to include them in the training set for building a new version of the estimation model. The majority of the work is empirically based, applying computational intelligence techniques to some COCOMO model variations with respect to a publicly available cost estimation data set in the PROMISE repository.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5316020,no
Predicting defects with program dependencies,2009,"Software development is a complex and error-prone task. An important factor during the development of complex systems is the understanding of the dependencies that exist between different pieces of the code. In this paper, we show that for Windows Server 2003 dependency data can predict the defect-proneness of software elements. Since most dependencies of a component are already known in the design phase, our prediction models can support design decisions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5316024,no
Cognitive factors in perspective-based reading (PBR): A protocol analysis study,2009,"The following study investigated cognitive factors involved in applying the Perspective-Based Reading (PBR) technique for defect detection in software inspections. Using the protocol analysis technique from cognitive science, the authors coded concurrent verbal reports from novice reviewers and used frequency-based analysis to consider existing research on cognition in software inspections from within a cognitive framework. The current coding scheme was able to describe over 98% of the cognitive activities reported during inspection at a level of detail capable of validating multiple hypotheses from literature. A number of threats to validity are identified for the protocol analysis method and the parameters of the current experiment. The authors conclude that protocol analysis is a useful tool for analyzing cognitively intense software engineering tasks such as software inspections.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5316027,no
A detailed examination of the correlation between imports and failure-proneness of software components,2009,"Research has provided evidence that type usage in source files is correlated with the risk of failure of software components. Previous studies that investigated the correlation between type usage and component failure assigned equal blame to all the types imported by a component with a failure history, regardless of whether a type is used in the component, or associated to its failures. A failure-prone component may use a type, but it is not always the case that the use of this type has been responsible for any of its failures. To gain more insight about the correlation between type usage and component failure, we introduce the concept of a failure-associated type to represent the imported types referenced within methods fixed due to failures. We conducted two studies to investigate the tradeoffs between the equal-blame approach and the failure-associated type approach. Our results indicate that few of the types or packages imported by a failure-prone component are associated with its failures - less than 25% of the type imports, and less than 55% of the packages whose usage were reported to be highly correlated with failures by the equal-blame approach, were actually correlated with failures when we looked at the failure-associated types.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5316047,no
A probability-based approach for measuring external attributes of software artifacts,2009,"The quantification of so-called external software attributes, which are the product qualities with real relevance for developers and users, has often been problematic. This paper introduces a proposal for quantifying external software attributes in a unified way. The basic idea is that external software attributes can be quantified by means of probabilities. As a consequence, external software attributes can be estimated via probabilistic models, and not directly measured via software measures. This paper discusses the reasons underlying the proposals and shows the pitfalls related to using measures for external software attributes. We also show that the theoretical bases for our approach can be found in so-called ldquoprobability representations,rdquo a part of Measurement Theory that has not yet been used in Software Engineering Measurement. By taking the definition and estimation of reliability as reference, we show that other external software attributes can be defined and modeled by a probability-based approach.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5316048,no
Original content extraction oriented to anti-plagiarism,2009,"In order to reduce the impact of inclusion of citations and references during the detection of plagiarism in academic theses, and extract the original content, the author created three ways to extract original content and remove the citation: 1) Removal of normative citations by symbol features; 2) removal tacit citations by Bayesian method based on the minimum risk and thesis structure; 3) removal common knowledge base on domain public knowledge base. The research results show that during the extraction of original content, the precision decreases as the risk coefficient increases, while the recall rate increases with the risk coefficient. When the risk coefficient is 60, the whole performance achieves the optimum. Plagiarism detection after extracting the original content presents a fault rate decrease from 9.09% to 4.52%.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5317530,no
Improve the Portability of J2ME Applications: An Architecture-Driven Approach,2009,"The porting of J2ME applications is usually difficult because of diverse device features, limited device resources, and specific issues like device bugs. Therefore, achieving high efficiency in J2ME application porting can be challenging, tedious and error-prone. In this paper, we propose an architecture-driven approach to help address these issues through improving the portability of J2ME applications. It abstracts and models the features that affect porting tasks using component model named NanoCM (nano component model). The model is described in an architecture description language named NanoADL. Several open source J2ME applications are used as the case studies, and are evaluated using metrics indicating coupling, comprehensibility and complexity. Experiment results show that our approach effectively improves the portability of J2ME applications.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5318924,no
A Passive Mode QoS Measurer for ISP,2009,"Most network QoS measurement methods are user-oriented: measurements are triggered by network users, to obtain some performance / QoS metrics of a specified network path. Obviously these methods are not suitable for Internet services providers (ISPs), for they take ISPs too much time to finish a performance poll for the whole network. ISPs need a rapid method to measure and verify some important QoS metrics of their own network and partner's network. We introduce a real-time, passive mode measurement method based on high speed traffic monitoring and tracing mechanism. This method make it possible for ISPs to obtain QoS metrics for most network paths within a very short term at only one measuring point. The method is easy to deploy in backbone and works well in Gbps networks. We also introduce several new QoS metrics and according estimation algorithms, which may be more useful to ISPs than current end-to-end QoS metrics. The method is implemented in a tool called RPPM.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5319116,no
Integrated Evaluation Model of Software Reliability Based on Multi-information,2009,"The software reliability prediction model based on software quality measurement (program characteristics) and the software reliability growth model based on statistics characteristic (failure data) are the main quantification evaluation and prediction methods of software reliability. The modern creditability theory based on Bayesian estimation was introduced into the multi-information integrated modeling of software reliability evaluation and prediction. By this means, the prediction result of software reliability based on the metrics of software quality and the evaluation result of software reliability growth models based on failure data are combined reasonably via the credibility factor. Thus the extensive information related to software reliability was utilized and integrated so as to make the evaluated result of software reliability more reasonable. According to the modeling means of variance decomposition and optimal linear non-homogeneous credibility estimator, the mathematic expression of this integrated evaluation model of software reliability based on Bayesian estimation was described in detail, and the solution of credibility factor was illustrated. The data experiment testified its rationality and feasibility of this multi-information integrated model.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5319439,no
Description and Selection Model Based on Constraint QoS for Web Service,2009,"Along with the rapid growth of the Web service, the research of non-functional properties of Web service(QoS) becomes a hotspot. The work in this paper is based on an extended service model. It puts forward a metric policy of comprehensive QoS, and the matching algorithm including two aspects: non-QoS constraint and QoS constraint. The experimental results prove that the algorithm is reliable and sensitive to constraint conditions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5319491,no
Research on Testing-Based Software Credibility Measurement and Assessment,2009,"It's one of the important approaches to measure and assess software credibility by testing in trustworthy software study. From the perspective of effective management and credibility analysis supporting the test process, this article describes a basic framework for its management and discusses the assessment techniques and methods of credible test process and software product.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5319501,no
Progress and Quality Modeling of Requirements Analysis Based on Chaos,2009,"It is important and difficult for us to know the progress and quality of requirements analysis. We introduce chaos and software requirements complexity to the description of requirements decomposing, and get a method which can help us to evaluate the progress and quality. The model shows that requirements decomposing procedure has its own regular pattern which we can describe in a equation and track in a trajectory. The requirements analysis process of a software system can be taken as normal if its trajectory coincide with the model. We may be able to predict the time we need to finish all requirements decomposition in advance based on the model. We apply the method in the requirements analysis of home phone service management system, and the initial results show that the method is useful in the evaluation of requirements decomposition.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5319522,no
Soft Measurement Modeling Based on Improved Simulated Annealing Neural Network for Sewage Treatment,2009,"Considering the issues that the sewage treatment process is a complicated and nonlinear system, and the key parameters of sewage treatment quality can not be detected on-line, a soft measurement modeling method based on improved simulated annealing neural network (ISANN) is presented in this paper. First the simulated annealing algorithm with the best reserve mechanism is introduced and it is organic combined with Powell algorithm to form improved simulated annealing mixed optimize algorithm, instead of gradient falling algorithm of BP network to train network weight. It can get higher accuracy and faster convergence speed. We construct the network structure. With the ability of strong self-learning and faster convergence of ISANN, the soft measurement modeling method can truly detect and assess the quality of sewage treatment in real time by learning the sewage treatment parameter information of sensors acquired. The experimental results show that this method is feasible and effective.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5319556,no
Research on Fabric Image Registration Technique Based on Pattern Recognition Using Cross-Power Spectrum,2009,"This paper designs a fabric image registration method after researching texture of fabric. It can find the commonality area of two images quickly and accurately. Generally speaking, it is a good choice to solve the image registration with the relativity of two images in random texture images. However, repeated on high-resolution images to do Fourier Transform will decrease real-time of detection system. In order to enhance the real-time performance of fabric inspection system, the paper designs an effective method in the premise of ensuring matching effect. The method combines the excellence of frequency domain and space domain using predictable commonality area in the process of template matching, so arithmetic with favorable matching effect is put forward.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5319635,no
Measurement of the Complexity of Variation Points in Software Product Lines,2009,"Feature models are used in member product configuration in software product lines. A valid product configuration must satisfy two kinds of constraints, multiplicity of each variation point and dependencies among the variants in a product line. The combined impact of the two kinds of constrains on product configuration should be well understood. In this paper we propose a measurement, called VariationRank, that combines the two kinds of constraints to assess the complexity of variation points in software product lines. Based on the measurement we can identify those variation points with the highest impact on product configurations in a product line. This information could be used as guidance in product configuration as well as for feature model optimization in software product lines. A case study is presented and discussed in this paper as well.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5319645,no
The digital algorithm processors for the ATLAS Level-1 Calorimeter Trigger,2009,"The ATLAS Level-1 Calorimeter Trigger identifies high-E<sub>T</sub> jets, electrons/photons and hadrons, and measures total and missing transverse energy in proton-proton collisions at the Large Hadron Collider. Two subsystems-the Jet/Energy-sum Processor (JEP) and the Cluster Processor (CP)-process data from every crossing, and report feature multiplicities and energy sums to the ATLAS Central Trigger Processor, which produces a Level-1 Accept decision. Locations and types of identified features are read out to the Level-2 Trigger as regions-of-interest, and quality-monitoring information is read out to the ATLAS data acquisition system. The JEP and CP subsystems share a great deal of common infrastructure, including a custom backplane, several common hardware modules, and readout hardware. Some of the common modules use FPGAs with selectable firmware configurations based on their location in the system. This approach saved substantial development effort and provided a uniform model for firmware and software development. We present an in-depth description of the two subsystems as manufactured and installed. We compare and contrast the JEP and CP systems, and discuss experiences during production, installation and commissioning. We also briefly present results of recent tests that suggest an interesting upgrade path for higher luminosity running at the LHC.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5321731,no
The LHCb readout system and real-time event management,2009,"The LHCb experiment is a hadronic precision experiment at the LHC accelerator aimed at mainly studying b-physics by profiting from the large b-anti-b-production at LHC. The challenge of high trigger efficiency has driven the choice of a readout architecture allowing the main event filtering to be performed by a software trigger with access to all detector information on a processing farm based on commercial multicore PCs. The readout architecture therefore features only a relatively relaxed hardware trigger with a fixed and short latency accepting events at 1 MHz out of a nominal proton collision rate of 30 MHz, and high bandwidth with event fragment assembly over gigabit Ethernet. A fast central system performs the entire synchronization, event labelling and control of the readout, as well as event management including destination control, dynamic load balancing of the readout network and the farm, and handling of special events for calibrations and luminosity measurements. The event filter farm processes the events in parallel and reduces the physics event rate to about 2 kHz which are formatted and written to disk before transfer to the offline processing. A spy mechanism allows processing and reconstructing a fraction of the events for online quality checking. In addition a 5 Hz subset of the events are sent as express stream to offline for checking calibrations and software before launching the full offline processing on the main event stream. In this paper, we will give an overview of the readout system, and describe the real-time event management and the experience with the system during the commissioning phase with cosmic rays and first LHC beams.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5321781,no
The ALICE Data Quality Monitoring system,2009,"ALICE is one of the four experiments installed at the CERN Large Hadron Collider (LHC), especially designed for the study of heavy-ion collisions. The online Data Quality Monitoring (DQM) is an important part of the data acquisition (DAQ) software. It involves the online gathering, the analysis by user-defined algorithms and the visualization of monitored data. This paper presents the final design, as well as the latest and coming features, of the ALICE's specific DQM software called AMORE (Automatic Monitoring Environment). It describes the challenges we faced during its implementation, including the performances issues, and how we tested and handled them, in particular by using a scalable and robust publish-subscribe architecture. We also review the on-going and increasing adoption of this tool amongst the ALICE collaboration and the measures taken to develop, in synergy with their respective teams, efficient monitoring modules for the sub-detectors. The related packaging and release procedure needed by such a distributed framework is also described. We finally overview the wide range of usages people make of this framework, and we review our own experience, before and during the LHC start-up, when monitoring the data quality on both the sub-detectors and the DAQ side in a real-world and challenging environment.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5322152,no
The ALICE DAQ online databases,2009,"ALICE (A Large Ion Collider Experiment) is the heavy-ion detector designed to study the physics of strongly interacting matter and the quark-gluon plasma at the CERN Large Hadron Collider (LHC). The ALICE Data Acquisition system (DAQ) is made of a large number of distributed hardware and software components, which rely on several online databases: the configuration database, describing the counting room machines, some detector-specific electronics settings and the DAQ and Experiment Control System runtime parameters; the log database, centrally collecting reports from running processes; the experiment logbook, tracking the run statistics filled automatically and the operator entries; the online archive of constantly updated data quality monitoring reports; the file indexing services, including the status of transient files for permanent storage and the calibration results for offline export; the user guides (Wiki); test databases to check the interfaces with external components; reference data sets used to restore known configurations. With 35 GB of online data hosted on a MySQL server and organized in more than 500 relational tables for a total of 40 million rows, this information is populated and accessible through various frontends, including C library for efficient repetitive access, Tcl/TK GUIs for configuration editors and log browser, HTML/PHP pages for the logbook, and command line tools for scripting and expert debugging. Exhaustive hardware benchmarks have been conducted to select the appropriate database server architecture. Secure access from private and general networks was implemented. Ad-hoc monitoring and backup mechanisms have been designed and deployed. We discuss the implementation of these complex databases and how the inhomogeneous requirements have been addressed. We also review the performance analysis outcome after more than one year in production and show results of data mining from this central information source.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5322155,no
Quality Assurance Testing for Magnetization Quality Assessment of BLDC Motors used in Compressors,2009,"Quality assurance testing of mass-produced electrical appliances is critical for the reputation of the manufacturer since defective units will have a negative impact on safety, reliability, efficiency, and performance of the end product. It has been observed at a brushless DC compressor motor manufacturing facility that improper magnetization of the rotor permanent magnet is one of the leading causes of motor defects. A new technique for post-manufacturing assessment the magnetization quality of concentrated winding brushless DC compressor motors for quality assurance, is proposed in this paper. The new method evaluates the data acquired during the test runs performed after motor assembly to observe anomalies in the zero crossing pattern of the back-emf voltages for screening motor units with defective magnetization. An experimental study on healthy and defective 250 W brushless DC compressor motor units shows that the proposed technique provides sensitive detection of magnetization defects that existing tests were not capable of finding. The proposed algorithm does not require additional hardware for implementation since it can be added to the existing test run inverter of the quality assurance system as a software algorithm.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5324825,no
Software Reliability Prediction and Analysis Using Queueing Models with Multiple Change-Points,2009,"Over the past three decades, many software reliability growth models (SRGMs) were proposed and they are aimed at predicting and estimating software reliability. One common assumption of these conventional SRGMs is that detected faults will be removed immediately. In reality, this assumption may not be reasonable and may not always occur. Developers need time to identify the root causes of detected faults and then fix them. Besides, during debugging the fault correction rate may not be a constant and could be changed at some certain points as time proceeds. Consequently, in this paper, we will explore and study how to apply queueing model to investigate the fault correction process during software development. We propose an extended infinite server queueing model with multiple change-points to predict and assess software reliability. Experimental results based on real failure data show that proposed model can depicts the change of fault correction rates and predict the behavior of software development more accurately than traditional SRGMs.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5325373,no
A Trust-Based Detecting Mechanism against Profile Injection Attacks in Recommender Systems,2009,"Recommender systems could be applied in grid environment to help grid users select more suitable services by making high quality personalized recommendations. Also, recommendation could be employed in the virtual machines managing platform to measure the performance and creditability of each virtual machine. However, such systems have been shown to be vulnerable to profile injection attacks (shilling attacks), attacks that involve the insertion of malicious profiles into the ratings database for the purpose of altering the system's recommendation behavior. In this paper we introduce and evaluate a new trust-based detecting algorithm for protecting recommender systems against profile injection attacks. Moreover, we discuss the combination of our trust-based metrics with previous metrics such as RDMA in profile-level and item-level respectively. In the end, we show these metrics can lead to improved detecting accuracy experimentally.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5325392,no
Architectural Availability Analysis of Software Decomposition for Local Recovery,2009,"Non-functional properties, such as timeliness, resource consumption and reliability are of crucial importance for today's software systems. Therefore, it is important to know the non-functional behavior before the system is put into operation. Preferably, such properties should be analyzed at design time, at an architectural level, so that changes can be made early in the system development process. In this paper, we present an efficient and easy-to-use methodology to predict - at design time - the availability of systems that support local recovery. Our analysis techniques work at the architectural level, where the software designer simply inputs the software modules' decomposition annotated with failure and repair rates. From this decomposition we automatically generate an analytical model (i.e. a continuous-time Markov chain), from which various performance and dependability measures are then computed, in a way that is completely transparent to the user. A crucial step is the use of intermediate models in the Input/Output Interactive Markov Chain formalism, which makes our techniques, efficient, mathematically rigorous, and easy to adapt. In particular, we use aggressive minimization techniques to keep the size of the generated state spaces small. We have applied our methodology on a realistic case study, namely the MPlayer open source software. We have investigated four different decomposition alternatives and compared our analytical results with the measured availability on a running MPlayer. We found that our predicted results closely match the measured ones.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5325395,no
Looking for Product Line Feature Models Defects: Towards a Systematic Classification of Verification Criteria,2009,"Product line models (PLM) are important artifacts in product line engineering. Due to their size and complexity, it is difficult to detect defects in PLMs. The challenge is however important: any error in a PLM will inevitably impact configuration, generating issues such as incorrect product models, inconsistent architectures, poor reuse, difficulty to customize products, etc. Surveys on feature-based PLM verification approaches show that there are many verification criteria, that these criteria are defined in different ways, and that different ways of working are proposed to look for defect. The goal of this paper is to systematize PLM verification. Based on our literature review, we propose a list of 23 verification criteria that we think cover those available in the literature.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5328480,no
Evaluating the Completeness and Granularity of Functional Requirements Specifications: A Controlled Experiment,2009,"Requirements engineering (RE) is a relatively young discipline, and still many advances have been achieved during the last decades. In particular, numerous RE methods have been proposed. However, there is a growing concern for empirical validations that assess RE proposals and statements. This paper is related to the evaluation of the quality of functional requirements specifications, focusing on completeness and granularity. To do this, several concepts related to conceptual model quality are presented; these concepts lead to the definition of metrics that allow measuring certain aspects of a requirements model quality (e.g. degree of functional encapsulations completeness with respect to a reference model, number of functional fragmentation errors). A laboratory experiment with master students has been carried out, in order to compare (using the proposed metrics) two RE approaches; namely, Use Cases and Communication Analysis. Results indicate greater quality (in terms of completeness and granularity) when communication analysis guidelines are followed. Moreover, interesting issues arise from experimental results, which invite further research.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5328548,no
Double Redundant Fault-Tolerance Service Routing Model in ESB,2009,"With the development of the Service Oriented Architecture (SOA), the Enterprise Service Bus (ESB) is becoming more and more important in the management of mass services. The main function of it is service routing which focuses on delivery of message among different services. At present, some routing patterns have been implemented to finish the messaging, but they are all static configuration service routing. Once one service fails in its operation, the whole service system will not be able to detect such fault, so the whole business function will also fail finally. In order to solve this problem, we present a double redundant fault tolerant service routing model. This model has its own double redundant fault tolerant mechanism and algorithm to guarantee that if the original service fails, another replica service that has the same function will return the response message instead automatically. The service requester will receive the response message transparently without taking care where it comes from. Besides, the state of failed service will be recorded for service management. At the end of this article, we evaluated the performance of double redundant fault tolerant service routing model. Our analysis shows that, by importing double redundant fault tolerance, we can improve the fault-tolerant capability of the services routing apparently. It will solve the limitation of existent static service routing and ensure the reliability of messaging in SOA.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5328575,no
Reasoning on Non-Functional Requirements for Integrated Services,2009,"We focus on non-functional requirements for applications offered by service integrators; i.e., software that delivers service by composing services, independently developed, managed, and evolved by other service providers. In particular, we focus on requirements expressed in a probabilistic manner, such as reliability or performance. We illustrate a unified approach-a method and its support tools-which facilitates reasoning about requirements satisfaction as the system evolves dynamically. The approach relies on run-time monitoring and uses the data collected by the probes to detect if the behavior of the open environment in which the application is situated, such as usage profile or the external services currently bound to the application, deviates from the initially stated assumptions and whether this can lead to a failure of the application. This is achieved by keeping a model of the application alive at run time, automatically updating its parameters to reflect changes in the external world, and using the model's predictive capabilities to anticipate future failures, thus enabling suitable recovery plans.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5328588,no
SQUAD: Software Quality Understanding through the Analysis of Design,2009,"Object-oriented software quality models usually use metrics of classes and of relationships among classes to assess the quality of systems. However, software quality does not depend on classes solely: it also depends on the organization of classes, i.e., their design. Our thesis is that it is possible to understand how the design of systems affects their quality and to build quality models that take into account various design styles, in particular design patterns, antipatterns, and code smells. To demonstrate our thesis, we first analyze how playing roles in design patterns, antipatterns, and code smells impacts quality; specifically change-proneness, fault-proneness, and maintenance costs. Second, we build quality models and apply and validate them on open-source and industrial object-oriented systems to show that they allow a more precise evaluation of the quality than traditional models,like Bansiya et al.'s QMOOD.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5328744,no
A New Metric for Automatic Program Partitioning,2009,"Software reverse engineering techniques are most often applied to reconstruct the architecture of a program with respect to quality constraints, or non-functional requirements such as maintainability or reusability. However, there has been no effort to assess the architecture of a program from the performance viewpoint and reconstruct this architecture in order to improve the program performance. In this paper, a novel Actor-Oriented Program reverse engineering approach, is proposed to reconstruct an object-oriented program architecture based on a high performance model such as actor model. Since actors can communicate with each other asynchronously, reconstructing the program architecture based on this model may result in the concurrent execution of the program invocations and consequently increasing the overall performance of the program when enough processors are available.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5329092,no
Performance Analysis of Datamining Algorithms for Software Quality Prediction,2009,"Data mining techniques are applied in building software fault prediction models for improving the software quality. Early identification of high-risk modules can assist in quality enhancement efforts to modules that are likely to have a high number of faults. Classification tree models are simple and effective as software quality prediction models, and timely predictions of defects from such models can be used to achieve high software reliability. In this paper, the performance of five data mining classifier algorithms named J48, CART, Random Forest, BFTree and Naive Bayesian classifier (NBC) are evaluated based on 10 fold cross validation test. Experimental results using KC2 NASA software metrics dataset demonstrates that decision trees are much useful for fault predictions and based on rules generated only some measurement attributes in the given set of the metrics play an important role in establishing final rules and for improving the software quality by giving correct predictions. Thus we can suggest that these attributes are sufficient for future classification process. To evaluate the performance of the above algorithms Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), Receiver Operating Characteristic (ROC) and Accuracy measures are applied.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5329377,no
A Novel Fast Multiple Reference Frames Selection Method for H.264/AVC,2009,"Multiple reference frames motion estimation is one of the new features introduced in H.264/AVC. Although this new feature improves video coding performance, it is extremely computational intensive. Full search scheme is adopted in reference software JM, and the increased computation load is in proportion to the number of searched reference frames. In this paper, we propose a novel fast multiple reference frames selection method. It can decrease the number of reference frames for motion estimation and reduce the complexity of coding adaptively according to the features of video sequences. The experiment results show that our algorithm achieves 42% coding time saving on average with negligible quality loss.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5329402,no
A platform for testing and comparing of real-time decision-support algorithms in mobile environments,2009,"The unavailability of a flexible system for real-time testing of decision-support algorithms in a pre-hospital clinical setting has limited their use. In this study, we describe a plug-and-play platform for real-time testing of decision-support algorithms during the transport of trauma casualties en route to a hospital. The platform integrates a standard-of-care vital-signs monitor, which collects numeric and waveform physiologic time-series data, with a rugged ultramobile personal computer. The computer time-stamps and stores data received from the monitor, and performs analysis on the collected data in real-time. Prior to field deployment, we assessed the performance of each component of the platform by using an emulator to simulate a number of possible fault scenarios that could be encountered in the field. Initial testing with the emulator allowed us to identify and fix software inconsistencies and showed that the platform can support a quick development cycle for real-time decision-support algorithms.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5332403,no
A new validity measure for a correlation-based fuzzy c-means clustering algorithm,2009,"One of the major challenges in unsupervised clustering is the lack of consistent means for assessing the quality of clusters. In this paper, we evaluate several validity measures in fuzzy clustering and develop a new measure for a fuzzy c-means algorithm which uses a Pearson correlation in its distance metrics. The measure is designed with within-cluster sum of square, and makes use of fuzzy memberships. In comparing to the existing fuzzy partition coefficient and a fuzzy validity index, this new measure performs consistently across six microarray datasets. The newly developed measure could be used to assess the validity of fuzzy clusters produced by a correlation-based fuzzy c-means clustering algorithm.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5332582,no
"A wearable, low-power, health-monitoring instrumentation based on a programmable system-on-chip<sup>TM</sup>",2009,"Improvement in quality and efficiency of health and medicine, at home and in hospital, has become of paramount importance. The solution of this problem would require the continuous monitoring of several key patient parameters, including the assessment of autonomic nervous system (ANS) activity using non-invasive sensors, providing information for emotional, sensorial, cognitive and physiological analysis of the patient. Recent advances in embedded systems, microelectronics, sensors and wireless networking enable the design of wearable systems capable of such advanced health monitoring. The subject of this article is an ambulatory system comprising a small wrist device connected to several sensors for the detection of the autonomic nervous system activity. It affords monitoring of skin resistance, skin temperature and heart activity. It is also capable of recording the data on a removable media or sending it to computer via a wireless communication. The wrist device is based on a programmable system-on-chip (PSoCtrade) from Cypress: PSoCs are mixed-signal arrays, with dynamic, configurable digital and analogical blocks and an 8-bit microcontroller unit (MCU) core on a single chip. In this paper we present first of all the hardware and software architecture of the device, and then results obtained from initial experiments.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5332662,no
SleepMinder: An innovative contact-free device for the estimation of the apnoea-hypopnoea index,2009,"We describe an innovative sensor technology (SleepMindertrade) for contact-less and convenient measurement of sleep and breathing in the home. The system is based on a novel non-contact biomotion sensor and proprietary automated analysis software. The biomotion sensor uses an ultra low-power radio-frequency transceiver to sense the movement and respiration of a subject. Proprietary software performs a variety of signal analysis tasks including respiration analysis, sleep quality measurement and sleep apnea assessment. This paper measures the performance of SleepMinder as a device for the monitoring of sleep-disordered breathing (SDB) and the provision of an estimate of the apnoea-hypopnoea index (AHI). The SleepMinder was tested against expert manually scored PSG data of patients gathered in an accredited sleep laboratory. The comparison of SleepMinder to this gold standard was performed across overnight recordings of 129 subjects with suspected SDB. The dataset had a wide demographic profile with the age ranging between 20 and 81 years. Body weight included subjects with normal weight through to the very obese (Body Mass Index: 21-44 kg/m<sup>2</sup>). SDB severity ranged from subjects free of SDB to those with severe SDB (AHI: 0.8-96 events/hours). SleepMinder's AHI estimation has a correlation of 91% and can detect clinically significant SDB (AHI>15) with a sensitivity of 89% and a specificity of 92%.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5332909,no
A hybrid platform based on EOG and EEG signals to restore communication for patients afflicted with progressive motor neuron diseases,2009,"An efficient alternative channel for communication without overt speech and hand movements is important to increase the quality of life for patients suffering from Amiotrophic Lateral Sclerosis or other illnesses that prevent correct limb and facial muscular responses. Often, such diseases leave the ocular movements preserved for a relatively long time. The aim of this study is to present a new approach for the hybrid system which is based on the recognition of electrooculogram (EOG) and electroencephalogram (EEG) measurements for efficient communication and control. As a first step we show that the EOG-based side of the system for communication and controls is useful for patients. The EOG side of the system has been equipped with an interface including a speller to notify of messages. A comparison of the performance of the EOG-based system has been made with a BCI system that uses P300 waveforms. As a next step, we plan to integrate EOG and EEG sides. The final goal of the project is to realize a unique noninvasive device able to offer the patient the partial restoration of communication and control abilities with EOG and EEG signals.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5333742,no
An Assessment of GPS-based precise point positioning of the low earth-orbiting satellite CHAMP,2009,"Precise point positioning (PPP) with the international GNSS service (IGS) products, which consist of precise orbits and clock correction information, has been demonstrated by several investigators to achieve a centimeter-decimeter level positioning accuracy in real-time for land and aerial vehicular navigation. The purpose of this paper is to present one phase of study conducted at National Defense Academy (NDA) on performance evaluation when such a PPP technique is extended for orbit determination of the low Earth orbiters (LEO) in post-flight processing or even in real-time. In the present study the satellite CHAMP was selected as an LEO since high accuracy orbit is prerequisite to that mission. A PPP filter with a simple white noise plant model was designed, and a basic PPP software which was originally developed at NDA for land and aerial vehicles was intensively modified to process GPS data taken by the dual-frequency receiver ldquoBlackJackrdquo onboard the CHAMP satellite. To generate a reference orbit of CHAMP, we proposed a hybrid orbit determination (OD) method consisting of classical least-squares and Hill-Clohessy-Wildshire (HCW) solution, where a least-squares point positioning solution of 3D coordinates from pseudoranges of CHAMP was employed as the pseudo-observations. It was shown that our PPP filter produced an overall positioning accuracy of better than a sub-meter for transverse and normal components and 90 cm for radial component for 3-hour data arc, and in particular of a few decimeters for each component over the last one-hour segment of good-quality data (no data gap), the implication of the precision real-time on-orbit satellite navigation using only a single, dual-frequency GPS receiver, along with the prediction portion of IGS rapid or ultra-rapid products.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5334336,no
Use of threat image projection (TIP) to enhance security performance,2009,"Threat Image Projection (TIP) is a software system that is used at airports to project images of threat items amongst the passenger baggage being screened by X-ray. The use of TIP is becoming more widespread and is increasingly being included as part of security regulation. This is due to its purported benefits of improved attention and vigilance, and increased exposure to threat items that are linked to improvements in threat detection performance. Further, the data collected by the TIP system can be used to assess individual performance, provide feedback to screeners, and tailor training to specific performance weaknesses; which can generate further performance improvements. However, TIP will only be successful in enhancing security performance if it is used and managed effectively. In this paper the key areas of effective TIP use and management that enable security performance to be enhanced are highlighted. These include the optimisation of TIP settings, such as the TIP to bag ratio, and image library management. Appropriate setting of these components can lead to improved performance as a facet result of increasing exposure to a suitable range of threat images. The key elements of TIP training are highlighted including the importance of communicating TIP related information and the role of the supervisor in ensuring TIP is used appropriately. Finally, the use of TIP data are examined, including the effective use of TIP for performance assessment and screener feedback and in defining training. The provision of feedback regarding TIP scores has been shown to enhance performance in excess of that achieved by using TIP in isolation. To date, the vast majority of TIP research has been conducted in relation to the screening of carry-on baggage. In the final part of this presentation the use of TIP to enhance performance in other areas such as Hold Baggage Screening (HBS) and Cargo are considered. HBS TIP is associated with different challenges due to its alternative - method of operation (to present complete images of a bag and threat item) which imposes demands for the operational set-up and the construction of the image library. The use of TIP in Cargo is associated with a different set of challenges as a result of the diverse nature of items scanned and the screening environment. However, in both these domains, the use of TIP has been associated with the realisation of benefits in line with those achieved for carry-on baggage screening. Through understanding differences in the context in which TIP is used it is possible to understand the differing requirements for its use and management that will enable the benefits of TIP to be realised, enhancing security performance across locations and screening contexts.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5335565,no
Applying Code Coverage Approach to an Infinite Failure Software Reliability Model,2009,"An approach to software reliability modeling based on code coverage is used to derive the Infinite Failure software reliability Model Based on Code Coverage - IFMBC. Our aim was to verify the soundness of the approach under different assumptions. The IFMBC was assessed with test data from a real application, making use of the following structural testing criteria: all-nodes, all-edges, and potential-uses - a data-flow based family of criteria. The IFMBC was shown to be as good as the Geometric Model - GEO, found to be the best traditional time-based model that fits the data. Results from the analysis also show that the IFMBC is as good as the BMBC - Binomial software reliability Model Based on Coverage - a model previously derived using the code coverage approach, indicating it to be effective under different modeling assumptions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5336084,no
Proactive estimation of the video streaming reception quality in WiFi networks using a cross-layer technique,2009,"The quality of service (QoS) in a Wireless Fidelity (WiFi) network can not be guaranteed due to intermittent disconnections, interferences and so forth because they reduce considerably the necessary QoS for multimedia applications. For that reason it is essential a software as the one we present in this paper that evaluates if there are the appropriate conditions to receive the streaming measuring the congestion, radio coverage, throughput, lost and received packets rate, and so on. Our software makes a proactive estimation using a bottom-up cross-layer technique measuring physical level parameters, Round Trip Time (RTT) and getting traffic statistics for the overall WiFi network and for each ongoing Real Time Streaming Protocol/Real Time Protocol (RTSP/RTP) streaming session. Experimental results show a high percentage of good decisions of the software to detect the wireless channel degradation and its recovery.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5336638,no
TPSS: A two-phase sleep scheduling protocol for object tracking in sensor networks,2009,"Lifetime maximization is an important factor in the design of sensor networks for object tracking applications. Some techniques of node scheduling have been proposed to reduce energy consumption. By exploiting the redundancy of network coverage, they turn off unnecessary nodes, or make nodes work in turn, which require high nodes density or sacrificing tracking quality. We present TPSS, a two-phase sleep scheduling protocol, which divides the whole tracking procedure into two phases and assigns different scheduling policies at each phase. To balance energy savings and tracking quality, we further optimize the node scheduling protocol in terms of network coverage and nodes state prediction. We evaluate our method in an indoor environment with 36 sensor nodes. Comparing with the existing methods, all the experimental results show that our method has a high performance in term of energy savings and tracking quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5336966,no
Throughput and delay analysis of the IEEE 802.15.3 CSMA/CA mechanism considering the suspending events in unsaturated traffic conditions,2009,"Unlike in IEEE 802.11, the CSMA/CA traffic conditions in IEEE 802.15.3 are typically unsaturated. This paper presents an extended analytical model based on Bianchi's model in IEEE 802.11, by taking into account the device suspending events, unsaturated traffic conditions, as well as the effects of error-prone channels. Based on this model we re-derive a closed form expression of the average service time. The accuracy of the model is validated through extensive simulations. The analysis is also instructional for IEEE 802.11 networks under limited load.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5337030,no
Mutation Sensitivity Testing,2009,"Computational scientists often encounter code-testing challenges not typically faced by software engineers who develop testing techniques. Mutation sensitivity testing addresses these challenges, showing that a few well-designed tests can detect many code faults and that reducing error tolerances is often more effective than running additional tests.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5337643,no
Engineering the Software for Understanding Climate Change,2009,"Climate scientists build large, complex simulations with little or no software engineering trainingâ€”and don't readily adopt the latest software engineering tools and techniques. This ethnographic study of climate scientists shows that their culture and practices share many features of agile and open source projects, but with highly customized software validation and verification techniques.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5337646,no
Measuring the quality of interfaces using source code entropy,2009,"Global enterprises face an increasingly high complexity of software systems. Although size and complexity are two different aspects of a software system, traditionally, various size metrics have been established to indicate their complexity. In fact, many developed software metrics correlate with the number of lines of code. Moreover, a combination of multiple metrics collected on bottom layers into one comprehensible and meaningful indicator for an entire system is not a trivial task. This paper proposes a novel interpretation of an entropy-based metric to assess the design of a software system in terms of interface quality and understandability. The proposed metric is independent of the system size and delivers one single value eliminating the unnecessary aggregation step. Further, an industrial case study has been conducted to illustrate the usefulness of this metric.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5344433,no
Performance evaluation of service-oriented architecture through stochastic Petri nets,2009,"The service-oriented architecture (SOA) has become an unifying technical architecture that can be embodied with Web service technologies, in which the Web service is thought as a fundamental building block. This paper proposes a simulation modeling approach based on stochastic Petri nets to estimate the performance of SOA applications. Using the proposed model it is possible to predict resource consumption and service levels degradation in scenarios with different compositions and workloads. A case study was conducted to validate the approach and to compare the results against an existing analytical modeling approach.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5346593,no
Using designer's effort for user interface evaluation,2009,"Designing Human Computer Interfaces is one of the more important and difficult design tasks. The tools for verifying the quality of the interface are frequently expensive or provide feedback too far after the design of the interface as to make it meaningless. To improve the interface usability, designers need a verification tool providing immediate feedback at a low cost. Using an effort-based measure of usability, it is possible for a designer to estimate the effort a subject might expend to complete a specific task. In this paper, we develop the notion of designer's effort for evaluating interface usability for new designs and Commercial-Off-The-Shelf software. Designer's effort provides a technique to evaluate human interface before completing the development of the software and provides feedback from usability tests conducted using the effort-based evaluation technique.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5346725,no
Application of a seeded hybrid genetic algorithm for user interface design,2009,"Studies have established that computer user interface (UI) design is a primary contributor to people's experiences with modern technology; however, current UI development remains more art than quantifiable science. In this paper, we study the use of search algorithms to predict optimal display layouts early in system design. This has the potential to greatly reduce the cost and improve the quality of UI development. Specifically, we demonstrate a hybrid genetic algorithm and pattern search optimization process that makes use of human performance modeling to quantify known design principles. We show how this approach can be tailored by capturing contextual factors in order to properly seed and tune the genetic algorithm. Finally, we demonstrate the ability of this process to discover superior layouts as compared to manual qualitative methods.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5346789,no
Discovering the best web service: A neural network-based solution,2009,"Differentiating between Web services that share similar functionalities is becoming a major challenge into the discovery of Web services. In this paper we propose a framework for enabling the efficient discovery of Web services using artificial neural networks (ANN) best known for their generalization capabilities. The core of this framework is applying a novel neural network model to Web services to determine suitable Web services based on the notion of the quality of Web service (QWS). The main concept of QWS is to assess a Web service's behaviour and ability to deliver the requested functionality. Through the aggregation of QWS for Web services, the neural network is capable of identifying those services that belong to a variety of class objects. The overall performance of the proposed method shows a 95% success rate for discovering Web services of interest. To test the robustness and effectiveness of the neural network algorithm, some of the QWS features were excluded from the training set and results showed a significant impact in the overall performance of the system. Hence, discovering Web services through a wide selection of quality attributes can considerably be influenced with the selection of QWS features used to provide an overall assessment of Web services.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5346817,no
Automation component aspects for efficient unit testing,2009,"Automation systems software must provide sufficient diagnosis information for testing to enable early defect detection and quality measurement. However, in many automation systems the aspects of automation, testing, and diagnosis are intertwined in the code. This makes the code harder to read, modify, and test. In this paper we introduce the design of a test-driven automation (TDA) component with separate aspects for automation, diagnosis, and testing to improve testability and test efficiency. We illustrate with a prototype, how automation component aspects allow flexible configuration of a Â¿Â¿system under testÂ¿Â¿ for test automation. Major result of the pilot application is that the TDA concept was found usable and useful to improve testing efficiency.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5347022,no
A novel error resilient joint source-channel coding scheme for image transmission over error prone channels,2009,"In this paper, we proposed a double-level error resilient joint source-channel coding scheme for image transmission. Characteristically, we inserted one coordinative component named error resilient entropy coding (EREC) between source coding and channel coding to achieve additional error resilient capability. Based on the novel architecture, we proved that, in the aspects of computational complexity, coding redundancy and furthermore the decoding performance of image transmission, our scheme outperformed either separate source and channel coding scheme or joint source-channel coding scheme with synchronization words.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5347861,no
On line sensor planning for tracking in camera networks,2009,"Sensor planning chiefly applies to optimizing surveillance tasks, such as persistent tracking by designing and utilizing camera placement strategies. Against substituting new optimized camera networks for those still in usage, online sensor planning hereby involves the design of vision algorithms that not only select cameras which yield the best results, but also improve the quality with which surveillance tasks are performed. In previous literatures about coverage problem in sensor planning, targets (e.g., persons) are justly simplified as a 2-D point. However in actual application scene, cameras are always heterogeneous such as fixed with different height and action radii, and the monitored objects has 3-D features (e.g., height). This paper presents a new sensor planning formulation addressing the efficiency enhancement of active visual tracking in camera networks that track and detect people traversing a region. The numerical results show that this online sensor planning approach can improve the active tracking performance of the system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5347877,no
Control of dynamic voltage restorer (DVR) based on introducing a new approach for the on-line estimation of symmetrical components,2009,The main duty of dynamic voltage restorer (DVR) is protection of sensitive loads against voltage disturbances especially voltage sags and swells. DVR should possess necessary response time in restoring load bus voltage to avoid load interruptions. One the most important factors that affects the performance of DVR is the estimation algorithm which is used in control unit of this compensator. This paper presents a new fast-response algorithm for the on-line estimation of symmetrical components in harmonic distortion condition which provides adequate response time and accuracy for dynamic voltage restorer. Furthermore performance of this algorithm would be compared with some well-known estimation techniques using MATLAB/SIMULINK. Application of the proposed estimation approach is realized by representing a new control unit for compensation of different voltage sags and swells. Finally the simulation results which obtained using PSCAD/EMTDC confirm the efficiency of suggested control unit.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5347919,no
Service Redundancy Strategies in Service-Oriented Architectures,2009,"Redundancy can improve the availability of components in service-oriented systems. However, predicting and quantifying the effects of different redundancy strategies can be a complex task. In our work, we have taken an architecture based approach to the modeling, predicting and monitoring of properties in distributed software systems.This paper proposes redundancy strategies for service-oriented systems and models services with their associated protocols. We derive formal models from these high-level descriptions that are embedded in our fault-tolerance testing framework.We describe the general framework of our approach, develop two service redundancy strategies and report about the preliminary evaluation results in measuring performance and availability of such services. While the assumptions for the chosen case study are limiting, our evaluation is promising and encourages the extension of our testing framework to cater for more complex, hybrid, fault-tolerance strategies and architectural compositions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5349836,no
What Software Repositories Should Be Mined for Defect Predictors?,2009,"The information about which modules in a software system's future version are potentially defective is a valuable aid for quality managers and testers. Defect prediction promises to indicate these defect-prone modules. Constructing effective defect prediction models in an industrial setting involves the decision from what data source the defect predictors should be derived. In this paper we compare defect prediction results based on three different data sources of a large industrial software system to answer the question what repositories to mine. In addition, we investigate whether a combination of different data sources improves the prediction results. The findings indicate that predictors derived from static code and design analysis provide slightly yet still significant better results than predictors derived from version control, while a combination of all data sources showed no further improvement.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5349842,no
Experiences and Results from Establishing a Software Cockpit at BMD Systemhaus,2009,"What is the degree of completion of the current iteration? Are we on track? How accurate are estimates compared to actual effort? Software cockpits (software project control centers) provide systematic support for answering such questions. Therefore, like a cockpit in an aircraft, software cockpits integrate and visualize accurate and timely information from various data sources for operative and strategic decision making. Important aspects are to track progress, to visualize team activities, and to provide transparency about the status of a project. In this paper we present our experiences from implementing and introducing a software cockpit in a leading Austrian software development company. The introduction has been supported by a small-scale process improvement and a GM-based measurement initiative. Furthermore, the paper discusses challenges and potential pitfalls in establishing software cockpits and shares some lessons learned relevant for the practitioner confronted with introducing a software cockpit in an industrial setting.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5349843,no
Synthetic Metrics for Evaluating Runtime Quality of Software Architectures with Complex Tradeoffs,2009,"Runtime quality of software, such as availability and throughput, depends on architectural factors and execution environment characteristics (e.g. CPU speed, network latency). Although the specific properties of the underlying execution environment are unknown at design time, the software architecture can be used to assess the inherent impact of the adopted design decisions on runtime quality. However, the design decisions that arise in complex software architectures exhibit non trivial interdependences. This work introduces an approach that discovers the most influential factors, by exploiting the correlation structure of the analyzed metrics via factor analysis of simulation data. A synthetic performance metric is constructed for each group of correlated metrics. The variability of these metrics summarizes the combined factor effects hence it is easier to assess the impact of the analyzed architecture decisions on the runtime quality. The approach is applied on experimental results obtained with the ACID Sim Tools framework for simulating transaction processing architectures.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5349844,no
CQML Scheme: A Classification Scheme for Comprehensive Quality Model Landscapes,2009,"Managing quality during the development, operation, and maintenance of software(-intensive) systems and services is a challenging task. Although many organizations need to define, control, measure, and improve various quality aspects of their development artifacts and processes, nearly no guidance is available on how to select, adapt, define, combine, use, and evolve quality models. Catalogs of models as well as selection and tailoring processes are widely missing. A first step towards better support for selecting and adapting quality models can be seen in a classification of existing quality models, especially with respect to their suitability for different purposes and contexts. This article presents so-called comprehensive quality model landscapes (CQMLs), which provide such a classification scheme and help to get an overview of existing models and their relationships. The article focuses on the description and justification of essential concepts needed to define quality models and landscapes. In addition, the article describes typical usage scenarios, illustrates the concept with examples, and sketches open questions and future work.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5349845,no
Fault Analysis in OSS Based on Program Slicing Metrics,2009,"In this paper, we investigate the barcode OSS using two of Weiser's original slice-based metrics (tightness and overlap) as a basis, complemented with fault data extracted from multiple versions of the same system. We compared the values of the metrics in functions with at least one reported fault with fault-free modules to determine a) whether significant differences in the two metrics would be observed and b) whether those metrics might allow prediction of faulty functions. Results revealed some interesting traits of the tightness metric and, in particular, how low values of that metric seemed to indicate fault-prone functions. A significant difference was found between the tightness metric values for faulty functions when compared to fault-free functions suggesting that tightness is the `better' of the two metrics in this sense. The overlap metric seemed less sensitive to differences between the two types of function.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5349887,no
A Framework for the Balanced Optimization of Quality Assurance Strategies Focusing on Small and Medium Sized Enterprises,2009,"The Quality Improvement Paradigm (QIP) offers a general framework for systematically improving an organization's development processes in a continuous manner. In the context of the LifeCycleQM project, the general QIP framework was concretized for a specific application area, namely, the balanced improvement of quality assurance (QA) strategies, i.e., a set of systematically applied QA activities. Especially with respect to small and medium-sized enterprises, the encompassing QIP framework presents limited guidance for easy and concrete application. Therefore, individual activities within the QIP framework were refined focusing on QA strategies, and proven measurement procedures such as the defect flow model and knowledge from the area of process improvement (e.g., with regard to individual QA procedures) was integrated for reuse in a practice-oriented manner. The feasibility of the developed approach was initially explored by its application at IBS AG, a medium-sized enterprise, where improvement goals were defined, a corresponding measurement program was established, improvement potential was identified, and concrete improvement suggestions for the QA strategy were derived, assessed, and implemented.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5349940,no
A Multi-Tier Provenance Model for Global Climate Research,2009,"Global climate researchers rely upon many forms of sensor data and analytical methods to help profile subtle changes in climate conditions. The U.S. Department of Energy Atmospheric Radiation Measurement (ARM) program provides researchers with curated Value Added Products (VAPs) resulting from continuous instrumentation streams, data fusion, and analytical profiling. The ARM operational staff and software development teams (data producers) rely upon a number of techniques to ensure strict quality control (QC) and quality assurance (QA) standards are maintained. Climate researchers (data consumers) are highly interested in obtaining as much provenance evidence as possible to establish data trustworthiness. Currently all the evidence is not easily attainable or identifiable without significant efforts to extract and piece together information from configuration files, log files, codes, or status information on the ARM website. Our objective is to identify a provenance model that serves the needs of both the VAP producers and consumers. This paper shares our initial results - a comprehensive multi-tier provenance model. We describe how both ARM operations staff and the climate research community can greatly benefit from this approach to more effectively assess and quantify the data historical record.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5349958,no
"RAMS Analysis of a Bio-inspired Traffic Data Sensor (""Smart Eye"")",2009,"The Austrian Research Centers have developed a compact low-power embedded vision system ""Smart Eye TDS"", capable of detecting, counting and measuring the velocity of passing vehicles simultaneously on up to four lanes of a motorway. The system is based on an entirely new bio-inspired wide dynamic Ã‚Â¿silicon retinaÃ‚Â¿ optical sensor. Each of the 128 ÃƒÂ— 128 pixels operates autonomously and delivers asynchronous events representing relative changes in illumination with low latency, high temporal resolution and independence of scene illumination. The resulting data rate is significantly lower and reaction significantly faster than for conventional vision systems. In ADOSE, an FP7 project started 2008 (see acknowledgment at the end of the paper), the sensor will be tested on-board for pre-crash warning and pedestrian protection systems. For safety-related control applications, it is evident that dependability issues are important. Therefore a RAMS analysis was performed with the goal of improving the quality of this new traffic data sensor technology, in particular with respect to reliability and availability. This paper describes the methods used and the results found by applying a RAMS analysis to this specific case of a vision system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5350013,no
Simulation of Target Range Measurement Process by Passive Optoelectronic Rangefinder,2009,"Active rangefinders used for measurement of longer distances of objects (targets), e.g. pulsed laser rangefinders, emit radiant energy, being in conflict with hygienic restrictions in various applications. Having been applied in security and military area there is a serious defect: irradiation can be detected by target. Used passive optoelectronic rangefinders (POERF) can fully eliminate mentioned above defects. The development started initially on a department of Military Academy in Brno (since 2004 University of Defence) in the year 2001 in cooperation with the company OPROX, a.s., Brno. The POERF development resulted in making special tool being able to test individual algorithms required for the target range measurement. It is the Test POERF simulation program, which is permanent created by authors of this contribution. Its 3rd version has just been finalized. This contribution is dealing with the simulation software and covers short comments on the simulation experiments results.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5350107,no
Instruction Precomputation for Fault Detection,2009,"Fault tolerance (FT) is becoming increasingly important in computing systems. This work proposes and evaluates the instruction precomputation technique to detect hardware faults. Applications are profiled off-line, and the most frequent instruction instances with their operands and results are loaded into the precomputation table when executing. The precomputation-based error detection technique is used in conjunction with another method that duplicates all instructions and compares the results. In the precomputation-enabled version, whenever possible, the instruction compares its result with a precomputed value, rather than executing twice. Another precomputation-based scheme does not execute the precomputed instructions at all, assuming that precomputation provides sufficient reliability. Precomputation improves the fault coverage (including permanent and some other faults) and performance of the duplication method. The proposed method is compared to an instruction memoization-based technique. The performance improvements of the precomputation- and memoization-based schemes are comparable, while precomputation has a better long-lasting fault coverage and is considerably cheaper.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5350222,no
Safety criteria and development methodology for the safety critical railway software,2009,"The main part of the system is operated by the software. Besides the safety critical system such as railways, airplanes, and nuclear power plants is applied by the software. The software can perform more varying and highly complex functions efficiently because software can be flexibly designed and implemented. But the flexible design makes it difficult to predict the software failures. We need to show that the safety critical railway software is developed to ensure the safety. This paper is suggested safety criteria and software development methodology to enhance safety for the safety critical railway system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5351957,no
A Multi-Agent Based Service Restoration in Distribution Network with Distributed Generations,2009,"The amount of distributed generation (DG) integrations in the distribution network is increasing. In order to enhance the service reliability, intentional islanding operation and coordinated operations of DG with supporting feeders can be considered to assist service restoration during fault. This paper presents a multi-agent based fault restoration system with and without DG assistance and compares its performance with centralized processing scheme. Network simulation results indicate that for fault detection, isolation and service restoration, a multi-agent system could outperform the centralized processing system. Equipped with adequate synchronizing equipment in the distribution system, coordinated operations of DG and supporting feeders could minimize the un-served load during fault.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5352847,no
On-line Transformer Condition Monitoring through Diagnostics and Anomaly Detection,2009,"This paper describes the end-to-end components of an on-line system for diagnostics and anomaly detection. The system provides condition monitoring capabilities for two in service transmission transformers in the UK. These transformers are nearing the end of their design life, and it is hoped that intensive monitoring will enable them to stay in service for longer. The paper discusses the requirements on a system for interpreting data from the sensors installed on site, as well as describing the operation of specific diagnostic and anomaly detection techniques employed. The system is deployed on a substation computer, collecting and interpreting site data on-line.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5352860,no
An Alternative Pre-Processing Technique Applied to Power Quality Disturbances Identification,2009,"This research presents the development of an alternative pre-processing technique of signals based on the fractal dimension calculation, entropy and signal energy that will be applied to disturbances classification occurring in an electrical power system (EPS). With respect to the power quality disturbances, the voltage sags, voltage swells, oscillatory transients and interruptions were considered for this application. In order to test and validate the proposed technique, a representative database has been obtained through computer simulations of a real EPS using the ATP software. Through windows data and the pre-processing technique proposed, the data were directed to artificial neural network (ANN) architecture to classify the power quality events. The results shown that, combining both the data pre-processing techniques and ANNs, a satisfactory performance of all the proposed methodology can be obtained.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5352934,no
ADEM: Automating deployment and management of application software on the Open Science Grid,2009,"In grid environments, the deployment and management of application software presents a major practical challenge for end users. Performing these tasks manually is error-prone and not scalable to large grids. In this work, we propose an automation tool, ADEM, for grid application software deployment and management, and demonstrate and evaluate the tool on the Open Science Grid. ADEM uses Globus for basic grid services, and integrates the grid software installer Pacman. It supports both centralized Ã‚Â¿prebuildÃ‚Â¿ and on-site Ã‚Â¿dynamic-buildÃ‚Â¿ approaches to software compilation, using the NMI Build and Test system to perform central prebuilds for specific target platforms. ADEM's parallel workflow automatically determines available grid sites and their platform Ã‚Â¿signaturesÃ‚Â¿, checks for and integrates dependencies, and performs software build, installation, and testing. ADEM's tracking log of build and installation activities is helpful for troubleshooting potential exceptions. Experimental results on the Open Science Grid show that ADEM is easy to use and more productive for users than manual operation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5353083,no
A benchmark for fault monitors in distributed systems,2009,"Fault monitoring is one of the main activities of fault tolerant distributed systems. It is required to determine the suspected/crashed component and proactively take the recovery steps to keep the system alive. The main objective of the fault monitoring activity is to quickly and correctly identify the faults. There are many techniques for fault monitoring which have general and specific parameters which influence their performance. In this paper we find the parameters that can help us classify the fault monitoring techniques. We created a benchmark ACI (adaptation, convergence, intelligence) and applied it on current techniques.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5353193,no
Model based predictive Rate Controller for video streaming,2009,"Due to the heterogeneous structure of networks, video streaming over lossy IP networks is very important issues. Infrastructure of the Internet exhibits variable bandwidths, delays, congestions and time-varying packet losses. Video streaming applications should not only have a good end-to-end transport performance but also have a robust rate control, because of variable attributes of the Internet. Furthermore video streaming applications should have a multipath rate allocation mechanism. So for providing the video streaming service quality, some other components such as Bandwidth Estimation and Rate Controller should be taken into consideration. This paper gives an overview of video streaming concept and bandwidth estimation tools and then introduces special architectures for bandwidth adaptive video streaming. A bandwidth estimation algorithm-pathChirp, optimized rate controllers and multipath rate allocation algorithm are considered as all-in-one solution for video streaming problem. This solution is directed and optimized by a decision center which is designed for obtaining the maximum quality at the receiving side.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5355244,no
Investigation on the effectiveness of classifying the voltage sag using support vector machine,2009,"Support vector machine (SVM), which is based on statistical learning theory, is a universal machine learning method. This paper proposes the application of SVM in classifying the causes of voltage sag in power distribution system. Voltage sag is among the major power quality disturbances that can cause substantial loss of product and also can attribute to malfunctions, instabilities and shorter lifetime of the load. Voltage sag can be caused by fault in power system, starting of induction motor and transformer energizing. An IEEE 30 bus system is modeled using the PSCAD software to generate the data for different type of voltage sag namely, caused by fault and starting of induction motor. Feature extraction using the wavelet transformation for the SVM input has been performed prior to the classification of the voltage sag cause. Two kernels functions are used namely radial basis function (RBF) and polynomial function. The minimum and maximum of the wavelet energy are used as the input to the SVM and analysis on the performance of these two kernels are presented. In this paper, it has been found that the polynomial kernel performed better as compared to the RBF in classifying the cause of voltage sag in power system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5356311,no
Dynamic Voltage Restorer based on stacked multicell converter,2009,"Voltage sags are major power quality problems in distribution systems. To overcome these problems, different solutions have been proposed to compensate these sags and protect sensitive loads. As a solution, dynamic voltage restorers (DVRs) have been proposed to provide higher power quality. However, the quality of DVR output voltage, such as THD, is important. So, in this paper a configuration of DVR based on stacked multicell (SM) converter is proposed. The main properties of SM converter, which causes increase in the number of output voltage levels, are transformer-less operation and natural self-balancing of flying capacitors voltages. The proposed DVR consists of a set of series converter and shunt rectifier connected back-to-back. To guarantee the proper operation of shunt rectifier and maintaining the dc link voltage at the desired value, which results in suitable performance of DVR, the DVR is characterized by installing the series converter on the source-side and the shunt rectifier on the load-side. Also, the pre-sag compensation strategy and proposed synchronous reference frame (SRF) based voltage sag detection and reference series injected voltage determination methods are adopted as the control system. The proposed DVR is simulated using PSCAD/EMTDC software and simulation results are presented to validate its effectiveness.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5356441,no
Speeding up motion estimation in modern video encoders using approximate metrics and SIMD processors,2009,"In the past, efforts have been devoted to the amelioration of motion estimation algorithms to speed up motion compensated video coding. Now, efforts are increasingly being directed at exploiting the underlying architecture, in particular, single instruction, multiple data (SIMD) instruction sets. The resilience of motion estimation algorithms to various error metrics allows us to propose new high performance approximate metrics based on the sum of absolute differences (SAD). These new approximate metrics are amenable to efficient branch-free SIMD implementations which yield impressive speed-ups, up to 11:1 in some cases, while sacrificing image quality for less than 0.1 dB on average.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5356471,no
Modeling of large-scale point model,2009,"This paper proposes efficient data structures for point-based rendering and a real-time and high quality rendering algorithm for large-scale point models. As a preprocessing, large-scale point model is subdivided into multiple blocks and a hierarchical structure with minimal bounding box (MBB) property is built for each block. A 3D R-tree index is constructed by those MBB properties. A linear binary tree is created in every block data. During rendering, the model is deal with block by block. Fast view-frustum detection based on respective MBB and 3D R-tree index are first performed to determine invisible data blocks. For visibility detection, this project proposes three algorithms which are back point visibility detection, view point-dependent visibility detection and depth-dependent visibility detection. Visible blocks are then rendered by choosing appropriate rendering model and view point-dependent level-of-detail. For determined level-of-detail, corresponding point geometry is accessed from the 3D R-tree and the linear binary tree (K-D tree). Adaptive distance-dependent rendering is accomplished to select point geometry, yielding better performance without loss of quality. The experiment system is developed in C# program language and CSOpenGL 3D graphic library. The point-cloud data sampled from several great halls of Forbidden City are used in experiment. Experimental results show that our approach can not only design to allow easy access to point data stored in Oracle databases, but also realize real-time rendering for huge datasets in consumer PCs. Those are the grounds for the modeling and computer simulation with point-cloud data.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5357654,no
Investigating the Effect of Refactoring on Software Testing Effort,2009,"Refactoring, the process of improving the design of existing code by changing its internal structure without affecting its external behavior, tends to improve software quality by improving design, improving readability, and reducing bugs. There are many different refactoring methods, each having a particular purpose and effect. Consequently, the effect of refactorings on software quality attribute may vary. Software testing is an external software quality attributes that takes lots of time and effort to make sure that the software performs as intended. In this paper, we propose a classification of refactoring methods based on their measurable effect on software testing effort. This, in turn, helps the software developers decide which refactoring methods to apply in order to optimize a software system with regard to the testing effort.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5358476,no
An Effective Path Selection Strategy for Mutation Testing,2009,"Mutation testing has been identified as one of the most effective techniques, in detecting faults. However, because of the large number of test elements that it introduces, it is regarded as rather expensive for practical use. Therefore, there is a need for testing strategies that will alleviate this drawback by selecting effective test data that will make the technique more practical. Such a strategy based on path selection is reported in this paper. A significant influence on the efficiency associated with path selection strategies is the number of test paths that must be generated in order to achieve a specified level of coverage, and it is determined by the number of paths that are found to be feasible. Specifically, a path selection strategy is proposed that aims at reducing the effects of infeasible paths and conversely developing effective and efficient mutation based tests. The results obtained from applying the method to a set of program units are reported and analysed presenting the flexibility, feasibility and practicality of the proposed approach.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5358804,no
Prioritizing Use Cases to Aid Ordering of Scenarios,2009,"Models are used as the basis for design and testing of software. The unified modeling language (UML) is used to capture and model the requirements of a software system. One of the major requirements of a development process is to detect defects as early as possible. Effective prioritization of scenarios helps in early detection of defects as well maximize effort and utilization of resources. Use case diagrams are used to represent the requirements of a software system. In this paper, we propose using data captured from the primitives of the use case diagrams to aid in prioritization of scenarios generated from activity diagrams. Interactions among the primitives in the diagrams are used to guide prioritization. Customer prioritization of use cases is taken as one of the factors. Preliminary results on a case study indicate that the technique is effective in prioritization of test scenarios.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5358815,no
An Automatic Compliance Checking Approach for Software Processes,2009,"A lot of knowledge has been accumulated and documented in the form of process models, standards, best practices, etc. The knowledge tells how a high quality software process should look like, in other words, which constrains should be fulfilled by a software process to assure high quality software products. Compliance checking for a predefined process against proper constrains is helpful to quality assurance. Checking the compliance of an actual performed process against some constrains is also helpful to process improvement. Manual compliance checking is time-consuming and error-prone, especially for large and complex processes. In this paper, we record the process knowledge by means of process pattern. We provide an automatic compliance checking approach for process models against constrains defined in process patterns. Checking results indicate where and which constrains are violated, and therefore suggests the focuses of future process improvement. We have applied this approach in three real projects and the experimental results are also presented.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5358833,no
Hierarchical Understandability Assessment Model for Large-Scale OO System,2009,"Understanding software, especially in large-scale, is an important issue for software modification. In large-scale software systems, modularization provides help for understanding them. But, even if a system has a well-modularized design, the modular design can be deteriorated by system change over time. Therefore it is needed to assess and manage modularization in the view of understandability. However, there are rarely studies of a quality assessment model for understandability in the module-level. In this paper, we propose a hierarchical model to assess understandability of modularization in large-scale object-oriented software. To assess understandability, we define several design properties, which capture the characteristics influencing on understandability, and design metrics based on the properties, which are used to quantitatively assess understandability. We validate our model and its usefulness by applying the model to an open-source software system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5358881,no
Improved Fusion Machine Based on T-norm Operators for Robot Perception,2009,"Map reconstruction for autonomous mobile robots navigation needs to deal with uncertainties, imprecisions and even imperfections due to the limited sensors quality and knowledge acquisition. An improved fusion machine is proposed by replacing the classical conjunctive operator with T-norm operator in Dezert-Smarandache Theory (DSmT)framework for building grid map using noisy sonar measurements. An experiment using a Pioneer II mobile robot with 16 sonar detectors on board is done in a small indoor environment, and a 2D Map is built online with our self-developing software platform. A quantitative comparison of the results of our new method for map reconstruction with respect to the classical fusion machine is proposed. We show how the new approach developed in this work outperforms the classical one.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5359114,no
A FPGA-Based Reconfigurable Software Architecture for Highly Dependable Systems,2009,"Nowadays, systems-on-chip are commonly equipped with reconfigurable hardware. The use of hybrid architectures based on a mixture of general purpose processors and reconfigurable components has gained importance across the scientific community allowing a significant improvement of computational performance. Along with the demand for performance, the great sensitivity of reconfigurable hardware devices to physical defects lead to the request of highly dependable and fault tolerant systems. This paper proposes an FPGA-based reconfigurable software architecture able to abstract the underlying hardware platform giving an homogeneous view of it. The abstraction mechanism is used to implement fault tolerance mechanisms with a minimum impact on the system performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5359382,no
Automated Model Checking of Stochastic Graph Transformation Systems,2009,"Non-functional requirements like performance and reliability play a prominent role in distributed and dynamic systems. To measure and predict such properties using stochastic formal methods is crucial. At the same time, graph transformation systems are a suitable formalism to formally model distributed and dynamic systems. Already, to address these two issues, Stochastic Graph Transformation Systems (SGTS) have been introduced to model dynamic distributed systems. But most of the researches so far are concentrated on SGTS as a modeling means without considering the need for suitable analysis tools. In this paper, we present an approach to verify this kind of graph transformation systems using PRISM (a stochastic model checker). We translate the SGTS to the input language of PRISM and then PRISM performs the model checking and returns the results back to the designers.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5359771,no
Real Time Optical Network Monitoring and Surveillance System,2009,"Optical diagnosis, performance monitoring, and characterization are essential for achieving high performance and ensuring high quality of services (QoS) of any efficient and reliable optical access network. In this paper, we proposed a practical in-service transmission surveillance scheme for passive optical network (PON). The proposed scheme is designed and simulated by using the OptiSystem CAD program with the system sensitivity - 35 dBm. A real time optical network monitoring software tool named smart access network _ testing, analyzing and database (SANTAD) is introduced for providing remote controlling, centralized monitoring, system analyzing, and fault detection features in large scale network infrastructure management and fault diagnostic. 1625 nm light source is assigned to carry the troubleshooting signal for line status monitoring purposes in the live network system. Three acquisition parameters of optical pulse: distance range, pulse width, and acquisition time, is contributed to obtain the high accuracy in determining the exact failure location. The lab prototype of SANTAD was implemented in the proposed scheme for analyzing the network performance. The experimental results showed SANTAD is able to detect any occurrence of fault and address the failure location within 30 seconds. The main advantages of this work are to manage network efficiently, reduce hands on workload, minimize network downtime and rapidly restore failed services when problems are detected and diagnosed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5360160,no
Two Heads Better Than One: Metric+Active Learning and its Applications for IT Service Classification,2009,"Large IT service providers track service requests and their execution through problem/change tickets. It is important to classify the tickets based on the problem/change description in order to understand service quality and to optimize service processes. However, two challenges exist in solving this classification problem: 1) ticket descriptions from different classes are of highly diverse characteristics, which invalidates most standard distance metrics; 2) it is very expensive to obtain high-quality labeled data. To address these challenges, we develop two seemingly independent methods 1) discriminative neighborhood metric learning (DNML) and 2) active learning with median selection (ALMS), both of which are, however, based on the same core technique: iterated representative selection. A case study on real IT service classification application is presented to demonstrate the effectiveness and efficiency of our proposed methods.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5360350,no
A Complexity Reliability Model,2009,"A model of software complexity and reliability is developed. It uses an evolutionary process to transition from one software system to the next, while complexity metrics are used to predict the reliability for each system. Our approach is experimental, using data pertinent to the NASA satellite systems application environment. We do not use sophisticated mathematical models that may have little relevance for the application environment. Rather, we tailor our approach to the software characteristics of the software to yield important defect-related predictors of quality. Systems are tested until the software passes defect presence criteria and is released. Testing criteria are based on defect count, defect density, and testing efficiency predictions exceeding specified thresholds. In addition, another type of testing efficiency - a directed graph representing the complexity of the software and defects embedded in the code - is used to evaluate the efficiency of defect detection in NASA satellite system software. Complexity metrics were found to be good predictors of defects and testing efficiency in this evolutionary process.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5361943,no
Wavelet-Based Approach for Estimating Software Reliability,2009,"Recently, wavelet methods have been frequently used for not only multimedia information processing but also time series analysis with high speed and accuracy requirements. In this paper we apply the wavelet-based techniques to estimate software intensity functions in non-homogeneous Poisson process based software reliability models. There are two advantages for use of the wavelet-based estimation; (i) it is a non-parametric estimation without specifying a parametric form of the intensity function under any software debugging scenario, (ii) the computational overhead arising in statistical estimation is rather small. Especially, we apply two kinds of data transforms, called Anscombe transform and Fisz transform, and four kinds of thresholding schemes for empirical wavelet coefficients, to non-parametric estimation of software intensity functions. In numerical validation test with real software fault data, we show that our wavelet-based estimation method can provide higher goodness-of-fit performances than the conventional maximum likelihood estimation and the least squares estimation in some cases, in spite of its non-parametric nature.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5362077,no
Optimal Adaptive System Health Monitoring and Diagnosis for Resource Constrained Cyber-Physical Systems,2009,"Cyber-physical systems (CPS) are complex net-centric hardware/software systems that can be applied to transportation, healthcare, defense, and other real-time applications. To meet the high reliability and safety requirements for these systems, proactive system health monitoring and management (HMM) techniques can be used. However, to be effective, it is necessary to ensure that the operation of the underlying HMM system does not adversely impact the normal operation of the system being monitored. In particular, it must be ensured that the operation of the HMM system will not lead to resource contentions that may prevent the system being monitored from timely completion of critical tasks. This paper presents an adaptive HMM system model that defines the fault diagnosis quality metrics and supports diagnosis requirement specifications. Based on the model, the sensor activation decision problem (SADP) is defined along with a steepest descent based heuristic algorithm to make the HMM configuration decisions that best satisfy the diagnosis quality requirements. Evaluation results show that the technique reduces the overall system resource consumption without adversely impacting the diagnosis capability of the HMM.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5362083,no
Approximating Deployment Metrics to Predict Field Defects and Plan Corrective Maintenance Activities,2009,"Corrective maintenance activities are a common cause of schedule delays in software development projects. Organizations frequently fail to properly plan the effort required to fix field defects. This study aims to provide relevant guidance to software development organizations on planning for these corrective maintenance activities by correlating metrics that are available prior to release with parameters of the selected software reliability model that has historically best fit the product's field defect data. Many organizations do not have adequate historical data, especially historical deployment and field usage information. The study identifies a set of metrics calculable from available data to approximate these missing predictor categories. Two key metrics estimable prior to release surfaced with potentially useful correlations, (1) the number of periods until the next release and (2) the peak deployment percentage. Finally, these metrics were used in a case study to plan corrective maintenance efforts on current development releases.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5362089,no
Variance Analysis in Software Fault Prediction Models,2009,"Software fault prediction models play an important role in software quality assurance. They identify software subsystems (modules,components, classes, or files) which are likely to contain faults. These subsystems, in turn, receive additional resources for verification and validation activities. Fault prediction models are binary classifiers typically developed using one of the supervised learning techniques from either a subset of the fault data from the current project or from a similar past project. In practice, it is critical that such models provide a reliable prediction performance on the data not used in training. Variance is an important reliability indicator of software fault prediction models. However, variance is often ignored or barely mentioned in many published studies. In this paper, through the analysis of twelve data sets from a public software engineering repository from the perspective of variance, we explore the following five questions regarding fault prediction models: (1) Do different types ofclassification performance measures exhibit different variance? (2) Does the size of the data set imply a more (or less) accurate prediction performance? (3) Does the size of training subset impact model's stability? (4) Do different classifiers consistently exhibit different performance in terms of model's variance? (5) Are there differences between variance from 1000 runs and 10 runs of 10-fold cross validation experiments? Our results indicate that variance is a very important factor in understanding fault prediction models and we recommend the best practice for reporting variance in empirical software engineering studies.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5362090,no
Putting It All Together: Using Socio-technical Networks to Predict Failures,2009,"Studies have shown that social factors in development organizations have a dramatic effect on software quality. Separately, program dependency information has also been used successfully to predict which software components are more fault prone. Interestingly, the influence of these two phenomena have only been studied separately. Intuition and practical experience suggests,however, that task assignment (i.e. who worked on which components and how much) and dependency structure (which components have dependencies on others)together interact to influence the quality of the resulting software. We study the influence of combined socio-technical software networks on the fault-proneness of individual software components within a system. The network properties of a software component in this combined network are able to predict if an entity is failure prone with greater accuracy than prior methods which use dependency or contribution information in isolation. We evaluate our approach in different settings by using it on Windows Vista and across six releases of the Eclipse development environment including using models built from one release to predict failure prone components in the next release. We compare this to previous work. In every case, our method performs as well or better and is able to more accurately identify those software components that have more post-release failures, with precision and recall rates as high as 85%.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5362091,no
Fault Tree Analysis of Software-Controlled Component Systems Based on Second-Order Probabilities,2009,"Software is still mostly regarded as a black box in the development process, and its safety-related quality ensured primarily by process measures. For systems whose lion share of service is delivered by (embedded) software, process-centred methods are seen to be no longer sufficient. Recent safety norms (for example, ISO 26262) thus prescribe the use of safety models for both hardware and software. However, failure rates or probabilities for software are difficult to justify. Only if developers take good design decisions from the outset will they achieve safety goals efficiently. To support safety-oriented navigation of the design space and to bridge the existing gap between qualitative analyses for software and quantitative ones for hardware, we propose a fault-tree-based approach to the safety analysis of software-controlled systems. Assigning intervals instead of fixed values to events and using Monte-Carlo sampling, probability mass functions of failure probabilities are derived. Further analysis of PMF lead to estimates of system quality that enable safety managers to take an optimal choice between design alternatives and to target cost-efficient solutions in every phase of the design process.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5362097,no
Metrics for Evaluating Coupling and Service Granularity in Service Oriented Architecture,2009,"Service oriented architecture (SOA) is becoming an increasingly popular architectural style for many organizations due to the promised agility, flexibility benefits. Although the concept of SOA has been described in research and industry literature, there are currently few SOA metrics designed to measure the appropriateness of service granularity and service coupling between services and clients. This paper defines the metrics centered around service design principles concerning loosely-coupled and well-chosen granularity. The metrics are based on information-theoretic principles, and are used to predict the quality of the final software product. The usefulness of the metrics is illustrated through a case study.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5362767,no
Wood Nondestructive Test Based on Artificial Neural Network,2009,"It is important to detect defects in wood, when it reduce the performance. The data and signal processing technology providing researchers with more damage identification problem solution ideas and methods. This article explore the wavelet analysis and artificial neural network for the wood defects based on non-destructive testing, and build an artificial neural network model for wood non-destructive testing technology. After wavelet packet decomposition to extract the different frequency bands of energy levels characteristic of the signal, as the neural network input samples, the network training and learning. Training of the BP network model can be achieved on the different locations automatic recognition of defects, defects of the middle of more than 90% recognition rate on the left and right side of the recognition rate of over 80%.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5363106,no
A Novel Method for the Fingerprint Image Quality Evaluation,2009,"The performance of automatic fingerprint identification system relies heavily on the quality of the captured fingerprint images. A novel method for fingerprint image quality analysis has been presented, which overcomes the shortcoming most of existing methods have, considering the correlation of each quality feature as linear and paying no attention to the clarity of local texture. In this paper, ten features are extracted from the fingerprint image and then Fuzzy Relation Classifier is trained to classify the fingerprint images, which includes the unsupervised clustering and supervised classification to care more about the revelation of the data structure than other classifiers. Experimental results show that the proposed method has a good performance in evaluating the quality of the fingerprint images.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5363327,no
Evaluation of Text Clustering Based on Iterative Classification,2009,"Text clustering is a useful and inexpensive way to organize vast text repositories into meaningful topics categories. Although text clustering can be seen as an alternative to supervised text categorization, the question remains of how to determine if the resulting clusters are of sufficient quality in a real-life application. However, it is difficult to evaluate a given clustering of documents. Furthermore, the existing quality measures rely on the labor standard, which is difficult and time-consuming. The need for fair methods that can assess the validation of clustering results is becoming more and more critical. In this paper, we propose and experiment an innovative evaluation measure that allows one to effectively and correctly assess the clustering results.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5364099,no
Application of Entire Synchronization in Frequency Measurement for SAW Sensors,2009,"The high precision measure frequency is a key technology in SAW gas sensors, because the output of SAW CO gas sensors is frequency. This paper takes the SAW CO gas sensor as an example. Based on the analyzing currently method of measure frequency, the principle of high quality measure frequency for SAW CO gas sensors is brought forward with the aid of the entire synchronized mechanism. And the system structure of measure frequency and electronic element are presented.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5364442,no
Software Reliability Prediction Based on Discrete Wavelet Transform and Neural Network,2009,"Effective prediction of the software reliability is one of the active pares of software engineering. This paper proposes a novel approach based on wavelet transform and neural network (NN). Using this approach, the time series of software faults can be decomposed into four components information, and then predict them by NN respectively. The experience results show that the performance of novel software reliability prediction approach is satisfactory.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5364584,no
Exploring Software Quality Classification with a Wrapper-Based Feature Ranking Technique,2009,"Feature selection is a process of selecting a subset of relevant features for building learning models. It is an important activity for data preprocessing used in software quality modeling and other data mining problems. Feature selection algorithms can be divided into two categories, feature ranking and feature subset selection. Feature ranking orders the features by a criterion and a user selects some of the features that are appropriate for a given scenario. Feature subset selection techniques search the space of possible feature subsets and evaluate the suitability of each. This paper investigates performance metric based feature ranking techniques by using the multilayer perceptron (MLP) learner with nine different performance metrics. The nine performance metrics include overall accuracy (OA), default F-measure (DFM), default geometric mean (DGM), default arithmetic mean (DAM), area under ROC (AUC), area under PRC (PRC), best F-measure (BFM), best geometric mean (BGM) and best arithmetic mean (BAM). The goal of the paper is to study the effect of the different performance metrics on the feature ranking results, which in turn influences the classification performance. We assessed the performance of the classification models constructed on those selected feature subsets through an empirical case study that was carried out on six data sets of real-world software systems. The results demonstrate that AUC, PRC, BFM, BGM and BAM as performance metrics for feature ranking outperformed the other performance metrics, OA, DFM, DGMand DAM, unanimously across all the data sets and therefore are recommended based on this study. In addition, the performances of the classification models were maintained or even improved when over 85 percent of the features were eliminated from the original data sets.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5364717,no
Comparison of Two Fitness Functions for GA-Based Path-Oriented Test Data Generation,2009,"Automatic path-oriented test data generation is not only a crucial problem but also a hot issue in the research area of software testing today. As a robust metaheuritstic search method in complex spaces, genetic algorithm (GA) has been used to path-oriented test data generation since 1992 and outperforms other approaches. A fitness function based on branch distance (BDBFF) and another based on normalized extended Hamming distance (SIMILARITY) are both applied in GA-based path-oriented test data generation. To compare performance of these two fitness functions, a triangle classification program was chosen as the example. Experimental results show that BDBFF-based approach can generate path-oriented test data more effectively and efficiently than SIMILARITY- based approach does.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5365103,no
A Multi-instance Model for Software Quality Estimation in OO Systems,2009,"In this paper, a problem of object-oriented (OO) software quality estimation is investigated with a multi-instance (MI) perspective. In detail, each set of classes that have inheritance relation, named `class hierarchy', is regarded as a bag in the training, while each class in the bag is regarded as an instance. The task of the software quality estimation in this study is to predict the label of unseen bags, i.e. the fault-proneness of untested class hierarchies. It is stipulated that a fault-prone class hierarchy contains at least one fault-prone (negative) class, while a not fault-prone (positive) one has no negative class. Based on the modification records (MR) of previous project releases and OO software metrics, the fault-proneness of untested class hierarchy can be predicted. A MI kernel specifically designed for MI data was utilized to build the OO software quality prediction model. This model was evaluated on five datasets collected from an industrial optical communication software project. Among the MI learning algorithms applied in our empirical study, the support vector algorithms combined with dedicated MI kernel led others in accurately and correctly predicting the fault-proneness of the class hierarchy.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5365451,no
High-Dimensional Software Engineering Data and Feature Selection,2009,"Software metrics collected during project development play a critical role in software quality assurance. A software practitioner is very keen on learning which software metrics to focus on for software quality prediction. While a concise set of software metrics is often desired, a typical project collects a very large number of metrics. Minimal attention has been devoted to finding the minimum set of software metrics that have the same predictive capability as a larger set of metrics - we strive to answer that question in this paper. We present a comprehensive comparison between seven commonly-used filter-based feature ranking techniques (FRT) and our proposed hybrid feature selection (HFS) technique. Our case study consists of a very high-dimensional (42 software attributes) software measurement data set obtained from a large telecommunications system. The empirical analysis indicates that HFS performs better than FRT; however, the Kolmogorov-Smirnov feature ranking technique demonstrates competitive performance. For the telecommunications system, it is found that only 10% of the software attributes are sufficient for effective software quality prediction.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5365755,no
Performance Appraisal System for Academic Staff in the Context of Digital Campus of Higer Education Institutions: Design and Implementation,2009,"Academic staff at higher education institutions are crucial for quality education and research, the core competiveness of a top research-oriented university. Efforts have been made at many universities to work on a system to effectively and accurately assess the performance of academic staff to promote the overall teaching and research activities. The paper presents an appraisal information system developed for School of Economics and Management of Beihang University, which has taken into consideration the contextual factors of research-oriented universities. The appraisal information system, based on service-oriented architecture, is characterized by a multi-layer system in digital campus. The implementation of the system has significantly promoted the overall teaching and research quality at the school.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5365938,no
An Improving Approach for Recovering Requirements-to-Design Traceability Links,2009,"Requirement tracing is an important activity for its helpfulness to effective system quality assurance, impact analyzing of changes and software maintenance. In this paper, we propose an automatic approach called LGRTL to recover traceability links between high-level requirements and low-level design elements. This approach treats the recovery process as Bayesian classification process. Meanwhile, we add a synonym process to the preprocessing phase, and improve the Bayesian model for performing better. To evaluate the validity of the method, we perform a case study and the experimental results show that our method can enhance the effect to a certain extent.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5366024,no
An Efficient Forward Recovery Checkpointing Scheme in Dissimilar Redundancy Computer System,2009,"Roll-forward checkpointing schemes (RFCS) are developed in order to avoid rollback in the presence of independent faults and increase the possibility that a task completes within a tight deadline. But the assumption of RFCS does not exist in most time. Run the same software on the same hardware may result in correlated faults. Another question is these RFCS schemes may lose useful build-in self detection information results in performance degradation. In this paper, we propose a twice dissimilar redundancy computer based roll-forward recovery scheme (TDCS) that can avoid the correlated faults and realize fault-tolerance, without extra process. At last we use a novel technique based on a Markov reward model, to reveal our TDCS performance is quite better than the RFCS in average completion time when build-in self detection coverage be high.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5366252,no
Software Reliability Growth Models Based on Non-Homogeneous Poisson Process,2009,"Non-homogeneous Poisson process (NHPP) model with typical reliability growth patterns is an important technology about evaluating software reliability. Two parameters which affect software reliability are original failures number and failure-detected rate. The paper firstly defines the software failure distributed and discusses a few kinds' software reliability models. Non-homogeneous Poisson process model is presented. The models with different parameters are discussed and analyzed. The model's restrict condition and some of parameters, original failures number and failure-detected ratio, are extrapolated. Assessment methodology about key parameters is given.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5366273,no
Memory Leak Dynamic Monitor Based On HOOK Technique,2009,"Memory leak can cause performance decrease or even breakdown of computer system. According to the unavailability of COM component, this paper analyses COM component's memory leak mechanism, propose a testing architecture and provide a memory leak detection method based on HOOK technique. This method can locate functions which cause memory leak and get details of the leaking process. The experiment shows that the memory leak faults can be triggered and monitored effectively.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5366368,no
A Kind of Novel Constrained Multipath Routing Algorithm in Mobile Ad Hoc Networks,2009,Ad hoc network is a collection of wireless mobile nodes dynamically forming a temporary network without the use of any existing network infrastructure or centralized administration.. This paper proposes a novel constrained entropy-based multipath routing algorithm in adhoc (NCEMR). The key idea of NCEMR algorithm is to construct the new metric-entropy and select the stability path with the help of entropy metric to reduce the number of route reconstruction so as to provide QoS guarantee in the ad hoc network. It is typically proposed in order to increase the reliability of data transmission or to provide load balancing. The simulation results show that the proposed approach and parameters provide an accurate and efficient method of estimating and evaluating the route stability in dynamic ad hoc.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5366598,no
Image Denoising Using Information Measure and Support Vector Machines,2009,"Image denoising is one of important steps in a number of image processing applications. However, available methods mainly present by conducting filter of restoration on whole observation image, resulting in many image detail information have been lost. So, how to obtain the balance of remove noises from the smooth regions and preserved more image detail at high frequency regions have still worth to pay more attention. It is presents a novel approaches that can improve image quality by reducing corrupted pixels, but leave good pixels unchanged. First, information measure method is introduced to extract noise features from observation image. And then, a support vector machines (SVM) based classifier which is employed to divided noise corrupted image into noise candidates pixels and good pixels, so that a noise map is generated that can be used to guide the mixed mean and media filter (MMMF), which is designed to conduct restoration filter just for corrupted pixels. Three typical numerical experimental are reported and results show that the proposed algorithm can achieve better performance both on vision effect and a higher mark on objective criterion (peak signal and noise ratio, PSNR).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5366628,no
Post-Forecast OT: A Novel Method of 3D Model Compression,2009,"In this paper, we present Post-Forecast OT (Octree), a novel method of 3D model compression codec based on Octree. Vertices of 3D meshes are re-classified according to the Octree rule. All the nodes of the Octree are statistically analyzed to identify the type of nodes with the max proportion and are encoded with fewer bits. The vertices positions are predicted and recorded, which can effectively reduce the error between the decoded vertices and the corresponding vertices in the original 3D model. Compared with prior 3D model progressive codec method with severe distortion at low bit rates, Post-Forecast OT has better performance while providing a pleasant visual quality. We also encode the topology and attribute information corresponding to the geometry information, which enables progressive transmission of all encoded 3D data over the Internet.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5366715,no
Fingerprint Chromatogram and Fuzzy Calculation for Quality Control of Shenrong Tonic Wine,2009,"With computer software and fuzzy calculation, we determined several key compounds and established fingerprint to control the stability of food products. High performance liquid chromatography (HPLC) was used to establish the fingerprint chromatogram of Shenrong tonic wine to control its quality and stability, and to detect possible counterfeits. We used reverse phase C-18 column, equivalent elution and detection wavelength at 259 nm. The chromatographic fingerprint was established by using sample chromatography of 10 different production batches to calculate the relative retention time Ã‚Â¿ and the relative area Ar of each peak respectively, and 11 peaks of common characteristic had been found. Cosine method was used to calculate the similarity by which the comparative study was done on various batches. The results showed that the method was convenient and applicable for the quality evaluation of Chinese herb tonic wine.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5366783,no
Parameters Tuning of Fuzzy Controller for Rotated Pendulum Based on Improved Particle Swarm Optimization,2009,"The Particle Swarm Optimization (PSO) technique, which refines its search by attracting the particles to positions with good solutions, has ever since turned out to be a competitor in the field of numerical optimization. The PSO can generate high-quality solutions within shorter computation time and have more stable convergence charactoristic than other stochastic methods. In this article, an improved Particle Swarm Optimization is used to tune the parameters of fuzzy-PID controller. A new method to estimate scaling factors of the fuzzy PID controller for the rotated inverted pendulum is investigated. The performance of the fuzzy PID controller is sensitive to the variety of scaling factors. The design procedure dwells on the use of particle swarm optimization and estimate algorithm. The tuning of the scaling factors of the fuzyy PID controller is essential to the entire optimization process. And the scaling factors of the fuzyy PID controller is estimated by means of regression polynomials. Numerical studies verify the effectivenss.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5367001,no
Automatic Test Data Generation Based on Ant Colony Optimization,2009,Software testing is a crucial measure used to assure the quality of software. Path testing can detect bugs earlier because of it performs higher error coverage. This paper presents a model of generating test data based on an improved ant colony optimization and path coverage criteria. Experiments show that the algorithm has a better performance than other two algorithms and improve the efficiency of test data generation notably.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5367156,no
Using Probabilistic Model Checking to Evaluate GUI Testing Techniques,2009,"Different testing techniques are being proposed in software testing to improve systems quality and increase development productivity. However, it is difficult to determine from a given set of testing techniques, which is the most effective testing technique for a certain domain, particularly if they are random-based. We are proposing a strategy and a framework that can evaluate such testing techniques. Our framework is defined compositionally and parametric ally. This allows us to characterize different aspects of systems in an incremental way as well as test specific hypothesis about the system under test. In this paper we focus on GUI-based systems. That is, the specific internal behavior of the system is unknown but it can be approximated by probabilistic behaviors. And the empirical evaluation is based on the probabilistic model checker PRISM.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5368109,no
Quantitative RAS Comparison of Sun CMT/Solaris and X86/Linux Servers,2009,"By incorporating the chip multi-threading (CMT) and operating system predictive self-healing technologies, the Sun CMT/Solaris based servers are not only cost/performance effective, but also more robust in reliability, availability, and serviceability (RAS) than the X86/Linux servers with similar performance. The differentiators include higher levels of hardware integration, more fault tolerance provisions in processors, the Solaris memory page retirement (MPR), and the Solaris/SPARC processor offlining (PO) capabilities of the CMT/Solaris server. This study applies analytical models, with parameters calibrated by field experience, to quantitatively compare system RAS, against hardware faults, between the CMT/Solaris and X86/Linux servers. The results show significant RAS benefits of the CMT, MPR, and PO technologies.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5368203,no
Fault Injection Scheme for Embedded Systems at Machine Code Level and Verification,2009,"In order to evaluate software from the third party whose source codes are not available, after a careful analysis of the statistic data sorted by orthogonal defect classification, and the corresponding relation between patterns of high level language programs and machine codes, we propose a fault injection scheme at machine code level suitable respectively to the IA32 ARM and MIPS architecture, which takes advantage of mutating machine code. To prove the feasibility and validity of this scheme, two sets of programs are chosen as our experimental target: Set I consists of two different versions of triangle testing algorithms, and Set II is a subset of the Mibench which is a collection of performance benchmark programs designed for embedded systems; we inject both high level faults into the source code written in C language and the corresponding machine code level faults directly into the executables, and monitor their running on Linux. The results from experiments show that at least 96% of total similarity degree is obtained. Therefore, we conclude that the effect of injecting corresponding faults on both the source code level and machine code level are mostly the same. Therefore, our scheme is rather useful in analyzing system behavior under faults.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5368226,no
Evaluating the Use of Reference Run Models in Fault Injection Analysis,2009,"Fault injection (FI) has been shown to be an effective approach to assessing the dependability of software systems. To determine the impact of faults injected during FI, a given oracle is needed. Oracles can take a variety of forms, including (i) specifications, (ii) error detection mechanisms and (iii) golden runs. Focusing on golden runs, in this paper we show that there are classes of software which a golden run based approach can not be used to analyse. Specifically, we demonstrate that a golden run based approach can not be used in the analysis of systems which employ a main control loop with an irregular period. Further, we show how a simple model, which has been refined using FI experiments, can be employed as an oracle in the analysis of such a system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5368242,no
Mining Rank-Correlated Associations for Recommendation Systems,2009,"Recommendation systems, best known for their use in e-commerce or social network applications, predict users' preferences and output item suggestions. Modern recommenders are often faced with many challenges, such as covering high volume of volatile information, dealing with data sparsity, and producing high-quality results. Therefore, while there are already several strategies of this category, some of them can still be refined. Association rules mining is one of the widely applied techniques of recommender implementation. In this paper, we propose a tuned method, trying to overcome some defects of existing association rules based recommendation systems by exploring rank correlations. It builds a model for preference prediction with the help of rank correlated associations on numerical values, where traditional algorithms of such kind would choose to do discretization. An empirical study is then conducted to see the efficiency of our method.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5368358,no
Trustworthy Evaluation of a Safe Driver Machine Interface through Software-Implemented Fault Injection,2009,"Experimental evaluation is aimed at providing useful insights and results that constitute a confident representation of the system under evaluation. Although guidelines and good practices exist and are often applied, the uncertainty of results and the quality of the measuring system is rarely discussed. To complement such guidelines and good practices in experimental evaluation, metrology principles can contribute in improving experimental evaluation activities by assessing the measuring systems and the results achieved. In this paper we present the experimental evaluation by software-implemented fault injection of a safe train-borne driver machine interface (DMI), to evaluate its behavior in presence of faults. The measuring system built for the purpose and the results obtained on the assessment of the DMI are scrutinized along basic principles of metrology and good practices of fault injection. Trustfulness in results has been estimated satisfactory and the experimental campaign has shown that the safety mechanisms of the DMI correctly identify the faults injected and that a proper reaction is executed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5368539,no
A New Algorithm for Fabric Defect Detection Based on Image Distance Difference,2009,"This paper brings forward a new method of detection of fabric defect, namely image distance difference arithmetic. The system permit user to set appropriate control parameter of fabric defect defection based on the type of the fabric. It can detect more than 30 kinds of common defects, which has advantages of high identification correctness and fast inspection speed. Finally, using some image processing technology to score the grade of piece to satisfy the quality and elevate the finished product ratio.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5369042,no
Zapmem: A Framework for Testing the Effect of Memory Corruption Errors on Operating System Kernel Reliability,2009,"While monolithic operating system kernels are composed of many subsystems, during runtime they all share a common address space, making fault propagation a serious issue. The code quality of each subsystem is different, as OS development is a complex task commonly divided by different groups with different degrees of expertise. Since the memory space into which this code runs is shared, the occurrence of bugs or errors in one of the subsystems may propagate to others and affect general OS reliability. It is necessary, then, to test how errors propagate between the different kernel subsystems and how they affect reliability. This work presents a simple new technique to inject memory corruption faults and Zapmem, a fault injection tool which uses such technique to test the effect on reliability from memory corruption of statically allocated kernel data. Zapmem associates the runtime memory addresses to the corresponding high level (source code) memory structure definitions, which indicate which kernel subsystem allocated that memory region, and the tool has minimal intrusiveness, as our technique does not require kernel instrumentation. The efficacy of our approach and preliminary results are also presented.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5369146,no
A Performance Monitoring Tool for Predicting Degradation in Distributed Systems,2009,"Continuous performance monitoring is critical for detecting software aging and enabling performance tuning. In this paper we design and develop a performance monitoring system called PerfMon. It makes use of the /proc virtual file system's kernel-level mechanisms and abstractions in Linux-based operating system, which provides the building blocks for implementation of efficient, scalable and multi-level performance monitoring. Using PerfMon, we show that (1) monitoring functionality can be customized according to clients' requirements, (2) by filtering of monitoring information, the trade-offs can be attained between the quality of the information monitored and the associated overheads, and (3) by performing monitoring at application-level, we can predict software aging by taking into account the multiple resources used by applications. Finally, we evaluate PerfMon by experiments.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5369361,no
A New Strategy for Pairwise Test Case Generation,2009,"Pairwise testing has become an important approach to software testing because it often provides effective error detection at low cost, and a key problem of it is the test case generation method. As the part of an effort to develop an optimized strategy for pairwise testing, this paper proposes an efficient pairwise test case generation strategy, called VIPO (Variant of In-Parameter-order), which is a variant of IPO strategy. We compare its effectiveness with some existing strategies including IPO, Tconfig, Pict and AllPairs. Experimental results demonstrate that VIPO outperformed them in terms of the number of generated test case within reasonable execution times, in most cases.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5369430,no
Application of Particle Swarm Optimization and RBF Neural Network in Fault Diagnosis of Analogue Circuits,2009,"BP neural network has the shortcoming of over-fitting, local optimal solution, which affects the practicability of BP neural network. RBF neural network is a feedforward neural network, which has the global optimal closing ability. However, the parameters in RBF neural network need determination. Particle swarm optimization is presented to choose the parameters of RBF neural network. The particle swarm optimization-RBF neural network method has high classification performance, and is applied to fault diagnosis of analogue circuits. Finally, the result of fault diagnosis cases shows that the particle swarm optimization - RBF neural network method has higher classification than BP neural network.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5369551,no
Chinese Named Entity Recognition with Inducted Context Patterns,2009,"Since whether or not a word is a name is determined mostly by the context of the word, the context pattern induction plays an important role in name entity recognition (NER). We present a NER method based on the context pattern induction. It induces high-precision context patterns in an unsupervised way starting with some entity seeds. Then it uses directly the matched context patterns, instead of extracted entities by inducted patterns, as the features of a CRF-based NER model. The experiments show that the proposed method improves the performance of the high quality named entity recognizer, and achieves higher accuracy and recall rate.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5370151,no
Design and Realization of Feeder Terminal Unit Based on DSP,2009,"Distribution automation is an effective means to improve the reliability and energy quality of power supply. Feeder automation (FA) is one of the important contents in the distribution automation, and feeder terminal unit (FTU) is the key intelligent equipment and basic control unit of feeder automation. In recent years, with the rapid development of detection and embedded technology, higher request on reliability and accuracy of FTU is put forward. In this paper, based on the design schemes of lots of manufacturers, a FTU using DSP as the hardware core is designed. It uses AC sampling theory to measure electrical network's voltage and current, uses AC sampling value to calculate virtual value of voltage and current, power, power factor and other electrical parameters, and has several wave record functions. The software is based on uC/OS-II, enhances the system's modularization and expansibility.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5370183,no
An Improved Spatial Error Concealment Algorithm Based on H.264,2009,"The losses of packets are inevitable when the video is transported over error-prone networks. Error concealment methods can reduce the quality degradation of the received video by masking the effects of such errors. This paper presents a novel spatial error concealment algorithm based on the directional entropy in the available neighboring Macro Blocks (MBs), which can adaptively switch between weighted pixel average (WPA) adopted in H.264 and an improved directional interpolation algorithm to recover the lost MBs. In this work, the proposed algorithm was evaluated on H.264 reference software JM8.6. The illustrative examples demonstrate that the proposed method can achieve better Peak to Signal-to-Noise Ratio (PSNR) performance and visual quality, compared with WPA and the conventional directional interpolation algorithm respectively.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5370364,no
Side-Channel Attacks on Cryptographic Software,2009,"When it comes to cryptographic software, side channels are an often-overlooked threat. A side channel is any observable side effect of computation that an attacker could measure and possibly influence. Crypto is especially vulnerable to side-channel attacks because of its strict requirements for absolute secrecy. In the software world, side-channel attacks have sometimes been dismissed as impractical. However, new system architecture features, such as larger cache sizes and multicore processors, have increased the prevalence of side channels and quality of measurement available to an attacker. Software developers must be aware of the potential for side-channel attacks and plan appropriately.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5370703,no
Monitoring Workflow Applications in Large Scale Distributed Systems,2009,"This paper presents the design, implementation and testing of the monitoring solution created for integration with a workflow execution platform. The monitoring solution constantly checks the system evolution in order to facilitate performance tuning and improvement. Monitoring is accomplished at application level, by monitoring each job from each workflow and at system level, by aggregating state information from each processing node. The solution also computes aggregated statistics that allow an improvement to the scheduling component of the system, with which it will interact. The improvement on the performance of distributed application is obtained using the realtime information to compute estimates of runtime which are used to improve scheduling. Another contribution is an automated error detection systems, which can improve the robustness of grid by enabling fault recovery mechanisms to be used. These aspects can benefit from the particularization of the monitoring system for a workflow-based application: the scheduling performance can be improved through better runtime estimation and the error detection can automatically detect several types of errors. The proposed monitoring solution could be used in the SEEGRID project as a part of the satellite image processing engine that is being built.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5370938,no
An empirical analysis for evaluating the link quality of robotic sensor networks,2009,"This paper presents a comprehensive metric to evaluate the link quality for the distributed control of robotic swarms to maintain the communication links. The mobile robots dynamically reconfigure themselves to maintain reliable end-to-end communication links. Such applications require online measurements of communication link quality in real-time and require a connection between link quality and robot positions. In this paper, we present the empirical results and analysis of a link variability study for an indoor and outdoor environments including received signal strength indicator(RSSI), active throughput and packet loss rate(PLR) using the off-the-shelf software and hardware. RSSI is adpoted to reflect the basic performance of link quality in robotic sensor networks. Throughput acting as a decaying threshold generator is used to estimate the critical point for smooth and stable communication. Meanwhile, The metric PLR is adopted to reckon the cut-off point in end-to-end communication. The assessment of link quality acts as a feedback for cooperative control of mobile robots. The experimental results have shown the effectiveness of evaluation for the link quality of robotic sensor networks.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5371588,no
Software-Based Hardware Fault Tolerance for Many-Core Architectures,2009,"This presentation will point out the new opportunities and challenges for applying software-based hardware fault tolerance to emerging many-core architectures. The paper will discuss the tradeoff between the application of these techniques and the classical hardware-based fault tolerance in terms of fault coverage, overhead, and performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5372255,no
On the Functional Qualification of a Platform Model,2009,"This work focuses on the use of functional qualification for measuring the quality of co-verification environments for hardware/software (HW/SW) platform models. Modeling and verifying complex embedded platforms requires co-simulating one or more CPUs running embedded applications on top of an operating system, and connected to some hardware devices. The paper describes first a HW/SW co-simulation framework which supports all mechanisms used by software, in particular by device drivers, to access hardware devices so that the target CPU's machine code can be simulated. In particular, synchronization between hardware and software is performed by the co-simulation framework and, therefore, no adaptation is required in device drivers and hardware models to handle synchronization messages. Then, CertitudeÃ‚Â¿, a flexible functional qualification tool, is introduced. Functional qualification is based on the theory of mutation analysis, but it is extended by considering a mutation to be killed only if a testcase fails. CertitudeÃ‚Â¿ automatically inserts mutants into the HW/SW models and determines if the verification environment can detect these mutations. A known mutant that cannot be detected points to a verification weakness. If a mutant cannot be detected, there is evidence that actual design errors would also not be detected by the co-verification environment. This is an iterative process and functional qualification solution provides the verifier with information to improve the co-verification environment quality. The proposed approach has been successfully applied on an industrial platform as shown in the experimental result section.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5372259,no
Parallelization of render engine for global illumination of graphics scenes,2009,"Photorealistic image synthesis plays a central role in computer graphics since it has wide range of application areas such as scientific visualization, film industry, gaming and architectural modeling. Global illumination algorithms used for photorealistic image synthesis such as ray-tracing, radiosity, and photon mapping suffer from speed because of complex computational operations and hardware limitations. In this study, a parallelization technique for the photon mapping algorithm has been developed to reduce the rendering time of the images by distributing the jobs over computing units. To increase the visual quality of the output images, a multilevel dynamic anti-aliasing algorithm has also been implemented to smooth the jagged edges caused by the bucket render. Our render engine supports various geometry and material properties as well as hard-to-simulate visual effects such as caustics. System is tested on several three dimensional scene files to measure and analyze its performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5372551,no
Traffic Provisioning for HTTP Applications in WiFi Networks,2009,"Access networks based on cooper cables are costly to build and maintain. For this reason, last-mile access networks based on wireless technologies are gaining considerable attention. Among the wireless technologies being employed, the IEEE802.11 is common place. In such scenarios it is important to have well defined mechanisms to better evaluate traffic characteristics and overall system performance. Such understanding can help network designers to better estimate the resources needed to provide basic services with a reasonable level of quality (QoS). This task, however, has been shown to be non-trivial. The main contribution of this work is to present techniques than can be applied to estimate the throughput and the access pattern for basic services in the context of the IEEE802.11 based networks.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5372760,no
Two-dimensional software reliability measurement technologies,2009,"This paper discusses two-dimensional software reliability measurement technologies, which describe a software reliability growth process depending on two-types of software reliability growth factors: Testing-time and testing-effort factors. From the view point of actual software failure-occurrence mechanism, it is natural to consider that a software reliability growth process depends on not only testing-time factor but also other software reliability growth factors, i.e., testing-effort factors, such as test-execution time, testing-skill of test engineers, testing-coverage. From such reason, the two-dimensional software reliability measurement technologies enable us to conduct more feasible software reliability assessment than the one-dimensional (conventional) software reliability measurement approach, in which it is assumed that the software reliability growth process depends only on testing-time. We discuss two-types of two-dimensional software reliability growth modeling approaches for feasible software reliability assessment, and conduct goodness-of-fit comparisons of our models with existing one-dimensional software reliability growth models. Finally, as one of the examples for two-dimensional software reliability analysis, we show examples of the application of our two-dimensional software reliability growth model by using actual data.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5373378,no
The selection of proper discriminative cognitive tasks â€?A necessary prerequisite in high-quality BCI applications,2009,"While in brain computer interface (BCI) field the research is focused basically on finding improved processing methods leading to both high classification rates and high bit transfer rates, in this paper the same BCIs performances are addressed but, this time, with the emphasis set on the subject-specific discriminative cognitive tasks selection process. In this respect, a set of twelve electroencephalographic (EEG)-discriminative mental tasks was proposed to be studied in conjunction with four different subjects. For each subject, a particular set of four mental tasks was selected. The classification performances corresponding to these particular sets of tasks were obtained using some standard processing methods (i.e., the autoregressive model of the EEG signals and a multilayer perceptron classifier trained with back-propagation algorithm). The superior classification rates achieved for the selected sets compared to other set of mental tasks commonly used in the 4-class BCI studies (i.e. the set proposed by Keirn and Aunon) promote the idea of subject-oriented mental tasks selection process as a necessary preliminary step in any high-quality BCI application.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5373706,no
On the application of predictive control techniques for adaptive performance management of computing systems,2009,"This paper addresses adaptive performance management of real-time computing systems. We consider a generic model-based predictive control approach that can be applied to a variety of computing applications in which the system performance must be tuned using a finite set of control inputs. The paper focuses on several key aspects affecting the application of this control technique to practical systems. In particular, we present techniques to enhance the speed of the control algorithm for real-time systems. Next we study the feasibility of the predictive control policy for a given system model and performance specification under uncertain operating conditions. The paper then introduces several measures to characterize the performance of the controller, and presents a generic tool for system modeling and automatic control synthesis. Finally, we present a case study involving a real-time computing system to demonstrate the applicability of the predictive control framework.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5374030,no
An Adaptive Service Composition Performance Guarantee Mechanism in Peer-to-Peer Network,2009,"Service composition is becoming a hot research point in recent years, especially in distributed peer-to-peer (P2P) network. However, how to guarantee the high-quality composition service performance becomes the key technology and difficulty in P2P network, which draws lots of attention in computer research field. According to the characteristics of distributed network, an adaptive service composition performance guarantee mechanism in P2P network is presented. Firstly, the composition services are divided into preferred service and backup one; secondly, BP neural network is introduced to detect and evaluate the service performance exactly; finally, the backup service will adaptively replace the preferred one when the preferred composition service performance is less than the threshold value. Except that, the half-time window punishment mechanism is introduced to increase the punishment to the low-quality service and encourage users to provide more high-quality one, which further improves the service composition performance in P2P network. The simulations verify our method to be effective and enrich the proposed theory.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5374543,no
Research and Implement of the Key Technology for IP QoS Based on Network Processor,2009,"Along with the rapid development of network technology, the application of internet is wider. The network service also presents the characteristics of diversification. Thus the traditional real-time services have not already people's need. Immediately what to produce is various multi-media businesses such as, video, audio, the television meeting etc. with high request of real time. The traditional service of IP net can provide"" do the best one can"" only which can not satisfy the request of low time delay. So the trends of IP provide service of classification and guarantee the quality of service in network equipment. The network is developed rapidly, Router is the main network equipment and the control unit is network processor, so the research and implement of the quality of service is important orientation. The principal content of this dissertation is realization of the IP QoS based on network processor. According to the characteristic of network and the performance of network processor, a IP QoS system model is designed. The IP QoS in data transmit lay is mainly researched and realized, the validity of the QoS system proposed in this dissertation of is also proved in Router.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5374583,no
An Investigation of the Effect of Discretization on Defect Prediction Using Static Measures,2009,"Software repositories with defect logs are main resource for defect prediction. In recent years, researchers have used the vast amount of data that is contained by software repositories to predict the location of defect in the code that caused problems. In this paper we evaluate the effectiveness of software fault prediction with Naive-Bayes classifiers and J48 classifier by integrating with supervised discretization algorithm developed by Fayyad and Irani. Public datasets from the promise repository have been explored for this purpose. The repository contains software metric data and error data at the function/method level. Our experiment shows that integration of discretization method improves the software fault prediction accuracy when integrated with Naive-Bayes and J48 classifiers.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5375760,no
EVM measurement techniques for MUOS,2009,"Physical layer simulations and analysis techniques were used to develop the error vector magnitude (EVM) metric specifying transmitter signal quality. These tools also proved to be very useful in specifying lower level hardware unit performance and predicting mobile user objective system (MUOS) satellite 8-PSK transmitter performance before the hardware was built. However, the verification of EVM compliance at Ka frequencies posed challenges. Initial measurements showed unacceptably high levels of EVM which exceeded specification. Attempts to remove the contribution of the test equipment distortion and isolate the device-under-test distortion using commercial oscilloscope VSA software were unsuccessful. In this paper, we describe methods used to develop an accurate EVM measurement. The transmitted modulated signal was first recorded using a digitizing scope. In-house system identification, equalization, demodulation and analysis algorithms were then used to remove signal distortion due to the test equipment. Results from EVM measurements on MUOS single-channel hardware are given and performance is shown to be consistent with estimates made three years earlier. The results reduce technical risk and verify transmitter design by demonstrating signal quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5379776,no
Characterization of Software Projects by Restructuring Parameters for Usability Evaluation,2009,"Every software project progresses through a development process with necessary involvement of developers and users both. For evaluation of the software to confirm its phase wise goals, objectives and quality, developers and customers must reach to a win-win condition. It has been observed that the user satisfaction is the most important factor that must be emphasized in software development and thus, usability is an indispensable feature of a software project. Inclusion of usability dictates determination of existence of usability at different stages of development for measurement of user performance and user satisfaction. Various criteria known as software parameters are employed to evaluate the software for realizing certain level of usability. These criteria may be applied for software estimations and are also involved in several assessments during software development process. Variety of well defined and well structured parameters such as size, duration, files used, external inputs, external outputs, external queries etc. are available to support development of desirable software. Further, based upon these parameters other important software estimations may be derived. Though there exist various dimensions of employing the parameters based on the requirements of a software project, these parameters must be redefined and restructured to evaluate usability of software to produce usable software. In this paper, we restructure the software project parameters for evaluation of software usability. It will lead to overall characterization of the projects in usability perspective. Also, it will be useful for ranking usability attributes and measuring usability subsequently.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5380543,no
A New Fault Tolerance Heuristic for Scientific Workflows in Highly Distributed Environments Based on Resubmission Impact,2009,"Even though highly distributed environments such as Clouds and Grids are increasingly used for e-science high performance applications, they still cannot deliver the robustness and reliability needed for widespread acceptance as ubiquitous scientific tools. To overcome this problem, existing systems resort to fault tolerance mechanisms such as task replication and task resubmission. In this paper we propose a new heuristic called resubmission impact to enhance the fault tolerance support for scientific workflows in highly distributed systems. In contrast to related approaches, our method can be used effectively on systems even in the absence of historic failure trace data. Simulated experiments of three real scientific workflows in the Austrian Grid environment show that our algorithm drastically reduces the resource waste compared to conservative task replication and resubmission techniques, while having a comparable execution performance and only a slight decrease in the success probability.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5380852,no
Early Software Fault Prediction Using Real Time Defect Data,2009,"Quality of a software component can be measured in terms of fault proneness of data. Quality estimations are made using fault proneness data available from previously developed similar type of projects and the training data consisting of software measurements. To predict faulty modules in software data different techniques have been proposed which includes statistical method, machine learning methods, neural network techniques and clustering techniques. The aim of proposed approach is to investigate that whether metrics available in the early lifecycle (i.e. requirement metrics), metrics available in the late lifecycle (i.e. code metrics) and metrics available in the early lifecycle (i.e. requirement metrics) combined with metrics available in the late lifecycle (i.e. code metrics) can be used to identify fault prone modules by using clustering techniques. This approach has been tested with three real time defect datasets of NASA software projects, JM1, PC1 and CM1. Predicting faults early in the software life cycle can be used to improve software process control and achieve high software reliability. The results show that when all the prediction techniques are evaluated, the best prediction model is found to be the fusion of requirement and code metric model.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381121,no
Keynote 2,2009,"Summary form only given. In the 2006 report of the SEI International Process Research Consortium we identified nine driving forces which are shaping the need for research into software process and product. These were the increased push for value-add, business diversification, technology change, systems complexity, product quality, regulation, security and safety, and globalization. Four research themes were developed of which only one will be explored in this presentation. In the theme entitled ""The relationships between processes and product qualities"" the authors document some twenty six broad research questions which remain as challenges for the software engineering research community. This presentation outlines research carried out since 2006 which addresses some of these questions. In particular the presentation explores research into the meaning of software product quality, the performance quality prediction of heterogeneous systems of systems, and the relationships between defined software process and enacted process. Examples of conclusions which can be drawn are that our research should no longer benchmark development process productivity without consideration of product quality. In another example it shows that we will need to evaluate heterogeneous SOA architectures in the context of processes and workflows. In order to achieve significant progress it is argued that we need to pursue our research at a much deeper and inclusive level of understanding than often seen in the past in terms of process and process / product relationships.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381328,no
Non-homogeneous Inverse Gaussian Software Reliability Models,2009,"In this paper we consider a novel software reliability modeling framework based on non-homogeneous inverse Gaussian processes (NHIGPs). Although these models are derived in a different way from the well-known non-homogeneous Poisson processes (NHPPs), they can be regarded as interesting stochastic point processes with both arbitrary time non-stationary properties and the inverse Gaussian probability law. In numerical examples with two real software fault data, it is shown that the NHIGP-based software reliability models could outperform the goodness-of-fit and the predictive performances more than the existing NHPP-based models.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381369,no
RiTMO: A Method for Runtime Testability Measurement and Optimisation,2009,"Runtime testing is emerging as the solution for the integration and assessment of highly dynamic, high availability software systems where traditional development-time integration testing is too costly, or cannot be performed. However, in many situations, an extra effort will have to be invested in implementing appropriate measures to enable runtime tests to be performed without affecting the running system or its environment. This paper introduces a method for the improvement of the runtime testability of a system, which provides an optimal implementation plan for the application of measures to avoid the runtime tests' interferences. This plan is calculated considering the trade-off between testability and implementation cost. The computation of the implementation plan is driven by an estimation of runtime testability, and based on a model of the system. Runtime testability is estimated independently of the test cases and focused exclusively on the architecture of the system at runtime.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381399,no
An Approach to Measure Value-Based Productivity in Software Projects,2009,"Nowadays, after a lot of evolution in software engineering area, there is not yet a simple and direct answer to the question: What is the best software productivity metric? The simplest and most commonly used metric is the SLOC and its derivations, but these are admittedly problematic for this purpose. In another way, there are some indications of maturation in this topic, the new studies point to the use of more labored models, which are based on multiple dimensions and in the idea of produced value. Based in this tendency and on the evidence that each organization must define its own way to assess their productivity, we defined a process to support the definition of productivity measurement models in software organizations. We also discuss, in this paper, some issues about the difficulties related with the process adoption in real software organizations.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381400,no
A Neural Network Approach to Forecasting Computing-Resource Exhaustion with Workload,2009,"Software aging refers to the phenomenon that applications will show growing failure rate or performance degradation after longtime execution. It is reported that this phenomenon usually has close relationship with computing-resource exhaustion. This paper analyzes computing-resource usage data collected on a LAN, and quantitatively investigates the relationship between computing-resource exhaustion trend and workload. First, we discuss the definition of workload, and then a multi-layer back propagation neural network is trained to construct the nonlinear relationship between input (workload) and output (computing-resource usage). Then we use the trained neural network to forecast the computing-resource usage, i.e., free memory and used swap, with workload as its input. Finally, the results were benchmarked against those obtained without regard to influence of workload reported in the literatures, such as non-parametric statistical techniques or parametric time series models.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381423,no
Reuse Strategies in Distributed Complex Event Detection,2009,"In a Pub/Sub system, the procedure of distributed event detection can be divided into two interacting phases: the subscription matching and the subscription/event routing. Also, the performance of the system is greatly influenced by these two parts. Adopting a reuse strategy in matching and routing can reduce matching workload and network traffic. However, few of the existing researches provide a reuse solution comprehensively in distributed environments. This paper focuses on exploiting reuse to improve distributed complex event detection. The Pub/Sub system OGENS is introduced, in which the reuses are exploited in the channel-based subscription/event routing and the complex event matching.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381424,no
Quality Assessment of Mission Critical Middleware System Using MEMS,2009,"Architecture evaluation methods provide general guidelines to assess quality attributes of systems, which are not necessarily straightforward to practice with. With COTS middleware based systems, this assessment process is further complicated by the complexity of middleware technology and a number of design and deployment options. Efficient assessment is key to produce accurate evaluation results for stakeholders to ensure good decisions are made on system acquisition. In this paper, a systematic evaluation method called MEMS is developed to provide some structure to this assessment process. MEMS produces the evaluation plan with thorough design of experiments, definition of metrics and development of techniques for measurement. This paper presents MEMS and its application to a mission critical middleware system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381448,no
Increasing Diversity in Coverage Test Suites Using Model Checking,2009,"Automated test case generation often results in test suites containing significant redundancy such as test cases that are duplicates, prefixes of other test cases, or cover the same test requirements. In this paper we consider the fact that items described by a coverage criterion can be covered in different ways. We introduce a technique where each created test case is guaranteed to cover a test requirement in a new way, even if it has previously been covered. This increases the diversity of how test objectives are satisfied, thus reducing the redundancy in test suites, improving their fault detection ability, and usually also decreasing the number of test cases generated. This approach is based in a scenario of specification based testing using model checkers as test case generation tools, and evaluation is performed on three different case study specifications.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381459,no
ADAM: Web Anomaly Detection Assistant Based on Feature Matrix,2009,"Importance of web security cannot be overemphasized in the era of web-based economy. Although anomaly detection has long been considered a promising alternative to signature-based misuse detection technique, most studies to date used either small scale or artificially generated attack data. In this paper, based on security analysis applied on anonymous www.microsoft.com log of about 250 GB, we propose Anomaly Feature Matrix (AFM) as an effective framework to characterize anomalies. Feature selection of AFM is based on the characteristics of well-known (e.g., DDoS) attacks as well as patterns of anomalous logs found in the Microsoft data. Independent security analysis performed on the same data by Microsoft security engineers concluded that 1) We did not miss any major attacks; and 2) AFM is a general enough framework to characterize likely web attacks. In order to assist AFM-based anomaly analysis in large organizations, we implemented an interactive and visual analysis tool named ADAM (Anomaly Detection Assistant based on feature Matrix). Integrated with mapping software such as Virtual Earth, ADAM enables efficient and focused security analysis on web logs.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381495,no
A Hybrid Approach to Detecting Security Defects in Programs,2009,"Static analysis works well at checking defects that clearly map to source code constructs. Model checking can find defects of deadlocks and routing loops that are not easily detected by static analysis, but faces the problem of state explosion. This paper proposes a hybrid approach to detecting security defects in programs. Fuzzy inference system is used to infer selection among the two detection approaches. A cluster algorithm is developed to divide a large system into several clusters in order to apply model checking. Ontology based static analysis employs logic reasoning to intelligently detect the defects. We also put forwards strategies to improve performance of the static analysis. At last, we perform experiments to evaluate the accuracy and performance of the hybrid approach.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381537,no
SmartClean: An Incremental Data Cleaning Tool,2009,"This paper presents the SmartClean tool. The purpose of this tool is to detect and correct the data quality problems (DQPs). Compared with existing tools, SmartClean has the following main advantage: the user does not need to specify the execution sequence of the data cleaning operations. For that, an execution sequence was developed. The problems are manipulated (i.e., detected and corrected) following that sequence. The sequence also supports the incremental execution of the operations. In this paper, the underlying architecture of the tool is presented and its components are described in detail. The tool's validity and, consequently, of the architecture is demonstrated through the presentation of a case study. Although SmartClean has cleaning capabilities in all other levels, in this paper are only described those related with the attribute value level.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381543,no
What Makes Testing Work: Nine Case Studies of Software Development Teams,2009,Recently there has been a focus on test first and test driven development; several empirical studies have tried to assess the advantage that these methods give over testing after development. The results have been mixed. In this paper we investigate nine teams who tested during coding to examine the effect it had on the external quality of their code. Of the top three performing teams two used a documented testing strategy and the other an ad-hoc approach to testing. We conclude that their success appears to be related to a testing culture where the teams proactively test rather than carry out only what is required in a mechanical fashion.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381629,no
Boundary Value Testing Using Integrated Circuit Fault Detection Rule,2009,"Boundary value testing is a widely used functional testing approach. This paper presents a new boundary value selection approach by applying fault detection rules for integrated circuits. Empirical studies based on Redundant Strapped-Down Inertial Measurement Unit of the 34 program versions and 426 mutants compare the new approach to the current boundary value testing methods. The results show that the approach proposed in this paper is remarkably effective in conquering test blindness, reducing test cost and improving fault coverage.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381653,no
Using TTCN-3 in Performance Test for Service Application,2009,"Service applications are applicable to provide services for requests of users from network. Due to the fact that they have to endure a big number of concurrent requests, the performance of service applications running under specific arrival rate of requests should be assessed. To measure the performance of a service application, multi-party testing context is needed to simulate a number of concurrent requests and collect the responses. TTCN-3 is a test description language; it provides basic language elements for multi-party testing context that can be used in performance tests. This paper proposes a general approach of using TTCN-3 in multi-party performance testing service application. To this aim, a model of service application is presented, and performance testing framework for service applications is discussed. This testing framework is realized for a typical application by developing a reusable TTCN-3 abstract test suite.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381754,no
Hierarchical Stability-Based Model Selection for Clustering Algorithms,2009,"We present an algorithm called HS-means which is able to learn the number of clusters in a mixture model. Our method extends the concept of clustering stability to a concept of hierarchical stability. The method chooses a model for the data based on analysis of clustering stability; it then analyzes the stability of each component in the estimated model and chooses a stable model for this component. It continues this recursive stability analysis until all the estimated components are unimodal. In so doing, the method is able to handle hierarchical and symmetric data that existing stability-based algorithms have difficulty with. We test our algorithm on both synthetic datasets and real world datasets. The results show that HS-means outperforms a popular stability-based model selection algorithm, both in terms of handling symmetric data and finding high-quality clusterings in the task of predicting CPU performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381839,no
Feature Selection with Imbalanced Data for Software Defect Prediction,2009,"In this paper, we study the learning impact of data sampling followed by attribute selection on the classification models built with binary class imbalanced data within the scenario of software quality engineering. We use a wrapper-based attribute ranking technique to select a subset of attributes, and the random undersampling technique (RUS) on the majority class to alleviate the negative effects of imbalanced data on the prediction models. The datasets used in the empirical study were collected from numerous software projects. Five data preprocessing scenarios were explored in these experiments, including: (1) training on the original, unaltered fit dataset, (2) training on a sampled version of the fit dataset, (3) training on an unsampled version of the fit dataset using only the attributes chosen by feature selection based on the unsampled fit dataset, (4) training on an unsampled version of the fit dataset using only the attributes chosen by feature selection based on a sampled version of the fit dataset, and (5) training on a sampled version of the fit dataset using only the attributes chosen by feature selection based on the sampled version of the fit dataset. We compared the performances of the classification models constructed over these five different scenarios. The results demonstrate that the classification models constructed on the sampled fit data with or without feature selection (case 2 and case 5) significantly outperformed the classification models built with the other cases (unsampled fit data). Moreover, the two scenarios using sampled data (case 2 and case 5) showed very similar performances, but the subset of attributes (case 5) is only around 15% or 30% of the complete set of attributes (case 2).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381844,no
Wrapper-Based Feature Ranking for Software Engineering Metrics,2009,"The application of feature ranking to software engineering datasets is rare at best. In this study, we consider wrapper-based feature ranking where nine performance metrics aided by a particular learner are evaluated. We consider five learners and take two different approaches, each in conjunction with one of two different methodologies: 3-fold Cross-Validation (CV) and 3-fold Cross-Validation Risk Impact (CV-R). The classifiers are Naive Bayes (NB), Multi Layer Perceptron (MLP), k- Nearest Neighbors (kNN), Support Vector Machines (SVM), and Logistic Regression (LR). The performance metrics used as ranking techniques are Overall Accuracy (OA), F-Measure(FM), Geometric Mean (GM), Arithmetic Mean (AM), Area under ROC (AUC), Area under PRC (PRC), Best F-Measure (BFM), Best Geometric Mean (BGM), and Best Arithmetic Mean (BAM). To evaluate the classifier performance after feature selection has been applied, we use AUC as the performance evaluator. This paper represents a preliminary report on our proposed wrapper-based feature ranking approach to software defect prediction problems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381847,no
Fault Diagnosis for Analogy Circuits Based on Support Vector Machines,2009,"When it is hard to obtain training samples, the fault classifier based on support vector machine (SVM) can diagnose faults with high accuracy. It can easily be generalized and put to practical use. In this paper, a fault classifier based on support vector machine (SVM) is proposed for analog circuits. It can classify the faults in the target circuit effectively and accurately. In order to test the algorithm, an analog circuit fault diagnosis system based on SVM is designed for the measurement circuit that approximates the square curve with a broken line. After being trained with practical measurement data, the system is shown to be capable of diagnosing faults hidden in real measurement data accurately. Therefore, the effectiveness of the algorithm is verified.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381893,no
On the Reliability of Wireless Sensors with Software-Based Attestation for Intrusion Detection,2009,"Wireless sensor nodes are widely used in many areas, including military operation surveillance, natural phenomenon monitoring, and medical diagnosis data collection. These applications need to store and transmit sensitive or secret data, which requires intrusion detection mechanisms be deployed to ensure sensor node health, as well as to maintain quality of service and survivability. Because wireless sensors have inherent resource constraints, it is crucial to reduce energy consumption due to intrusion detection activities. In this paper by means of a probability model, we analyze the best frequency at which intrusion detection based on probabilistic code attestation on the sensor node should be performed so that the sensor reliability is maximized by exploiting the trade-off between the energy consumption and intrusion detection effectiveness. When given a set of parameter values characterizing the operational and networking conditions, a sensor can dynamically set its intrusion detection rate identified by the mathematical model to maximize its reliability and the expected sensor lifetime.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381928,no
High-Performance Cloud Computing: A View of Scientific Applications,2009,"Scientific computing often requires the availability of a massive number of computers for performing large scale experiments. Traditionally, these needs have been addressed by using high-performance computing solutions and installed facilities such as clusters and super computers, which are difficult to setup, maintain, and operate. Cloud computing provides scientists with a completely new model of utilizing the computing infrastructure. Compute resources, storage resources, as well as applications, can be dynamically provisioned (and integrated within the existing infrastructure) on a pay per use basis. These resources can be released when they are no more needed. Such services are often offered within the context of a service level agreement (SLA), which ensure the desired quality of service (QoS). Aneka, an enterprise cloud computing solution, harnesses the power of compute resources by relying on private and public clouds and delivers to users the desired QoS. Its flexible and service based infrastructure supports multiple programming paradigms that make Aneka address a variety of different scenarios: from finance applications to computational science. As examples of scientific computing in the cloud, we present a preliminary case study on using Aneka for the classification of gene expression data and the execution of fMRI brain imaging workflow.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381983,no
An efficient error concealment method for mobile TV broadcasting,2009,"Nowadays, TV broadcasting has found its application in mobile terminal, however, due to the prediction structure of video coding standards, compressed video bitstreams are vulnerable to wireless channel disturbances for real-time transmission. In this paper, we propose a novel temporal error concealment method for mobile TV sequences. The proposed ordering methods utilizes continuity feature among adjacent frames, so that both inter and intra error propagation are alleviated. Combined with our proposed fuzzy metric based boundary matching algorithm (FBMA) which provides more accurate distortion function, experiment results show our proposal achieves better performance under error-prone channel, compared existing error concealment algorithms.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5383889,no
Ricean Factor Estimation and Performance Analysis,2009,"In wireless communication, Ricean K factor which is the relative strength of the direct and scattered components of the received signal, is an important reflection of channel quality parameters. Therefore, the estimate of Ricean factor is great significance. Based on the introduction of the estimation methods of K from envelope and from the I/Q components, there is an academic analysis about the precision of these methods by simulations, and the complexity of the first- and second-order moments method and the second- and fourth-order moments method in engineering in the paper. Furthermore, we come to the conclusion is that: while could obtain high estimation accuracy, the second- and fourth-order moments method consume much less time and is used more widely in engineering applications.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5384711,no
Application of Fuzzy Data Mining Algorithm in Performance Evaluation of Human Resource,2009,"The assessment of human resource performance objectively, thoroughly, and reasonably is critical to choosing managerial personnel suited for organizational development. Therefore, an efficient tool should be able to deal with various employees' data and assist managers to make decision and strategic plan. As an effect mathematic tool to deal with the vagueness and uncertainty, fuzzy data mining is considered as a highly desirable tool being applied to many application areas. In this paper we applied the fuzzy data mining technique to make the assessment and selection of human resource in enterprise. We present and justify the capabilities of fuzzy data mining technology in the evaluation of human resource in enterprise by proposing a practical model for improving the efficiency and effectiveness of human resource management. Firstly, the paper briefly explained the basic fuzzy data mining theory, and proposed the fuzzy data mining algorithm in detail. We gave process steps and the flow chart of algorithm in this part. Secondly, we used the human resource management data as illustration to implement the algorithm. We used the maximal tree to cluster the human resource. Then the raw data of human resource management is compared with each cluster and calculate the proximal values based on the equation in fuzzy data mining algorithm. At last we determined the evaluation of human resource. The whole process was easy to be completed. The results of this study indicated that the methodology was practical and feasible. It could help managers in enterprise assess performance of the human resource swiftly and effectively.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5385062,no
Spectral diagnosis of plasma jet at atmospheric pressure,2009,"A new approach to surface modification of materials using dielectric barrier discharge (DBD) plasma jet at atmospheric pressure is presented in this paper. The emission spectral lines of argon plasma jet at atmospheric pressure were recorded by the grating spectrograph HR2000 and computer software. The argon plasma emission spectra, whose range is from 300nm to 1000nm, were measured at different applied voltage. Comparing to air plasma emission spectra under the same circumstance, it is shown that all of the spectral lines are attributed to neutral argon atoms. The spectral line 763.51mn and 772.42nm are chosen to estimate the electron excitation temperature. The purpose of the study is to research the relationship between the applied voltage and temperature to control the process of materials' surface modification promptly. The results show that electron excitation temperature is in the range of 0.1eV-0.5eV and it increases with increasing applied voltage. In the process of surface modification under the plasma jet, the infrared radiation thermometer was used to measure the material surface temperature under the plasma jet. The results show that the material surface temperature is in the range of 50~100 degrees centigrade and it also increases with increasing applied voltage. Because the material surface was under the plasma jet and its temperature was decided by the plasma, the material surface temperature increases with increasing the macro-temperature of plasma jet, the relationship of the surface temperature and applied voltage indicates the relationship of the macro-temperature of the plasma jet and the applied voltage approximately. The experimental results indicate that DBD plasma jet at atmospheric pressure is a new approach to improve the quality of materials' surface modification, and spectral diagnose is proved to be a kind of workable method by choosing suitable applied voltage.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5386273,no
Reputation-Aware Scheduling for Storage Systems in Data Grids,2009,"Data grids provide such data-intensive applications with a large virtual storage framework with unlimited power. However, conventional scheduling algorithms for data grids are unable to meet the reputation service requirements of data-intensive applications. In this paper we address the problem of scheduling data-intensive jobs on data grids subject to reputation service constraints. Using the reputation-aware technique, the dynamic scheduling strategy is proposed to improve the capability of predicting the reliability and credibility for data-intensive applications. To incorporate reputation service into job scheduling, we introduce a new performance metric, degree of reputation sufficiency, to quantitatively measure quality of reputation service provided by data grids. Experimental results based on a simulated grid show that the proposed scheduling strategy significantly is capable of significantly satisfying the reputation service requirements and guaranteeing the desired response times .",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5392916,no
A Heuristic Approach with Branch Cut to Service Substitution in Service Orchestration,2009,"With the rapidly growing number of Web services throughout the Internet, Service Oriented Architecture (SOA) enables a multitude of service providers (SP) to provide loosely coupled and inter-operable services at different Quality of Service (QoS) levels. This paper considers the services are published to a QoS-aware registry. The structure of composite service is described as a Service Orchestration that allows atomic services to be brought together into one business process; This paper considers the problem of finding a set of substitution atomic services to make the Service Orchestration re-satisfies the given multi-QoS constraints when one QoS metric went unsatisfied at runtime. This paper leverage hypothesis test to detect possible fault atomic services, and propose heuristic algorithms with different level branch cut to determine the Service Orchestration substitutions. Experiments are given to testify the algorithms are effective and efficient, and the probability cut algorithm reaches a cut/search ratio of 137.04% without loss solutions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5392937,no
Using Hessian Locally Linear Embedding for autonomic failure prediction,2009,"The increasing complexity of modern distributed systems makes conventional fault tolerance and recovery prohibitively expensive. One of the promising approaches is online failure prediction. However, the process of feature extraction depends on the experienced administrators and their domain knowledge to filtering and compressing error events into a form that is easy for failure prediction. In this paper, we present a novel performance-centric approach to automate failure prediction with Manifold Learning techniques. More specifically, we focus on methods that use Supervised Hessian Locally Embedding algorithm to achieve autonomic failure prediction. In our experimental work we found that our method can automatically predict more than 60% of the CPU and memory failures, and around 70% of the network failure based on the runtime monitoring of the performance metrics.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5393880,no
How much input vectors affect nano-circuit's reliability estimates,2009,"As the sizes of (nano-)devices are aggressively scaled deep towards the nanometer range, the design and manufacturing of future (nano-)circuits will become extremely complex and inevitably introduce more defects, while their functioning will be adversely affected by (transient) faults. Therefore, accurately calculating the reliability of future designs will become a very important factor for (nano-)circuit designers as they investigate several alternatives for optimizing the tradeoffs between the conflicting metrics of area-power-energy-delay versus reliability. This paper studies the effect of the input vectors on the (nano-)circuit's reliability, and introduces a time-efficient method for quickly and accurately identifying the lower/upper reliability bounds. Simulations results support the claim that the absolute difference between the lowest and the highest achievable reliability is of one-to-two orders of magnitude. Therefore, future designs should consider the worst case input vector(s) in order to guarantee the required reliability margins.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5394482,no
Creating On-Demand Service-Oriented Applications from Intentions Model,2009,"The recent years have seen a flurry of research inspired by social and biological models to achieve the software autonomy. This has been prompted by the need to automate laborious administration tasks, recovery from unanticipated systems failure, and provide self-protection from security vulnerabilities, whilst guaranteeing predictable autonomic software behavior. However, runtime assured adaptation of software to new requirement is still an outstanding issue for research. This paper presents a supporting language for process-oriented programming of autonomic software. The paper starts by a review of the state-of-the-art into runtime software adaptation. This is followed by an outline of a developed Neptune framework and language support, which is here described via an illustrative example pet shop benchmark. The paper ends with a discussion and some concluding remarks leading to suggested further works.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5395113,no
Field-Based Branch Prediction for Packet Processing Engines,2009,"Network processors have exploited many aspects of architecture design, such as employing multi-core, multi-threading and hardware accelerator, to support both the ever-increasing line rates and the higher complexity of network applications. Micro-architectural techniques like superscalar, deep pipeline and speculative execution provide an excellent method of improving performance without limiting either the scalability or flexibility, provided that the branch penalty is well controlled. However, it is difficult for traditional branch predictor to keep increasing the accuracy by using larger tables, due to the fewer variations in branch patterns of packet processing. To improve the prediction efficiency, we propose a flow-based prediction mechanism which caches the branch histories of packets with similar header fields, since they normally undergo the same execution path. For packets that cannot find a matching entry in the history table, a fallback gshare predictor is used to provide branch direction. Simulation results show that the our scheme achieves an average hit rate in excess of 97.5% on a selected set of network applications and real-life packet traces, with a similar chip area to the existing branch prediction architectures used in modern microprocessors.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5395272,no
Reliable Software Distributed Shared Memory Using Page Migration,2009,"Reliability has recently become an important issue in PC cluster technology. This research proposes a software distributed shared memory system, named SCASH-FT, as an execution platform for high performance and highly reliable parallel system for commodity PC clusters. To achieve fault tolerance, each node has redundant page data that allows recovery from node failure using SCASH-FT. All page data is checkpointed and duplicated to another node when a user explicitly calls the checkpoint function. When failure occurs, SCASH-FT invokes the rollback function by restarting an execution from the last checkpoint data. SCASH-FT takes charge of processes such as detecting failure and restarting execution. So, all you have to do is just adding checkpoint function calls in the source code to determine the timing of each checkpoint. Evaluation results show that the checkpoint cost and the rollback penalty depend on the data access pattern and the checkpoint frequency. Thus, users can control their application performance by adjusting checkpoint frequency.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5395317,no
A Quantitative Evaluation of Software Quality Enhancement by Refactoring Using Dependency Oriented Complexity Metrics,2009,"The maintainability of software depends on the quality of software. Software under evolution is modified and enhanced to cater the new requirements. Due to this the software becomes more complex and deviates from its original design, in turn lowering the quality. Refactoring makes object oriented software systems maintainable. Effective refactoring requires proper metrics to quantitatively ascertain the improvement in the quality after refactoring. In this direction, we have made an effort to quantitatively evaluate the quality enhancement by refactoring using dependency oriented complexity metrics. In this paper three experimental cases are given. The metrics have successfully indicated quantitatively the presence of defects and the improvement in the quality of designs after refactoring. These metrics have acted as quality indicators with respect to designs considered under ripple effects before and after refactoring. Metrics as quality indicators help to estimate the required maintenance efforts.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5395508,no
PSO approach to preview tracking control systems,2009,"Preview Control is a field well suited for application to systems that have reference signals known a priori. The use of advance knowledge of reference signal can improve the tracking quality of the concerned control system. The classical solution to the Preview Control problem is obtained using the Algebraic Riccati Equation. The solution obtained is good but it is not optimal and has a scope of improvement. This paper presents a novel method of design of HÃ‚Â¿ Preview Controller using Particle Swarm Optimization (PSO) technique. The procedure is based on improving the performance by minimizing the objective function i.e. IAE (Integral of Absolute Error). The procedure is tested for two systems - Altitude Control System of second order and an industrial system of fifth order using MATLAB software environment. The results show that the solutions of PSO based technique are better in terms of the control characteristics of transient response of the system for various preview lengths, stability and also in terms of the objective function Integral of Absolute Error (IAE).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5395843,no
A novel approach to minimizing the risks of soft errors in mobile and ubiquitous systems,2009,"A novel approach to minimizing the risks of soft errors at modeling level of mobile and ubiquitous systems is outlined. From a pure dependability viewpoint, critical components, whose failure is likely to impact on system functionality, attract more attention of protection/prevention mechanisms (against soft errors) than others do. Tolerating soft errors can be much improved if critical components can be identified at an early design phase and measures are taken to lower their criticalities at that stage. This improvement is achieved by presenting a criticality ranking (among the components) formed by combining a prediction of soft errors, consequences of them, and a propagation of failures at system modeling phase; and pointing out the ways to apply changes in the model to minimize the risks of degradation of desired functionalities. Case study results are given to illustrate and validate the approach.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5395903,no
An approach to automatic verification of stochastic graph transformations,2009,"Non-functional requirements like performance and reliability play a prominent role in distributed and dynamic systems. To measure and predict such properties using stochastic formal methods is crucial. At the same time, graph transformation systems are a suitable formalism to formally model distributed and dynamic systems. Already, to address these two issues, stochastic graph transformation systems (SGTS) have been introduced to model dynamic distributed systems. But most of the researches so far are concentrated on SGTS as a modelling means without considering the need for suitable analysis tools. In this paper, we present an approach to verify this kind of graph transformation systems using PRISM (a stochastic model checker). We translate the SGTS to the input language of PRISM and then PRISM performs the model checking and returns the results back to the designers.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5395989,no
A multi-scaling based admission control approach for network traffic flows,2009,"In this paper, we propose an analytical expression for estimating byte loss probability at a single server queue with multi-scale traffic arrivals. We extend our investigation on the application potentiality of the estimation method and possible its quality in connection admission control mechanisms. Extensive experimental tests validate the efficiency and accuracy of the proposed loss probability estimation approach and its superior performance for admission control applications in network connection with respect to some well-known approaches suggested in the literature.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5397689,no
Diagnostic models for sensor measurements in rocket engine tests,2009,"This paper presents our ongoing work in the area of using virtual reality (VR) environments for the Integrated Systems Health Management (ISHM) of rocket engine test stands. Specifically, this paper focuses on the development of an intelligent valve model that integrates into the control center at NASA Stennis Space Center. The intelligent valve model integrates diagnostic algorithms and 3D visualizations in order to diagnose and predict failures of a large linear actuator valve (LLAV). The diagnostic algorithm uses auto-associative neural networks to predict expected values of sensor data based on the current readings. The predicted values are compared with the actual values and drift is detected in order to predict failures before they occur. The data is then visualized in a VR environment using proven methods of graphical, measurement, and health visualization. The data is also integrated into the control software using an ActiveX plug-in.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5398535,no
Nonlinear adaptive flight control law design and handling qualities evaluation,2009,"This paper considers the design of a stability and control augmentation system for a modern fighter aircraft. The aim of the flight control system is to offer the pilot consistent good flying and handling qualities over a specified flight envelope and to provide robustness to model uncertainties. A nonlinear adaptive backstepping method is proposed to directly deal with the nonlinearities and the uncertainties of the system. B-spline neural networks are used to partition the flight envelope into multiple connecting regions. In each partition a locally valid linear-in-the-parameters nonlinear aircraft model is defined, of which the unknown parameters are approximated online by Lyapunov based update laws. These update laws take aircraft state and input constraints into account so that they do not corrupt the parameter estimation process. The desired aircraft response characteristics are enforced with command filters and verified by applying conventional handling qualities analysis techniques to numerical simulation data. Simulation results show that the controller is capable of giving desired closed-loop nominal and robust performance in the presence of aerodynamic uncertainties.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5400209,no
Predicting Defect-Prone Software Modules at Different Logical Levels,2009,"Effective software defect estimation can bring cost reduction and efficient resources allocation in software development and testing. Usually, estimation of defect-prone modules is based on the supervised learning of the modules at the same logical level. Various practical issues may limit the availability or quality of the attribute-value vectors extracting from the high-level modules by software metrics. In this paper, the problem of estimating the defect in high-level software modules is investigated with a multi-instance learning (MIL) perspective. In detail, each high-level module is regarded as a bag of its low-level components, and the learning task is to estimate the defect-proneness of the bags. Several typical supervised learning and MIL algorithms are evaluated on a mission critical project from NASA. Compared to the selected supervised schemas, the MIL methods improve the performance of the software defect estimation models.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5401286,no
CBCT-subsystem performance of the multi-modality Brightview XCT system (M09-26),2009,"The new Brightview XCT system uses a flat-panel detector to perform CBCT imaging for attenuation correction and localization. Features include a small footprint due to an offset-detector geometry, advanced scatter correction - both software and hardware - isotropic voxels, and GPU-accelerated reconstruction. System performance characteristics of the CBCT system such as spatial resolution, HU linearity, uniformity, noise, low-contrast detectability, and dose measurements are discussed in this paper.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5401571,no
Improving lesion detectability of a PEM system with post-reconstruction filtering,2009,"We present a method to quantify the image quality of a positron emission mammography (PEM) imaging system through the metric of lesion detectability. For a customized image quality phantom, we assess the impact of different post-reconstruction filters on the acquired PEM image. We acquired six image quality phantom images on a Naviscan PEM scanner using different scan durations which gave differing amounts of background noise. The image quality phantom has dimensions of 130 mm ÃƒÂ— 130 mm ÃƒÂ— 66 mm and consists of 15 hot rod inserts with diameters of approximately 10 mm, 5 mm, 4 mm, 3 mm, and 2 mm filled with activity ratios of 3.5, 6.8 and 12.7 times the background activity. One region of the phantom had no inserts so as to measure the uniformity of the background noise. Lesion detectability was determined for each background uniformity and each activity ratio by extrapolating a fit of the recovery coefficients to the point where the lesion would be lost in the noise of the background (defined as 3 times the background's standard deviation). The data were reconstructed by the system's standard clinical software using an MLEM algorithm with 5 iterations. We compare the lesion detectability of an unfiltered image to the image after applying one of five common post-reconstruction filters. Two of the filters were found to improve lesion detectability: a bilateral filter (9% improvement) and a Perona-Malik filter (8% improvement). One filter was found to have negligible effect: a Gaussian filter showed a 1% decrease in lesion detectability. The other two filters tested were found to worsen lesion detectability: a median filter (8% decrease) and a Stick filter (7% decrease).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5401712,no
The Utah PET lesion detection database,2009,"Task-based assessment of image quality is a challenging but necessary step in evaluating advancements in PET instrumentation, algorithms, and processing. We have been developing methods of evaluating observer performance for detecting and localizing focal warm lesions using experimentally-acquired whole-body phantom data designed to mimic oncologic FDG PET imaging. This work describes a new resource of experimental phantom data that is being developed to facilitate lesion detection studies for the evaluation of PET reconstruction algorithms and related developments. A new large custom-designed thorax phantom has been constructed to complement our existing medium thorax phantom, providing two whole-body setups for lesion detection experiments. The new phantom is ~50% larger and has a removable spine/rib-cage attenuating structure that is held in place with low water resistance open cell foam. Several series of experiments have been acquired, with more ongoing, including both 2D and fully-3D acquisitions on tomographs from multiple vendors, various phantom configurations and lesion distributions. All raw data, normalizations, and calibrations are collected and offloaded to the database, enabling subsequent retrospective offline reconstruction with research software for various applications. The offloaded data are further processed to identify the true lesion locations in preparation for use with both human observers and numerical studies using the channelized non-prewhitened observer. These data have been used to study the impact of improved statistical algorithms, point spread function modeling, and time-of-flight measurements upon focal lesion detection performance, and studies on the effects of accelerated block-iterative algorithms and advanced regularization techniques are currently ongoing. Interested researchers are encouraged to contact the author regarding potential collaboration and application of database experiments to their projects.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5401862,no
Data Quality Monitoring of the CMS Tracker,2009,"The data quality monitoring (DQM) of the Compact Muon Solenoid (CMS) silicon tracking detectors (tracker) at the Large Hadron Collider (LHC) at CERN is a software based system designed to monitor the detector and reconstruction performance, to identify problems and to certify the collected data for physics analysis. It uses the framework provided by the central CMS DQM as well as tools developed especially for the CMS tracker DQM. This paper describes aim, framework conditions, tools and work flows of the CMS tracker DQM and shows examples of its successful use during the recent commissioning phase of the CMS experiment.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5401987,no
Online monitoring system for Double Chooz experiment,2009,"Double Chooz is a reactor-neutrino experiment at Chooz nuclear power plant in France to measure the unknown neutrino mixing angle Ã‚Â¿<sub>13</sub> with a better sensitivity than the current best limit. A remote-monitoring framework has been developed to check status of DAQ systems, which enables collaborators off-site to access online monitoring data through the internet. As for these graphical user interfaces (GUI) are developed with platform independent technologies to be available through the internet. As fault tolerant monitoring systems in Double Chooz, Gaibu (external in Japanese) system and Log-Message system have been developed. Gaibu system works as an alarm message provider working outside DAQs and Log-Message system is a log file manager. Developments of these systems are almost finished and we are preparing software commissioning at the far detector site now.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5401999,no
CZT quasi-hemispherical detectors with improved spectrometric characteristics,2009,"At present time various CZT detectors of different designs and sizes are widely and successfully used for different applications due to its favorable detection properties. Among them there are hemispherical or quasi-hemispherical detectors. These detectors have rather simple design and do not require special electronics for application. Development of quasi-hemispherical detectors fabrication methods allows noticeable improves detector's spectrometric performance due to a charge collection optimization. This will allow increasing the yield of high quality detectors. For example energy resolution at 662 keV line and peak-to-Compton ratio of the quasi-hemispherical detector with volume of 150 mm<sup>3</sup> were improved from 27 keV and 2.5 to 13 keV and 4.9 correspondingly. Total absorption peaks efficiencies in all energy range remained without changes. At that, improvements were obtained without increasing of detectors high operation voltages and without application of any pulse shape correction/selection schemes.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5402238,no
Iterative layer-based raytracing on CUDA,2009,"A raytracer consists in an application capable of tracing rays from a point into a scene in order to determine the closest sphere intersection along the ray's direction. Because of their recursive nature, raytracing algorithms are hard to implement on architectures which do not support recursion, such as the NVIDIA CUDA architecture. Even if the recursive portion of a typical raytracing algorithm can be rewritten iteratively, naively doing so could tamper with the image generation process, preventing the parallel algorithm's results from maintaining high fidelity to the sequential algorithm's and resulting, in many cases, in lower quality images. In this paper we address both issues by presenting a novel approach for expressing the recursive structure of raytracer algorithms iteratively, while still maintaining high fidelity to the images generated by the sequential algorithm, and leveraging the processing power of the GPU for parallelizing the image generation process. Our work focuses on designing and implementing a raytracer that renders arbitrary scenes and the reflections among the objects contained in it. However, it can be easily extended to implement other natural phenomena, such as light refraction, and to aid the iterative implementation of recursive algorithms in architectures like CUDA, which do not support recursive function calls.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5403843,no
Automatic Testing of Financial Charting Software Component: A Case Study on Point and Figure Chart,2009,"<div style=""font-variant: small-caps; font-size: .9em;"">First Page of the Article</div><img class=""img-abs-container"" style=""width: 95%; border: 1px solid #808080;"" src=""/xploreAssets/images/absImages/05404375.png"" border=""0"">",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5404375,no
A new approach for DCT coefficients estimation and super resolution of compressed video,2009,"Reconstructing high resolution images from compressed low resolution video is a hot issue now. In many real applications, such as surveillance users often only get low quality videos which are hard to identify interested objects, super resolution restoration is a effective tool to enhance the quality of images. Quantization loss is the major cause for losing images details, this paper propose an new method to estimate the quantization noise in frequency domain using Laplace model for DCT coefficients, then the distribution parameter of Laplace is treated as variable simultaneously iterated with the object high resolution images. Experiments indicate that this algorithm in compressed video restoration details due to quantization noise have better performance than others.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5406640,no
Quality prediction model of object-oriented software system using computational intelligence,2009,"Effective prediction of the fault-proneness plays a very important role in the analysis of software quality and balance of software cost, and it also is an important problem of software engineering. Importance of software quality is increasing leading to development of new sophisticated techniques, which can be used in constructing models for predicting quality attributes. In this paper, we use fuzzy c-means clustering (FCM) and radial basis function neural network (RBFNN) to construct prediction model of the fault-proneness, RBFNN is used as a classificatory, and FCM is as a cluster. Object-oriented software metrics are as input variables of fault prediction model. Experiments results confirm that designed model is very effective for predicting a class's fault-proneness, it has a high accuracy, and its implementation requires neither extra cost nor expert's knowledge. It also is automated. Therefore, proposed model was very useful in predicting software quality and classing the fault-proneness.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5406941,no
Adaptation of ATAM<sup>SM</sup> to software architectural design practices for organically growing small software companies,2009,"The architecture of a software application determines the degree of success of both operation and development of software. Adopted architectural options not only affect the functionality and performance of the software, but they also affect delivery related factors such as cost, time, changeability, scalability, and maintainability. It is thus very important to find appropriate means of assessing benefits as well as liabilities of different architectural options to maximize the life-time benefit and reduce the overall cost of ownership of a software application. The Architecture Tradeoff Analysis Method (ATAM<sup>SM</sup>) developed by Software Engineering Institute (SEI) is that kind of tool. Considerably this is a very big framework for dealing with architectural tradeoff issues faced by large companies for developing large as well as complex software applications. The practicing of full blown ATAM without taking into consideration of diverse forces affecting the value addition from its practice does not maximize benefits from its adoption. Related forces faced by small software companies are significantly different than those faced by large software companies. Therefore, ATAM should be adapted to make it suitable for the practice by small software companies. This paper presents the information about the architectural practice level of organically grown small software companies within the context of ATAM followed by the gap analysis between the industry practices and ATAM, and adaptation recommendations. Both literature review and field investigation based on key informant interview have been performed for this purpose. Based on the findings of this study an adaptation process of ATAM for the small companies has been proposed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5407288,no
Nonparametric multivariate anomaly analysis in support of HPC resilience,2009,"Large-scale computing systems provide great potential for scientific exploration. However, the complexity that accompanies these enormous machines raises challenges for both, users and operators. The effective use of such systems is often hampered by failures encountered when running applications on systems containing tens-of-thousands of nodes and hundreds-of-thousands of compute cores capable of yielding petaflops of performance. In systems of this size failure detection is complicated and root-cause diagnosis difficult. This paper describes our recent work in the identification of anomalies in monitoring data and system logs to provide further insights into machine status, runtime behavior, failure modes and failure root causes. It discusses the details of an initial prototype that gathers the data and uses statistical techniques for analysis.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5407992,no
Study on performance testing of index server developed as ISAPI Extension,2009,"A major concern of most businesses is their ability to meet customers' performance requirements. Correspondingly, in order to ensure the information system to provide service with high quality, it's necessary to test the performance before information system issued. This paper proposes an approach to performance testing of Index Server, which is developed as ISAPI Extension. This paper mainly focuses on a case study that demonstrates the approach to a security updating index server. With avalanche and testing in the test lab, we assessed the performance of the system under both current workloads and those likely to be encountered in the future. In addition, this leds to find out the bottleneck of its performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5408130,no
Electrical equipment fault diagnosis system based on the decomposition products of SF6,2009,"This paper presents electrical equipment fault diagnosis system based on the decomposition products of SF6, and makes an introduction of a method that electrical equipment fault diagnosis system of hardware and software implementation. The hardware uses ATmega128 series single-chip platform, and the software uses advanced wavelet neural network fault diagnosis method. To prove the superiority of this algorithm, we make the simulation and comparison with others. A good synergy of hardware and software is used in SF<sub>6</sub> electrical equipment fault diagnosis, and analysis of SF<sub>6</sub> gas content of decomposition products to judge if the electrical equipment fault happens and to make a fault prediction. In this paper we will introduce the hardware design method and the detailed design of the software just because of strong electromagnetic interference environment and it is very important to the system design, finally by giving the experimental data to prove system reliability and practicality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5410561,no
A HW/SW mixed mechanism to improve the dependability of a stack processor,2009,"In this paper we are presenting a journaling mechanism to improve dependability of a stack processor. This approach is based on a HW/SW mixed mechanism, using hardware error detection and software error correction. The SW correction is based on a rollback mechanism and relies on a journal. The journal is located between processor and the main memory in a way that all the data written into the main memory must pass through it. In case of error detection the rollback mechanism is executed in the journal. Therefore processor re-execute from the last sure states. In this way only validated data is written in the main memory. In order to evaluate the performance of our proposed architecture, the clocks per instruction are measured for different benchmarks in presence of high error rate.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5410845,no
Multilayer Architecture Based on HMM and SVM for Fault Classification,2009,"In order to solve the problems of current machine learning in fault diagnosing system of the chemical plants, a better and effective multilayer architecture model is used in this paper. Hidden Markov model (HMM) is good at dealing with dynamic continuous data and support vector machine (SVM) shows superior performance for classification, especially for limited samples. Combining their respective virtues, we propose a new multilayer architecture model to improve classification accuracy for a fault diagnosis example. The simulation result shows that this two level architecture framework combining HMM and SVM is better than the single HMM method in high classification accuracy with small training samples.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5412442,no
Application of pre-function information in software testing based on defect patterns,2009,"In order to improve precision of software static testing based on defect patterns, inter-function information was extended and applied in software static testing. Pre-function information includes two parts, the effect of context to invoked function and the constraint of invoked function to context, which can be used to detect the common defects, such as null pointer defect, un-initial defect, dangling pointer defect, illegal operation defect, out of bounds defect and so on. Experiments show that, pre-function information can reduce false negative in software static testing effectively.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5412894,no
Computer simulation of the grey system prediction method and its application in water quality prediction,2009,"The water quality prediction is an important basis to implement water pollution control programs. In this paper, the gray system method is used to build a mathematical model of water quality prediction for the Yangtze River and using computer to simulate it to complete the implementation of the method for solving the model. Finally we find the method to solve problem of predicting the water quality of Yangtze River. The key problems are focused on the following two issues. (1) If we do not take more effective control measures, we make prediction analysis of the future development trendbased on past data. (2). According to the prediction analysis, we use computer to simulate that how much sewage we need to address each year.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5412927,no
Analog circuit fault diagnosis based on artificial neural network and embedded system,2009,Analog circuit fault diagnosis system based on S3C2410 embedded board is achieved in this paper. The hardware and software design are presented. Momentum addition BP neural network algorithm with embedded system is applied to that system. Real-time data collection and on-line detection of analog circuit fault condition are designed as the basic functions in the embedded system. Diagnosis system of intelligence and minimization is realized actually.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5412941,no
Hierarchical parametric test metrics estimation: A Î£Î” converter BIST case study,2009,"In this paper we propose a method for evaluating test measurements for complex circuits that are difficult to simulate. The evaluation aims at estimating test metrics, such as parametric test escape and yield loss, with parts per million (ppm) accuracy. To achieve this, the method combines behavioral modeling, density estimation, and regression. The method is demonstrated for a previously proposed Built-In Self-Test (BIST) technique for Î£Î” Analog-to-Digital Converters (ADC) explaining in detail the derivation of a behavioral model that captures the main nonidealities in the circuit. The estimated test metrics are further analyzed in order to uncover trends in a large device sample that explain the source of erroneous test decisions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5413173,no
A no-reference perceptual blur metric using histogram of gradient profile sharpness,2009,"No-reference measurement of blurring artifacts in images is a challenging problem in image quality assessment field. One of the difficulties is that the inherently blurry regions in some natural images may disturb the evaluation of blurring artifacts. In this paper, we study the image gradients along local image structures and propose a new perceptual blur metric to deal with the above problem. The gradient profile sharpness of image edge is efficiently calculated along horizontal or vertical direction. Then the sharpness distribution histogram rectified by just noticeable distortion (JND) threshold is used to evaluate the blurring artifacts and assess the image quality. Experimental results show that the proposed method can achieve good image quality prediction performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5413545,no
Resource prediction and quality control for parallel execution of heterogeneous medical imaging tasks,2009,"We have established a novel control system for combining the parallel execution of deterministic and non-deterministic medical imaging applications on a single platform, sharing the same constrained resources. The control system aims at avoiding resource overload and ensuring throughput and latency of critical applications, by means of accurate resource-usage prediction. Our approach is based on modeling the required computation tasks, by employing a combination of weighted moving-average filtering and scenario-based Markov chains to predict the execution. Experimental validation on medical image processing shows an accuracy of 97%. As a result, the latency variation within non-deterministic analysis applications is reduced by 70% by adaptively splitting/merging of tasks. Furthermore, the parallel execution of a deterministic live-viewing application features constant throughput and latency by dynamically switching between quality modes. Interestingly, our solution can successfully be reused for alternative applications with several parallel streams, like in surveillance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5414222,no
Logo insertion transcoding for H.264/AVC compressed video,2009,"H.264/AVC quickly gains ground in many aspects of video applications, due to its superior coding performance. Inserting a company logo into H.264/AVC compressed video streams has been a highly desirable application in the TV telecasting industry. In this paper, we propose a novel and efficient logo-insertion scheme for H.264/AVC compressed videos. Our proposed scheme overcomes the numerous coding dependencies, and minimizes the changes to the original compressed videos. Experimental results show that our proposed transcoding scheme achieves extraordinary video quality and significantly reduces the bit rate and computational cost. Compared with the cascaded transcoding scheme, our proposed logo-insertion method achieves an average of 1.16 dB PSNR increase, or a 68.6% bit-rate reduction. Our scheme also dramatically reduces the total transcoding time and the motion-estimation time by at least 67.2% and 97.4%, respectively.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5414225,no
Development of an Electrical Power Quality Monitor based on a PC,2009,"This paper describes an electric power quality monitor developed at the University of Minho. The hardware of the monitor is constituted by four current sensors based on Rogowski effect, four voltage sensors based on Hall effect, a signal conditioning board and a computer. The software of the monitor consists of several applications, and it is based on LabVIEW. The developed applications allow the equipment to function as a digital scope, analyze harmonic content, detect and record disturbances in the voltage (wave shapes, sags, swells, and interruptions), measure energy, power, unbalances, power factor, register and visualize strip charts, register a great number of data in the hard drive and generate reports. This article also depicts an electrical power quality monitor integrated into active power filters developed at the University of Minho.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5415142,no
Tutorial proposal efficient solving of optimization problems using advanced boolean satisfiability and Integer Linear Programming techniques,2009,"Recent years have seen a tremendous growth in the number of research and development groups at universities, research labs, and companies that have started using Boolean Satisfiability (SAT) algorithms for solving different decision and optimization problems in Computer Science and Engineering. This has lead to the development of highly-efficient SAT solvers that have been successfully applied to solve a wide-range of problems in Electronic Design Automation (EDA), Artificial Intelligence (AI), Networking, Fault Tolerance, Security, and Scheduling. Examples of such problems include automatic test pattern generation for stuck-at faults (ATPG), formal verification of hardware and software, circuit delay computation, FPGA routing, power leakage minimization, power estimation, circuit placement, graph coloring, wireless communications, wavelength assignment, university classroom scheduling, and failure diagnosis in wireless sensor networks. SAT solvers have recently been extended to handle Pseudo-Boolean (PB) constraints which are linear inequalities with integer coefficients. This feature allowed SAT solvers to handle optimization problems, as opposed to only decision problems, and to be applied to a variety of new applications. Recent work has also showed that free open source SAT-based PB solvers can compete with the best generic Integer Linear Programming (ILP) commercial solvers such as CPLEX. This tutorial is aimed at introducing the latest advances in S AT technology. Specifically, we describe the simple new input format of SAT solvers and the common SAT algorithms used to solve decision/optimization problems. In addition, we highlight the use of SAT algorithms in solving a variety of EDA decision and optimization problems and compare its performance to generic ILP solvers. This should guide researchers in solving their existing optimization problems using the new SAT technology. Finally, we provide a prospective on future work on SAT.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5418586,no
Chaos Immune Particle Swarm Optimization Algorithm with Hybrid Discrete Variables and its Application to Mechanical Optimization,2009,"During the iterative process of standard particle swarm optimization (PSO), the premature convergence of particles decreases the algorithm's searching ability. Through analyzing the reason of particle premature convergence during the renewal process, by introducing the selection strategy based on antibody density and initiation based on equal probability chaos, chaos immune particle swarm optimization (CIPSO) algorithm with hybrid discrete variables model was proposed, and its program CIPSO1.0 with Matlab software was developed. Initiation based on chaos makes initial particles possess good performance and the selection strategy based on antibody density makes the particles of immune particle swarm optimization (CIPSO) maintain the diversity during the iterative process, thus overcomes the defect of premature convergence. Example for mechanical optimization indicates that compared with the exiting algorithms, CIPSO gets better result, thus certify the improvement of the algorithm's searching ability by immunity mechanism and chaos initiation particle swarm.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5419466,no
"A multi-tenant oriented performance monitoring, detecting and scheduling architecture based on SLA",2009,"Software as a Service (SaaS) is thriving as a new mode of service delivery and operation with the development of network technology and the maturity of application software. SaaS application providers offer services for multiple tenants through the Ã‚Â¿single-instance multi-tenancyÃ‚Â¿ model, which can effectively reduce service costs due to scale effect. Meanwhile, the providers allocate resources according to the SLA signed with tenants to meet the different needs of service quality. However, the service quality of some tenants will be affected by some abnormal consumption of system resources since both hardware and software resources are shared by tenants. In order to deal with this issue, we proposed a multi-tenant oriented monitoring, detecting and scheduling architecture based on SLA for performance isolation. It would monitor service quality of per tenant, discover abnormal status and dynamically adjust the use of resources based on quantization of SLA parameters to ensure the full realization of SLA tasks.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5420114,no
An Airborne imaging Multispectral Polarimeter (AROSS-MSP),2009,"Transport of sediment and organisms in rivers, estuaries and the near-shore ocean is dependent on the dynamics of waves, tides, turbulence, and the currents associated with these interacting bodies of water. We present measurements of waves, currents and turbulence from color and polarization remote sensing in these regions using our Airborne Remote Optical Spotlight System-Multispectral Polarimeter (AROSS-MSP). AROSS-MSP is a 12-channel sensor system that measures 4 color bands (RGB-NTR) and 3 polarization states for the full linear polarization response of the imaged scene. Color and polarimetry, from airborne remotely-sensed time-series imagery, provide unique information for retrieving dynamic environmental parameters relating to sediment transport processes over a larger area than is possible with typical in situ measurements. Typical image footprints provide area coverage on the water surface on the order of 2 square kilometers with 2 m ground sample distance. A significant first step, in advanced sensing systems supporting a wide range of missions for organic UAVs, has been made by the successful development of the Airborne Remote Optical Spotlight System (AROSS) family of sensors. These sensors, in combination with advanced algorithms developed in the Littoral Remote Sensing (LRS) and Tactical Littoral Sensing (TLS) Programs, have exhibited a wide range of important environmental assessment products. An important and unique aspect of this combination of hardware and software has been the collection and processing of time-series imaging data from militarily-relevant standoff ranges that enable characterization of riverine, estuarine and nearshore ocean areas. However, an optimal EO sensor would further split the visible and near-infrared light into its polarimetric components, while simultaneously retaining the spectral components. AROSS-MSP represents the third generation of sophistication in the AROSS series, after AROSS-MultiChannel (AROSS-MC) which was d- veloped to collect and combine time-series image data from a 4-camera sensor package. AROSS-MSP extends the use of color or polarization filters on four panchromatic cameras that was provided by AROSS-MC to 12 simultaneous color and polarization data channels. This particular field of optical remote sensing is developing rapidly, and data of this much more general form is expected to enable the development of a number of additional important environmental data products. Important examples that are presently being researched are: minimizing surface reflections to image the sub-surface water column at greater depth, detecting objects in higher environmental clutter, improving ability to image through marine haze and maximizing wave contrast to improve oce?anographie parameter retrievals such as wave spectra and water depth and currents. These important capabilities can be supported using AROSS-MSP. The AROSS-MSP design approach utilizes a yoke-style positioner, digital framing cameras, and integrated Global Positioning System/Inertial Measurement Unit (GPS/IMU), with a computer-based data acquisition and control system. Attitude and position information are provided by the GPS/IMU, which is mounted on the sensor payload rather than on the airframe. The control system uses this information to calculate the camera pointing direction and maintain the intended geodetic location of the aim point in close proximity to the center of the image while maintaining a standoff range suitable for military applications. To produce high quality images for use in quantitative analysis, robust individual camera and inter-camera calibrations are necessary. AROSS-MSP is optimally focused and imagery is corrected for lens vignetting, non-uniform pixel response, relative radiometry and geometric distortion. The cameras are aligned with each other to sub-pixel accuracy for production of multichannel imagery products and with the IMU for mapping to a geodetic surface. The mapped, corrected",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5422152,no
Real-time and long-term monitoring of phosphate using the in-situ CYCLE sensor,2009,"Dissolved nutrient dynamics broadly affect issues related to public health, ecosystem status and resource sustainability. Modeling ecosystem dynamics and predicting changes in normal variability due to potentially adverse impacts requires sustained and accurate information on nutrient availability. On site sampling is often resource limited which results in sparse data sets with low temporal and spatial density. For nutrient dynamics, sparse data sets will bias analyses because critical time scales for the relevant biogeochemical processes are often far shorter and spatially limited than sampling regimes. While data on an areal basis will always be constrained economically, an in-situ instrument that provides coherent data at a sub-tidal temporal scale can provide a significant improvement in the understanding of nutrient dynamics and biogeochemical cycles. WET Labs has developed an autonomous in-situ phosphate analyzer which is able to monitor variability in the dissolved reactive phosphate concentration (orthophosphate) for months with a sub-tidal sampling regime. The CYCLE phosphate sensor is designed to meet the nutrient monitoring needs of the community using a standard wet chemical method (heteropoly blue) and minimal user expertise. The heteropoly blue method for the determination of soluble reactive phosphate in natural waters is based on the reaction of phosphate ions with an acidified molybdate reagent to yield molybdophosphoric acid, which is then reduced with ascorbic acid to a highly colored blue phosphomolybdate complex. This method is selective, insensitive to most environmental changes (e.g., pH, salinity, temperature), and can provide detection limits in the nM range. The CYCLE sensor uses four micropumps that deliver the two reagents (ascorbic acid and acidified molybdate), ambient water, and a phosphate standard. The flow system incorporates an integrated pump manifold and fluidics housing that includes controller and mixing assemblies virtually i- nsensitive to bubble interference. A 5-cm pathlength reflective tube absorption meter measures the absorption at 880 nm associated with reactive phosphate concentration. Reagents and an on-board phosphate standard for quality assurance are delivered using a novel and simple-to-use cartridge system that eliminates the user's interaction with the reagents. The reagent cartridges are sufficient for more than 1000 samples. The precision of the CYCLE sensor is ~50 nM phosphate, with a dynamic range from ~0 to 10 ?M. The CYCLE sensor operates using 12 VDC input, and has a low current draw (milliamps). CYCLE also has 1 GB on-board data storage capacity, and communicates using a serial interface. The host software for the CYCLE sensor includes a variety of features, including deployment planning and sensor configuration, data processing, plotting of raw and processed data, tracking of reagent usage and a pre and post deployment calibration utility. The instrument has been deployed in a variety of sampling situations: freshwater, estuarine, and ocean. Deployments are typically for over 1000 samples worth of continuous run time without maintenance (4-12 wks). Using the CYCLE phosphate sensor, a sufficient sampling rate (~20-30 minutes per sample) is realized to monitor in-situ nutrient variability over a broad range of time scales including tidal cycles, runoff events, and phytoplankton bloom dynamics. We present a time series of phosphate data collected in Yaquina Bay, Oregon. Combining this data with complimentary measurements, the CYCLE phosphate provides a missing link in understanding nutrient dynamics in Yaquina Bay. We demonstrate that by correlating phosphate variability with nitrate, chlorophyll, dissolved oxygen, turbidity, CDOM, conductivity, and temperature, a greater understanding of the factors influencing nutrient flux in the bay is possible. What nutrients limit production and whether anthropogenic or oceanic sources of nutrients dominate bloom dynamics can",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5422184,no
Implementations of the Navy Coupled Ocean Data Assimilation system at the Naval Oceanographic Office,2009,"The Naval Oceanographic Office uses the Navy Coupled Ocean Data Assimilation (NCODA) system to perform data assimilation for ocean modeling. Currently the system uses a 3D multivariate optimum interpolation (3D MVOI) algorithm to produce outputs of temperature, salinity, geopotential, and u/v velocity. NCODA is run in a standalone mode to support automated ocean data quality control (NCODA OcnQC) and to test software updates. NCODA is also coupled with the Regional/Global Navy Coastal Ocean Model (RNCOM/GNCOM). The RNCOM/NCODA system is being used as part of an Adaptive Sampling and Prediction (ASAP) pre-operational project, that makes use of the Ensemble Transform (ET) and Ensemble Transform Kalman Filter (ET KF) applied to ensemble runs of the RNCOM. The ET KF is used to predict the posterior error covariances resulting from possible profile measurements. These results aid in predicting the impact of ocean observations on the future analysis, and thus allow the direction of limited assets to areas that will have the maximum gain (for applications such as ocean acoustics). A review of these systems will be given as well as examples of the metrics used for the RNCOM/NCODA system, ensemble modeling, and ASAP.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5422232,no
Instrumentation for continuous monitoring in marine environments,2009,"Continuous monitoring data are a useful source of information for the understanding of seasonal chemical and biological changes in marine environments. They are useful to estimate nutrient dynamics, primary and secondary production as well as to assess C, N, P fluxes associated with biogeochemical cycling. More and better water quality data is needed to calculate Maximum Permissible Loading of coastal waters and we need better data to assess trends, to determine current status and impairments, and to test water quality models. For a long time these requirements were not met satisfactorily due to the absence of suitable instrumentation in the market. SYSTEA has tried to bridge this gap since ten years with the development of several field analyzers (NPA, NPA Plus and Pro, DPA series and latest is the WIZ-probe) and by participating in several R&D European projects (EXOCET/D, WARMER) we have proven our ability to build reliable and efficient in-situ probes which are now commercially available. The choice to work in collaboration with scientific institutions specialized in marine ecosystem study has been made very early by SYSTEA and is actually the only company able to offer a complete range of in-situ probes for continuous nutrient analysis, using its exclusive ?-LFA technology fully developed by SYSTEA, in collaboration with Sysmedia S.r.l with remote management capabilities. These innovative technical solutions allow deploying their DPA probe down to - 1500 m depth, maintaining a high level of accuracy and robustness as proved during the European project EXOCET/D in 2006. The WIZ probe is the latest development of SYSTEA, the state of the art portable ""in-situ"" probe, to measure up to four chemical parameters continuously in surface waters or marine environments. The innovative design allows an easy handling and field deployment by the user. WIZ probe allows, in the standard configuration, the detection of four nutrient parameters (orthophosphate, ammonia, nit- ite and nitrate) in low concentrations while autonomously managing the well tested spectrophotometric wet chemistries, and an advanced fluorimetric method for ammonia measurement. Analytical methods have been developed for several other parameters including silicates, iron and trace metals. Results are directly recorded in concentration units; all measured values are stored with date, time and sample optical density (O.D.). The same data are remotely available through a serial communication port, which allows the complete probe configuration and remote control using the external Windows<sup>?</sup> based Wiz Control Panel software.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5422370,no
Modeling and analysis of two-component cluster system,2009,"Businesses today are becoming increasingly dependent on information technology (IT) to meet business-critical demands. The more available a computer system is, the more value it can provide to its users. In many cases high availability (HA) requirement becomes as critical as high performance. Cluster computing has been attracting more and more attention from both the industrial and the academic world for its enormous computing power and scalability. High availability features need to be included to ensure that cluster computing environments can provide continuous services. In order to validate and compare the different techniques, it is often necessary to build models of the system. A typical availability modeling method is based on analytical formalisms such as fault tree, Markov chains, Stochastic Petri Net, etc. In this paper we describe (1) a two-component cluster configuration, and (2) an availability model of that system using Markov modeling techniques.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5423130,no
On the Objective Evaluation of Real-Time Networked Games,2009,"With the recent evolution of network-based multiplayer games and the increasing popularity of online games demanding strict real-time interaction among players - like First Person Shooter (FPS) -, game providers face the problem to correlate network conditions with quality of gaming experience. This paper addresses the problem of the estimation gameplay quality during real-time games; in particular, we focus on FPS ones. Current literature usually considers end-to-end delay as the only important parameter and deducts system performance indexes from graphical ones. Player satisfaction, on the other hand, is usually evaluated in a subjective way: asking the player, or measuring how long he/she stays connected. In this paper we use a testbed with synthetic players (bots) to directly correlate network end-to-end delay and jitter with expected players' satisfaction. Running extensive experiments we argue about effective in-game performances degradation of penalized players. Performances are measured in terms of score and number of actions kills, actually - performed per minute.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5426032,no
Application of single pole auto reclosing in distributaion networks with high pentration of DGS,2009,"Due to continued penetration of DG into existing distribution network, the tripping of DG during network fault condition may affect the network stability and reliability. Operation of DG should be remain during network temporary fault as far as possible in order to maximise the benefits of interconnection of DG and increase reliability of power supply service to consumers. Conventional three pole auto recloser in distribution network become incompatible with the presence of DG because it interrupts the operation of DG during network temporary single phase to earth fault event and cause unnecessary disconnection of DG. Therefore the application of single pole auto reclosing scheme (SPAR) which is widely used in transmission network should be considered in distribution network with DG. Literature survey shows that no investigation has been carried out for the application of fault identification and phase selection techniques to single pole auto reclosing scheme in distribution networks with high penetration of DG. This paper presents the development of an adaptive fault identification and phase selection scheme to be used in the implementation of SPAR in power distribution network with DG. The proposed method uses only three line current measurements at the relay point. The value of line current during prefault condition and transient period of fault condition is processed using condition rules with IF-THEN in order to determine the faulty phase and to initiate single pole auto reclosure. The analysis of the proposed method is performed using PSCAD/EMTDC power system software. Test results show that the proposed method can correctly detect the faulty phase within one cycle. The validity of the proposed method has been tested for different fault locations and network operating modes.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5429509,no
How simulation languages should report results: A modest proposal,2009,"The focus of simulation software environments is on developing simulation models; much less consideration is placed on reporting results. However, the quality of the simulation model is irrelevant if the results are not interpreted correctly. The manner in which results are reported, along with a lack of standardized guidelines for reports, could contribute to the misinterpretation of results. We propose a hierarchical report structure where each reporting level provides additional detail about the simulated performance. Our approach utilizes two recent developments in output analysis: a procedure for omitting statistically meaningless digits in point estimates, and a graphical display called a MORE Plot, which conveys operational risk and statistical error in an intuitive manner on a single graph. Our motivation for developing this approach is to prevent or reduce misinterpretation of simulation results and to provide a foundation for standardized guidelines for reporting.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5429684,no
Using similarity measures for test scenario selection,2009,"Specification based testing involves using specification as the basis for generating test cases. The Unified Modeling Language (UML) consists of diagrams to capture the static and dynamic behaviour of a system. Generating test scenarios using UML activity diagrams produces all possible scenarios which is impossible to test exhaustively. Test scenario selection involves selecting an effective subset of scenarios for testing. This paper presents a new metric based on common subscenario between scenarios, their lengths, and weights based on the position of the common subscenario in the scenario. The aim of this strategy is to select scenarios that are least similar and at the same time provide high coverage. The method is compared with results of random selection to study effectiveness of our technique.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5429829,no
Optical measurement and management system,2009,"This paper proposes an appreciate approach to alleviate the critical issues of using optical time domain reflectometer (OTDR) for monitoring and troubleshooting a conventional passive optical network (PON) equipped with passive branching device. In this enhancement, OTDR is located at central office (CO) and a tapper circuit is designed to allow the OTDR pulse bypassing the passive branching device in a conventional PON when emitted from CO towards customer sites in downstream direction. This provides a faster, simpler, and less complex way for surveillance applications. A simulation software named Smart Access Network _ Testing, Analyzing and Database (SANTAD) is developed for providing remote controlling, optical monitoring, fault detection, and centralized troubleshooting features to assist networks operators to manage PON more efficiency and deliver high quality of services (QoS) for end-users. SANTAD increases the workforce productivity and facilitates the network management of network through centralized monitoring and troubleshooting from CO.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5431491,no
Automatically Recommending Triage Decisions for Pragmatic Reuse Tasks,2009,"Planning a complex software modification task imposes a high cognitive burden on developers, who must juggle navigating the software, understanding what they see with respect to their task, and deciding how their task should be performed given what they have discovered. Pragmatic reuse tasks, where source code is reused in a white-box fashion, is an example of a complex and error-prone modification task: the developer must plan out which portions of a system to reuse, extract the code, and integrate it into their own system. In this paper we present a recommendation system that automates some aspects of the planning process undertaken by developers during pragmatic reuse tasks. In a retroactive evaluation, we demonstrate that our technique was able to provide the correct recommendation 64% of the time and was incorrect 25% of the time. Our case study suggests that developer investigative behaviour is positively influenced by the use of the recommendation system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5431754,no
Instant-X: Towards a generic API for multimedia middleware,2009,"The globalisation of our society leads to an increasing need for spontaneous communication. However, the development of such applications is a tedious and error-prone process. This results from the fact that in general only basic functionality is available in terms of protocol implementations and encoders/decoders. This leads to inflexible proprietary software systems implementing unavailable functionality on their own. In this work we introduce Instant-X, a novel component-based middleware platform for multimedia applications. Unlike related work, Instant-X provides a generic programming model with an API for essential tasks of multimedia applications with respect to signalling and data transmission. This API abstracts from concrete component implementations and thus allows replacing specific protocol implementations without changing the application code. Furthermore, Instant-X supports dynamic deployment, i.e., unavailable components can be automatically loaded at runtime. To show the feasibility of our approach we evaluated our Instant-X prototype regarding code complexity and performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5439483,no
Comparison of different ANN techniques for automatic defect detection in X-Ray images,2009,"X-ray imaging is extensively used in the NDT. In the conventional method, interpretation of the large number of radiographs for defect detection and evaluation is carried out manually by operator or expert, which makes the system subjective. Also interpretation of large number of images is tedious and may lead to misinterpretation. Automation of Non-Destructive evaluation techniques is gaining greater relevance but automatic analysis of X-Ray images is still a complex problem, as the images are noisy, low contrast with a number of artifacts. ANN's are systems which can be trained to analyze input data based on conditions provided to derive required output. This makes the system automatic reducing the subjective interference in analysis of data. Artificial neural network based systems are thus a feasible solution to this problem of X-Ray NDT. Due to complex nature of input images and noise present, Noise removal becomes a problem in X-Ray images. Preprocessing techniques based on statistical analysis have shown improvement in image noise reduction. Pixels/group of pixels, which deviate from the general structural pattern and grey scale distribution are located. The statistically processed pixel values are used to obtain the features vector from defective as well as from non-defective areas. Software for pre-processing and analyzing NDT images has been developed. Software allows user to train neural networks for defect detection. Once trained satisfactorily, the software scans the new input image and uses the trained ANN for defect detection. The final image with defect regions marked will be displayed. This system can be used to obtain the probable defective areas in a given input image. This paper presents performance of MLP and RBF for detection of defect. The effect of different types of input viz. template and moments on performance of ANN is discussed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5441138,no
Directional relaying during power swing and single-pole tripping,2009,"This paper presents a directional relaying scheme during power swing and single-pole tripping condition using fuzzy logic approach. If any balanced or unbalanced fault occurs during power swing condition, conventional directional relaying algorithm finds limitation due to complex nature of signal. Once a fault is identified as being only on a phase, then the single-pole tripping takes place. During this situation the sequence component based directional relay will have a tendency to trip incorrectly due to the unbalanced loading condition. Again if a single-pole tripping situation happens during power swing, the situation becomes more complex. This paper highlights these issues and proposes a fault direction estimation technique using fuzzy logic approach where feature selection using mutual information technique is emphasized. Performance of the technique is evaluated using the test system simulated through PSCAD/EMTP software.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5442711,no
A biosignal analysis system applied for developing an algorithm predicting critical situations of high risk cardiac patients by hemodynamic monitoring,2009,"A software system for efficient development of high quality biosignal processing algorithms and its application for the Computers in Cardiology Challenge 2009 Predicting Acute Hypotensive Episodes is described. The system is part of a medical research network, and supports import of several standard and non-standard data formats, a modular and therefore extremely flexible signal processing architecture, remote and parallel processing, different data viewers and a framework for annotating signals and for validating and optimizing algorithms. Already in 2001, 2004 and 2006 the system was used for implementing algorithms that successfully took part in the Challenges, respectively. In 2009 we received a perfect score of 10 out of 10 in event 1 and a score of 33 out of 40 in event 2 of the Challenge.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5445306,no
The impact of virtualization on the performance of Massively Multiplayer Online Games,2009,"Today's highly successful Massively Multiplayer Online Games (MMOGs) have millions of registered users and hundreds of thousands of active concurrent users. As a result of the highly dynamic MMOG usage patterns, the MMOG operators pre-provision and then maintain throughout the lifetime of the game tens of thousands of compute resources in data centers located across the world. Until recently, the difficulty of porting the MMOG software services to different platforms made it impractical to dynamically provision resources external to the MMOG operators' data centers. However, virtualization is a new technology that promises to alleviate this problem by providing a uniform computing platform with minimal overhead. To investigate the potential of this new technology, in this paper we propose a new hybrid resource provisioning model that uses a smaller and less expensive set of self-owned data centers, complemented by virtualized cloud computing resources during peak hours. Using real traces from RuneScape, one of the most successful contemporary MMOGs, we evaluate with simulations the effectiveness of the on-demand cloud resource provisioning strategy for MMOGs. We assess the impact of provisioning of virtualized cloud resources, analyze the components of virtualization overhead, and compare provisioning of virtualized resources with direct provisioning of data center resources.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5446227,no
Effectiveness and Cost of Verification Techniques: Preliminary Conclusions on Five Techniques,2009,"A group of 17 students applied 5 unit verification techniques in a simple Java program as training for a formal experiment. The verification techniques applied are desktop inspection, equivalence partitioning and boundary-value analysis, decision table, linearly independent path, and multiple condition coverage. The first one is a static technique, while the others are dynamic. JUnit test cases are generated when dynamic techniques are applied. Both the defects and the execution time are registered. Execution time is considered as a cost measure for the techniques. Preliminary results yield three relevant conclusions. As a first conclusion, performance defects are not easily found. Secondly, unit verification is rather costly and the percentage of defects it detects is low. Finally desktop inspection detects a greater variety of defects than the other techniques.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5452554,no
Adaptive Cross-Diamond Search Algorithm for Fast Block Motion Estimation in H.264,2009,"Motion estimation plays a key role in the H.264. It is the most computationally intensive module. In this paper, we proposed a novel adaptive cross-diamond search (ACDS) algorithm for fast block motion estimation in H.264. The proposed algorithm adaptively chooses different search pattern according to the seven types of macro block division in H.264 and the motion vector distribution characteristics of video sequence. Experimental results show that the proposed ACDS algorithm is much faster than the UMHexagonS algorithm, whereas similar prediction quality is still maintained.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5454596,no
Research of E-Commerce Software Quality Evaluation Using Rough Set Theory,2009,"In order to overcome the subjectivity on weight allocation of the traditional evaluation methods in e-commerce software quality evaluation, based on the object classification capability of rough set theory, the paper presents a completely data-driven e-commerce software quality evaluation method using measurement methods of knowledge dependence and attribute importance in rough set theory. It overcomes the subjectivity and ambiguity of traditional evaluation methods. An example is presented to explain the evaluation process.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5454693,no
Predicting Object-Oriented Software Maintainability Using Projection Pursuit Regression,2009,"This paper presents ongoing work on using projection pursuit regression model to predict object-oriented software maintainability. The maintainability is measured as the number of changes made to code during a maintenance period by means of object-oriented software metrics. To evaluate the benefits of using PPR over nonlinear modeling techniques, we also build artificial neural network model, and multivariate adaptive regression splines model. The models performance is evaluated and compared using leave-one-out cross-validation with RMSE. The results suggest that PPR can predict more accurately than the other two modeling techniques. The study also provided the useful information on how to constructing software quality model.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5455686,no
Automatic configuration of spectral dimensionality reduction methods for 3D human pose estimation,2009,"In this paper, our main contribution is a framework for the automatic configuration of any spectral dimensionality reduction methods. This is achieved, first, by introducing the mutual information measure to assess the quality of discovered embedded spaces. Secondly, we overcome the deficiency of mapping function in spectral dimensionality reduction approaches by proposing data projection between spaces based on fully automatic and dynamically adjustable Radial Basis Function network. Finally, this automatic framework is evaluated in the context of 3D human pose estimation. We demonstrate mutual information measure outperforms all current space assessment metrics. Moreover, experiments show the mapping associated to the induced embedded space displays good generalization properties. In particular, it allows improvement of accuracy by around 30% when refining 3D pose estimates of a walking sequence produced by an activity independent method.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5457457,no
"Efficient, high-quality image contour detection",2009,"Image contour detection is fundamental to many image analysis applications, including image segmentation, object recognition and classification. However, highly accurate image contour detection algorithms are also very computationally intensive, which limits their applicability, even for offline batch processing. In this work, we examine efficient parallel algorithms for performing image contour detection, with particular attention paid to local image analysis as well as the generalized eigensolver used in Normalized Cuts. Combining these algorithms into a contour detector, along with careful implementation on highly parallel, commodity processors from Nvidia, our contour detector provides uncompromised contour accuracy, with an F-metric of 0.70 on the Berkeley Segmentation Dataset. Runtime is reduced from 4 minutes to 1.8 seconds. The efficiency gains we realize enable high-quality image contour detection on much larger images than previously practical, and the algorithms we propose are applicable to several image segmentation approaches. Efficient, scalable, yet highly accurate image contour detection will facilitate increased performance in many computer vision applications.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5459410,no
Mixing Simulated and Actual Hardware Devices to Validate Device Drivers in a Complex Embedded Platform,2009,"The structure and the functionalities of a device driver are strongly influenced by the target platform architecture, as well as by the device communication protocol. This makes the generation of device drivers designed for complex embedded platforms a very time consuming and error prone activity. Validation becomes then a nodal point in the design flow. The aim of this paper is to present a co-simulation framework that allows validation of device drivers. The proposed framework supports all mechanisms used by device drivers to communicate with HW devices so that both modeled and actual components can be included in the simulated embedded platform. In this way, the generated code can be tested and validated even if the final platform is not ready yet. The framework has been applied to some examples to highlight the performance and effectiveness of this approach.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5460811,no
Performance and scalability of M/M/c based queuing model of the SIP Proxy Server - a practical approach,2009,"In recent years, Session Initiation Protocol (SIP) based Voice over IP (VoIP) based applications are alternative to the traditional Public Switched Telephone Networks (PSTN) because of its flexibility in the implementation of new features and services. The Session Initiation Protocol (SIP) is becoming a popular signaling protocol for Voice over IP (VoIP) based applications. The SIP Proxy server is a software application that provides call routing services by parsing and forwarding all the incoming SIP packets in an IP telephony network. The efficiency of this process can create large scale, highly reliable packet voice networks for service providers and enterprises. Since, SIP Proxy server performance can be characterized by its transaction states of each SIP session, we proposed the M/M/c performance model of the SIP Proxy Server and studied some of the key performance benchmarks such as server utilization, queue size and memory utilization. Provided the comparative results between the predicted results with the experimental results conducted in a lab environment.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5464717,no
Vehicle speed detection system,2009,"This research intends to develop the vehicle speed detection system using image processing technique. Overall works are the software development of a system that requires a video scene, which consists of the following components: moving vehicle, starting reference point and ending reference point. The system is designed to detect the position of the moving vehicle in the scene and the position of the reference points and calculate the speed of each static image frame from the detected positions. The vehicle speed detection from a video frame system consists of six major components: 1) Image Acquisition, for collecting a series of single images from the video scene and storing them in the temporary storage. 2) Image Enhancement, to improve some characteristics of the single image in order to provide more accuracy and better future performance. 3) Image Segmentation, to perform the vehicle position detection using image differentiation. 4) Image Analysis, to analyze the position of the reference starting point and the reference ending point, using a threshold technique. 5) Speed Detection, to calculate the speed of each vehicle in the single image frame using the detection vehicle position and the reference point positions, and 6) Report, to convey the information to the end user as readable information. The experimentation has been made in order to assess three qualities: 1) Usability, to prove that the system can determine vehicle speed under the specific conditions laid out. 2) Performance, and 3) Effectiveness. The results show that the system works with highest performance at resolution 320Ã—240. It takes around 70 seconds to detect a moving vehicle in a video scene.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5478629,no
A New approach of Rate-Quantization modeling for Intra and Inter frames in H.264 rate control,2009,"Video encoding rate control has been the research focus in the recent years. The existing rate control algorithms use Rate-Distortion (R-D) or Rate-Quantization (R-Q) models. These latter assume that the enhancement of the bit allocation process, the quantization parameter determination and the buffer management are essentially based on the improvement of complexity measures estimation. Inaccurate estimation leads to wrong quantization parameters and affects significantly the global performance. Therefore, several improved frame complexity measures are proposed in literature. The efficiency of such measures is however limited by the linear prediction model which remains still inaccurate to encode complexity between two neighbour frames. In this paper, we propose a new approach of Rate-Quantization modeling for both Intra and Inter frame without any complexity measure estimation. This approach results from extensive experiments and proposes two Rate-Quantization models. The first one (M1) aims at determining an optimal initial quantization parameter for Intra frames based on sequence target bit-rate and frame rate. The second model (M2) determines the quantization parameter of Inter coding unit (Frame or Macroblock) according to the statistics of the previous coded ones. This model substitutes both linear and quadratic models used in H.264 rate controller. The simulations have been carried out using both JM10.2 and JM15.0 reference softwares. Compared to JM10.2, M1 alone, improves the PSNR up to 1.93dB, M2 achieves a closer output bit-rate and similar quality while the combined model (M1+M2) minimizes the computational complexity. (M1+M2) outperforms both JM10.2 and JM15.0 in terms of PSNR.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5478701,no
Design and implementation of adaptive scalable streaming system over heterogeneous network,2009,"Distribution of multimedia content through the internet has gain popularity with the advancement of multimedia technology through software application with different transmission capabilities. Current conventional system implementation works perfectly for a guaranteed network bandwidth. However, when the packet experience losses due to drop in bandwidth, it could not preserve an acceptable video quality over the heterogeneous network. Thus, if the bandwidth could be determined in real-time, an application that controls the scalability of video stream sent over the channel can be used to control the video quality. This is done by reducing the bit-rate of the video to be sent according to current available bandwidth. In this paper, a scalable video streaming system with automate bandwidth control over heterogeneous network, using spatial scalability approach and end-to-end available bandwidth estimation are designed and implemented. Distribution of multimedia data streaming would be improved without any jitter and could fit to any devices with various performance capabilities.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5478725,no
Flying capacitor multicell converter based dynamic voltage restorer,2009,"One of the major power quality problems in distribution systems are voltage sags. This paper deals with a dynamic voltage restorer (DVR) as one of the different solutions to compensate these sags and protect sensitive loads. However, the quality of DVR output voltage itself, such as THD, is important. So, in this paper a configuration of DVR based on flying capacitor multicell (FCM) converter is proposed. The main properties of FCM converter, which causes increase in the number of output voltage levels, are transformer-less operation and natural self-balancing of flying capacitors voltages. The proposed DVR consists of a set of series converter and shunt rectifier connected back-to-back. To guarantee the proper operation of shunt rectifier and maintaining the dc link voltage at the desired value, which results in suitable performance of DVR, the DVR is characterized by installing the series converter on the source-side and the shunt rectifier on the load-side. Also, the pre-sag compensation strategy and the proposed voltage sag detection and DVR reference voltages determination methods based on synchronous reference frame (SRF) are adopted as the control system. The proposed DVR is simulated using PSCAD/EMTDC software and simulation results are presented to validate its effectiveness.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5484074,no
"Integrated Detection of Attacks Against Browsers, Web Applications and Databases",2009,"Anomaly-based techniques were exploited successfully to implement protection mechanisms for various systems. Recently, these approaches have been ported to the web domain under the name of ""web application anomaly detectors"" (or firewalls) with promising results. In particular, those capable of automatically building specifications, or models, of the protected application by observing its traffic (e.g., network packets, system calls, or HTTP requests and responses) are particularly interesting, since they can be deployed with little effort. Typically, the detection accuracy of these systems is significantly influenced by the model building phase (often called training), which clearly depends upon the quality of the observed traffic, which should resemble the normal activity of the protected application and must be also free from attacks. Otherwise, detection may result in significant amounts of false positives (i.e., benign events flagged as anomalous) and negatives (i.e., undetected threats). In this work we describe Masibty, a web application anomaly detector that have some interesting properties. First, it requires the training data not to be attack-free. Secondly, not only it protects the monitored application, it also detects and blocks malicious client-side threats before they are sent to the browser. Third, Masibty intercepts the queries before they are sent to the database, correlates them with the corresponding HTTP requests and blocks those deemed anomalous. Both the accuracy and the performance have been evaluated on real-world web applications with interesting results. The system is almost not influenced by the presence of attacks in the training data and shows only a negligible amount of false positives, although this is paid in terms of a slight performance overhead.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5494330,no
The criteria of system development methodology choice for the minimization of the time and resources,2009,"Due to decreasing of demand on the corporate informational systems becomes an important question the improvement of price-quality relation of the software product. It requires increasing of the quality level in the same time with prime cost decreasing. The aim of the given work is to describe the possibility of resource usage optimization applying different system development methodologies. In the analysis were used waterfall, iteration, spiral, and agile methodologies. The criteria of choice from them are described taking into account necessity of project's time and cost spending minimization. In the work were analyses risks that can appear at application of different methodologies, and ways of their minimization. As a result of the performed analysis in the article is proposed to choose methodology not for the company as a whole but for each separate project. In the article are given practical recommendations that can be used in the real projects.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5501150,no
Estimation of program reverse semantic traceability influence at program reliability with assistance of object-oriented metrics,2009,"In article the approach to the estimation of program reverse semantic traceability (RST) influence on program reliability with assistance of object-oriented metrics is proposed. In order to estimate reasonability of RST usage it is naturally to define how it influences on major adjectives of software development project: project cost, quality of the developed application, etc. At present object-oriented metrics of Chidamber and Kemerer are widely used for predictive estimation of software reliability at early stage of life cycle. In number of works, for example, it is proposed to use logistic regression for estimation of probability Ï€ (that a module will have a fault). The parameters of this model are found by maximal likelihood method with calculation of object-oriented metrics. The paper shows how to change the software reliability model parameters, that was received using logistic regression, in order to estimate influence of program RST on program reliability.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5501175,no
Automatic defects detection in industrial C/C++ software,2009,"The solution to the problem of automatic defects detection in industrial software is covered in this paper. The results of the experiments with the existing tools are presented. These results stand for inadequate efficiency of the implemented analysis. Existing source code static analysis methods and defects detection algorithms are covered. The program model and the analysis algorithms based on existing approaches are proposed. The problems of co-execution of different analysis algorithms are explored. The ways for improvement of analysis precision and algorithms performance are proposed. Advantages of the approaches developed are: soundness of a solution, full support of the features of target programming languages and analysis of the programs lacking full source code using annotations mechanism. The algorithms proposed in the paper are implemented in the automatic defects detection tool.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5501189,no
The ATLAS calorimeter trigger commissioning using cosmic ray data,2009,"While waiting for the first LHC collisions, the ATLAS detector is undergoing integration exercises performing cosmic ray data taking runs. The ATLAS calorimeter trigger software uses a common framework capable of retrieving, decoding and providing data to different particle identification algorithms. This work reports on the results obtained during the first data-taking period of 2009 with the calorimeter trigger. Experts from detector areas such as calorimetry, data flow management, data quality monitoring and the trigger worked together to reach stable data taking conditions. Issues like noise readout channels were addressed and the global trigger robustness was evaluated.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5503673,no
Simulation Model of Team Software Process Using Temporal Parallel Automata,2009,"Team Software Process (TSP) is brought forward to help improve the efficiency of software development process. Further, establishing simulation model for TSP can facilitate developers' understanding, forecast possible shortcomings and bottlenecks in process and assist developers' decision-making, supervise and control project development process. In this paper, firstly Temporal Parallel Automata (TPA) theory expanded from the finite state automata theory is applied into software process modeling. Secondly the concepts and the process control structures of TSP which are divided into sequence, AND-join, AND-split, OR-join, OR-split and Loop structure are mapped to TPA, thereby the TSP simulation model based on TPA is built. Thirdly the definition of soundness is given for soundness verification of process model. Finally, an instance is used to verify the validity of process model. It is proved that activity planning, resource allocation and schedule control of software process can be implemented effectively by the model.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5521589,no
A new critical variable analysis in processor-based systems,2009,"Determining the dependability of integrated systems with respect to soft errors is necessary for a growing number of applications. The most critical information must be identified when selective hardening is necessary to achieve good efficiency/cost trade-offs. In processor-based systems, the most critical variables must thus be identified in the application program. An improved algorithm for critical variable identification is described and validated with respect to fault injection results.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5994671,no
VGrADS: enabling e-Science workflows on grids and clouds with fault tolerance,2009,"Today's scientific workflows use distributed heterogeneous resources through diverse grid and cloud interfaces that are often hard to program. In addition, especially for time-sensitive critical applications, predictable quality of service is necessary across these distributed resources. VGrADS' virtual grid execution system (vgES) provides an uniform qualitative resource abstraction over grid and cloud systems. We apply vgES for scheduling a set of deadline sensitive weather forecasting workflows. Specifically, this paper reports on our experiences with (1) virtualized reservations for batchqueue systems, (2) coordinated usage of TeraGrid (batch queue), Amazon EC2 (cloud), our own clusters (batch queue) and Eucalyptus (cloud) resources, and (3) fault tolerance through automated task replication. The combined effect of these techniques was to enable a new workflow planning method to balance performance, reliability and cost considerations. The results point toward improved resource selection and execution management support for a variety of e-Science applications over grids and cloud systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6375523,no
On improving dependability of the numerical GPC algorithm,2009,The paper studies the dependability of software implementation of the numerical Generalized Predictive Control (GPC) Model Predictive Control (MPC) algorithm. The algorithm is implemented for a control system of a multivariable chemical reactor - a process with strong cross-couplings. Fault sensitivity of the proposed implementations is verified in experiments with a software implemented fault injector. Experimental methodology of disturbing the software implementation of the control algorithm is presented. The impact of faults on the quality of the controller performance is analysed and techniques improving the dependability of the algorithm implementation are proposed. The experimental results prove the efficiency of the software improvements.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7074598,no
Trade-off between safety and normal-case control performance based on probabilistic safety management of control laws,2009,"This paper presents a probabilistic safety management framework for control laws to provide a balance between normal-case performance, safety and fault-case performance according to the international standard on safety, IEC 61508. It is based on multiobjective design for simultaneous problems for each context to optimize only normal-case performance out of the whole including fault-case performance. Also the framework establishes the existence of trade-off between them quantitatively for the first time ever.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7074785,no
Integrated process and control system model for product quality control - A soft-sensor based application,2009,"In the near future of chemical industry, communication between design, manufacturing, marketing and management should be centered on modeling and simulation, which could integrate the whole product and process development chains, process units and subdivisions of the company. Solutions to this topic often set aside one or more component from product, process and control models, hence as a novel know-how, an information system methodology was developed. Its structure integrates models of these components with process data warehouse where integration means information, location, application and time integrity. It supports complex engineering tasks related to analysis of system performance, process optimization, operator training systems (OTS), decision support systems (DSS), reverse engineering or software sensors (soft-sensors). The case study in this article presents the application of the proposed methodology for product quality soft-sensor application by on-line melt index prediction of an operating polymerization technology.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7074941,no
A fast method for scaling color images,2009,"Image scaling is an important processing step in any digital imaging chain containing a camera sensor and a display. Although resolutions of the mobile displays have increased to the level that is usable for imaging, images often have larger resolution than the mobile displays. In this paper we propose a software based fast and good quality image scaling procedure, which is suitable for mobile implementations. We describe our method in detail and we present experiments showing the performances of our approach on real images.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7077414,no
An extensive comparison of bug prediction approaches,2010,"Reliably predicting software defects is one of software engineering's holy grails. Researchers have devised and implemented a plethora of bug prediction approaches varying in terms of accuracy, complexity and the input data they require. However, the absence of an established benchmark makes it hard, if not impossible, to compare approaches. We present a benchmark for defect prediction, in the form of a publicly available data set consisting of several software systems, and provide an extensive comparison of the explanative and predictive power of well-known bug prediction approaches, together with novel approaches we devised. Based on the results, we discuss the performance and stability of the approaches with respect to our benchmark and deduce a number of insights on bug prediction models.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463279,yes
Assessing UML design metrics for predicting fault-prone classes in a Java system,2010,"Identifying and fixing software problems before implementation are believed to be much cheaper than after implementation. Hence, it follows that predicting fault-proneness of software modules based on early software artifacts like software design is beneficial as it allows software engineers to perform early predictions to anticipate and avoid faults early enough. Taking this motivation into consideration, in this paper we evaluate the usefulness of UML design metrics to predict fault-proneness of Java classes. We use historical data of a significant industrial Java system to build and validate a UML-based prediction model. Based on the case study we have found that level of detail of messages and import coupling-both measured from sequence diagrams, are significant predictors of class fault-proneness. We also learn that the prediction model built exclusively using the UML design metrics demonstrates a better accuracy than the one built exclusively using code metrics.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463285,yes
An empirical approach for software fault prediction,2010,"Measuring software quality in terms of fault proneness of data can help the tomorrow's programmers to predict the fault prone areas in the projects before development. Knowing the faulty areas early from previous developed projects can be used to allocate experienced professionals for development of fault prone modules. Experienced persons can emphasize the faulty areas and can get the solutions in minimum time and budget that in turn increases software quality and customer satisfaction. We have used Fuzzy C Means clustering technique for the prediction of faulty/ non-faulty modules in the project. The datasets used for training and testing modules available from NASA projects namely CM1, PC1 and JM1 include requirement and code metrics which are then combined to get a combination metric model. These three models are then compared with each other and the results show that combination metric model is found to be the best prediction model among three. Also, this approach is compared with others in the literature and is proved to be more accurate. This approach has been implemented in MATLAB 7.9.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5578698,yes
New Conceptual Coupling and Cohesion Metrics for Object-Oriented Systems,2010,"The paper presents two novel conceptual metrics for measuring coupling and cohesion in software systems. Our first metric, Conceptual Coupling between Object classes (CCBO), is based on the well-known CBO coupling metric, while the other metric, Conceptual Lack of Cohesion on Methods (CLCOM5), is based on the LCOM5 cohesion metric. One advantage of the proposed conceptual metrics is that they can be computed in a simpler (and in many cases, programming language independent) way as compared to some of the structural metrics. We empirically studied CCBO and CLCOM5 for predicting fault-proneness of classes in a large open source system and compared these metrics with a host of existing structural and conceptual metrics for the same task. As the result, we found that the proposed conceptual metrics, when used in conjunction, can predict bugs nearly as precisely as the 58 structural metrics available in the Columbus source code quality framework and can be effectively combined with these metrics to improve bug prediction.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5601833,yes
Change Bursts as Defect Predictors,2010,"In software development, every change induces a risk. What happens if code changes again and again in some period of time? In an empirical study on Windows Vista, we found that the features of such change bursts have the highest predictive power for defect-prone components. With precision and recall values well above 90%, change bursts significantly improve upon earlier predictors such as complexity metrics, code churn, or organizational structure. As they only rely on version history and a controlled change process, change bursts are straight-forward to detect and deploy.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635057,yes
Self-Consistent MPI Performance Guidelines,2010,"Message passing using the Message-Passing Interface (MPI) is at present the most widely adopted framework for programming parallel applications for distributed memory and clustered parallel systems. For reasons of (universal) implementability, the MPI standard does not state any specific performance guarantees, but users expect MPI implementations to deliver good and consistent performance in the sense of efficient utilization of the underlying parallel (communication) system. For performance portability reasons, users also naturally desire communication optimizations performed on one parallel platform with one MPI implementation to be preserved when switching to another MPI implementation on another platform. We address the problem of ensuring performance consistency and portability by formulating performance guidelines and conditions that are desirable for good MPI implementations to fulfill. Instead of prescribing a specific performance model (which may be realistic on some systems, under some MPI protocol and algorithm assumptions, etc.), we formulate these guidelines by relating the performance of various aspects of the semantically strongly interrelated MPI standard to each other. Common-sense expectations, for instance, suggest that no MPI function should perform worse than a combination of other MPI functions that implement the same functionality, no specialized function should perform worse than a more general function that can implement the same functionality, no function with weak semantic guarantees should perform worse than a similar function with stronger semantics, and so on. Such guidelines may enable implementers to provide higher quality MPI implementations, minimize performance surprises, and eliminate the need for users to make special, nonportable optimizations by hand. We introduce and semiformalize the concept of self-consistent performance guidelines for MPI, and provide a (nonexhaustive) set of such guidelines in a form that could be automat- - ically verified by benchmarks and experiment management tools. We present experimental results that show cases where guidelines are not satisfied in common MPI implementations, thereby indicating room for improvement in today's MPI implementations.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5184825,no
Recent Developments in Fault Detection and Power Loss Estimation of Electrolytic Capacitors,2010,"This paper proposes a comparative study of current-controlled hysteresis and pulsewidth modulation (PWM) techniques, and their influence upon power loss dissipation in a power-factor controller (PFC) output filtering capacitors. First, theoretical calculation of low-frequency and high-frequency components of the capacitor current is presented in the two cases, as well as the total harmonic distortion of the source current. Second, we prove that the methods already used to determine the capacitor power losses are not accurate because of the capacitor model chosen. In fact, a new electric equivalent scheme of electrolytic capacitors is determined using genetic algorithms. This model, characterized by frequency-independent parameters, redraws with accuracy the capacitor behavior for large frequency and temperature ranges. Thereby, the new capacitor model is integrated into the converter, and then, software simulation is carried out to determine the power losses for both control techniques. Due to this model, the <i>equivalent series resistance</i> (ESR) increase at high frequencies due to the skin effect is taken into account. Finally, for hysteresis and PWM controls, we suggest a method to determine the value of the series resistance and the remaining time to failure, based on the measurement of the output ripple voltage at steady-state and transient-state converter working.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5200514,no
On the Quality of Service of Crash-Recovery Failure Detectors,2010,We model the probabilistic behavior of a system comprising a failure detector and a monitored crash-recovery target. We extend failure detectors to take account of failure recovery in the target system. This involves extending QoS measures to include the recovery detection speed and proportion of failures detected. We also extend estimating the parameters of the failure detector to achieve a required QoS to configuring the crash-recovery failure detector. We investigate the impact of the dependability of the monitored process on the QoS of our failure detector. Our analysis indicates that variation in the MTTF and MTTR of the monitored process can have a significant impact on the QoS of our failure detector. Our analysis is supported by simulations that validate our theoretical results.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5210115,no
Software Industry Performance: What You Measure Is What You Get,2010,"The software industry's overall performance is uneven and, at first sight, puzzling. Delivery to time and budget is notoriously poor, and productivity shows limited improvement over time, yet quality can be amazingly good. Customers largely bear the costs of the poor aspects of performance. Many factors drive this performance. This article explores whether causal links exist between the overall observed performance and the commonly used performance metrics, estimating methods and processes, and the way these incentivize suppliers. The author proposes a set of possible improvements to current metrics and estimating methods and processes, and concludes that software professionals must educate their customers on the levers that are available to obtain a better all-round performance from their suppliers.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5235133,no
An Online and Noninvasive Technique for the Condition Monitoring of Capacitors in Boost Converters,2010,"Capacitors usually determine the overall lifetime of power converters since they are mainly responsible for breakdowns. Their failure results from the deterioration of their dielectric, the production of gases, and, eventually, their explosion. This process leads to an increase in the capacitor equivalent series resistance (ESR) and a decrease in its capacitance value for both electrolytic and metalized polypropylene film (MPPF) capacitors. In this paper, a novel noninvasive technique for capacitor diagnostic in Boost converters is presented. It can easily be applied online and even in real time, and it is suitable for the operation in both continuous current mode (CCM) and discontinuous current mode (DCM). The technique is based on the double estimations of the ESR and the capacitance, improving the diagnostic reliability. This way, predictive maintenance is provided, and it is possible to alarm for capacitor replacement, avoiding downtime. As the method is intended for railway high-power applications, it has been conceived neither to add any additional hardware in the power stage nor to even slightly modify it. To demonstrate and validate the effectiveness and accuracy of the proposed technique, several simulations and experimental results are discussed in a small prototype. In this prototype, the software for real-time estimation is programmed in a low-cost digital signal processor (DSP).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290120,no
An Integrated Data-Driven Framework for Computing System Management,2010,"With advancement in science and technology, computing systems are becoming increasingly more complex with a growing number of heterogeneous software and hardware components. They are thus becoming more difficult to monitor, manage, and maintain. Traditional approaches to system management have been largely based on domain experts through a knowledge acquisition solution that translates domain knowledge into operating rules and policies. This process has been well known as cumbersome, labor intensive, and error prone. In addition, traditional approaches for system management are difficult to keep up with the rapidly changing environments. There is a pressing need for automatic and efficient approaches to monitor and manage complex computing systems. In this paper, we propose an integrated data-driven framework for computing system management by acquiring the needed knowledge automatically from a large amount of historical log data. Specifically, we apply text mining techniques to automatically categorize the log messages into a set of canonical categories, incorporate temporal information to improve categorization performance, develop temporal mining techniques to discover the relationships between different events, and take a novel approach called event summarization to provide a concise interpretation of the temporal patterns.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5313888,no
Learning a Metric for Code Readability,2010,"In this paper, we explore the concept of code readability and investigate its relation to software quality. With data collected from 120 human annotators, we derive associations between a simple set of local code features and human notions of readability. Using those features, we construct an automated readability measure and show that it can be 80 percent effective and better than a human, on average, at predicting readability judgments. Furthermore, we show that this metric correlates strongly with three measures of software quality: code changes, automated defect reports, and defect log messages. We measure these correlations on over 2.2 million lines of code, as well as longitudinally, over many releases of selected projects. Finally, we discuss the implications of this study on programming language design and engineering practice. For example, our data suggest that comments, in and of themselves, are less important than simple blank lines to local judgments of readability.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5332232,no
Architectural Enhancement and System Software Support for Program Code Integrity Monitoring in Application-Specific Instruction-Set Processors,2010,"Program code in a computer system can be altered either by malicious security attacks or by various faults in microprocessors. At the instruction level, all code modifications are manifested as bit flips. In this paper, we present a generalized methodology for monitoring code integrity at run-time in application-specific instruction-set processors. We embed monitoring microoperations in machine instructions, so the processor is augmented with a hardware monitor automatically. The monitor observes the processor's execution trace at run-time, checks whether it aligns with the expected program behavior, and signals any mismatches. Since the monitor works at a level below the instructions, the monitoring mechanism cannot be bypassed by software or compromised by malicious users. We discuss the ability and limitation of such monitoring mechanism for detecting both soft errors and code injection attacks. We propose two different schemes for managing the monitor, the operating system (OS) managed and application controlled, and design the constituent components within the monitoring architecture. Experimental results show that with an effective hash function implementation, our microarchitectural support can detect program code integrity compromises at a high probability with small area overhead and little performance degradation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5332238,no
Low-Complexity Transcoding of JPEG Images With Near-Optimal Quality Using a Predictive Quality Factor and Scaling Parameters,2010,"A common transcoding operation consists of reducing the file size of a JPEG image to meet bandwidth or device constraints. This can be achieved by reducing its quality factor (QF) or reducing its resolution, or both. In this paper, using the structural similarity (SSIM) index as the quality metric, we present a system capable of estimating the QF and scaling parameters to achieve optimal quality while meeting a device's constraints. We then propose a novel low-complexity JPEG transcoding system which delivers near-optimal quality. The system is capable of predicting the best combination of QF and scaling parameters for a wide range of device constraints and viewing conditions. Although its computational complexity is an order of magnitude smaller than the system providing optimal quality, the proposed system yields quality results very similar to those of the optimal system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5342473,no
Modulation Quality Measurement in WiMAX Systems Through a Fully Digital Signal Processing Approach,2010,"The performance assessment of worldwide interoperability for microwave access (WiMAX) systems is dealt with. A fully digital signal processing approach for modulation quality measurement is proposed, which is particularly addressed to transmitters based on orthogonal frequency-division multiplexing (OFDM) modulation. WiMAX technology deployment is rapidly increasing. To aid researchers, manufactures, and technicians in designing, realizing, and installing devices and apparatuses, some measurement solutions are already available, and new ones are being released on the market. All of them are arranged to complement an <i>ad hoc</i> digital signal processing software with an existing specialized measurement instrument such as a real-time spectrum analyzer or a vector signal analyzer. Furthermore, they strictly rely on a preliminary analog downconversion of the radio-frequency input signal, which is a basic front-end function provided by the cited instruments, to suitably digitize and digitally process the acquired samples. In the same way as the aforementioned solutions, the proposed approach takes advantage of existing instruments, but different from them, it provides for a direct digitization of the radio-frequency input signal. No downconversion is needed, and the use of general-purpose measurement hardware such as digital scopes or data acquisition systems is thus possible. A proper digital signal processing algorithm, which was designed and implemented by the authors, then demodulates the digitized signal, extracts the desired measurement information from its baseband components, and assesses its modulation quality. The results of several experiments conducted on laboratory WiMAX signals show the effectiveness and reliability of the approach with respect to the major competitive solutions; its superior performance in special physical-layer conditions is also highlighted.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5353716,no
Hybrid Simulated Annealing and Its Application to Optimization of Hidden Markov Models for Visual Speech Recognition,2010,"We propose a novel stochastic optimization algorithm, <i>hybrid simulated annealing</i> (SA), to train hidden Markov models (HMMs) for visual speech recognition. In our algorithm, SA is combined with a local optimization operator that substitutes a better solution for the current one to improve the convergence speed and the quality of solutions. We mathematically prove that the sequence of the objective values converges in probability to the global optimum in the algorithm. The algorithm is applied to train HMMs that are used as visual speech recognizers. While the popular training method of HMMs, the expectation-maximization algorithm, achieves only local optima in the parameter space, the proposed method can perform global optimization of the parameters of HMMs and thereby obtain solutions yielding improved recognition performance. The superiority of the proposed algorithm to the conventional ones is demonstrated via isolated word recognition experiments.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5373955,no
Performability Analysis of Multistate Computing Systems Using Multivalued Decision Diagrams,2010,"A distinct characteristic of multistate systems (MSS) is that the systems and/or their components may exhibit multiple performance levels (or states) varying from perfect operation to complete failure. MSS can model behaviors such as shared loads, performance degradation, imperfect fault coverage, standby redundancy, limited repair resources, and limited link capacities. The nonbinary state property of MSS and their components as well as dependencies existing among different states of the same component make the analysis of MSS difficult. This paper proposes efficient algorithms for analyzing MSS using multivalued decision diagrams (MDD). Various reliability, availability, and performability measures based on state probabilities or failure frequencies are considered. The application and advantages of the proposed algorithms are demonstrated through two examples. Furthermore, experimental results on a set of benchmark examples are presented to illustrate the advantages of the proposed MDD-based method for the performability analysis of MSS, as compared to the existing methods.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5374373,no
Evaluation of Accuracy in Design Pattern Occurrence Detection,2010,"Detection of design pattern occurrences is part of several solutions to software engineering problems, and high accuracy of detection is important to help solve the actual problems. The improvement in accuracy of design pattern occurrence detection requires some way of evaluating various approaches. Currently, there are several different methods used in the community to evaluate accuracy. We show that these differences may greatly influence the accuracy results, which makes it nearly impossible to compare the quality of different techniques. We propose a benchmark suite to improve the situation and a community effort to contribute to, and evolve, the benchmark suite. Also, we propose fine-grained metrics assessing the accuracy of various approaches in the benchmark suite. This allows comparing the detection techniques and helps improve the accuracy of detecting design pattern occurrences.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5374428,no
Assessing Software Service Quality and Trustworthiness at Selection Time,2010,"The integration of external software in project development is challenging and risky, notably because the execution quality of the software and the trustworthiness of the software provider may be unknown at integration time. This is a timely problem and of increasing importance with the advent of the SaaS model of service delivery. Therefore, in choosing the SaaS service to utilize, project managers must identify and evaluate the level of risk associated with each candidate. Trust is commonly assessed through reputation systems; however, existing systems rely on ratings provided by consumers. This raises numerous issues involving the subjectivity and unfairness of the service ratings. This paper describes a framework for reputation-aware software service selection and rating. A selection algorithm is devised for service recommendation, providing SaaS consumers with the best possible choices based on quality, cost, and trust. An automated rating model, based on the expectancy-disconfirmation theory from market science, is also defined to overcome feedback subjectivity issues. The proposed rating and selection models are validated through simulations, demonstrating that the system can effectively capture service behavior and recommend the best possible choices.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5383370,no
A New Approach and a Related Tool for Dependability Measurements on Distributed Systems,2010,"In recent years, experts in the field of dependability are recognizing experimental measurements as an attractive option for assessing distributed systems; contrary to simulation, measurement allows monitoring the real execution of a system in its real usage environment. However, the results of a recent survey have highlighted that the way measurements are carried out and measurement results are expressed is far from being in line with the approach commonly adopted by metrology. The scope of this paper is twofold. The first goal is to extend the discussion on the increasing role that measurements play in dependability and on the importance of cross-fertilization between the dependability and the instrumentation and measurement communities. The second objective is to present a different approach to dependability measurements, in line with the common practices in metrology. With regard to this, the paper presents a tool for dependability measurements in distributed systems that allows evaluating the uncertainty of measurement results. The tool is an enhancement of NekoStat, which is a powerful highly portable Java framework that allows analyzing distributed systems and algorithms. Together with the description of the tool and its innovative features, two experimental case studies are presented.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5404891,no
A Genetic Programming Approach for Software Reliability Modeling,2010,"Genetic programming (GP) models adapt better to the reliability curve when compared with other traditional, and non-parametric models. In a previous work, we conducted experiments with models based on time, and on coverage. We introduced an approach, named genetic programming and Boosting (GPB), that uses boosting techniques to improve the performance of GP. This approach presented better results than classical GP, but required ten times the number of executions. Therefore, we introduce in this paper a new GP based approach, named (Â¿Â¿ + Â¿Â¿) GP. To evaluate this new approach, we repeated the same experiments conducted before. The results obtained show that the (Â¿Â¿ + Â¿Â¿) GP approach presents the same cost of classical GP, and that there is no significant difference in the performance when compared with the GPB approach. Hence, it is an excellent, less expensive technique to model software reliability.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5409534,no
Judy - a mutation testing tool for java,2010,"Popular code coverage measures, such as branch coverage, are indicators of the thoroughness rather than the fault detection capability of test suites. Mutation testing is a fault-based technique that measures the effectiveness of test suites for fault localisation. Unfortunately, use of mutation testing in the software industry is rare because generating and running vast numbers of mutants against the test cases is time-consuming and difficult to do without an automated, fast and reliable tool. Our objective is to present an innovative approach to mutation testing that takes advantage of a novel aspect-oriented programming mechanism, called `pointcut and advice`, to avoid multiple compilation of mutants and, therefore, to speed up mutation testing. An empirical comparison of the performance of the developed tool, called Judy, with the MuJava mutation testing tool on 24 open-source projects demonstrates the value of the presented approach. The results show that there is a statistically significant (<i>t</i>(23) = -12.28, <i>p</i> < 0.0005, effect size <i>d</i> = 3.43) difference in the number of mutants generated per second between MuJava (<i>M</i> = 4.15, SD = 1.42) and Judy (<i>M</i> = 52.05, SD = 19.69). Apart from being statistically significant, this effect is considered very large and, therefore, represents a substantive finding. This therefore allows us to estimate the fault detection effectiveness of test suites of much larger systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5415508,no
The Theory of Relative Dependency: Higher Coupling Concentration in Smaller Modules,2010,"Our observations on several large-scale software products has consistently shown that smaller modules are proportionally more defect prone. These findings, challenge the common recommendations from the literature suggesting that quality assurance (QA) and quality control (QC) resources should focus on larger modules. Those recommendations are based on the unfounded assumption that a monotonically increasing linear relationship exists between module size and defects. Given that complexity is correlated with the size.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5420801,no
Automatic Facial Expression Recognition Using Gabor Filter and Expression Analysis,2010,"Facial expression extraction is the essential step of facial expression recognition. The paper presents a system that uses 28 facial feature key-points in images detection and Gabor wavelet filter provided with 5 frequencies, 8 orientations. In according to actual demand, It can extract the feature of low quality facial expression image target, and have good robust for automatic facial expression recognition. Experimental results show that the performance of the proposed method achieved excellent average recognition rates, when it is applied to facial expression recognition system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5421091,no
A Novel Method of Avionics Circuit Simulation,2010,"As the drastic advancement in the computer and software technologies, simulation of avionics system is gradually moving towards the use of modular avionics for an important component of the aero plane simulator. Traditionally, avionics simulators use unrelated identifier of system to identify the different fault information and establish various operational functions without providing the circuit structure, this potentially leading to the simulation modular being implemented irrelevantly, which would not conducive to the cosmically systems realization. Basis on the characteristics of the avionics system, an avionics circuit model is designed, and it modeled avionics circuit structure and their connection according as the circuit schematic, application of A* algorithm and recursive iterative algorithm achieved circuit depth-traversing. It supports the multimeter to measure the circuit voltage, resistance and current, and estimating the value is right or not to find the fault point and replace the components. The simulation of the avionics system both simulated the operation state of aircraft various subsystems and realized the function of fault diagnosis.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5421253,no
Assess Content Comprehensiveness of Ontologies,2010,"This paper proposes a novel method to assess and evaluate content comprehensiveness of ontologies. Comparing to other researchers methods which just count the number of classes and properties, the method concerns about the actual content coverage of ontologies. By applying statistical analysis to a corpus, we assign different weights to different terms chosen from the corpus. These terms are then used for evaluating ontologies. Afterwards, a score is generated for each ontology to mark its content comprehensiveness. Experiments are then appropriately designed to evaluate the qualities of typical ontologies to show the effectiveness of the proposed evaluation method.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5421334,no
Simulation Environment for IT Service Support Processes: Supporting Service Providers in Estimating Service Levels for Incident Management,2010,"Because of a steadily increasing market for IT services, providers need to set apart from their competitors in order to successfully assert with their service offerings in the market. As compliance with negotiated service levels is becoming more and more important for service providers, the improvement of the quality of operational IT service support processes is coming to the fore. In this paper we present an approach based on discrete-event simulation in order to do a priori estimations on ticket workloads within a skills-based IT service desk. Service providers can perform tool-supported capability planning (analysis & optimization) during design time based on the potential impact of ticket workloads to agreed service levels. We additionally show how obtained results can be taken into account for determining service levels in Service Level Agreements (SLA).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5430045,no
Business Process Automation Based on Dependencies,2010,"Given a business language, this study presents a dependency-based mapping approach to automate the process of generating a workflow. First, the classification of various operators is carried out in terms of their usage in generating different flows. Second, an internally developed backward flow generating algorithm is used. In order to measure the effectiveness and accuracy of the approach, a forward flow generating algorithm is also developed to compare the results using an example on both approaches.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5430048,no
Collaborative Educational System Analysis and Assessment,2010,"The paper presents definitions of collaborative systems, their classification and the study of collaborative systems in education. It describes the key concepts of collaborative educational systems. There are listed main properties and quality characteristics of collaborative educational systems. It analyzes the application for the assessment of text entities orthogonality within a collaborative system in education, represented by a virtual campus. It implements a metric for evaluating the orthogonality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5430106,no
Per-Thread Cycle Accounting,2010,"Resource sharing unpredictably affects per-thread performance in multithreaded architectures, but system software assumes all coexecuting threads make equal progress. Per-thread cycle accounting addresses this problem by tracking per-thread progress rates for each coexecuting thread. This approach has the potential to improve Quality Of Service (QoS), Service-Level Agreements (SLA), performance predictability, service differentiation, and proportional-share performance on multithreaded architectures.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5430741,no
Research on Reliability Model of Practical Software,2010,"The modeling technology of software reliability is analyzed in detail in the paper. According to practical software development, the evaluation standard of reliability model of practical software will be explored. The methods of constructing reliability model of practical software is discussed from the point of view of practical application.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5431846,no
Context-Aware Adaptive Applications: Fault Patterns and Their Automated Identification,2010,"Applications running on mobile devices are intensely context-aware and adaptive. Streams of context values continuously drive these applications, making them very powerful but, at the same time, susceptible to undesired configurations. Such configurations are not easily exposed by existing validation techniques, thereby leading to new analysis and testing challenges. In this paper, we address some of these challenges by defining and applying a new model of adaptive behavior called an Adaptation Finite-State Machine (A-FSM) to enable the detection of faults caused by both erroneous adaptation logic and asynchronous updating of context information, with the latter leading to inconsistencies between the external physical context and its internal representation within an application. We identify a number of adaptation fault patterns, each describing a class of faulty behaviors. Finally, we describe three classes of algorithms to detect such faults automatically via analysis of the A-FSM. We evaluate our approach and the trade-offs between the classes of algorithms on a set of synthetically generated Context-Aware Adaptive Applications (CAAAs) and on a simple but realistic application in which a cell phone's configuration profile changes automatically as a result of changes to the user's location, speed, and surrounding environment. Our evaluation describes the faults our algorithms are able to detect and compares the algorithms in terms of their performance and storage requirements.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5432224,no
Low Overhead Incremental Checkpointing and Rollback Recovery Scheme on Windows Operating System,2010,"Implementation of a low overhead incremental checkpointing and rollback recovery scheme that consists of incremental checkpointing combines copy-on-write technique and optimal checkpointing interval is addressed in this article. The checkpointing permits to save process state periodically during failure-free execution, and the recovery scheme maintains to normally execute the task when failure occurs in a PC-based computer-controlled system employed with Windows Operating System. Excess size of capturing state and arbitrary checkpointing results in either performance degradation or expensive recovery cost. For the objective of minimizing overhead, the checkpointing and recovery scheme is designed of Win32 API interception associated with incremental checkpointing and copy-on-write technique. Instead of saving entire process space, it only needs to save the modified pages and uses buffer to save state temporarily in the process of checkpointing so that the checkpointing overhead is reduced. While system is encountered with failure, the minimum expected time of the total overhead to complete a task is calculated by using probability to find the optimal checkpointing interval. From simulation results, the proposed checkpointing and rollback recovery scheme not only enhances the capability of the normal task executing but also reduces the overhead of checkpointing and recovery.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5432637,no
Performance Evaluation of Handoff Queuing Schemes,2010,"One of the main advantages of new wireless systems is the freedom to make and receive calls anywhere and at any time; handovers are considered a key element for providing this mobility. This paper presents the handoff queuing problem in cellular networks. We propose a model to study three handoff queuing schemes, and provide a performance evaluation of these ones. The paper begin by a presentation of the different handoff queuing scheme to evaluate. Then, gives an evaluation model with the different assumption considered in the simulations. The evaluation concerns the blocking probability for handoff and original calls. These simulations are conducted for each scheme, according to different offered loads, size of call (original and handoff) queue, and number of voice channels. A model is proposed and introduced in this paper for the study of three channel assignment schemes; namely, they are the non prioritized schemes (NPS), the original call queuing schemes, and the handoff call queuing schemes.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5437627,no
Failure Detection and Localization in OTN Based on Optical Power Analysis,2010,"In consideration of the new features of Optical Transport Networks (OTN), the failure detection and localization has become a new challenging issue in OTN management research area. This paper proposes a scheme to detect and locate the failures based on the optical power analysis. In failure detection section of the scheme, this paper propose a method to detect the performance degradation caused by possible failures based on real optical power analysis and build a status matrix which demonstrates the current optical power deviation of the fiber port of each node in OTN. In failure localization section of the scheme, this paper proposes the multiple failures location algorithm (MFLA), which deals with both single point failure and multi-point failures, to locate the multiple failures based on analyzing the status matrix and the switching relationship matrix. Then, an exemplary scenario is given to present the result of detecting and locating the fiber link failure and OXC device failure with the proposed scheme.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5437636,no
A Comparative Study of Six Software Packages for Complex Network Research,2010,"UCINET, Pajek, Networkx, iGraph, JUNG and statnet, are commonly used to perform analysis with complex network model. The scalability, and function coverage of these six software packages are assessed and compared. Some randomly generated datasets are used to evaluate the performance of these software packages with regard to input/output (I/O), basic graph algorithms, statistical metrics computation, graph generation, community detection, and visualization. A metric regarding both numbers of the nodes and the edges of complex networks, which is called Maximum Expected Network Processing Ability (MENPA), is proposed to measure the scalability of software packages. Empirical results show that these six software packages are complementary rather than competitive and the difference on the scalability among these six software packages may be attributed to the varieties in both of the programming languages and the network representations.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5437689,no
Security and Performance Aspects of an Agent-Based Link-Layer Vulnerability Discovery Mechanism,2010,"The identification of vulnerable hosts and subsequent deployment of mitigation mechanisms such as service disabling or installation of patches is both time-critical and error-prone. This is in part owing to the fact that malicious worms can rapidly scan networks for vulnerable hosts, but is further exacerbated by the fact that network topologies are becoming more fluid and vulnerable hosts may only be visible intermittently for environments such as virtual machines or wireless edge networks. In this paper we therefore describe and evaluate an agent-based mechanism which uses the spanning tree protocol (STP) to gain knowledge of the underlying network topology to allow both rapid and resource-efficient traversal of the network by agents as well as residual scanning and mitigation techniques on edge nodes. We report performance results, comparing the mechanism against a random scanning worm and demonstrating that network immunity can be largely achieved despite a very limited warning interval. We also discuss mechanisms to protect the agent mechanism against subversion, noting that similar approaches are also increasingly deployed in case of malicious code.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5438039,no
Estimating Error-probability and its Application for Optimizing Roll-back Recovery with Checkpointing,2010,"The probability for errors to occur in electronic systems is not known in advance, but depends on many factors including influence from the environment where the system operates. In this paper, it is demonstrated that inaccurate estimates of the error probability lead to loss of performance in a well known fault tolerance technique, Roll-back Recovery with checkpointing (RRC). To regain the lost performance, a method for estimating the error probability along with an adjustment technique are proposed. Using a simulator tool that has been developed to enable experimentation, the proposed method is evaluated and the results show that the proposed method provides useful estimates of the error probability leading to near-optimal performance of the RRC fault-tolerant technique.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5438676,no
A Smart CMOS Image Sensor with On-chip Hot Pixel Correcting Readout Circuit for Biomedical Applications,2010,"One of the most recent and exciting applications for CMOS image sensors is in the biomedical field. In such applications, these sensors often operate in harsh environments (high intensity, high pressure, long time exposure), which increase the probability for the occurrence of hot pixel defects over their lifetime. This paper presents a novel smart CMOS image sensor integrating hot pixel correcting readout circuit to preserve the quality of the captured images. With this approach, no extra non-volatile memory is required in the sensor device to store the locations of the hot pixels. In addition, the reliability of the sensor is ensured by maintaining a real-time detection of hot pixels during image capture.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5438703,no
Population-Based Algorithm Portfolios for Numerical Optimization,2010,"In this paper, we consider the scenario that a population-based algorithm is applied to a numerical optimization problem and a solution needs to be presented within a given time budget. Although a wide range of population-based algorithms, such as evolutionary algorithms, particle swarm optimizers, and differential evolution, have been developed and studied under this scenario, the performance of an algorithm may vary significantly from problem to problem. This implies that there is an inherent risk associated with the selection of algorithms. We propose that, instead of choosing an existing algorithm and investing the entire time budget in it, it would be less risky to distribute the time among multiple different algorithms. A new approach named population-based algorithm portfolio (PAP), which takes multiple algorithms as its constituent algorithms, is proposed based upon this idea. PAP runs each constituent algorithm with a part of the given time budget and encourages interaction among the constituent algorithms with a migration scheme. As a general framework rather than a specific algorithm, PAP is easy to implement and can accommodate any existing population-based search algorithms. In addition, a metric is also proposed to compare the risks of any two algorithms on a problem set. We have comprehensively evaluated PAP via investigating 11 instantiations of it on 27 benchmark functions. Empirical results have shown that PAP outperforms its constituent algorithms in terms of solution quality, risk, and probability of finding the global optimum. Further analyses have revealed that the advantages of PAP are mostly credited to the synergy between constituent algorithms, which should complement each other either over a set of problems, or during different stages of an optimization process.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5439827,no
Applying an effective model for VNPT CDN,2010,"Most operations of Content Distribution Network (CDN) have measured to evaluate the ability to serve users with content or services they want. Activity measurement process provides the ability to predict, monitor and ensure activities throughout the CDN. Five parameters or regular measurement units are often used by content providers to evaluate the operation of CDN including Cache hit ratio, reserved bandwidth, latency, surrogate server utilization and reliability. There are many ways to measure CDN activities, one which use simulation tools. The simulation CDN is implemented using software tools that have value for research and development, internal testing and diagnostic CDN performance, because of accessing real CDN traces and logs is not easy due to the proprietary nature of commercial CDN. In this article, we will apply a CDN simulation model (based on [CDNSim; 2007]) for design a CDN based on the network infrastructure of Vietnam Posts and Telecommunications Group (VNPT Network).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5440141,no
A Lightweight Sanity Check for Implemented Architectures,2010,"Software architecture has been loosely defined as the organizational structure of a software system, including the components, connectors, constraints, and rationale.1 Evaluating a system's software architecture helps stakeholders to check whether the architecture complies with their interests. Additionally, the evaluation can result in a common understanding of the architecture's strengths and weaknesses. All of this helps to determine which quality criteria the system meets because ""architectures allow or preclude nearly all of the system's quality attributes.""2",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5440164,no
Assessing communication media richness in requirements negotiation,2010,"A critical claim in software requirements negotiation regards the assertion that group performances improve when a medium with different richness level is used. Accordingly, the authors have conducted a study to compare traditional face-to-face communication, the richest medium and two less rich communication media, namely a distributed three-dimensional virtual environment and a text-based structured chat. This comparison has been performed with respect to the time needed to accomplish a negotiation. Furthermore, as the only assessment of the time could not be meaningful, the authors have also analysed the media effect on the issues arisen in the negotiation process and the quality of the negotiated software requirements.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5440854,no
Introducing Queuing Network-Based Performance Awareness in Autonomic Systems,2010,"This paper advocates for the introduction of performance awareness in autonomic systems. The motivation is to be able to predict the performance of a target configuration when a self-* feature is planning a system reconfiguration.We propose a global and partially automated process based on queues and queuing networks models. This process includes decomposing a distributed application into black boxes, identifying the queue model for each black box and assembling these models into a queuing network according to the candidate target configuration. Finally, performance prediction is performed either through simulation or analysis.This paper sketches the global process and focuses on the black box model identification step. This step is automated thanks to a load testing platform enhanced with a workload control loop. Model identification is then based on statistical tests. The model identification process is illustrated by experimental results.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5442613,no
Rate-distortion optimization using structural information in H.264 strictly Intra-frame encoder,2010,"In this paper we employ Structural Similarity Index (SSIM) metric in the rate-distortion optimizations of H.264 strictly I-frame encoder to choose the best prediction mode(s). The SSIM is designed to improve on traditional metrics like PSNR and MSE, which have been proved to be inconsistent with human eye perception. The required modifications are done on the JVT reference software JM92 program. The simulation results show that there is reduction in bit rate by 3% while maintaining almost the same video quality and better encoding time.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5442789,no
Performance analysis and comparison of JM 15.1 and Intel IPP H.264 encoder and decoder,2010,"Joint Model (JM) reference software is used for academic reference of H.264 and it was developed by JVT (Joint Video Team) of ISO/IEC MPEG & ITU-T VCEG (Video coding experts group). The Intel IPP (Integrated Performance Primitives) H.264 is a product of Intel which uses IPP libraries and SIMD instructions available on modern processors. The Intel IPP H.264 is multi-threaded and uses CPU optimized IPP routines. In this paper, JM 15.1 and Intel IPP H.264 codec are compared in terms of execution time and video quality of the output decoded sequence. The metrics used for comparison are SSIM (Structural Similarity Index Metric), PSNR (Peak-to-Peak Signal to Noise Ratio), MSE (Mean Square Error), motion estimation time, encoding time, decoding time and the compression ratio of the H.264 file size (encoded output). Intel IPP H.264 clearly emerges as the winner against JM 15.1 in all parameters except for the compression ratio of H.264 file size.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5442807,no
Simple Experimental Techniques to Characterize Capacitors in a Wide Range of Frequencies and Temperatures,2010,"The aim of this paper is to present two very simple, cheap, and practical experimental techniques that are able to estimate the capacitor equivalent circuit for a wide range of frequencies and temperatures. The capacitor equivalent circuit considerably changes with temperature, aging, and frequency. Therefore, knowledge of their equivalent circuit at their operating conditions can lead to better design proposals. In addition, knowledge of the evolution of the equivalent series resistance (ESR) with temperature is essential for the development of reliable online fault diagnosis techniques. This issue is particularly important for aluminum electrolytic capacitors, since they are the preferred capacitor type in power electronics applications and simultaneously one of the most critical components in such applications. To implement the first technique, it is necessary to put the capacitor under test in series with a resistor and connect it to a sinusoidal voltage. The second technique requires a simple charge-discharge circuit. After acquiring both capacitors' current and voltage through an oscilloscope, which is connected to a PC with <i>Matlab</i> software, it is possible to compute both capacitor capacitance and resistance using the least mean square (<i>LMS</i>) algorithm. To simulate the variation of capacitor case temperature, a very simple prototype was used. Several experimental results were obtained to evaluate the accuracy and precision of the two experimental techniques.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5443534,no
A Cubic 3-Axis Magnetic Sensor Array for Wirelessly Tracking Magnet Position and Orientation,2010,"In medical diagnoses and treatments, e.g., endoscopy, dosage transition monitoring, it is often desirable to wirelessly track an object that moves through the human GI tract. In this paper, we propose a magnetic localization and orientation system for such applications. This system uses a small magnet enclosed in the object to serve as excitation source, so it does not require the connection wire and power supply for the excitation signal. When the magnet moves, it establishes a static magnetic field around, whose intensity is related to the magnet's position and orientation. With the magnetic sensors, the magnetic intensities in some predetermined spatial positions can be detected, and the magnet's position and orientation parameters can be computed based on an appropriate algorithm. Here, we propose a real-time tracking system developed by a cubic magnetic sensor array made of Honeywell 3-axis magnetic sensors, HMC1043. Using some efficient software modules and calibration methods, the system can achieve satisfactory tracking accuracy if the cubic sensor array has enough number of 3-axis magnetic sensors. The experimental results show that the average localization error is 1.8 mm.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5443691,no
Prediction model of reservoir fluids properties using Sensitivity Based Linear Learning method,2010,"This paper presented a new prediction model for Pressure-Volume-Temperature (PVT) properties based on the recently introduced learning algorithm called Sensitivity Based Linear Learning Method (SBLLM) for two-layer feedforward neural networks. PVT properties are very important in the reservoir engineering computations. The accurate determination of these properties such as bubble-point pressure and oil formation volume factor is important in the primary and subsequent development of an oil field. In this work, we develop Sensitivity Based Linear Learning method prediction model for PVT properties using two distinct databases, while comparing forecasting performance, using several kinds of evaluation criteria and quality measures, with neural network and the three common empirical correlations. Empirical results from simulation show that the newly developed SBLLM based model produced promising results and outperforms others, particularly in terms of stability and consistency of prediction.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5444846,no
Investigation of higher order optical multiple pulse position modulation links over a highly dispersive optical channel,2010,"This study describes a performance analysis of eight different multiple pulse position modulation (PPM) systems with 4, 7, 12, 15, 17, 22, 28 and 33 slots operating over a plastic optical fibre (POF) channel. The receiver/decoder uses slope detection and a maximum likelihood sequence detector (MLSD). As the analysis of any multiple pulse position modulation (MPPM) system is extremely time consuming, especially when the number of slots is large, a software solution was used. A measure of coding quality that accounts for efficiency of coding and bandwidth expansion was applied and original results showed that the middle systems of these MPPM families are the most efficient for a wide range of bandwidths. Although the efficient Gray coding was used for this investigation, a close to optimum mapping is presented for MPPM systems of the families mentioned above. These mappings were found to be superior to the efficient Gray codes, linear coding and a series of random mappings.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5445249,no
An Efficient Duplicate Detection System for XML Documents,2010,"Duplicate detection, which is an important subtask of data cleaning, is the task of identifying multiple representations of a same real-world object and necessary to improve data quality. Numerous approaches both for relational and XML data exist. As XML becomes increasingly popular for data exchange and data publishing on the Web, algorithms to detect duplicates in XML documents are required. Previous domain independent solutions to this problem relied on standard textual similarity functions (e.g., edit distance, cosine metric) between objects. However, such approaches result in large numbers of false positives if we want to identify domain-specific abbreviations and conventions. In this paper, we present the process of detecting duplicate includes three modules, such as selector, preprocessor and duplicate identifier which uses XML documents and candidate definition as input and produces duplicate objects as output. The aim of this research is to develop an efficient algorithm for detecting duplicate in complex XML documents and to reduce number of false positive by using MD5 algorithm. We illustrate the efficiency of this approach on several real-world datasets.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5445601,no
Performance Evaluation of the Judicial System in Taiwan Using Data Envelopment Analysis and Decision Trees,2010,"A time-honored maxim says that the judicial system is the last line of defending justice. Its performance has a great impact on how the citizen trust or distrust their state apparatus in a democracy. Technically speaking, the judicial process and its procedures are very complicated and the purpose of the whole system is to go through the law and due process to protect civil liberties and rights and to defend the public good of the nation. Therefore, it is worthwhile to assess the performance of judicial institutions in order to advance the efficiency and quality of judicial verdict. This paper combines data envelopment analysis (DEA) and decision trees to achieve this objective. In particular, DEA is first of all used to evaluate the relative efficiency of 18 district courts in Taiwan. Then, the efficiency scores and the overall efficiency of each decision making units are then used to train a decision tree model. Specifically, C5.0, CART, and CHAID decision trees are constructed for comparisons. The decision rules in the best decision tree model can be used to distinguish between efficient units and inefficient units and allow us to understand important factors affecting the efficiency of judicial institutions. The experimental result shows that C5.0 performs the best for predicting (in) efficient judicial institutions, which provides 80.37% average accuracy.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5445658,no
A Partial Bandwidth Reservation Scheme for QoS Support in Ad Hoc Networks,2010,"With the fast development of information technology, the application environment and multimedia services in Ad Hoc networks, all of these are needed to support QoS, and bandwidth is the one of key points needed to be considered. In this paper, we propose a partial bandwidth reservation scheme adapted to reactive routing protocol. In each node, it can estimate available bandwidth in real time, and introduce access control scheme to meet services QoS metric. Simulations integrated with DSR protocol show that this partial bandwidth reservation scheme proposed has improved network capability, and provided QoS support for network services in a certain extent.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5445804,no
The LHCb Readout System and Real-Time Event Management,2010,"The LHCb Experiment is a hadronic precision experiment at the LHC accelerator aimed at mainly studying b-physics by profiting from the large b-anti-b-production at LHC. The challenge of high trigger efficiency has driven the choice of a readout architecture allowing the main event filtering to be performed by a software trigger with access to all detector information on a processing farm based on commercial multi-core PCs. The readout architecture therefore features only a relatively relaxed hardware trigger with a fixed and short latency accepting events at 1 MHz out of a nominal proton collision rate of 30 MHz, and high bandwidth with event fragment assembly over Gigabit Ethernet. A fast central system performs the entire synchronization, event labelling and control of the readout, as well as event management including destination control, dynamic load balancing of the readout network and the farm, and handling of special events for calibrations and luminosity measurements. The event filter farm processes the events in parallel and reduces the physics event rate to about 2 kHz which are formatted and written to disk before transfer to the offline processing. A spy mechanism allows processing and reconstructing a fraction of the events for online quality checking. In addition a 5 Hz subset of the events are sent as express stream to offline for checking calibrations and software before launching the full offline processing on the main event stream.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5446526,no
Post-TRL6 Dependable Multiprocessor technology developments,2010,"Funded by the NASA New Millennium Program (NMP) Space Technology 8 (ST8) project since 2004, the Dependable Multiprocessor (DM) project is a major step toward NASA's and DoD's long-held desire to fly Commercial-Off-The-Shelf (COTS) technology in space to take advantage of the higher performance and lower cost of COTS-based onboard processing solutions. The development of DM technology represents a significant paradigm shift. For applications that only need to be radiation tolerant, DM technology allows the user to fly 10x - 100x the processing capability of designs implemented with radiation hardened technologies. As a software-based, platform and technology-independent technology, DM allows space missions to keep pace with COTS developments. As a result, the processing technologies used in space applications no longer need to be 2 - 3 generations behind state-of-the-art terrestrial processing technologies. The DM project conducted its TRL6 Technology Validation in 2008 and 2009. The preliminary DM TRL6 technology validation demonstration was conducted in September of 2008. The results of the preliminary TRL6 technology validation demonstration were described in a paper presented at the 2009 IEEE Aerospace Conference. The final TRL6 results are provided. This paper includes a brief overview of DM technology and a brief overview of the overall TRL6 effort, but focuses on the 2009 TRL6 effort. The 2009 effort includes system-level radiation testing and post-TRL6 technology enhancements. Post-TRL6 enhancements include an upgraded DM implementation with enhanced data integrity protection, coordinated checkpointing, repetitive transient fault detection, Open MPI, true open system HAM (High Availability Middleware), and an updated flight configuration which is not subject to the constraints of the original ST8 spacecraft.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5446658,no
Model-based validation of safety-critical embedded systems,2010,"Safety-critical systems have become increasingly software reliant and the current development process of Ã‚Â¿build, then integrateÃ‚Â¿ has become unaffordable. This paper examines two major contributors to today's exponential growth in cost: system-level faults that are not discovered until late in the development process; and multiple truths of analysis results when predicting system properties through model-based analysis and validating them against system implementations. We discuss the root causes of such system-level problems, and an architecture-centric model-based analysis approach of different operational quality aspects from an architecture model. A key technology is the SAE Architecture Analysis & Design Language (AADL) standard for embedded software-reliant system. It supports a single source approach to analysis of operational qualities such as responsiveness, safety-criticality, security, and reliability through model annotations. The paper concludes with a summary of an industrial case study that demonstrates the feasibility of this approach.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5446809,no
Validation of health-monitoring algorithms for civil aircraft engines,2010,"Snecma builds CFM engines with GE for commercial aircrafts and now faces with the challenge of providing assistance to the maintenance operations of its wide fleet. Some years ago, Snecma engaged in the development of a set of algorithmic applications to monitor engine subsystems. Some health-monitoring (HM) application examples developed for Snecma's engines are presented below. An architecture proposition for HM applications is given with a list of quality indicators used in validation process. Finally, the problem of how to reach the drastic requirements in use for civil aircrafts is addressed. The conclusion sketches the methodology and software solution tested by Snecma's HM team to manage the algorithms.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5446815,no
Modeling and performance considerations for automated fault isolation in complex systems,2010,"The purpose of this paper is to document the modeling considerations and performance metrics that were examined in the development of a large-scale Fault Detection, Isolation and Recovery (FDIR) system. The FDIR system is envisioned to perform health management functions for both a launch vehicle and the ground systems that support the vehicle during checkout and launch countdown by using a suite of complimentary software tools that alert operators to anomalies and failures in real-time. The FDIR team members developed a set of operational requirements for the models that would be used for fault isolation and worked closely with the vendor of the software tools selected for fault isolation to ensure that the software was able to meet the requirements. Once the requirements were established, example models of sufficient complexity were used to test the performance of the software. The results of the performance testing demonstrated the need for enhancements to the software in order to meet the demands of the full-scale ground and vehicle FDIR system. The paper highlights the importance of the development of operational requirements and preliminary performance testing as a strategy for identifying deficiencies in highly scalable systems and rectifying those deficiencies before they imperil the success of the project.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5446817,no
Autonomic Composite-service Architecture with MAWeS,2010,"The highly distributed nature and the load sensitivity of Service Oriented Architectures (SOA) make it very difficult to guarantee performance requirements under rapidly-changing load conditions. This paper deals with the development of service oriented autonomic systems that are capable to optimize themselves using a feed forward approach, by exploiting automatically generated performance predictions. The MAWeS (MetaPL/HeSSE Autonomic Web Services) framework allows the development of self-tuning applications that proactively optimize themselves by simulating the execution environment. After a discussion on the possible design choices for the development of autonomic web services applications, a soft real-time test application is presented and the performance results obtained in a composite-service execution scenario are commented.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5447260,no
Resilient Critical Infrastructure Management Using Service Oriented Architecture,2010,"The SERSCIS project aims to support the use of interconnected systems of services in Critical Infrastructure (CI) applications. The problem of system interconnectedness is aptly demonstrated by 'Airport Collaborative Decision Making' (A-CDM). Failure or underperformance of any of the interlinked ICT systems may compromise the ability of airports to plan their use of resources to sustain high levels of air traffic, or to provide accurate aircraft movement forecasts to the wider European air traffic management systems. The proposed solution is to introduce further SERSCIS ICT components to manage dependability and interdependency. These use semantic models of the critical infrastructure, including its ICT services, to identify faults and potential risks and to increase human awareness of them. Semantics allows information and services to be described in such a way that makes them understandable to computers. Thus when a failure (or a threat of failure) is detected, SERSCIS components can take action to manage the consequences, including changing the interdependency relationships between services. In some cases, the components will be able to take action autonomously -- e.g. to manage 'local' issues such as the allocation of CPU time to maintain service performance, or the selection of services where there are redundant sources available. In other cases the components will alert human operators so they can take action instead. The goal of this paper is to describe a Service Oriented Architecture (SOA) that can be used to address the management of ICT components and interdependencies in critical infrastructure systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5447380,no
Wavelet Coherence and Fuzzy Subtractive Clustering for Defect Classification in Aeronautic CFRP,2010,"Despite their high specific stiffness and strength, carbon fiber reinforced polymers, stacked at different fiber orientations, are susceptible to interlaminar damages. They may occur in the form of micro-cracks and voids, and leads to a loss of performance. Within this framework, ultrasonic tests can be exploited in order to detect and classify the kind of defect. The main object of this work is to develop the evolution of a previous heuristic approach, based on the use of Support Vector Machines, proposed in order to recognize and classify the defect starting from the measured ultrasonic echoes. In this context, a real-time approach could be exploited to solve real industrial problems with enough accuracy and realistic computational efforts. Particularly, we discuss the cross wavelet transform and wavelet coherence for examining relationships in time-frequency domains between. For our aim, a software package has been developed, allowing users to perform the cross wavelet transform, the wavelet coherence and the Fuzzy Inference System. Since the ill-posedness of the inverse problem, Fuzzy Inference has been used to regularize the system, implementing a data-independent classifier. Obtained results assure good performances of the implemented classifier, with very interesting applications.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5447413,no
Localized QoS Routing with Admission Control for Congestion Avoidance,2010,"Localized Quality of Service (QoS) routing has been recently proposed for supporting the requirements of multimedia applications and satisfying QoS constraints. Localized algorithms avoid the problems associated with the maintenance of global network state by using statistics of flow blocking probabilities. Using local information for routing avoids the overheads of global information with other nodes. However, localized QoS routing algorithms perform routing decisions based on information updated from path request to path request. This paper proposes to tackle a combined localized routing and admission control in order to avoid congestion. We introduce a new Congestion Avoidance Routing algorithm (CAR) in localized QoS routing which make a decision of routing in each connection request using an admission control to route traffic away from congestion. Simulations of various network topologies are used to illustrate the performance of the CAR. We compare the performance of the CAR algorithm against the Credit Based Routing (CBR) algorithm and the Quality Based Routing (QBR) under various ranges of traffic loads.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5447423,no
Fault Tolerance and Recovery in Grid Workflow Management Systems,2010,"Complex scientific workflows are now commonly executed on global grids. With the increasing scale complexity, heterogeneity and dynamism of grid environments the challenges of managing and scheduling these workflows are augmented by dependability issues due to the inherent unreliable nature of large-scale grid infrastructure. In addition to the traditional fault tolerance techniques, specific checkpoint-recovery schemes are needed in current grid workflow management systems to address these reliability challenges. Our research aims to design and develop mechanisms for building an autonomic workflow management system that will exhibit the ability to detect, diagnose, notify, react and recover automatically from failures of workflow execution. In this paper we present the development of a Fault Tolerance and Recovery component that extends the ActiveBPEL workflow engine. The detection mechanism relies on inspecting the messages exchanged between the workflow and the orchestrated Web Services in search of faults. The recovery of a process from a faulted state has been achieved by modifying the default behavior of ActiveBPEL and it basically represents a non-intrusive checkpointing mechanism. We present the results of several scenarios that demonstrate the functionality of the Fault Tolerance and Recovery component, outlining an increase in performance of about 50% in comparison to the traditional method of resubmitting the workflow.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5447462,no
A Multidimensional Array Slicing DSL for Stream Programming,2010,"Stream languages offer a simple multi-core programming model and achieve good performance. Yet expressing data rearrangement patterns (like a matrix block decomposition) in these languages is verbose and error prone. In this paper, we propose a high-level programming language to elegantly describe n-dimensional data reorganization patterns. We show how to compile it to stream languages.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5447483,no
Fault Tolerance by Quartile Method in Wireless Sensor and Actor Networks,2010,"Recent technological advances have lead to the emergence of wireless sensor and actor networks (WSAN) which sensors gather the information for an event and actors perform the appropriate actions. Since sensors are prone to failure due to energy depletion, hardware failure, and communication link errors, designing an efficient fault tolerance mechanism becomes an important issue in WSAN. However, most research focus on communication link fault tolerance without considering sensing fault tolerance on paper survey. In this situation, actor may perform incorrect action by receiving error sensing data. To solve this issue, fault tolerance by quartile method (FTQM) is proposed in this paper. In FTQM, it not only determines the correct data range but also sifts the correct sensors by data discreteness. Therefore, actors could perform the appropriate actions in FTQM. Moreover, FTQM also could be integrated with communication link fault tolerance mechanism. In the simulation results, it demonstrates FTQM has better predicted rate of correct data, the detected tolerance rate of temperature, and the detected temperature compared with the traditional sensing fault tolerance mechanism. Moreover, FTQM has better performance when the real correct data rate and the threshold value of failure are varied.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5447511,no
An integrated life cycle-based software reliability assurance approach for NASA projects,2010,This paper proposes a software reliability assurance approach for NASA projects. The approach provides a success road-map of integrated system risk management from early development phases for timely identification of valued proactive improvement on software while striving for achieving mission reliability goals. The informed decision making process throughout the life cycle is supported to ensure successful deployment of the system.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5448000,no
FMEA at a residential care facility,2010,"In the healthcare industry, reliability of the care provided at residential care facilities has come to the forefront as a major concern of residents and their families. Management at these facilities understands the high liability involved with residents' safety, but they do not perform formal analyses to mitigate the risk. Failure Modes and Effects Analyses (FMEAs) have become a very popular method to mitigate the risk involved with health care products, but can also be extended to analyze an entire process at a residential care facility which provides older adults the high quality residential living and healthcare services they need. One particular residential care facility understands and shares the concern for the well-being of its residents, so its management decided to perform a Failure Modes and Effects Analysis (FMEA) on their Emergency Alert System (EAS) and supporting processes. The EAS is a campus-wide notification system that is activated by pendants worn by residents, pull cords, and motion detectors in the event that a resident requires emergency assistance. When activated, the system alerts the main security station. The security guard radios the appropriate facility staff who respond to the location of the call. The alert system requires the proper operation of the hardware and software components as well as the proper response of various key staff members. In this paper, we describe an environment where a process FMEA was performed. The FMEA included the functional failures of an electronic system, as well as the human factors that could cause a failure in the process. A Risk Priority Number (RPN) was calculated for each failure mode so that management could focus on mitigating the risk associated with the most critical failure modes.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5448001,no
Qualitative-Quantitative Bayesian Belief Networks for reliability and risk assessment,2010,This paper presents an extension of Bayesian belief networks (BBN) enabling use of both qualitative and quantitative likelihood scales in inference. The proposed method is accordingly named QQBBN (Qualitative-Quantitative Bayesian Belief Networks). The inclusion of qualitative scales is especially useful when quantitative data for estimation of probabilities are lacking and experts are reluctant to express their opinions quantitatively. In reliability and risk analysis such situation occurs when for example human and organizational root causes of systems are modeled explicitly. Such causes are often not quantifiable due to limitations in the state of the art and lack of proper quantitative metrics. This paper describes the proposed QQBBN framework and demonstrates its uses through a simple example.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5448022,no
Numerical Simulation of the Unsteady Flow and Power of Horizontal Axis Wind Turbine using Sliding Mesh,2010,Horizontal axis wind turbine (hereafter HAWT) is the common equipment in wind turbine generator systems in recent years. The paper relates to the numerical simulation of the unsteady airflow around a HAWT of the type Phase VI with emphasis on the power output. The rotor diameter is 10-m and the rotating speed is 72 rpm. The simulation was undertaken with the Computational Fluid Dynamics (CFD) software FLUENT 6.2 using the sliding meshes controlled by user-defined functions (UDF). Entire mesh node number is about 1.8 million and it is generated by GAMBIT 2.2 to achieve better mesh quality. The numerical results were compared with the experimental data from NREL UAE wind tunnel test. The comparisons show that the numerical simulation using sliding meshes can accurately predict the aerodynamic performance of the wind turbine rotor.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5448767,no
The Research of Power Quality Real Time Monitor for Coal Power System Based on Wavelet Analysis and ARM Chip,2010,"In order to prevent coal power system fault for safety production, a novel power quality real time monitor is researched in this paper. According to the coal power system special characteristics and safety production standard, the harmonic of the coal power system is analyzed first based on the wavelet theory, and then the monitoring system is designed with ARM LPC2132 as the client computer to fulfill the common power parameter data acquisition. The whole monitoring system is composed of the signal transform module, data processing module, communication module, host computer interfaces and their function modules. The system software is designed with the platform of LabWindowns/CVI. The research result shows that the power quality monitor can detect the harmonic states in real time.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5448921,no
A model for early prediction of faults in software systems,2010,"Quality of a software component can be measured in terms of fault proneness of data. Quality estimations are made using fault proneness data available from previously developed similar type of projects and the training data consisting of software measurements. To predict faulty modules in software data different techniques have been proposed which includes statistical method, machine learning methods, neural network techniques and clustering techniques. Predicting faults early in the software life cycle can be used to improve software process control and achieve high software reliability. The aim of proposed approach is to investigate that whether metrics available in the early lifecycle (i.e. requirement metrics), metrics available in the late lifecycle (i.e. code metrics) and metrics available in the early lifecycle (i.e. requirement metrics) combined with metrics available in the late lifecycle (i.e. code metrics) can be used to identify fault prone modules using decision tree based Model in combination of K-means clustering as preprocessing technique. This approach has been tested with CM1 real time defect datasets of NASA software projects. The high accuracy of testing results show that the proposed Model can be used for the prediction of the fault proneness of software modules early in the software life cycle.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5451695,no
Performance-effective operation below Vcc-min,2010,"Continuous circuit miniaturization and increased process variability point to a future with diminishing returns from dynamic voltage scaling. Operation below Vcc-min has been proposed recently as a mean to reverse this trend. The goal of this paper is to minimize the performance loss due to reduced cache capacity when operating below Vcc-min. A simple method is proposed: disable faulty blocks at low voltage. The method is based on observations regarding the distributions of faults in an array according to probability theory. The key lesson, from the probability analysis, is that as the number of uniformly distributed random faulty cells in an array increases the faults increasingly occur in already faulty blocks. The probability analysis is also shown to be useful for obtaining insight about the reliability implications of other cache techniques. For one configuration used in this paper, block disabling is shown to have on the average 6.6% and up to 29% better performance than a previously proposed scheme for low voltage cache operation. Furthermore, block-disabling is simple and less costly to implement and does not degrade performance at or above Vcc-min operation. Finally, it is shown that a victim-cache enables higher and more deterministic performance for a block-disabled cache.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5452017,no
Spatially Scalable Video Coding Based on Hybrid Epitomic Resizing,2010,"Scalable video coding (SVC) is considered as a potentially promising solution to enable the adaptability of video to heterogeneous networks and various devices. In spatially scalable video encoder, how to resize the captured high-resolution video to get low-resolution video has great effect on the quality of experience (QoE) in the clients receiving low-resolution video. In this paper, we propose a new resizing algorithm called hybrid epitomic resizing (HER), which can make the resized image preserve the same `physical' resolution with original image by the way of utilizing texture similarity inside image and highlight regions of interest while avoiding potential artifacts. For hybrid epitomic resizing, we also design two new inter-layer prediction methods to eliminate the redundancy between adjacent spatial layers instead of conventional inter-layer prediction. Experimental results show that HER can get resized images with perceptually much better quality and the performance of new inter-layer prediction are comparable to that of conventional inter-layer prediction in H.264 SVC.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5453456,no
Generating Test Plans for Acceptance Tests from UML Activity Diagrams,2010,"The Unified Modeling Language (UML) is the standard to specify the structure and behaviour of software systems. The created models are a constitutive part of the software specification that serves as guideline for the implementation and the test of software systems. In order to verify the functionality which is defined within the specification documents, the domain experts need to perform an acceptance test. Hence, they have to generate test cases for the acceptance test. Since domain experts usually have a low level of software engineering knowledge, the test case generation process is challenging and error-prone. In this paper we propose an approach to generate high-level acceptance test plans automatically from business processes. These processes are modeled as UML Activity Diagrams (ACD). Our method enables the application of an all-path coverage criterion to business processes for testing software systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5457786,no
Object oriented design metrics and tools a survey,2010,"The most important measure that must be considered in any software product is its design quality. The design phase takes only 5-10 % of the total effort but a large part (up to 80%) of total effort goes into correcting bad design decisions. If bad design is not fixed, the cost for fixing it after software delivery is between 5 and 100 times or higher. Researches on object oriented design metrics have produced a large number of metrics that can be measured to identify design problems and assess design quality attributes. However the use of these design metrics is limited in practice due to the difficulty of measuring and using a large number of metrics. This paper presents a survey of object-oriented design metrics. The goal of this paper is to identify a limited set of metrics that have significant impact on design quality attributes. We adopt the notion of defining design metrics as independent variables that can be measured to assess their impact on design quality attributes as dependent variables. We also present survey of existing object oriented design metrics tools that can be used to automate the measurement process. We present our conclusions on the set of important object oriented design metrics that can be assessed using these tools.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5461764,no
Applying an estimation framework in software projects a local experience,2010,"This paper discusses our experience in building a framework for effort estimation and applying it in a local environment of sample Egyptian companies. The framework focuses on minimizing effort variance by enhancing the adjustments made to the functional sizing techniques. A special focus was made on the adjustment factors, which reflects the application's complexity and the actual environment in which this application will be implemented. We introduced the idea of grouping the adjustment factors to simplify the process of adjustment and to ensure more consistency in the adjustments. We have also studied how the quality of requirements impact effort estimation. We introduced the quality of requirements as an adjustment factor in our proposed framework. We have applied the proposed framework on a sample of a group of Egyptian companies with an objective to enhance effort estimation in these companies.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5461785,no
An approach to measure the Hurst parameter for the Dhaka University network traffic,2010,"The main goal of this work was to analyze the network traffic of the University of Dhaka and find out the Hurst parameter to assess the degree of self similarity. For this verification a number of tests and analyses were performed on the data collected from the University Gateway router. The conclusions were supported by a rigorous statistical analysis of 7.5 millions of data packets of high quality Ethernet traffic measurements collected between Aug '07 and March'08 and the data were analyzed using both visual and statistical experimentation. Busy hour traffic and non-busy hour traffic, both were considered. All the software was coded using MATLAB and can be used as a tool to determine the inherent self similarity of a data traffic.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5461793,no
Utilizing CK metrics suite to UML models: A case study of Microarray MIDAS software,2010,"Software metrics provide essential means for software practitioners to assess its quality. However, to assess software quality, it is important to assess its UML models because of UML wide and recent usage as an object-oriented modeling language. But the issue is which type of software metrics can be utilized on UML models. One of the most important software metrics suite is Chidamber and Kemerer metrics suite, known by CK suite. In the current work, an automated tool is developed to compute the six CK metrics by gathering the required information from class diagrams, activity diagrams, and sequence diagrams. In addition, extra information is collected from system designer, such as the relation between methods and their corresponding activity diagrams and which attributes they use. The proposed automated tool operates on XMI standard file format to provide independence from a specific UML tool. To evaluate the applicability and quality of this tool, it has been applied to two examples: an online registration system and one of the bioinformatics Microarray tools (MIDAS).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5461798,no
When process data quality affects the number of bugs: Correlations in software engineering datasets,2010,"Software engineering process information extracted from version control systems and bug tracking databases are widely used in empirical software engineering. In prior work, we showed that these data are plagued by quality deficiencies, which vary in its characteristics across projects. In addition, we showed that those deficiencies in the form of bias do impact the results of studies in empirical software engineering. While these findings affect software engineering researchers the impact on practitioners has not yet been substantiated. In this paper we, therefore, explore (i) if the process data quality and characteristics have an influence on the bug fixing process and (ii) if the process quality as measured by the process data has an influence on the product (i.e., software) quality. Specifically, we analyze six Open Source as well as two Closed Source projects and show that process data quality and characteristics have an impact on the bug fixing process: the high rate of empty commit messages in Eclipse, for example, correlates with the bug report quality. We also show that the product quality - measured by number of bugs reported - is affected by process data quality measures. These findings have the potential to prompt practitioners to increase the quality of their software process and its associated data quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463286,no
Perspectives on bugs in the Debian bug tracking system,2010,"Bugs in Debian differ from regular software bugs. They are usually associated with packages, instead of software modules. They are caused and fixed by source package uploads instead of code commits. The majority are reported by individuals who appear in the bug database once, and only once. There also exists a small group of bug reporters with over 1,000 bug reports each to their name. We also explore our idea that a high bug-frequency for an individual package might be an indicator of popularity instead of poor quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463288,no
Validity of network analyses in Open Source Projects,2010,"Social network methods are frequently used to analyze networks derived from Open Source Project communication and collaboration data. Such studies typically discover patterns in the information flow between contributors or contributions in these projects. Social network metrics have also been used to predict defect occurrence. However, such studies often ignore or side-step the issue of whether (and in what way) the metrics and networks of study are influenced by inadequate or missing data. In previous studies email archives of OSS projects have provided a useful trace of the communication and co-ordination activities of the participants. These traces have been used to construct social networks that are then subject to various types of analysis. However, during the construction of these networks, some assumptions are made, that may not always hold; this leads to incomplete, and sometimes incorrect networks. The question then becomes, do these errors affect the validity of the ensuing analysis? In this paper we specifically examine the stability of network metrics in the presence of inadequate and missing data. The issues that we study are: 1) the effect of paths with broken information flow (i.e. consecutive edges which are out of temporal order) on measures of centrality of nodes in the network, and 2) the effect of missing links on such measures. We demonstrate on three different OSS projects that while these issues do change network topology, the metrics used in the analysis are stable with respect to such changes.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463342,no
THEX: Mining metapatterns from java,2010,"Design patterns are codified solutions to common object-oriented design (OOD) problems in software development. One of the proclaimed benefits of the use of design patterns is that they decouple functionality and enable different parts of a system to change frequently without undue disruption throughout the system. These OOD patterns have received a wealth of attention in the research community since their introduction; however, identifying them in source code is a difficult problem. In contrast, metapatterns have similar effects on software design by enabling portions of the system to be extended or modified easily, but are purely structural in nature, and thus easier to detect. Our long-term goal is to evaluate the effects of different OOD patterns on coordination in software teams as well as outcomes such as developer productivity and software quality. we present THEX, a metapattern detector that scales to large codebases and works on any Java bytecode. We evaluate THEX by examining its performance on codebases with known design patterns (and therefore metapatterns) and find that it performs quite well, with recall of over 90%.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463349,no
On Modeling of GUI Test Profile,2010,"GUI (Graphical User Interface) test cases contain much richer information than the test cases in non-GUI testing. Based on the information, the GUI test profiles can be represented in more forms. In this paper, we study the modeling of the test profiles in GUI testing. Several models of GUI test profiles are proposed. Then we present a methodology of studying the relationship between the test profiles and the fault detection in GUI testing. A control scheme based on this relationship that may be able to improve the efficiency of GUI testing is also proposed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463655,no
A Measurement Framework for Assessing Model-Based Testing Quality,2010,This paper proposes a measurement framework for assessing the relative quality of alternative approaches to system level model-based testing. The motivation is to investigate the types of measures that the MBT community should apply. The purpose of this paper is to provide a basis for discussion by proposing some initial ideas on where we should probe for MBT quality measurement. The centerpiece of the proposal offered here is the concept of an operational profile (OP) and its relevance to model-based testing.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463712,no
An Empirical Evaluation of the First and Second Order Mutation Testing Strategies,2010,"Various mutation approximation techniques have been proposed in the literature in order to reduce the expenses of mutation. This paper presents results from an empirical study conducted for first and second order mutation testing strategies. Its scope is to evaluate the relative application cost and effectiveness of the different mutation strategies. The application cost was based: on the number of mutants, the equivalent ones and on the number of test cases needed to expose them by each strategy. Each strategy's effectiveness was evaluated by its ability to expose a set of seeded faults. The results indicate that on the one hand the first order mutation testing strategies can be in general more effective than the second order ones. On the other hand, the second order strategies can drastically decrease the number of the introduced equivalent mutants, generally forming a valid cost effective alternative to mutation testing.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463722,no
Numerical simulations of thermo-mechanical stresses during the casting of multi-crystalline silicon ingots,2010,"Silicon is an important semiconductor substrate for manufacturing solar cells. The mechanical and electrical properties of multi-crystalline silicon (mc-Si) are primarily influenced by the quality of the feedstock material and the crystallization process. In this work, numerical calculations, applying finite element analysis (FEA) and finite volume methods (FVM) are presented, in order to predict thermo-mechanical stresses during the solidification of industrial size mc-Si ingots. A two-dimensional global model of an industrial multi-crystallization furnace was created for thermal stationary and time-dependent calculations using the software tool CrysMAS. Subsequent thermo-mechanical analyses of the silica crucible and the ingot were performed with the FEA code ANSYS, allowing additional calculations to define mechanical boundary conditions as well as material models. Our results show that thermal analyses are in good agreement with experimental measurements. Furthermore we show that our approach is suitable to describe the generation of thermo-mechanical stress within the silicon ingot.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5464525,no
Real-Time Ampacity and Ground Clearance Software for Integration Into Smart Grid Technology,2010,"Output from a real-time sag, tension, and ampacity program was compared with measurements collected on an outdoor test span. The test site included a laser range-finder, load cells, and weather station. A fiber optic distributed temperature sensing system was routed along the conductor and thermocouples were attached to the conductor's surface. Nearly 40 million data points were statistically compared with the computer output. The program provided results with a 95% confidence interval for conductor temperatures within 10Â°C and sags within 0.3 m for a conductor temperature of 75Â°C. Test data were also used to determine the accuracy of the IEEE Standard 738 models. The computer program and the Standard 738 transient model gave comparable temperatures for temperatures up to 160Â°C. Measured temperatures were used to estimate the radial and axial temperature gradients in the ACSR conductor. The effect of insulators and instrumentation attached to the conductor on the local conductor temperature was determined. The real-time rating program is an alternative to installing instrumentation on the conductor for measuring tension, sag, or temperature. The program avoids the problems of installing and maintaining expensive instrumentation on the conductor, and it will provide accurate information on the conductor's temperature and ground clearance in real-time.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5466080,no
Measurement and Analysis of Link Quality in Wireless Networks: An Application Perspective,2010,"Estimating the quality of wireless link is vital to optimize several protocols and applications in wireless networks. In realistic wireless networks, link quality is generally predicted by measuring received signal strength and error rates. Understanding the temporal properties of these parameters is essential for the measured values to be representative, and for accurate prediction of performance of the system. In this paper, we analyze the received signal strength and error rates in an IEEE 802.11 indoor wireless mesh network, with special focus to understand its utility to measurement based protocols. We show that statistical distribution and memory properties vary across different links, but are predictable. Our experimental measurements also show that, due to the effect of fading, the packet error rates do not always monotonically decrease as the transmission rate is reduced. This has serious implications on many measurement-based protocols such as rate-adaptation algorithms. Finally, we describe real-time measurement framework that enables several applications on wireless testbed, and discuss the results from example applications that utilize measurement of signal strength and error rates.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5466673,no
Evolutionary Optimization of Software Quality Modeling with Multiple Repositories,2010,"A novel search-based approach to software quality modeling with multiple software project repositories is presented. Training a software quality model with only one software measurement and defect data set may not effectively encapsulate quality trends of the development organization. The inclusion of additional software projects during the training process can provide a cross-project perspective on software quality modeling and prediction. The genetic-programming-based approach includes three strategies for modeling with multiple software projects: Baseline Classifier, Validation Classifier, and Validation-and-Voting Classifier. The latter is shown to provide better generalization and more robust software quality models. This is based on a case study of software metrics and defect data from seven real-world systems. A second case study considers 17 different (nonevolutionary) machine learners for modeling with multiple software data sets. Both case studies use a similar majority-voting approach for predicting fault-proneness class of program modules. It is shown that the total cost of misclassification of the search-based software quality models is consistently lower than those of the non-search-based models. This study provides clear guidance to practitioners interested in exploiting their organization's software measurement data repositories for improved software quality modeling.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5467094,no
Improving the performance of hypervisor-based fault tolerance,2010,"Hypervisor-based fault tolerance (HBFT), a checkpoint-recovery mechanism, is an emerging approach to sustaining mission-critical applications. Based on virtualization technology, HBFT provides an economic and transparent solution. However, the advantages currently come at the cost of substantial overhead during failure-free, especially for memory intensive applications. This paper presents an in-depth examination of HBFT and options to improve its performance. Based on the behavior of memory accesses among checkpointing epochs, we introduce two optimizations, read fault reduction and write fault prediction, for the memory tracking mechanism. These two optimizations improve the mechanism by 31.1% and 21.4% respectively for some application. Then, we present software-superpage which efficiently maps large memory regions between virtual machines (VM). By the above optimizations, HBFT is improved by a factor of 1.4 to 2.2 and it achieves a performance which is about 60% of that of the native VM.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5470357,no
A high-performance fault-tolerant software framework for memory on commodity GPUs,2010,"As GPUs are increasingly used to accelerate HPC applications by allowing more flexibility and programmability, their fault tolerance is becoming much more important than before when they were used only for graphics. The current generation of GPUs, however, does not have standard error detection and correction capabilities, such as SEC-DED ECC for DRAM, which is almost always exercised in HPC servers. We present a high-performance software framework to enhance commodity off-the-shelf GPUs with DRAM fault tolerance. It combines data coding for detecting bit-flip errors and checkpointing for recovering computations when such errors are detected. We analyze performance of data coding in GPUs and present optimizations geared toward memory-intensive GPU applications. We present performance studies of the prototype implementation of the framework and show that the proposed framework can be realized with negligible overheads in compute intensive applications such as N-body problem and matrix multiplication, and as low as 35% in a highly-efficient memory intensive 3-D FFT kernel.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5470473,no
An effective quality measure for prediction of context information,2010,"Pervasive systems must be able to adapt themselves to changing environments to ensure the QoS for the user and to optimize resource usage. In this respect, acting proactively before an event has occurred, like starting to heat the house based on the prediction that the user will arrive within the next hour, can yield better results than reacting at the moment it occurs. Especially for context prediction, the quality of context is important since a wrong prediction can be costly and diminish user satisfaction. In this work we evaluate algorithms with respect to the quality of their inference for higher-level context information and describe how these are realized as plug-ins for a middleware that enables context-aware, self-adaptive applications.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5470596,no
Comparison of exact static and dynamic Bayesian context inference methods for activity recognition,2010,"This paper compares the performance of inference in static and dynamic Bayesian Networks. For the comparison both kinds of Bayesian networks are created for the exemplary application activity recognition. Probability and structure of the Bayesian Networks have been learnt automatically from a recorded data set consisting of acceleration data observed from an inertial measurement unit. Whereas dynamic networks incorporate temporal dependencies which affect the quality of the activity recognition, inference is less complex for dynamic networks. As performance indicators recall, precision and processing time of the activity recognition are studied in detail. The results show that dynamic Bayesian Networks provide considerably higher quality in the recognition but entail longer processing times.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5470671,no
Resource management of enterprise cloud systems using layered queuing and historical performance models,2010,"The automatic allocation of enterprise workload to resources can be enhanced by being able to make `what-if' response time predictions, whilst different allocations are being considered. It is important to quantitatively compare the effectiveness of different prediction techniques for use in cloud infrastructures. To help make the comparison of relevance to a wide range of possible cloud environments it is useful to consider the following. 1.) urgent cloud customers such as the emergency services that can demand cloud resources at short notice (e.g. for our FireGrid emergency response software). 2.) dynamic enterprise systems, that must rapidly adapt to frequent changes in workload, system configuration and/or available cloud servers. 3.) The use of the predictions in a coordinated manner by both the cloud infrastructure and cloud customer management systems. 4.) A broad range of criteria for evaluating each technique. However, there have been no previous comparisons meeting these requirements. This paper, meeting the above requirements, quantitatively compares the layered queuing and (Ã‚Â¿HYDRAÃ‚Â¿) historical techniques - including our initial thoughts on how they could be combined. Supporting results and experiments include the following: i.) defining, investigating and hence providing guidelines on the use of a historical and layered queuing model; ii.) using these guidelines showing that both techniques can make low overhead and typically over 70% accurate predictions, for new server architectures for which only a small number of benchmarks have been run; and iii.) defining and investigating tuning a prediction-based cloud workload and resource management algorithm.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5470782,no
Experimental responsiveness evaluation of decentralized service discovery,2010,"Service discovery is a fundamental concept in service networks. It provides networks with the capability to publish, browse and locate service instances. Service discovery is thus the precondition for a service network to operate correctly and for the services to be available. In the last decade, decentralized service discovery mechanisms have become increasingly popular. Especially in ad-hoc scenarios - such as ad-hoc wireless networks - they are an integral part of auto-configuring service networks. Albeit the fact that auto-configuring networks are increasingly used in application domains where dependability is a major issue, these environments are inherently unreliable. In this paper, we examine the dependability of decentralized service discovery. We simulate service networks that are automatically configured by Zeroconf technologies. Since discovery is a time-critical operation, we evaluate responsiveness - the probability to perform some action on time even in the presence of faults - of domain name system (DNS) based service discovery under influence of packet loss. We show that responsiveness decreases significantly already with moderate packet loss and becomes practicably unacceptable with higher packet loss.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5470861,no
Optimizing RAID for long term data archives,2010,We present new methods to extend data reliability of disks in RAID systems for applications like long term data archival. The proposed solutions extend existing algorithms to detect and correct errors in RAID systems by preventing accumulation of undetected errors in rarely accessed disk segments. Furthermore we show how to change the parity layout of a RAID system in order to improve the performance and reliability in case of partially defect disks. All methods benefit of a hierarchical monitoring scheme that stores reliability related information. Our proposal focuses on methods that do not need significant hardware changes.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5470870,no
Meta-scheduling in advance using red-black trees in heterogeneous Grids,2010,"The provision of Quality of Service in Grid environments is still an open issue that needs attention from the research community. One way of contributing to the provision QoS in Grids is by performing meta-scheduling of jobs in advance, that is, jobs are scheduled some time before they are actually executed. In this way, the aproppriate resources will be available to run the job when needed, so that QoS requirements (i.e., deadline) are met. This paper presents two new techniques, implemented over the red-black tree data structure, to manage the idle/busy periods of resources. One of them takes into account the heterogeneity of resources when estimating the execution times of jobs. A performance evaluation using a real testbed is presented that illustrates the efficiency of this approach to meet the QoS requirements of users.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5470923,no
An extension of GridSim for quality of service,2010,GridSim is a well known and useful open software product through which users can simulate a Grid environment. At present Qualities of Service are not modeled in GridSim. When utilising a Grid a user may wish to make decisions about type of service to be contracted. For instance performance and security are two levels of service upon which different decisions may be made. Subsequently during operation a grid may not be able to fulfill its contractual obligations. In this case renegotiation is necessary. This paper describes an extension to GridSim that enables various Qualities of Service to be modeled together with Service Level Agreements and renegotiation of contract with associated costs. The extension is useful as it will allow users to make better estimates of potential costs and will also enable grid service suppliers to more accurately predict costs and thus provide better service to users.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5471948,no
Exploiting motion estimation resilience to approximated metrics on SIMD-capable general processors: From Atom to Nehalem,2010,"In the past, efforts to speed up motion estimation for video encoding were directed at finding better predictive search algorithms. Now, they are directed toward the shrewd exploitation of the machine's advanced architectural features such as multimedia extensions, especially for the computation of the error metric which is known to be expensive. In this paper, we extend previous work by further exploring efficient implementation of approximate fast metrics for motion estimation. We show that the proposed metrics can be implemented using SIMD instructions to yield impressive speed-ups, up to 12:1 relative to non-vectorized but otherwise optimized C code, while sacrificing less than 0.1 dB on image quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5473009,no
On the evaluation of the LTE-advanced proposal within the Canadian evaluation group (CEG) initiative: Preliminary work results,2010,"In this paper, we present a preliminary work within the CEG to evaluate the LTE-advanced proposal as a candidate for further improvements of LTE earlier releases. We consider an open-source LTE simulator that implements most of the LTE features. We focus on channel state variations and related link adaptation. We study the performance of channel quality report to the transmitter to track the downlink time-frequency channel variations among users. This process aims at performing link adaptation. To insure this procedure, we implement a CQI feedback scheme to an LTE open-source simulator. SINRs per subcarriers are computed supposing a perfect channel knowledge then mapped into a single SINR value for the whole bandwidth. This SINR is represented by a CQI value that allows for dynamic MCS allocation. CQI mapping is evaluated through different SINR mapping schemes.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5473014,no
Study on Impulse Magnetic Field of Magnetostrictive Position Sensor,2010,"The magnetostrictive position sensor is a kind of displacement sensor utilizing the magnetostrictive effect and inverse effect of magnetostrictive material. This essay discussed the distribution characteristic of the magnetostrictive position sensor under the impulse magnetic field. We analyzed the impulse magnetic field of magnetostrictive line influenced by the magnetic field of permanent magnet and the impulse current magnetic field, and constructed relative theory models. Applying the numerical simulation software ANSYS, the impulse magnetic field on the sensor line was emulated based on electromagnetic theories. With experiment data, the values of magnetic field and magnetic field direction angle in magnetostrictive line were calculated. Consequently, this essay should provide the theory basis and data for promoting the signal quality of the signal detection of the sensor.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5473238,no
The Study on the Fixed End Wave in Magnetostrictive Position Sensor,2010,"The magnetostrictive position sensor is a kind of displacement sensor utilizing the magnetostrictive effect and inverse effect of magnetostrictive material. This essay discussed the influence of the fixed end of the sensor system on the detected signal with the driver impulse. The fixed end waves (a kind of elastic wave) were described and defined in this essay. The mechanisms of torsional magnetic field on the long magnetostrictive material line and fixed end waves were discussed, and relative theory models were constructed in this paper. Experiments showed that the fixed end wave of the system could be generated and transmit along the line with impulse current, which is also a kind of noise wave should be removed or weakened. Consequently, this essay should provide the theory basis and data for promoting the signal quality of the signal detection of the sensor.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5473256,no
Real-Time Traffic Classification Based on Statistical and Payload Content Features,2010,"In modern networks, different applications generate various traffic types with diverse service requirements. Thereby the identification and classification of traffic play an important role for increasing the performance in network management. Primitive applications were using well-known ports in transport layer, so their traffic classification can be performed based on the port number. However, the recent applications progressively use unpredictable port numbers. Consequently the later methods are based on â€œdeep packet inspectionâ€? Notwithstanding proper accuracy, these methods impose heavy operational load and are vulnerable to encrypted flows. The recent methods classify the traffic based on statistical packet characteristics. However, having access to a little part of statistical flow information in real-time traffic may jeopardize the performance of these methods. Regarding the advantages and disadvantages of the two later methods, in this paper we propose an approach based on payload content and statistical traffic characteristics with Naive Bayes algorithm for real-time network traffic classification. The performance and low complexity of the propose approach confirm its competency for real-time traffic classification.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5473467,no
Designing Modular Hardware Accelerators in C with ROCCC 2.0,2010,"While FPGA-based hardware accelerators have repeatedly been demonstrated as a viable option, their programmability remains a major barrier to their wider acceptance by application code developers. These platforms are typically programmed in a low level hardware description language, a skill not common among application developers and a process that is often tedious and error-prone. Programming FPGAs from high level languages would provide easier integration with software systems as well as open up hardware accelerators to a wider spectrum of application developers. In this paper, we present a major revision to the Riverside Optimizing Compiler for Configurable Circuits (ROCCC) designed to create hardware accelerators from C programs. Novel additions to ROCCC include (1) intuitive modular bottom-up design of circuits from C, and (2) separation of code generation from specific FPGA platforms. The additions we make do not introduce any new syntax to the C code and maintain the high level optimizations from the ROCCC system that generate efficient code. The modular code we support functions identically as software or hardware. Additionally, we enable user control of hardware optimizations such as systolic array generation and temporal common subexpression elimination. We evaluate the quality of the ROCCC 2.0 tool by comparing it to hand-written VHDL code. We show comparable clock frequencies and a 18% higher throughput. The productivity advantages of ROCCC 2.0 is evaluated using the metrics of lines of code and programming time showing an average of 15Ã— improvement over hand-written VHDL.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5474060,no
Acceleration of a DWT-Based Algorithm for Short Exposure Stellar Images Processing on a HPRC Platform,2010,"Our objective is to provide an enhanced algorithm for the FASTCAM instrument, developed by the Instituto de AstrofiÌsica de Canarias in collaboration with the Universidad PoliteÌcnica de Cartagena. In this paper we propose an algorithm for the detection of astronomical objects and its implementation on a High Performance Reconfigurable Computer. Our algorithm introduces wavelet based preprocessing and post-processing stages that considerably enhance the image quality when compared to the initial algorithm.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5474062,no
Towards Understanding the Importance of Variables in Dependable Software,2010,"A dependable software system contains two important components, namely, error detection mechanisms and error recovery mechanisms. An error detection mechanism attempts to detect the existence of an erroneous software state. If an erroneous state is detected, an error recovery mechanism will attempt to restore a correct state. This is done so that errors are not allowed to propagate throughout a software system, i.e., errors are contained. The design of these software artefacts is known to be very difficult. To detect and correct an erroneous state, the values held by some important variables must be ensured to be suitable. In this paper we develop an approach to capture the importance of variables in dependable software systems. We introduce a novel metric, called importance, which captures the impact a given variable has on the dependability of a software system. The importance metric enables the identification of critical variables whose values must be ensured to be correct.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5474191,no
Software Fault Prediction Model Based on Adaptive Dynamical and Median Particle Swarm Optimization,2010,"Software quality prediction can play a role of importance in software management, and thus in improve the quality of software systems. By mining software with data mining technique, predictive models can be induced that software managers the insights they need to tackle these quality problems in an efficient way. This paper deals with the adaptive dynamic and median particle swarm optimization (ADMPSO) based on the PSO classification technique. ADMPSO can act as a valid data mining technique to predict erroneous software modules. The predictive model in this paper extracts the relationship rules of software quality and metrics. Information entropy approach is applied to simplify the extraction rule set. The empirical result shows that this method set of rules can be streamlined and the forecast accuracy can be improved.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5474404,no
Applications of Support Vector Mathine and Unsupervised Learning for Predicting Maintainability Using Object-Oriented Metrics,2010,"Importance of software maintainability is increasing leading to development of new sophisticated techniques. This paper presents the applications of support vector machine and unsupervised learning in software maintainability prediction using object-oriented metrics. In this paper, the software maintainability predictor is performed. The dependent variable was maintenance effort. The independent variable were five OO metrics decided clustering technique. The results showed that the Mean Absolute Relative Error (MARE) was 0.218 of the predictor. Therefore, we found that SVM and clustering technique were useful in constructing software maintainability predictor. Novel predictor can be used in the similar software developed in the same environment.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5474411,no
Wireless Intrusion Detection System Using a Lightweight Agent,2010,"The exponential growth in wireless network faults, vulnerabilities, and attacks make the Wireless Local Area Network (WLAN) security management a challenging research area. Deficiencies of security methods like cryptography (e.g. WEP) and firewalls, causes the use of more complex security systems, such as Intrusion Detection Systems, to be crucial. In this paper, we present a hybrid wireless intrusion detection system (WIDS). To implement the WIDS, we designed a simple lightweight agent. The proposed agent detect the most destroying and serious attacks; Man-In-The-Middle and Denial-of-Service; with the minimum selected feature set. To evaluate our proposed WIDS and its agent, we collect a complete data-set using open source attack generator softwares. Experimental results show that in comparison with similar systems, in addition of more simplicity, our WIDS provides high performance and precision.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5474532,no
Grid of Segment Trees for Packet Classification,2010,"Packet classification problem has received much attention and continued to be an important topic in recent years. In packet classification problem, each incoming packet should be classified into flows according to a set of pre-defined rules. Grid-of-tries (GoT) is one of the traditional algorithmic schemes for solving 2-dimensional packet classification problem. The advantage of GoT is that it uses the switch pointers to avoid backtracking operation during the search process. However, the primary data structure of GoT is base on binary tries. The traversal of binary tries decreases the performance of GoT due to the heights of binary tries are usually high. In this paper, we propose a scheme called GST (Grid of Segment Trees). GST modifies the original GoT by replacing the binary tries with segment trees. The heights of segment trees are much shorter than those of binary tries. As a result, the proposed GST can inherit the advantages of GoT and segment trees to achieve better performance. Experiments conducted on three different kinds of rule tables show that our proposed scheme performs better than traditional schemes, such as hierarchical tries and grid-of-tries.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5474843,no
A Statistical Method for Middleware System Architecture Evaluation,2010,"The architecture of complex software systems is a collection of decisions that are very expensive to change. This makes effective software architecture evaluation methods essential in today's system development for mission critical systems. We have previously developed MEMS for evaluating middleware architectures, which provides an effective assessment of important quality attributes and their characterizations. To provide additional quantitative assessments on the overall system performance using actual runtime data, we employed a set of statistical procedures in this work. Our proposed assessment procedures comprises a standard sensitivity analysis procedure that utilizes leverage statistics to identify and remove influential data points, and an estimator for evaluating system stability and a metric for evaluating system load capacity. Experiments were conducted using real runtime datasets. Results show that our procedures effectively identified and isolated abnormal data points, and provided valuable statistics to show system stability. Our approach thus provides a sound statistical basis to support software architecture evaluation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5475039,no
In Situ Software Visualisation,2010,"Software engineers need to design, implement, comprehend and maintain large and complex software systems. Awareness of information about the properties and state of individual artifacts, and the process being enacted to produce them, can make these activities less error-prone and more efficient. In this paper we advocate the use of code colouring to augment development environments with rich information overlays. These in situ visualisations are delivered within the existing IDE interface and deliver valuable information with minimal overhead. We present CODERCHROME, a code colouring plug-in for Eclipse, and describe how it can be used to support and enhance software engineering activities.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5475058,no
Managing Structure-Related Software Project Risk: A New Role for Project Governance,2010,"This paper extends recent research on the risk implications of software project organization structures by considering how structure-related risk might be managed. Projects, and other organizations involved in projects, are usually structured according to common forms. These organizational entities interact with each other, creating an environment in which risks relating to their structural forms can impact the project and its performance. This source of risk has previously been overlooked in software project research. The nature of the phenomenon is examined and an approach to managing structure-related risk is proposed, responsibility for which is assigned as a new role for project governance. This assignment is necessary because, due to the structural and relational nature of these risks, the project is poorly placed to manage such threats. The paper argues that risk management practices need to be augmented with additional analyses to identify, analyze and assess structural risks to improve project outcomes and the delivery of quality software. The argument is illustrated and initially validated with two project case studies. Implications for research and practice are drawn and directions for future research are suggested, including extending the theory to apply to other organization structures.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5475065,no
Towards Fully Automated Test Management for Large Complex Systems,2010,"Development of large and complex software intensive systems with continuous builds typically generates large volumes of information with complex patterns and relations. Systematic and automated approaches are needed for efficient handling of such large quantities of data in a comprehensible way. In this paper we present an approach and tool enabling autonomous behavior in an automated test management tool to gain efficiency in concurrent software development and test. By capturing the required quality criteria in the test specifications and automating the test execution, test management can potentially be performed to a great extent without manual intervention. This work contributes towards a more autonomous behavior within a distributed remote test strategy based on metrics for decision making in automated testing. These metrics optimize management of fault corrections and retest, giving consideration to the impact of the identified weaknesses, such as fault-prone areas in software.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477058,no
Searching for a Needle in a Haystack: Predicting Security Vulnerabilities for Windows Vista,2010,"Many factors are believed to increase the vulnerability of software system; for example, the more widely deployed or popular is a software system the more likely it is to be attacked. Early identification of defects has been a widely investigated topic in software engineering research. Early identification of software vulnerabilities can help mitigate these attacks to a large degree by focusing better security verification efforts in these components. Predicting vulnerabilities is complicated by the fact that vulnerabilities are, most often, few in number and introduce significant bias by creating a sparse dataset in the population. As a result, vulnerability prediction can be thought of us preverbally â€œsearching for a needle in a haystack.â€?In this paper, we present a large-scale empirical study on Windows Vista, where we empirically evaluate the efficacy of classical metrics like complexity, churn, coverage, dependency measures, and organizational structure of the company to predict vulnerabilities and assess how well these software measures correlate with vulnerabilities. We observed in our experiments that classical software measures predict vulnerabilities with a high precision but low recall values. The actual dependencies, however, predict vulnerabilities with a lower precision but substantially higher recall.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477059,no
Fault Detection Likelihood of Test Sequence Length,2010,"Testing of graphical user interfaces is important due to its potential to reveal faults in operation and performance of the system under consideration. Most existing test approaches generate test cases as sequences of events of different length. The cost of the test process depends on the number and total length of those test sequences. One of the problems to be encountered is the determination of the test sequence length. Widely accepted hypothesis is that the longer the test sequences, the higher the chances to detect faults. However, there is no evidence that an increase of the test sequence length really affect the fault detection. This paper introduces a reliability theoretical approach to analyze the problem in the light of real-life case studies. Based on a reliability growth model the expected number of additional faults is predicted that will be detected when increasing the length of test sequences.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477061,no
Satisfying Test Preconditions through Guided Object Selection,2010,"A random testing strategy can be effective at finding faults, but may leave some routines entirely untested if it never gets to call them on objects satisfying their preconditions. This limitation is particularly frustrating if the object pool does contain some precondition-satisfying objects but the strategy, which selects objects at random, does not use them. The extension of random testing described in this article addresses the problem. Experimentally, the resulting strategy succeeds in testing 56% of the routines that the pure random strategy missed; it tests hard routines 3.6 times more often; although it misses some of the faults detected by the original strategy, it finds 9.5% more faults overall; and it causes negligible overhead.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477072,no
"We're Finding Most of the Bugs, but What are We Missing?",2010,"We compare two types of model that have been used to predict software fault-proneness in the next release of a software system. Classification models make a binary prediction that a software entity such as a file or module is likely to be either faulty or not faulty in the next release. Ranking models order the entities according to their predicted number of faults. They are generally used to establish a priority for more intensive testing of the entities that occur early in the ranking. We investigate ways of assessing both classification models and ranking models, and the extent to which metrics appropriate for one type of model are also appropriate for the other. Previous work has shown that ranking models are capable of identifying relatively small sets of files that contain 75-95% of the faults detected in the next release of large legacy systems. In our studies of the rankings produced by these models, the faults not contained in the predicted most fault prone files are nearly always distributed across many of the remaining files; i.e., a single file that is in the lower portion of the ranking virtually never contains a large number of faults.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477073,no
An Application of Six Sigma and Simulation in Software Testing Risk Assessment,2010,"The conventional approach to Risk Assessment in Software Testing is based on analytic models and statistical analysis. The analytic models are static, so they don't account for the inherent variability and uncertainty of the testing process, which is an apparent deficiency. This paper presents an application of Six Sigma and Simulation in Software Testing. DMAIC and simulation are applied to a testing process to assess and mitigate the risk to deliver the product on time, achieving the quality goals. DMAIC is used to improve the process and achieve required (higher) capability. Simulation is used to predict the quality (reliability) and considers the uncertainty and variability, which, in comparison with the analytic models, more accurately models the testing process. Presented experiments are applied on a real project using published data. The results are satisfactorily verified. This enhanced approach is compliant with CMMI<sup>Â®</sup> and provides for substantial Software Testing performance-driven improvements.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477075,no
(Un-)Covering Equivalent Mutants,2010,"Mutation testing measures the adequacy of a test suite by seeding artificial defects (mutations) into a program. If a test suite fails to detect a mutation, it may also fail to detect real defects-and hence should be improved. However, there also are mutations which keep the program semantics unchanged and thus cannot be detected by any test suite. Such equivalent mutants must be weeded out manually, which is a tedious task. In this paper, we examine whether changes in coverage can be used to detect non-equivalent mutants: If a mutant changes the coverage of a run, it is more likely to be non-equivalent. In a sample of 140 manually classified mutations of seven Java programs with 5,000 to 100,000 lines of code, we found that: (a) the problem is serious and widespread-about 45% of all undetected mutants turned out to be equivalent; (b) manual classification takes time-about 15 minutes per mutation; (c) coverage is a simple, efficient, and effective means to identify equivalent mutants-with a classification precision of 75% and a recall of 56%; and (d) coverage as an equivalence detector is superior to the state of the art, in particular violations of dynamic invariants. Our detectors have been released as part of the open source JAVALANCHE framework; the data set is publicly available for replication and extension of experiments.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477100,no
"Toward High-Throughput, Multicriteria Protein-Structure Comparison and Analysis",2010,"Protein-structure comparison (PSC) is an essential component of biomedical research as it impacts on, e.g., drug design, molecular docking, protein folding and structure prediction algorithms as well as being essential to the assessment of these predictions. Each of these applications, as well as many others where molecular comparison plays an important role, requires a different notion of similarity that naturally lead to the multicriteria PSC (MC-PSC) problem. Protein (Structure) Comparison, Knowledge, Similarity, and Information (ProCKSI) (www.procksi.org) provides algorithmic solutions for the MC-PSC problem by means of an enhanced structural comparison that relies on the principled application of information fusion to similarity assessments derived from multiple comparison methods. Current MC-PSC works well for moderately sized datasets and it is time consuming as it provides public service to multiple users. Many of the structural bioinformatics applications mentioned above would benefit from the ability to perform, for a dedicated user, thousands or tens of thousands of comparisons through multiple methods in real time, a capacity beyond our current technology. In this paper, we take a key step into that direction by means of a high-throughput distributed reimplementation of ProCKSI for very large datasets. The core of the proposed framework lies in the design of an innovative distributed algorithm that runs on each compute node in a cluster/grid environment to perform structure comparison of a given subset of input structures using some of the most popular PSC methods [e.g., universal similarity metric (USM), maximum contact map overlap (MaxCMO), fast alignment and search tool (FAST), distance alignment (DaliLite), combinatorial extension (CE), template modeling alignment (TMAlign)]. We follow this with a procedure of distributed consensus building. Thus, the new algorithms proposed here achieve ProCKSI's similarity assessment quality but with a fraction of- - the time required by it. Our results show that the proposed distributed method can be used efficiently to compare: 1) a particular protein against a very large protein structures dataset (target-against-all comparison), and 2) a particular very large-scale dataset against itself or against another very large-scale dataset (all-against-all comparison). We conclude the paper by enumerating some of the outstanding challenges for real-time MC-PSC.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477191,no
The research of outlier data cleaning based on accelerating method,2010,"During the data integration process, it puts forward the accelerating trend comparison method to deal with the outlier data in this paper. Namely outlier data is discovered through the accelerating trend comparison. At the end of this article, it gives a specific description of the algorithm of outlier data cleaning results. It can improve the detection of outlier data and the data quality through the experiments.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477757,no
Evaluation approaches of information systems service quality,2010,"This paper has built a new evaluation approach (called SQ-Atten) based on summary and comparative study of the four traditional service quality assessment instruments. SQ-Atten has a new term attention, which has similar function with zone of tolerance (ZOT). It can analyze the users' satisfaction using the score of attention and performance, and has less difficult than that of ZOT in collecting data. It has data predictive ability and richer data than SERVPERF. Also, it measures the same 44 items as SERVQUAL, but has higher data reliability because of without measuring expectation and difference operation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477961,no
A software reliability prediction model based on benchmark measurement,2010,Software reliability is a very important and active research field in software engineering. There have been one hundred of prediction model since the first prediction model published. But most of them are adapted after software test and only few of them can be used before test. The paper proposed an idea that to predict the software reliability making use of the similar projects measurement data based on software process benchmark. Its prediction uses benchmark measurement and software process data before software test.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5478245,no
Predict protein subnuclear location with ensemble adaboost classifier,2010,"Protein function prediction with computational method is becoming an important research field in protein science and bioinformatics. In eukaryotic cells, the knowledge of subnuclear localization is essential for understanding the life function of nucleus. In this study, A novel ensemble classifier is designed incorporating three AdaBoost classifiers to predict protein subnuclear localization. The base classifier algorithms in AdaBoost classifier is fuzzy K nearest neighbors (FKNN). Three parts amino acid pair compositions with different spaces are computed to construct features vector for representing a protein sample. Jackknife cross-validation test are used to evaluate performance of proposed with two benchmark datasets. Compared with prior works, promising results obtained indicate that the proposed method is more effective and practical. Current approach may also be used to improve the prediction quality of other protein attributes. The software written in Matlab are available freely by contacting the corresponding author.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5478972,no
Developing Self-Managing Embedded Systems with ASSL,2010,"This research targets formal modeling of embedded systems capable of self-management. In our approach, we use the ASSL (Autonomic System Specification Language) framework as a development environment, where self-management features of embedded systems are formally specified and an implementation is automatically generated. ASSL exposes a rich set of specification constructs that help developers specify event-driven embedded systems. Hardware is sensed via special metrics intended to drive events and self-management policies that help the system handle critical situations in an autonomous reactive manner. We present this approach along with a simulation case study where ASSL is used to develop control software for the wide-angle camera carried on board NASA's Voyager II spacecraft.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5479522,no
Dynamic Policy-Driven Quality of Service in Service-Oriented Systems,2010,"Service-oriented architecture (SOA) middleware has emerged as a powerful and popular distributed computing paradigm due to its high-level abstractions for composing systems and hiding platform-level details. Control of some details hidden by SOA middleware is necessary, however, to provide managed quality of service (QoS) for SOA systems that need predictable performance and behavior. This paper presents a policy-driven approach for managing QoS in SOA systems. We discuss the design of several key QoS services and empirically evaluate their ability to provide QoS under CPU overload and bandwidth-constrained situations.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5479581,no
Levenberg-Marquardt neural network for gear fault diagnosis,2010,"In this study we are trying with the Levenberg-Marquardt neural network model to the problem of gear fault diagnosis. By using second derivative information, the network convergence speed is promoted and the generalization performance is enhanced. Taking a certain gearbox fault signal acquisition experimental system for instance, Matlab software and its neural network toolbox are used to model and simulate. The simulation result shows that Levenberg-Marquardt neural network has a good performance for the common gear fault diagnosis and it can identify various types of faults stably and accurately. Furthermore, compared with conventional BP neural network, the Levenberg-Marquardt neural network reduces training epochs and promotes diagnosis accuracy.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5479613,no
Trust-based rating prediction for recommendation in Web 2.0 collaborative learning social software,2010,"Benefiting from the advent of social software, information sharing becomes pervasive. Personalized rating systems have emerged to evaluate the quality of user-generated content in open environment and provide recommendation based on users' past experience. In this paper, a trust-based rating prediction approach for recommendation in Web 2.0 collaborative learning social software is proposed. Trust network is exploited in the rating prediction scheme and a multi-relational trust metric is developed in an implicit way. Finally the evaluation of the approach is performed using the dataset of collaborative learning social software, namely Remashed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5480038,no
Software Maintenance Prediction Using Weighted Scenarios: An Architecture Perspective,2010,"Software maintenance is considered one of the most important issues in software engineering which has some serious implications in term of cost and effort. It consumes enormous amount of organization's overall resources. On the other hand, software architecture of an application has considerable effect on quality factors such as maintainability, performance, reliability and flexibility etc. Using software architecture for quantification of certain quality factor will help organizations to plan resources accordingly. This paper is an attempt to predict software maintenance effort at architecture level. The method takes requirements, domain knowledge and general software engineering knowledge as input in order to prescribe application architecture. Once application architecture is prescribed, then weighted scenarios and certain factors (i.e. system novelty, turnover and maintenance staff ability, documentation quality, testing quality etc) that affect software maintenance are applied to application architecture to quantify maintenance effort. The technique is illustrated and evaluated using web content extraction application architecture.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5480420,no
Feature Selection for Medical Diagnosis Using Fuzzy Artmap Classification and Intersection Conflict,2010,"Studying complex systems including biological systems is a multi-disciplinary research area. It must be derived by the recent explosion of ICT including high-performance computing, high-throughput experiments, the Internet, knowledge discovery and Artificial Intelligence (AI). The goal of this research is to establish a computational architecture and tools to deal with complex systems based on such advanced technologies. Therefore in the case of medical diagnosis based on machine learning model, we need to reduce the number of variables according with their relevance and allowing to take decisions in real-time. This approach is realized in two stages. In the first one, we classify the unfaulty functioning data of system using the fuzzy-ARTMAP classification. In the second stage, a conflict is accounted between features of test data based on the hyper-cubes resulted in the first stage. Two features are in conflict if her intersection does not belong to the model elaborated by fuzzy-ARTMAP classification.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5480757,no
Evaluation of Error Control Mechanisms Based on System Throughput and Video Playable Frame Rate on Wireless Channel,2010,"Error control mechanisms are widely used in video communications over wireless channels. However for improving end-to-end video quality: they consume extra bandwidth and reduce effective system throughput. In this paper, considering the parameters of system throughput and playable frame rate as evaluating metrics, we investigate the efficiency of different error control mechanisms. We develop a throughput analytical model to present system effective throughput for different error control mechanisms under different conditions. For a given packet loss probability, both optimal retransmission times in adaptive ARQ and optimal number of redundant packets in adaptive FEC for each type of frames are derived by keeping the system throughput as a constant value. Also, end to end playable frame rates for the two schemes are computed. Then which error control scheme is the most suitable for which application condition is concluded. Finally empirical simulation experimental results with various data analysis are demonstrated.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5480940,no
Research on a Software Trustworthy Measure Model,2010,"Through analyzing the factors affecting software trustworthy, established the index system of trustworthy software estimation. Apply Analytic Hierarchy Process (AHP) to determine the relative importance of factors and indicator items affecting software trustworthy, and then determine the score of estimation index system through fuzzy estimation model, use the combination of both to measure software trustworthy. Finally, this paper presents an example to validate the validity and objectivity of this model.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5480976,no
Reliability Analysis of Embedded Applications in Non-Uniform Fault Tolerant Processors,2010,"Soft error analysis has been greatly aided by the concept of Architectural vulnerability Factor (AVF) and Architecturally Correct Execution (ACE). The AVF of a processor is defined as the probability that a bit flip in the processor architecture will result in a visible error in the final output of a program. In this work, we exploit the techniques of AVF analysis to introduce a software-level vulnerability analysis. This metric allows insight into the vulnerability of instruction and software to hardware faults with a micro-architectural involved fault injection method. The proposed metric can be used to make judgments about the reliability of different programs on different processors with regard to architectural and compiler guidelines for improving the processor reliability.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5482675,no
Centered Hyperspherical and Hyperellipsoidal One-Class Support Vector Machines for Anomaly Detection in Sensor Networks,2010,"Anomaly detection in wireless sensor networks is an important challenge for tasks such as intrusion detection and monitoring applications. This paper proposes two approaches to detecting anomalies from measurements from sensor networks. The first approach is a linear programming-based hyperellipsoidal formulation, which is called a centered hyperellipsoidal support vector machine (CESVM). While this CESVM approach has advantages in terms of its flexibility in the selection of parameters and the computational complexity, it has limited scope for distributed implementation in sensor networks. In our second approach, we propose a distributed anomaly detection algorithm for sensor networks using a one-class quarter-sphere support vector machine (QSSVM). Here a hypersphere is found that captures normal data vectors in a higher dimensional space for each sensor node. Then summary information about the hyperspheres is communicated among the nodes to arrive at a global hypersphere, which is used by the sensors to identify any anomalies in their measurements. We show that the CESVM and QSSVM formulations can both achieve high detection accuracies on a variety of real and synthetic data sets. Our evaluation of the distributed algorithm using QSSVM reveals that it detects anomalies with comparable accuracy and less communication overhead than a centralized approach.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5483231,no
A fault section detection method using ZCT when a single phase to ground fault in ungrounded distribution system,2010,"A single line to ground fault (SLG) detection in ungrounded network is very difficult, because fault current magnitude is very small. It is generated by a charging current between distribution line and ground. As it is very small, is not used for fault detection in case of SLG. So, SLG has normally been detected by switching sequence method which makes customers experience blackouts. A new fault detection algorithm based on comparison of zero-sequence current and line-to-line voltage phases is proposed. The algorithm uses ZCT installed to ungrounded distribution network. The proposed in this paper algorithm has the advantage that it can detect fault phase and distinguish a faulted section as well. The simulation tests of proposed algorithm were performed using Matlab Simulink and the results are presented in the paper.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5484544,no
Performance analysis of web service composition based on stochastic well-formed workflow,2010,"Quality-of-service (QoS) in Web services encompasses various non-functional issues such as performance, dependability and security, etc. As more and more Web services become available, QoS capability is becoming a decisive factor to distinguishing services. The management of QoS metrics directly impacts the success of services participating in workflow-based applications. Therefore, when services are created or managed using workflows or Web processes, the underlying workflow engine must be able to estimate, monitor, and control the QoS rendered to customers. In this paper, we present an approach to give QoS of service composition based on a decomposing algorism and the numerical analysis of stochastic well-formed workflow (SWWF) models of web service composition.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5484860,no
Speech compression using LPC and wavelet,2010,"The past decade has witnessed substantial progress towards the application of low-rate speech coders to civilian and military communications as well as computer-related voice applications. Central to this progress has been the development of new speech coders capable of producing high-quality speech at low data rates. Most of these coders incorporate mechanisms to represent the spectral properties of speech, provide for speech waveform matching, and optimize the coder's performance for the human ear. A number of these coders have already been adopted in national and international cellular telephony standards. In mobile communication systems, service providers are continuously met with the challenge of accommodating more users within a limited allocated bandwidth. For this reason, manufactures and service providers are continuously in search of low bit-rate speech coders that deliver toll-quality speech. In this paper the simulated low bit rate vocoder (LPC) using MATLAB was implemented. The result obtained from LPC was compared with other implemented voice compression using wavelet transform. From the results we see that the performance of wavelet transform was better than LPC.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5485348,no
Adaptive random testing of mobile application,2010,"Mobile applications are becoming more and more powerful yet also more complex. While mobile application users expect the application to be reliable and secure, the complexity of the mobile application makes it prone to have faults. Mobile application engineers and testers use testing technique to ensure the quality of mobile application. However, the testing of mobile application is time-consuming and hard to automate. In this paper, we model the mobile application from a black box view and propose a distance metric for the test cases of mobile software. We further proposed an ART test case generation technique for mobile application. Our experiment shows our ART tool can both reduce the number of test cases and the time needed to expose first fault when compared with random technique.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5485442,no
Evaluation of software testing process based on Bayesian networks,2010,"In this paper, we will introduce a Bayesian networks (BN) approach for probability evaluation method of software quality assurance. Then, we will present a method for transforming Fault Tree Analysis (FTA) to Bayesian networks and build an evaluation model based on Bayesian networks. Bayesian networks can perform forward risk prediction and backward diagnosis analysis by deduction on the model. Finally, we will illustrate the rationality and validity of the Bayesian networks through an example of evaluation of software testing process.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5485469,no
Defect association and complexity prediction by mining association and clustering rules,2010,"Number of defects remaining in a system provides an insight into the quality of the system. Software defect prediction focuses on classifying the modules of a system into fault prone and non-fault prone modules. This paper focuses on predicting the fault prone modules as well as identifying the types of defects that occur in the fault prone modules. Software defect prediction is combined with association rule mining to determine the associations that occur among the detected defects and the effort required for isolating and correcting these defects. Clustering rules are used to classify the defects into groups indicating their complexity: SIMPLE, MODERATE and COMPLEX. Moreover the defects are used to predict the effect on the project schedules and the nature of risk concerning the completion of such projects.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5485608,no
A source-based risk analysis approach for software test optimization,2010,"In this paper we introduce our proposed technique for software component test prioritization and optimization which is based on a source-code based risk analysis. Software test is one of the most critical steps in the software development. Considering that the time and human resources of a software project are limited, software test should be scheduled and planned very carefully. In this paper we introduce a classification approach that provides the developers with a risk model of the application which is specifically designed to assist the testing process by identifying the most important components and their corresponding test effort estimation. We designed an analyser tool to apply our technique to a test software project and we presented the results in this paper.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5485639,no
Keystroke identification with a genetic fuzzy classifier,2010,"This paper proposes the use of fuzzy if-then rules for Keystroke identification. The proposed methodology modifies Ishibuchi's genetic fuzzy classifier to handle high dimensional problems such as keystroke identification. High dimensional property of a problem increases the number of rules with low fitness. For decreasing them, rule initialization and coding are modified. Furthermore a new heuristic method is developed for improving the population quality while running GA. Experimental result demonstrates that we can achieve better running time, interpretability and accuracy with these modifications.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5485677,no
An efficient experimental approach for the uncertainty estimation of QoS parameters in communication networks,2010,"In communication networks setup and tuning activities, a key issue is to assess the impact of a new service running on the network on the overall Quality of Service. To this aim suitable figures of merit and test beds have to be adopted and time-consuming measurement campaigns generally should be carried out. A preliminary issue to be accomplished for is the metrological characterization of the test set-up aimed to provide a confidence level and a variability interval to the measurement results. This allows identifying and evaluating the intrinsic uncertainty to be considered in the experimental measurement of Quality of Service parameters. This paper proposes an original experimental approach suitable for the purpose. The uncertainty components involved in the measurement process are identified and experimentally quantified by means of effective statistical analyses. The proposed approach takes into account the general characteristics of the network topology, the number and type of devices involved, the characteristics of the current services operating on the network, and of the new services to be implemented, as well as the intrinsic uncertainties related to the set-up and to the measurement method. As an application example, the proposed approach has been adopted to the measurement of the packet jitter on a test bed involving a real computer network displaced on several kilometers. The obtained results show the effectiveness of the proposal.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5488077,no
Induction defectoscope based on uniform eddy current probe with GMRs,2010,"Defect detection in conductive plates represents an important issue. The present work proposes an induction defectoscope that includes a uniform eddy current probe with a rectangular excitation coil and a set of giant magnetoresistance sensors (GMR). The excitation current, the acquisition of the voltages delivered by the GMR and the signal processing of the acquired signal are performed by a real-time control and processing unit based on a TMS320C6713 digital signal processor (DSP). Different tests were carried out regarding the excitation coil position versus crack orientation and also regarding the GMR position inside the coil and the best response concerning the crack detection for a given aluminum plate specimen. Embedded software was developed using a NI LabVIEW DSP module including sinusoidal signal generation, amplitude and phase extraction using a sine-fitting algorithm and GUI for the induction defectoscope. Experimental results with probe characterization and detection of defects were included in the paper.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5488189,no
Trustable web services with dynamic confidence time interval,2010,"One part of trustfulness on web services application over the network is confidence of services that providers can guarantee to their customers. Therefore, after the development process, web services developers must be sure that the delivered services are qualified for availability and reliability during their execution. However, there is a critical problem when errors occur during the execution time of the service agent, such as the infinite loop problem in the service process. This unexpected problem of the web services software can cause critical damages in various aspects, especially lives and dead of people. Although there are various methods have been proposed to protect the unexpected errors, most of them are procedures in the verification and validation during the development processes. Nevertheless, these methods cannot completely solve the infinite loop problem since this problem is usually occurred by an unexpected values obtained from the execution of request and response processes . Therefore, this paper proposed a system architecture includes with a protection mechanism that completely detect and protect the unbound loop problem of web services when request services of each requester are under the dynamic situation. This proposed solution can guarantee that users will definitely be protected from a critical lost occurred from the unexpected infinite loop of the web services system. Consequently, all service agents with dynamic loop control condition can be trustable.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5488643,no
esrcTool: A Tool to Estimate the Software Risk and Cost,2010,"Function Point is a well known established method to estimate the size of software projects. There are several areas of the software engineering in which we can use the function point analysis (FPA) like project planning, project construction, software implementation etc. In this paper we have used the function point approach in order to develop the architecture of the esrcTool. This tool is used for two different purposes, firstly, to estimate the risk in the software and secondly to estimate the cost of the software. In the literature of software engineering there are so many models to estimate the risk in the software like Soft Risk Model, SRAM, SRAEM and so on. But in the esrcTool we have used SRAEM i.e. Software Risk Assessment and Estimation Model, because in this model FP is used as an input variable, and on the other hand side, in order to determine the cost of the software we have used the International Software Benchmarking Standards Group Release Report (ISBSG).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5489451,no
Complexity Estimation Approach for Debugging in Parallel,2010,Multiple faults in a software many times prevent debuggers from efficiently localizing a fault. This is mainly due to not knowing the exact number of faults in a failing program as some of the faults get obfuscated. Many techniques have been proposed to isolate different faults in a program thereby creating separate sets of failing program statements. To evenly divide these statements amongst debuggers we must know the level of work required to debug that slice. In this paper we propose a new technique to calculate the complexity of faulty program slices to efficiently distribute the work among debuggers for simultaneous debugging. The technique calculates the complexity of entire slice by taking into account the suspiciousness of every faulty statement. To establish the confidence in effectiveness and efficiency of proposed techniques we illustrate the whole idea with help of an example. Results of analysis indicate the technique will be helpful (a) for efficient distribution of work among debuggers (b) will allow simultaneous debugging of different faulty program slices (c) will help minimize the time and manual labor.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5489516,no
A Generic Model for the Specification of Software Interface Requirements and Measurement of Their Functional Size,2010,"The European ECSS-E-40 series of standards for the aerospace industry includes interfaces as one of 16 types of non functional requirement (NFR) for embedded and real-time software. An interface is typically described at the system level as a non functional requirement, and a number of concepts and terms are provided in that series to describe various types of candidate interfaces. This paper collects and organizes these interface-related descriptions into a generic model for the specification of software interface requirements, and to measure their functional size for estimation purposes using the COSMIC ISO 19761 standard.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5489831,no
Improved leak detection quality by automatic signal filtering,2010,"Constant improvement of leak detection techniques in pipe systems used for liquids transportation is a priority for companies and authorities around the world. If a pipe presents leakage problems, the liquid which is lost generates specific signals which are transmitted in the material of the pipe. These signals can be recorded using accelerometers. They are analyzed with the purpose of identifying the location of the leak. An important data analysis tool which helps in the process of leak position detection is the Cross Correlation Function (CCF). It is calculated between two simultaneously recorded specific leak signals with the purpose of time delay estimation. Time delay estimation calculations lead to the identification of the leak position. However, a traditional implementation of the CCF may not be satisfactory. Recorded signals are affected by noise coming from pipe elbows, junctions or traffic. All these unwanted noise sources can influence the aspect and accuracy of the CCF. Further improvements are necessary. This paper presents a Labview 8.5 implementation of an algorithm which improves the quality of the calculated CCF. Being part of a more complex software application, the algorithm uses the calculation of the coherence function. By analyzing the coherence the application will automatically select two frequency intervals (narrow bands) for filtering the leak signals. After the filtering process, the calculation of the CCF shows an increased quality. The application can be accessed from distance if a remote user needs to study or save the CCF results. No filtering settings are required and the method assures good results especially in the case of recorded signals which are highly affected by unwanted noise.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5491234,no
Portable artificial nose system for assessing air quality in swine buildings,2010,"To practice an efficient air quality management in livestocks, a standardized measurement technology has always been requested in order to assess the odor, of which results are acceptable by every party involved, i.e., the owner, the state and the public. This paper has reported on a prototype of portable electronic nose (e-nose) designed specially to assess malodors in swine buildings atmosphere in the pig farm. The briefcase formed e-nose consists of eight chemical gas sensors that are sensitive to gases usually presented in pig farm such as ammonia, hydrogen sulfide, hydrocarbons etc. The system contains gas flow controller, measurement circuit and data acquisition unit, all of which are automated and controlled by an in-house software on a notebook PC via a USB port. We have tested the functionality of this e-nose in a pig farm under a real project aimed specifically to reduce the odor emission from swine buildings in the pig farm. The e-nose was used to assess the air quality inside sampled swine buildings. Based on the results given in this paper, recommendations on appropriate feeding menu, buildings' cleaning schedule and emission control program have been made.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5491431,no
Using Cloud Constructs and Predictive Analysis to Enable Pre-Failure Process Migration in HPC Systems,2010,Accurate failure prediction in conjunction with efficient process migration facilities including some Cloud constructs can enable failure avoidance in large-scale high performance computing (HPC) platforms. In this work we demonstrate a prototype system that incorporates our probabilistic failure prediction system with virtualization mechanisms and techniques to provide a whole system approach to failure avoidance. This work utilizes a failure scenario based on a real-world HPC case study.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5493402,no
The Design and Implementation of IEEE 802.21 and Its Application on Wireless VoIP,2010,"Supporting a multimode mobile device to seamlessly switch its connections between various wireless access networks, such as WiMAX, Wi-Fi, and LTE, is an important research issue in 4G communications. The IEEE 802.21 working group defines the Media Independent Handover (MIH) standard to provide cross layer control and the information of network environment and services to optimize the handoff process. This work designs and implements the MIH middleware in a Linux platform to provide MIH event services (MIH-ES), MIH command services (MIH-CS) and MIH information services (MIH-IS). Based on the MIH software, we develop a high quality VoIP system which integrates Stream Control Transmission Protocol (SCTP), two MIH-IS based user motion detection (UMD) services and an adaptive QoS playout algorithm (AQP). The MIH services, multi-homing capability and dynamic address configuration extension of SCTP are applied in the VoIP system to perform seamless handoffs. Moreover, AQP adjusts the playout buffer by using the retransmission of partial reliable SCTP (PR-SCTP) and the MIH-ES to reduce the packet loss rate and improve speech quality. The experiment results indicate that MIH can enhance the performance of the handoff process and the quality of wireless VoIP services.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5493641,no
"The SlimSAR: A small, multi-frequency, Synthetic Aperture Radar for UAS operation",2010,"The SlimSAR is a small, low-cost, Synthetic Aperture Radar (SAR) and represents a new advancement in highperformance SAR. ARTEMIS employed a unique design methodology that exploits previous developments in designing the Slim-SAR to be smaller, lighter, and more flexible while consuming less power than typical SAR systems. With an L-band core, and frequency block converters, the system is very suitable for use on a number of small UAS's. Both linear-frequency-modulated continuous-wave (LFM-CW), which achieves high signal-to-noise ratio while transmitting with less power, and pulsed mode have been tested. The flexible control software allows us to change the radar parameters in flight. The system has a built-in high quality GPS/IMU motion measurement solution and can also be packaged with a small data link and a gimbal for high frequency antennas. Multi-frequency SAR provides day and night imaging through smoke, dust, rain, and clouds with the advantages of additional capabilites at different frequencies (i.e. dry ground and foliage penetration at low frequencies, and change detection at high frequencies.)",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5494612,no
An effective nonparametric quickest detection procedure based on Q-Q distance,2010,"Quickest detection schemes are geared toward detecting a change in the state of a data stream or a real-time process. Classical quickest detection schemes invariably assume knowledge of the pre-change and post-change distributions that may not be available in many applications. In this paper, we present a distribution free nonparametric quickest detection procedure based on a novel distance measure, referred to as the Q-Q distance calculated from the Q-Q plot, for detection of distribution changes. Through experimental study, we show that the Q-Q distance-based detection procedure presents comparable or better performance compared to classical parametric and other nonparametric procedures. The proposed procedure is most effective when detecting small changes.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5495849,no
Improved Internet traffic analysis via optimized sampling,2010,"Applications to evaluate Internet quality-of-service and increase network security are essential to maintaining reliability and high performance in computer networks. These applications typically use very accurate, but high cost, hardware measurement systems. Alternate, less expensive software based systems are often impractical for use with analysis applications because they reduce the number and accuracy of measurements using a technique called interrupt coalescence, which can be viewed as a form of sampling. The goal of this paper is to optimize the way interrupt coalescence groups packets into measurements so as to retain as much of the packet timing information as possible. Our optimized solution produces estimates of timing distributions much closer to those obtained using hardware based systems. Further we show that for a real Internet analysis application, periodic signal detection, using measurements generated with our method improved detection times by at least 36%.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5495909,no
Application of ANN in food safety early warning,2010,"In recent years, frequent occurrence of food safety crisis has seriously affected people's health, which causes widespread concern around the world. To effectively track and trace food has become an extremely urgent global issue. Early warning of food safety can prevent food safety crisis. However, there is still very few automatic tracking systems for the entire food supply chain. In the paper we propose a data mining technique to predict food quality using back-propagation (BP) neural network. Some prediction errors could occur when predicted data are near threshold values. To reduce errors, data near the threshold values are selected to train our system. Special care of threshold values and performance of our proposed algorithm are discussed in the paper.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5497450,no
Ultrasonic Waveguides Detection-based approach to locate defect on workpiece,2010,"Conventional ultrasonic techniques, such as pulse-echo, has been limited to testing relatively simple geometries or interrogating the region in the immediate vicinity of the transducer. A novel, efficiency methodology uses ultrasonic waveguides to examine structural components. The advantages of this technique include: its ability to detect the entire structure in a single measurement through long distance with little attenuation; and its capacity to test inaccessible regions of complex components. However, in practical work, this technique exists dispersion and mode conversion phenomena which makes poor signal to noise ratio, thereby, influences the actual application of this technique. In order to solve this problem, simulation with experiments can not only verifies the feasibility of this technique, but also has guiding significant for actual work. This paper reports on a novel approach in the simplification of the simulation of Ultrasonic Waveguides Detection. The first step is the selection of the frequency of signal which has the fastest group velocity and relatively small dispersion. The second step is the decision of Î” and l<sub>e</sub>. As the numerical analysis characteristics of general-purpose software ANSYS, two key parameters: time step Î”t and mesh element size l<sub>e</sub> need to be carefully selected. This report finds the balance point between the accuracy of results and calculation time to determine two key parameters which significantly influence the result of the simulation result. Finally, this report show the experiment results on two-dimensional flat panel structure and three-dimensional triangle-iron structure respectively. From the result shown, the error between the simulation and actual value is less than 0.4%, perfectly prove the feasibility of this approach.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5498125,no
Probabilistic fault prediction of incipient fault,2010,"In this work, a probabilistic fault prediction approach is presented for prediction of incipient fault in an uncertain way. The approach has two stages. In the first stage, normal data is analyzed by principle component analysis (PCA) to get control limits of the statistics of T<sup>2</sup> and SPE. In the second stage, fault data starts by PCA so as to derive the statistics of T<sup>2</sup> and SPE. Then, the samplings of these two statistics obeying some certain prediction distribution are obtained using Bayesian AR model on the basis of the Winbugs software. At last, one-step prediction fault probabilities are estimated by kernel density estimation method according to the statistics' corresponding control limits. The prediction performance of this approach is illustrated using the data from the simulator of the Tennessee Eastman process.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5498474,no
Model Based Testing Using Software Architecture,2010,"Software testing is an ultimate obstacle to the final release of software products. Software testing is also a leading cost factor in the overall construction of software products. On the one hand, model-based testing methods are new testing techniques aimed at increasing the reliability of software, and decreasing the cost by automatically generating a suite of test cases from a formal behavioral model of a system. On the other hand, the architectural specification of a system represents a gross structural and behavioral aspect of a system at the high level of abstraction. Formal architectural specifications of a system also have shown promises to detect faults during software back-end development. In this work, we discuss a hybrid testing method to generate test cases. Our proposed method combines the benefits of model-based testing with the benefits of software architecture in a unique way. A simple Client/Server system has been used to illustrate the practicality of our testing technique.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5501432,no
Improving Change Impact Analysis with a Tight Integrated Process and Tool,2010,"Change impact analysis plays an immanent role in the maintenance and enhancement of software systems, especially for defect prevention. In our previous work we have developed approaches to detect logical dependencies among artifacts in repositories and calculated different metrics. But that is not enough, because in order to use change impact analysis a detailed process with guidelines on the one hand, and appropriate tools on the other hand will be needed. To show the importance of such an approach, we have gathered problems and analyzed requirements in the field of a social insurance company. Based on these requirements we have developed a process and a tool which helps analysts in performing activities along the defined process. In this paper we present the tool for change impact analysis and the significance of the tight integration of the tool into the process.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5501508,no
Diagnosing Failures in Wireless Networks Using Fault Signatures,2010,"Detection and diagnosis of failures in wireless networks is of crucial importance. It is also a very challenging task, given the myriad of problems that plague present day wireless networks. A host of issues such as software bugs, hardware failures, and environmental factors, can cause performance degradations in wireless networks. As part of this study, we propose a new approach for diagnosing performance degradations in wireless networks, based on the concept of ``fault signatures''. Our goal is to construct signatures for known faults in wireless networks and utilize these to identify particular faults. Via preliminary experiments, we show how these signatures can be generated and how they can help us in diagnosing network faults and distinguishing them from legitimate network events. Unlike most previous approaches, our scheme allows us to identify the root cause of the fault by capturing the state of the network parameters during the occurrence of the fault.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5502483,no
Sensitivity of Two Coverage-Based Software Reliability Models to Variations in the Operational Profile,2010,"Software in field use may be utilized by users with diverse profiles. The way software is used affects the reliability perceived by its users, that is, software reliability may not be the same for different operational profiles. Two software reliability growth models based on structural testing coverage were evaluated with respect to their sensitivity to variations in operational profile. An experiment was performed on a real program (SPACE) with real defects, submitted to three distinct operational profiles. Distinction among the operational profiles was assessed by applying the Kolmogorov-Smirnov test. Testing coverage was measured according to the following criteria: all-nodes, all-arcs, all-uses, and all-potential-uses. Reliability measured for each operational profile was compared to the reliabilities estimated by the two models, estimated reliabilities were obtained using the coverage for the four criteria. Results from the experiment show that the predictive ability of the two models is not affected by variations in the operational profile of the program.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5502848,no
Software Reliability Modeling with Integrated Test Coverage,2010,"The models to predicate software reliability using test coverage (TC) have been widely studied in recent years. An increasing number of TC based software reliability models (TC-SRMs) have been developed. Meanwhile, to quantify the degree of effectiveness of software testing comprehensively, over ten kinds of TC measures have been proposed and each of them has its strength and weakness. A common problem with current TC-SRMs is that only a few classic TC are presented respectively. How to use all various TC metrics simultaneously in TC-SRMs has not been well discussed. This paper proposes a novel concept, integrated test coverage (ITC), to combine multi kinds of TC metrics as an integrated parameter for test effectiveness and reliability estimation. Then, the paper reviews the available TC-SRMs and classifies them into five categories by the modeling methods. Based on the weakness analysis of each category TC-SRMs respectively, we suggest that ITC can be applied to enhance these reported TC-SRMs.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5502851,no
"QoS management for NNEW: requirements, challenges, and solutions",2010,Presents a collection of slides covering the following topics:NNEW (NextGen network enabled weather); QoS management; and NextGen weather mission.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5503337,no
A Comprehensive Diagnosis Methodology for Complex Hybrid Systems: A Case Study on Spacecraft Power Distribution Systems,2010,"The application of model-based diagnosis schemes to real systems introduces many significant challenges, such as building accurate system models for heterogeneous systems with complex behaviors, dealing with noisy measurements and disturbances, and producing valuable results in a timely manner with limited information and computational resources. The Advanced Diagnostics and Prognostics Testbed (ADAPT), which was deployed at the NASA Ames Research Center, is a representative spacecraft electrical power distribution system that embodies a number of these challenges. ADAPT contains a large number of interconnected components, and a set of circuit breakers and relays that enable a number of distinct power distribution configurations. The system includes electrical dc and ac loads, mechanical subsystems (such as motors), and fluid systems (such as pumps). The system components are susceptible to different types of faults, i.e., unexpected changes in parameter values, discrete faults in switching elements, and sensor faults. This paper presents Hybrid Transcend, which is a comprehensive model-based diagnosis scheme to address these challenges. The scheme uses the hybrid bond graph modeling language to systematically develop computational models and algorithms for hybrid state estimation, robust fault detection, and efficient fault isolation. The computational methods are implemented as a suite of software tools that enable diagnostic analysis and testing through simulation, diagnosability studies, and deployment on the experimental testbed. Simulation and experimental results demonstrate the effectiveness of the methodology.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5504182,no
A simple and efficient way to compute depth maps for multi-view videos,2010,"This paper deals with depth maps extraction from multi-view video. Contrary to standard stereo matching-based approaches, depth maps are computed here using optical flow estimations between consecutive views. We compare our approach with the one proposed in the Depth Estimation Reference Software (DERS) for normalization purposes in the ISO-MPEG 3DV group. Experiments conducted on sequences provided to the normalization community show that the presented method provides high quality depth maps in terms of depth fidelity and virtual views synthesis. Moreover, being implemented on the GPU, it is far faster than the DERS.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5506333,no
Evaluation of depth compression and view synthesis distortions in multiview-video-plus-depth coding systems,2010,"Several quality evaluation studies have been performed for video-plus-depth coding systems. In these studies, however, the distortions in the synthesized views have been quantified in experimental setups where both the texture and depth videos are compressed. Nevertheless, there are several factors that affect the quality of the synthesized view. Incorporating more than one source of distortion in the study could be misleading; one source of distortion could mask (or be masked by) the effect of other sources of distortion. In this paper, we conduct a quality evaluation study that aims to assess the distortions introduced by the view synthesis procedure and depth map compression in multiview-video-plus-depth coding systems. We report important findings that many of the existing studies have overlooked, yet are essential to the reliability of quality evaluation. In particular, we show that the view synthesis reference software yields high distortions that mask those due to depth map compression, when the distortion is measured by average luma peak signal-to-noise ratio. In addition, we show what quality metric to use in order to reliably quantify the effect of depth map compression on view synthesis quality. Experimental results that support these findings are provided for both synthetic and real multiview-video-plus-depth sequences.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5506511,no
A software-based receiver sampling frequency calibration technique and its application in GPS signal quality monitoring,2010,"This paper has investigated the sampling frequency error impact on the signal processing in a software-correlator based GPS receiver as well as the periodic averaging technique in a pre-correlation GPS signal quality monitor. The refined signal model of receiver processing in the presence of clock error is established as the foundation of the performance analyses. A software-based method is developed to accurately calibrate both the digital IF and the sampling frequency simultaneously. The method requires no additional hardware other than the GPS receiver RF front end output samples. It enables inline calibration of the receiver measurements instead of complicated post-processing. The performance of the technique is evaluated using simulated signals as well as live GPS signals collected by several GPS data acquisition equipments, including clear time domain waveforms/eye patterns, amplitude probability density histograms, Power Spectrum Density (PSD) envelopes, and correlation-related characteristics. The results show that we can calibrate the sampling frequency with an accuracy resolution of 10<sup>âˆ?</sup> of the true sampling frequency online, and the pre-correlation SNR can be potentially improved by 39dB using periodic averaging.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5507224,no
A Quality Model in a Quality Evaluation Framework for MDWE methodologies,2010,"Nowadays, diverse development methodologies exist in the field of Model-Driven Web Engineering (MDWE), each of which covers different levels of abstraction on Model-Driven Architecture (MDA): CIM, PIM, PSM and Code. Given the high number of methodologies available, it is necessary to evaluate the quality of existing methodologies and provide helpful information to the developers. Furthermore, proposals are constantly appearing and the need may arise not only to evaluate the quality but also to find out how it can be improved. In this context, QuEF (Quality Evaluation Framework) can be employed to assess the quality of MDWE methodologies. This article presents the work being carried out and describes tasks to define a Quality Model component for QuEF. This component would be responsible for providing the basis for specifying quality requirements with the purpose of evaluating quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5507323,no
Software risk assessment and evaluation process (SRAEP) using model based approach,2010,"Software Risk Evaluation (SRE) is a process for identifying, analyzing, and developing mitigation strategies for risks in a software intensive system while it is in development. Risk assessment incorporates risk analysis and risk management, i.e. it combines systematic processes for risk identification and determination of their consequences, and how to deal with these risks? Many risk assessment methodologies exist, focusing on different types of risk or different areas of concern. Risk evaluation means to determine level of risk, prioritize the risk and categorize the risk. In this paper we have proposed a Software Risk Assessment and Evaluation Process (SRAEP) using model based approach. We have used model based approach because it requires correct description of the target system, its context and all security features. In SRAEP, we have used the software fault tree (SFT) to identify the risk. Finally we have compared the weaknesses of existing Software Risk Assessment and Estimation Model (SRAEM) with the proposed SRAEP in order to show the importance of software fault tree.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5508535,no
Algorithm-based fault tolerance for many-core architectures,2010,"Modern many-core architectures with hundreds of cores provide a high computational potential. This makes them particularly interesting for scientific high-performance computing and simulation technology. Like all nano scaled semiconductor devices, many-core processors are prone to reliability harming factors like variations and soft errors. One way to improve the reliability of such systems is software-based hardware fault tolerance. Here, the software is able to detect and correct errors introduced by the hardware. In this work, we propose a software-based approach to improve the reliability of matrix operations on many-core processors. These operations are key components in many scientific applications.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5512738,no
Microprocessor fault-tolerance via on-the-fly partial reconfiguration,2010,"This paper presents a novel approach to exploit FPGA dynamic partial reconfiguration to improve the fault tolerance of complex microprocessor-based systems, with no need to statically reserve area to host redundant components. The proposed method not only improves the survivability of the system by allowing the online replacement of defective key parts of the processor, but also provides performance graceful degradation by executing in software the tasks that were executed in hardware before a fault and the subsequent reconfiguration happened. The advantage of the proposed approach is that thanks to a hardware hypervisor, the CPU is totally unaware of the reconfiguration happening in real-time, and there's no dependency on the CPU to perform it. As proof of concept a design using this idea has been developed, using the LEON3 open-source processor, synthesized on a Virtex 4 FPGA.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5512759,no
A Practical Approach to Robust Design of a RFID Triple-Band PIFA Structure,2010,"This paper presents a practical methodology of obtaining a robust optimal solution for a multi U-slot PIFA (Planar Inverted F-Antenna) structure with triple bands of 433 MHz, 912 MHz and 2.45 GHz. Using evolutionary strategy, a global optimum is first sought out in terms of the dimensions of the slots and shorting strip. Then, starting with the optimized values, Taguchi quality method is carried out in order to obtain a robust design of the antenna against the changes of uncontrollable factors such as material property and feed position. To prove the validity of the proposed method, the performance of the antenna is predicted by general-purpose electromagnetic software and also compared to experimental results.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5512937,no
Correlation between Indoor Air Distribution and Pollutants in Natural Ventilation,2010,"The indoor air quality of lecture room in the university located in Xi'an city centre was assessed. The primary aim was to obtain correlations between the air distribution and pollutants. It was found that air distribution had important effect on the IAQ, and rational air distribution has played an important part in pollutants dispersion and attenuation. The visual air-flow field of lecture room was obtained via the computational fluid dynamics (CFD) method and Fluent software, meanwhile, some parameters of indoor pollutants was gained by field testing and the lecture room characteristics were investigated. The results showed that indoor pollutants can't be removed just depending on natural ventilation when there are vortexes in the air distribution. Furthermore, reasonable suggestions for creating healthy teaching environment and better IAQ are offered.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5514673,no
Early software reliability prediction based on support vector machines with genetic algorithms,2010,"With recent strong emphasis on rapid development of information technology, the decisions made on the basis of early software reliability estimation can have greatest impact on schedules and cost of software projects. Software reliability prediction models is very helpful for developers and testers to know the phase in which corrective action need to be performed in order to achieve target reliability estimate. In this paper, an SVM-based model for software reliability forecasting is proposed. It is also demonstrated that only recent failure data is enough for model training. Two types of model input data selection in the literature are employed to illustrate the performances of various prediction models.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5515129,no
Study from implementing algorithms for face detection and recognition for embedded systems,2010,"These Face detection and recognition research has attracted great attention in recent years. Automatic face detection has great potential in a large array of application areas, including banking and security system access control, video surveillance, and multimedia information retrieval. In this paper, we discuss the opportunity and methodology for implementation of an embedded face detection system like web-camera or digital camera. Face detection is a widely studied topic in computer vision, and advances in algorithms, low cost processing, and CMOS imagers make it practical for embedded consumer applications. As with graphics, the best cost-performance ratio is achieved with dedicated hardware. The challenges of face detection in embedded environments include bandwidth constraints set by low cost memory, or processor speed and a need to find optimal solution because applications need reliability, and low cost for final solution.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5520726,no
"Constrained-random test bench for synthesis: Technique, tools and results",2010,"This paper presents the technique and tools for automatization of the synthesis of constrained-random test-bench for verification of the synthesizable designs of microprocessors and microcontrollers. The structure and parameters of constrained-random test-bench is coded by a stochastic grammar that is specified by elaborated tools. The Application, called RandGen, generates test-bench instantiation which is inserted in the design for synthesis. Also, elaborated tools allow to estimate various constrained-random parameters. The performed test experiments have showed that the apriori estimations and aposteriori test results are in good agreement.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5520802,no
An indirect adaptive control strategy for a lactic fermentation bioprocess,2010,"This paper presents the design and the analysis of an indirect adaptive control strategy for a lactic acid production that is carried out in two cascaded continuous stirred tank bioreactors. The indirect adaptive control structure is based on the nonlinear process model and is derived by combining a linearizing control law with a new parameter estimator, which plays the role of the software sensor for on-line estimation of the bioprocess unknown kinetics. The behaviour and performance of both estimation and control algorithms are illustrated by simulations applied in the case of a lactic fermentation bioprocess for which kinetic dynamics are strongly nonlinear, time-varying and completely unknown.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5520893,no
Are Longer Test Sequences Always Better? - A Reliability Theoretical Analysis,2010,"One of the interesting questions currently discussed in software testing, both in practice and academia, is the role of test sequences on software testing, especially on fault detection. Previous work includes empirical research on rather small examples tested by relatively short test sequences. Belief is ""the longer the better"", i.e., the longer test sequences are, the more faults are detected. This paper extends those approaches applied to a large commercial application using test sequences of increasing length, which are generated and selected by graph-model-based techniques. Experiments applying many software reliability models of different categories deliver surprising results.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5521564,no
Quantitative Evaluation of Related Web-Based Vulnerabilities,2010,Current web application scanner reports contribute little to diagnosis and remediation when dealing with vulnerabilities that are related or vulnerability variants. We propose a quantitative framework that combines degree of confidence reports pre-computed from various scanners. The output is evaluated and mapped based on derived metrics to appropriate remediation for the detected vulnerabilities and vulnerability variants. The objective is to provide a trusted level of diagnosis and remediation that is appropriate. Examples based on commercial scanners and existing vulnerabilities and variants are used to demonstrate the framework's capability.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5521569,no
Studying the Impact of Social Structures on Software Quality,2010,"Correcting software defects accounts for a significant amount of resources such as time, money and personnel. To be able to focus testing efforts where needed the most, researchers have studied statistical models to predict in which parts of a software future defects are likely to occur. By studying the mathematical relations between predictor variables used in these models, researchers can form an increased understanding of the important connections between development activities and software quality. Predictor variables used in past top-performing models are largely based on file-oriented measures, such as source code and churn metrics. However, source code is the end product of numerous interlaced and collaborative activities carried out by developers. Traces of such activities can be found in the repositories used to manage development efforts. In this paper, we investigate statistical models, to study the impact of social structures between developers and end-users on software quality. These models use predictor variables based on social information mined from the issue tracking and version control repositories of a large open-source software project. The results of our case study are promising and indicate that statistical models based on social information have a similar degree of explanatory power as traditional models. Furthermore, our findings suggest that social information does not substitute, but rather augments traditional product and process-based metrics used in defect prediction models.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5521754,no
Development of a Novel Large Capacity Charger for Aircraft Battery,2010,"This paper presents a novel large capacity charger for aircraft battery, which can be used as maintainable equipment for lead-acid and cadmium-nickel batteries. The circuit scheme, control principle, software design, charger failure detection and protection circuit are also designed in detail. The main circuit of the proposed charger uses half-bridge DC-DC converter. The control circuit, which uses voltage and current double closed-loop control, takes the ADÎ¼C812 chip with A/D transformation as its core. It can control the charger in a state of constant current or trickle charge. Simultaneously, the battery charger's soft start circuit, over-current protection circuit, over-temperature protection circuit as well as the polarity detection circuit are also designed. A prototype with ADÎ¼C812 is developed, which can control the current of the charger in the state of constant current and trickle charge. Its superiority performance such as electric separation, simple frame, high efficiency, small volume, light weight is verified by experimental results.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5523084,no
Objective video quality assessment of mobile television receivers,2010,"The automated evaluation of mobile television receivers shall be facilitated by objective video quality assessment. Therefore, a test bench was set up, whose design and signal flow will be presented first. We describe and compare different full-reference video quality metrics and a simple no-reference metric from literature. The implemented metrics are evaluated and then used to assess the video quality of receivers for digital television broadcast by applying different RF scenarios. We present the achieved results with the different metrics for the purpose of receiver comparison.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5523722,no
A safety related analog input module based on diagnosis and redundancy,2010,"This paper introduces a safety-related analog input module to achieve data acquisition of 4-20mA current signals. It is an integral part of the safety-instrumented systems (SIS) which is used to provide critical control and safety applications for automation users. In order to ensure the performance of analog input circuit in good condition, a combination of hardware and software diagnosis should be carried out periodically. These kinds of internal self-diagnosis allow the device to detect improper operation within itself. If potentially dangerous process occurs, the AI has redundancy to maintain operation even when parts fail. The article presents special hardware, diagnostic software and full fault injection testing of the complete design. The test result shows that the safety-related AI is capable of detecting and locating of mostly potential faults and internal component failures.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5524110,no
A study of medical image tampering detection,2010,"Currently, methods of image tampering detection are divided into two categories, active detection and passive detection. In this paper, we try to review several detecting methods and hope this will offer some help to this field. We will focus on the passive detection method for medical images and show some results of our experiments in which we extract statistical features (IQM and HOWS based) of source images and their doctored version respectively. Manipulations we take to doctor the images include: brightness adjustment, rotation, scale, filtering, compression and so on, using fix manipulation parameter and random selected parameter. Different classifiers are chosen then to discriminate the source images from the doctored ones. We compare the performance of the classifiers to show that the passive detection methods are effective while dealing with medical image tapering detecting.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5528509,no
Overview of power system operational reliability,2010,"The traditional reliability evaluation assesses the long-term performance of power system but its constant failure rate cannot reflect the time-varying performances in an operational time frame. This paper studies the operational reliability of power system in the real-time operating conditions based on online operation information obtained from the Energy Management System (EMS). A framework of operational reliability evaluation is proposed systematically. The effects of components' inherence conditions, environment conditions and operating electrical conditions on failure rates are considered in their operational reliability models. To meet the restrictive requirement of real-time evaluation, a special algorithm is presented. The indices for operational reliability are defined as well. The software, Operational Reliability Evaluation Tools (ORET), is developed to implement the above described functions. The work reported in this paper can be used to assess the system risk in an operational timeframe and give warnings when the system reliability is low.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5528993,no
Using Aurora Road Network Modeler for Active Traffic Management,2010,"Active Traffic Management (ATM) is the ability to dynamically manage recurrent and nonrecurrent congestion based on prevailing traffic conditions. Focusing on trip reliability, it maximizes the effectiveness and efficiency of freeway corridors. ATM relies on fast and trustworthy traffic simulation software that can assess a large number of control strategies for a given road network, given various scenarios, in a matter of minutes. Effective traffic density estimation is crucial for the successful deployment of feedback algorithms for congestion control. Aurora Road Network Modeler (RNM) is an open-source macrosimulation tool set for operational planning and management of freeway corridors. Aurora RNM employs Cell Transmission Model (CTM) for road networks extended to support multiple vehicle classes. It allows dynamic filtering of measurement data coming from traffic sensors for the estimation of traffic density. In this capacity, it can be used for detection of faulty sensors. The virtual sensor infrastructure of Aurora RNM serves as an interface to the real world measurement devices, as well as a simulation of such measurement devices.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5530532,no
Analytically redundant controllers for fault tolerance: Implementation with separation of concerns,2010,"Diversity or redundancy based software fault tolerance encompasses the development of application domain specific variants and error detection mechanisms. In this regard, this paper presents an analytical design strategy to develop the variants for a fault tolerant real-time control system. This work also presents a generalized error detection mechanism based on the stability performance of a designed controller using the Lyapunov Stability Criterion. The diverse redundant fault tolerance is implemented with an aspect oriented compiler to separate and thus reduce this additional complexity. A Mathematical Model of an Inverted Pendulum System has been used as a case study to demonstrate the proposed design framework.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5531539,no
A new Resampling algorithm for generic particle filters,2010,"This paper is devoted to the resampling problem of particle filters. We firstly demonstrate the performance of classical Resampling algorithm (also called as systematic resampling algorithm) using a novel metaphor, through which the existing defects of Resampling algorithm is vividly reflected simultaneously. In order to avoid these defects, the exquisite resampling (ER) algorithm is induced which involves some exquisite actions such as comparing the weights by stages and generating the new particles based on quasi-Monte Carlo method. Simulations indicate that the proposed ER algorithm can reduce the sample impoverishment effectively and improve the accuracy of estimation evidently, which confirm that ER algorithm is a competitive alternative to Resampling algorithm.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5531576,no
A Modified History Based Weighted Average Voting with Soft-Dynamic Threshold,2010,"Voting is a widely used fault-masking technique for real time systems. Several voting algorithms exist in literature. In this paper, a survey on the few existing voting algorithms is presented and a modified history based weighted average voting algorithm with soft-dynamic threshold value is proposed with two different weight assignment techniques, which combines all the advantages of the surveyed voting algorithms but overcomes their deficiencies. The proposed algorithm with both type of weight assignment techniques, gives better performance compared to the existing history based weighted average voting algorithms in the presence of intermittent errors. In the presence of permanent errors, when all the modules are fault prone, the proposed algorithm with first type of weight assignment technique gives higher availability than all the surveyed voting algorithms. If at least one module is fault free, this algorithm gives almost 100 % safety and also higher range of availability than the other surveyed voting algorithms.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5532841,no
A Dynamic Adjustment Mechanism with Heuristic for Thread Pool in Middleware,2010,"Thread pooling is an important technique of performance optimization in middleware. With the consideration of the features of Internet applications, the configuration of thread pool in middleware needs to be adjusted dynamically on the basis of perceiving the run-time context. However, how to find out effective influencing factors which make the adjustment to have better adaptability remains to be discussed further. The paper firstly presents a thread pool model in context of Web application server based on M/M/1/K/âˆ?FCFS queuing system. A dynamic mechanism which imports certain of heuristic factors for reflecting the context at run-time is studied to adjust the size of thread pool so as to adapt to the changes of resources well. The prototypical experiments verify the effective influence of heuristic factors that exert on adjustment of thread pool size and show that the presented mechanism can be helpful for improving the performance of system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5533045,no
Business excellence in non-profit company,2010,"The success in achieving positive business results and employees satisfaction, which means overall company satisfaction, success and recognition on market, depends a lot on the quality of integrated business processes, their well estimated optimization and implemented measures for tracking and monitoring the processes. For this reason companies need much more time to dedicate not only to carry out routine jobs and procedures but also to analyze ways of performing them within the company and to note down each step in the jobs execution procedures, i.e. to record business processes, as the first step. The second step would be to analyze recorded processes and to optimize them as much as possible to become more efficient, to dissipate less time and give more satisfaction to employees. And the third step would be to introduce and establish the system for tracking and monitoring optimized processes, and to evaluate the results. This article gives an overview of procedure for introducing the process management, integrated with/within information system in the non-profit company.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5533329,no
Streamlining collection of training samples for object detection and classification in video,2010,"This paper is concerned with object recognition and detection in computer vision. Many promising approaches in the field exploit the knowledge contained in a collection of manually annotated training samples. In the resulting paradigm, the recognition algorithm is automatically constructed by some machine learning technique. It has been shown that the quantity and quality of positive and negative training samples is critical for good performance of such approaches. However, collecting the samples requires tedious manual effort which is expensive in time and prone to error. In this paper we present design and implementation of a software system which addresses these problems. The system supports an iterative approach whereby the current state-of-the-art detection and recognition algorithms are used to streamline the collection of additional training samples. The presented experiments have been performed in the frame of a research project aiming at automatic detection and recognition of traffic signs in video.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5533507,no
Real-time detection and recognition of traffic signs,2010,"Automated recognition of traffic signs is becoming a very interesting area in computer vision with clear possibilities of its application in automotive industry. For example, it would be possible to design a system which could recognize the current speed limit on the road and notify the driver in an appropriate manner. In this paper we deal with methods for automated localization of certain traffic signs, and classification of those signs according to the official designations. We propose two different approaches of determining the current speed limit after the sign was localized. A demo software system was developed to demonstrate the presented methods. Finally, we compare results obtained from the developed software, and discuss the influence of different parameters on recognition performance and quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5533516,no
Development on preventive maintenance management system for expressway asphalt pavements,2010,"In view of the status that there was no expressway pavement preventive maintenance management system at home and abroad at present, based on the technology theory obtained by the author and the demands and process of expressway asphalt pavement preventive maintenance management, preventive maintenance management system for expressway asphalt pavement (EPMMS (V1.0)) was developed. The work or functions of expressway maintenance quality evaluating, pavement performance predicting, optimum selecting of preventive maintenance treatment, the optimal timing determining, and post-evaluating could be realized or auxiliary realized, The theoretical foundation, development environment and the whole framework were expounded in the paper, the development and realization process of the five sub-systems was introduced in detail, the system testing and application were briefly introduced. The test results showed that this system could meet the demands of software products register test code. Primary application showed the system was correct and efficient, software support would be provided for expressway pavement preventive maintenance management, the pavement preventive maintenance management would be more standardized and convenient, and also the demands of the expressway maintenance management department could be greatly met by this system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5536248,no
Design and implementation of a direct RF-to-digital UHF-TV multichannel transceiver,2010,"This manuscript presents the design and implementation of a direct UHF digital transceiver that provides direct sampling of the UHF-TV input signal spectrum. Our SDR-based approach is based on a pair of high-speed ADC/DAC devices along with a channelizer, a dechannelizer and a channel management unit, which is capable of inserting, deleting and/or conmuting individual RF-TV channels. Simulation results and in-lab measurements assess that the proposed system is able to receive, manipulate and retransmit a real UHF-TV spectrum at a negligible quality penalty cost.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5537685,no
Perceptual-based coding mode decision,2010,"The framework of rate-distortion optimization (RDO) has been widely adopted for video coding to achieve a good trade-off between bit-rate and distortion. However, objective distortion metrics such as mean square error traditionally used in this framework are poorly correlated with perceptual video quality. To address this issue, we incorporate the structural similarity index as a quality metric into the framework and develop a predictive Lagrange multiplier selection technique to resolve the chicken-and-egg dilemma of perceptual-based RDO. The resulting perceptual-based RDO is then applied to H.264 intra mode decision as an illustration of the application of the proposed technique. Given a perceptual quality level, 5%-10% bit rate reduction over the JM reference software of H.264 is achieved. Subjective evaluation further confirms that, at the same bit-rate, the proposed perceptual RDO preserves image details and prevents block artifact better than the traditional RDO.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5537738,no
Bit-rate reduction using psychoacoustical masking model in Frequency Domain Linear Prediction based Audio Codec,2010,"Frequency Domain Linear Prediction (FDLP) gives an approximation of the Hilbert envelopes of a signal. FDLP based Codec works with long temporal segments and keeps the information carried by the time-domain envelopes very well. The codec gives good quality of the reconstructed signal, but coding efficiency is not enough. Here Frequency masking is introduced to FDLP based codec to reduce the bit-rate. Frequency masking is a hearing phenomenon that the hearing threshold of a sound will increase if an intense sound exists simultaneously. The psychoacoustics model is used to estimate the hearing threshold and the absolute threshold of hearing (ATH) of the FDLP carrier signals, and bit allocation for frequency sub-bands FDLP carrier signal is calculated according to the threshold and ATH. 5% bit-rate reduction is obtained with the application of the frequency masking. Perceptual Evaluation of Audio Quality (PEAQ) and Multiple Stimuli with Hidden Reference and Anchor (MUSHRA) test are carried out to evaluate the performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5538328,no
Using space time coding MIMO system for software-defined radio to improve BER,2010,"The demand for mobile communication systems with global coverage, interfacing with various standards and protocols, high data rates and improved link quality for a variety of applications has dramatically increased in recent years. The Software-Defined Radio (SDR) is the recent proposal to achieve these. In SDR, new concepts and methods, which can optimally exploit the limited resources, are necessary. Multiple antenna system is one of those, which resort to transmit strategies, referred to as Space-Time Codes (STCs). It gives high quality error performance by incorporating spatial and temporal redundancy. The paper discusses the space-time block codes and highlights the trade-off between the number of transmitter and receiver antennas and the bit error rate. We simulate MIMO systems using M-ary Phase Shift Keying (PSK) and compare the results with Single Input Single Output (SISO) systems. This analysis can be helpful in choosing the desired modulation scheme depending upon the Bit error rate (BER) and Signal to noise ratio (SNR) requirements of the service.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5540618,no
Metrics selection for fault-proneness prediction of software modules,2010,"It would be valuable to use metrics to identify the fault-proneness of software modules. It is important to select the most appropriate particular metric subset for fault-proneness prediction. We proposed an approach of metrics selection, which firstly utilized the correlation analysis to eliminate the high the correlation metrics and then ranked the remaining metrics based on the gray relational analysis. Three classifiers, that were logistic regression model, NaiveBayes, and J48, were utilized to empirically investigate the usefulness of selected metrics. Our results, based on a public domain NASA data set, indicate that 1) proposed method for metrics selection is effective, and 2) using 3-4 metrics gets the balanced performance for fault-proneness prediction of software modules.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5541206,no
Design of the pronunciation dictionary for an English CAPT system,2010,"Computer Assisted Pronunciation Training (CAPT) systems can judge the overall pronunciation quality and point out the pronunciation errors by recognition of the user's utterance to improve the users' oral ability. However, the performance of the current error detection systems can't satisfy the users' expectation. This paper carefully designs the pronunciation dictionary of the speech recognition engine of the CAPT system based on observation of the spectral properties of some phonemes. First, the phonetic symbol /R/ utilized in some modern English dictionaries is replaced by two symbols, /R/ (as in word result) and /ER/ (as in word bear), due to the large spectral deviation in these two cases. Moreover, rather than using a consonant sequence to represent a consonant cluster, we use one symbol to represent the whole for some consonant clusters, according to the phonetic properties of the consonants. Finally, we divide the phoneme /l/ into clear /l/ and dark /l/ according to the distinction in the manner of articulation and the place of articulation. So is /n/. As a result, we get a new pronunciation dictionary which is more suitable for the speech recognition engine of the CAPT system. The HMMs are trained by using the TIMIT database, and evaluated on a database involving 40 Chinese undergraduates. Experimental results illustrate the effectiveness of the new pronunciation dictionary.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5541403,no
Remote automatic selection of suitable frequency intervals for improved leak detection,2010,"Constant improvement of leak detection techniques in pipe systems used for liquids transportation is a priority for companies and authorities around the world. If a pipe presents leakage problems, the liquid which is lost generates specific signals which are transmitted in the material of the pipe. Using accelerometers these signals can be recorded and analyzed with the purpose of identifying the location of the leak. An important data analysis tool which helps in the process of leak detection is the Cross Correlation Function (CCF). It is calculated between two simultaneously recorded specific signals with the purpose of time delay estimation (TDE). TDE leads to the identification of the leak position. However, a traditional implementation of the CCF may not be satisfactory. Recorded signals are affected by noise coming from pipe elbows, junctions or traffic. Further improvements are necessary. This paper presents an implementation of an algorithm which improves the quality of the calculated CCF. Being part of a more complex software application, the algorithm is based on the calculation of the coherence function (CF). The application will automatically select two frequency intervals (bands) for filtering the leak signals. After the filtering process, the calculation of the CCF shows an increased quality. The application can be accessed from distance if a remote user needs to study or save the CCF results. No filtering settings are required and the method assures good results.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5541571,no
Reusable Specification of Agent-Based Models,2010,"This paper identifies and characterizes several important impediments to reusable agent-based models. It describes a new class of programming languages that address these problems by allowing abstract specification of incomplete but accurate models and by enabling automated construction of agent-based simulations from models. The primary innovation underlying this approach is the use of property-based types to avoid modeling limitations inherent in the object-oriented paradigm. A property-based approach enables independent validation of constituent models, abstract specification of actors independent of their simulations, and certain modifications to model without revalidation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5541951,no
Qualitative performance control in supervised IT infrastructures,2010,"Performability control of IT systems still lacks theoretically well-founded approaches that fit well to enterprise system management solutions. We propose a methodology for designing compact qualitative, state-based predictive performability control that use instrumentation provided by typical system monitoring frameworks. We identify the main systemic insufficiencies of current monitoring tools that hinder designing trustworthy fine-granular controls.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5542618,no
Quantifying effectiveness of failure prediction and response in HPC systems: Methodology and example,2010,"Effective failure prediction and mitigation strategies in high-performance computing systems could provide huge gains in resilience of tightly coupled large-scale scientific codes. These gains would come from prediction-directed process migration and resource servicing, intelligent resource allocation, and checkpointing driven by failure predictors rather than at regular intervals based on nominal mean time to failure. Given probabilistic associations of outlier behavior in hardware-related metrics with eventual failure in hardware, system software, and/or applications, this paper explores approaches for quantifying the effects of prediction and mitigation strategies and demonstrates these using actual production system data. We describe context-relevant methodologies for determining the accuracy and cost-benefit of predictors.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5542629,no
Analyzing router performance using network calculus with external measurements,2010,"In this paper we present results from an extensive measurement study of various hardware and (virtualized) software routers using several queueing strategies, i.e. First-Come-First-Served and Fair Queueing. In addition to well-known metrics such as packet forwarding performance, per packet processing time, and jitter, we apply network calculus models for performance analysis. This includes the Guaranteed Rate model for Integrated Services as well as the Packet Scale Rate Guarantee model for Differentiated Services. Using a measurement approach that provides a means to estimate rate and error term of a real node, we propose an interpretation of router performance based on these parameters taking packet queueing and scheduling into account. Such estimated parameters should be used to make the analysis of real networks more accurate. We underpin the applicability of this approach by comparing analytical results of concatenated routers to real world measurements.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5542749,no
Evaluating software reliability: Integration of MCDM and data mining,2010,"Although software reliability can be evaluated by applying data mining techniques in software engineering data to identify software defects or faults, it is difficult to select the best algorithm among the numerous data mining techniques. The goal of this paper is to propose a multiple criteria decision making (MCDM) framework for data mining algorithms selection in software reliability management. Through the application of MCDM method, this paper compares experimentally the performance of several popular data mining algorithms using 13 different performance metrics over 10 public domain software defect datasets from the NASA Metrics Data Program (MDP) repository. The results of the MCDM methods agree on top-ranked classification algorithms and differ about some classifiers for software defect datasets.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5542849,no
An empirical study of the influence of software Trustworthy Attributes to Software Trustworthiness,2010,"Software Trustworthiness is a hotspot problem in software engineering, and Software Trustworthy Attribute is the base of Software Trustworthiness. Software defect is the basic reason that influences Software Trustworthiness. Therefore, in this thesis we will attempt to utilize text classification technology to classify the historical software detects according to Software Trustworthy Attributes, in order to analyze the influence of Software Trustworthy Attributes to Software Trustworthiness, get the pivotal attribute of Software Trustworthy Attributes; and analyze the influence of Software Trustworthy Attributes to Software Trustworthiness in different version of Gentoo Linux.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5542851,no
A data mining based method: Detecting software defects in source code,2010,"With the expansion of software size and complexity, how to detect defects becomes a challenging problem. This paper proposes a defect detection method which applies data mining techniques in source code to detect two types of defects in one process. The two types of defects are rule-violating defects and copy-paste related defects which may include semantic defects. During the process, this method can also extract implicit programming rules without prior knowledge of the software and detect copy-paste segments with different granularities. The method is evaluated with the Linux kernel that contains more than 4 million lines of C code. The result shows that the resulting system can quickly detect many programming rules and violations to the rules. After using the novel pruning techniques, it will greatly reduce the effort of manually checking violations so as a large number of false positives are effectively eliminated. As an illustrative example of its effectiveness, a case study shows that among the top 50 violations reported by the proposed model, 11 defects can be confirmed after examining the source code.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5542852,no
Time series analysis for bug number prediction,2010,"Monitoring and predicting the increasing or decreasing trend of bug number in a software system is of great importance to both software project managers and software end-users. For software managers, accurate prediction of bug number of a software system will assist them in making timely decisions, such as effort investment and resource allocation. For software end-users, knowing possible bug number of their systems will enable them to take timely actions in coping with loss caused by possible system failures. To accomplish this goal, in this paper, we model the bug number data per month as time series and, use time series analysis algorithms as ARIMA and X12 enhanced ARIMA to predict bug number, in comparison with polynomial regression as the baseline. X12 is the widely used seasonal adjustment algorithm proposed by U.S. Census. The case study based on Debian bug data from March 1996 to August 2009 shows that X12 enhanced ARIMA can achieve the best performance in bug number prediction. Moreover, both ARIMA and X12 enhanced ARIMA outperform the baseline as polynomial regression.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5542853,no
GUI test-case generation with macro-event contracts,2010,"To perform a comprehensive GUI testing, a large number of test cases are needed. This paper proposes a GUI test-case generation approach that is suitable for system testing. The key idea is to extend high-level GUI scenarios with contracts and use the contracts to infer the ordering dependencies of the scenarios. From the ordering dependencies, a state machine of the system is constructed and used to generate test cases automatically. A case study is conducted to investigate the quality of the test cases generated by the proposed approach. The results showed that, in comparison to creating test cases manually, the proposed approach can detect more faults with less human effort.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5542936,no
A computer-vision-assisted system for Videodescription scripting,2010,"We present an application of video indexing/summarization to produce Videodescription (VD) for the blinds. Audio and computer vision technologies can automatically detect and recognize many elements that are pertinent to VD which can speed-up the VD production process. We have developed and integrated many of them into a first computer-assisted VD production software. The paper presents the main outcomes of this R&D activity started 5 years ago in our laboratory. Up to now, usability performance on various video and TV series types have shown a reduction of up to 50% in the VD time production process.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5543575,no
Numerical simulation of metal interconnects of power semiconductor devices,2010,"This paper presents a methodology and a software tool - R3D - for extraction, simulations, analysis, and optimization of metal interconnects of power semiconductor devices. This tool allows an automated calculation of large area device Rdson value, to analyze current density and potential distributions, to design sense device, and to optimize a layout to achieve a balanced and optimal design. R3D helps to reduce the probability of a layout error, and drastically speeds up and improves the quality of layout design.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5544002,no
A quality of service framework for adaptive and dependable large scale system-of-systems,2010,"There is growing recognition within industry that for system growth to be sustainable, the way in which existing assets are used must be improved. Future systems are being developed with a desire for dynamic behaviour and a requirement for dependability at mission critical and safety critical levels. These levels of criticality require predictable performance and as such have traditionally not been associated with adaptive systems. The software architecture proposed for such systems is based around a publish/subscribe model, an approach that, while adaptive, does not typically support critical levels of performance. There is, however, the scope for dependability within such architectures through the use of Quality of Service (QoS) methods. QoS is used in systems where the distribution of resources cannot be decided at design time. A QoS based framework is proposed for providing adaptive and dependable behaviour for future large-scale system-of-systems. Initial simulation results are presented to demonstrate the benefits of QoS.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5544115,no
A study of the internal and external effects of concurrency bugs,2010,"Concurrent programming is increasingly important for achieving performance gains in the multi-core era, but it is also a difficult and error-prone task. Concurrency bugs are particularly difficult to avoid and diagnose, and therefore in order to improve methods for handling such bugs, we need a better understanding of their characteristics. In this paper we present a study of concurrency bugs in MySQL, a widely used database server. While previous studies of real-world concurrency bugs exist, they have centered their attention on the causes of these bugs. In this paper we provide a complementary focus on their effects, which is important for understanding how to detect or tolerate such bugs at run-time. Our study uncovered several interesting facts, such as the existence of a significant number of latent concurrency bugs, which silently corrupt data structures and are exposed to the user potentially much later. We also highlight several implications of our findings for the design of reliable concurrent systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5544315,no
1<sup>st</sup> workshop on fault-tolerance for HPC at extreme scale FTXS 2010,2010,"With the emergence of many-core processors, accelerators, and alternative/heterogeneous architectures, the HPC community faces a new challenge: a scaling in number of processing elements that supersedes the historical trend of scaling in processor frequencies. The attendant increase in system complexity has first-order implications for fault tolerance. Mounting evidence invalidates traditional assumptions of HPC fault tolerance: faults are increasingly multiple-point instead of single-point and interdependent instead of independent; silent failures and silent data corruption are no longer rare enough to discount; stabilization time consumes a larger fraction of useful system lifetime, with failure rates projected to exceed one per hour on the largest systems; and application interrupt rates are apparently diverging from system failure rates.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5544426,no
Decoding STAR code for tolerating simultaneous disk failure and silent errors,2010,"As storage systems grow in size and complexity, various hardware and software component failures inevitably occur, resulting in disk malfunction in failures, as well as silent errors. Existing techniques and schemes overcome the failures and silent errors in a separate fashion. In this paper, we advocate using the STAR code as a unified and systematic mechanism to simultaneously tolerate failures on one disk and silent errors on another. By exploring the unique geometric structure of the STAR code, we propose a novel efficient decoding algorithm - EEL. Both theoretical and experimental performance evaluations show that EEL constantly outperforms a naive Try-and-Test approach by large factors in overall decoding throughput.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5544948,no
Assignments acceptance strategy in a Modified PSO Algorithm to elevate local optima in solving class scheduling problems,2010,Local optima in optimization problems describes a state where no small modification of the current best solution will produce a solution that is better. This situation will make the optimization algorithm unable to find a way to global optimum and finally the quality of the generated solution is not as expected. This paper proposes an assignment acceptance strategy in a Modified PSO Algorithm to elevate local optima in solving class scheduling problems. The assignments which reduce the value of objective function will be totally accepted and the assignment which increases or maintains the value of objective function will be accepted based on acceptance probability. Five combinations of acceptance probabilities for both types of assignments were tested in order to see their effect in helping particles moving out from local optima and also their effect towards the final penalty of the solution. The performance of the proposed technique was measured based on percentage penalty reduction (%PR). Five sets of data from International Timetabling Competition were used in the experiment. The experimental results shows that the acceptance probability of 1 for neutral assignment and 0 for negative assignments managed to produce the highest percentage of penalty reduction. This combination of acceptance probability was able to elevate the particle stuck at the local optima which is one of the unwanted situations in solving optimization problems.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5545252,no
A novel approach for Online signature verification using fisher based probabilistic neural network,2010,"The rapid advancements in communication, networking and mobility have entailed an urgency to further develop basic biometric capabilities to face security challenges. Online signature authentication is increasingly gaining interest thanks to the advent of high quality signature devices. In this paper, we propose a new approach for automatic authentication using dynamic signature. The key features consist in using a powerful combination of linear discriminant analysis (LDA) and probabibilistic neural network (PNN) model together with an appropriate decision making process. LDA is used to reduce the dimensionality of the feature space while maintining discrimination between users. Based on its results, a PNN model is constructed and used for matching purposes. Then a decision making process relying on an appropriate decision rule is performed to accept or reject a claimed identity. Data sets from SVC 2004 have been used to assess the performance of the proposed system. The results show that the proposed method competes with and even outperforms existing methods.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5546760,no
Joint throughput and packet loss probability analysis of IEEE 802.11 networks,2010,"Wireless networks have grown their popularity over the past number of years. Usually wireless networks operate according to IEEE 802.11, which specifies protocols of physical and MAC layers. A number of different studies have been conducted on performance of IEEE 802.11 wireless networks. However, questions of QoS control in such networks have not received sufficient attention from the research community. This paper considers modeling of QoS of IEEE 802.11 networks defined in terms of throughput requirements and packet loss probability limitations. An influence of sizes of packets being transmitted through the network on the QoS is investigated. Extensive simulations confirm results obtained from the mathematical model.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5546809,no
Resilient workflows for high-performance simulation platforms,2010,"Workflows systems are considered here to support large-scale multiphysics simulations. Because the use of large distributed and parallel multi-core infrastructures is prone to software and hardware failures, the paper addresses the need for error recovery procedures. A new mechanism based on asymmetric checkpointing is presented. A rule-based implementation for a distributed workflow platform is detailed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5547153,no
Distributed diagnosis in uncertain environments using Dynamic Bayesian Networks,2010,"Model-based diagnosis for industrial applications have to be efficient, and deal with modeling approximations and measurement noise. This paper presents a distributed diagnosis scheme, based on Dynamic Bayesian Networks (DBNs) that generates globally correct diagnosis results through local analysis, by only communicating a minimal number of measurements among diagnosers. We demonstrate experimentally that our distributed diagnosis scheme is computationally more efficient than its centralized counterpart, and it does not compromise the accuracy of the diagnosis results.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5547832,no
Locality considerations in exploring custom instruction selection algorithms,2010,"This paper explores custom instruction identifying methodologies for critical code segments of embedded applications, considering the locality in selection algorithms. We propose two policies for selection of custom instructions. In the first approach, called local selection, the structurally equal custom instructions are classified into the same group, called template, and the selection of custom instructions is done locally in each template. On the other hand, in the second approach no classification is made for the selection and the exploration is made globally on all the enumerated matches. We describe a design flow to establish the desired performance. We study the effects of locality on the overall speed and area of the system. Our experiments show that, locally selected custom instructions give better results in terms of both performance and performance per area.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5548232,no
Quantification of myocardial perfusion in 3D SPECT images- stress/rest volume differences,2010,"Our attempt consists of modelling the heart left ventricle at stress and rest situation, using the myocardial scintigraphic data and focuses on how to demonstrate differences in obtained 3D stress/rest images. 70 cardiac patients had completed myocardium tests by Tc-99m tetrofosmin and a GE-Starcam - 4000 SPECT gamma - camera. SPECT (Single Photon Emission Computed Tomography) slices were created and used. The myocardial perfusion was estimated by comparing those slices and the suspicion of an ischemia was indicated. 3D myocardium images were reconstructed by GE Volumetrix software in the GE Xeleris processing system by FBP reconstruction method, Hanning frequency 0.8 filter and a ramp filter and transferred in a Dicom format. The Dicom file, for each patient and each phase is imported to MATLAB 7.8 (R2009a). A series of isocontour surfaces were studied, in order to identify the appropriate threshold value, which isolates the myocardium surface from the rest area of the image. Based on the previously calculated threshold value, the myocardium volume was evaluated and be reconstructed in a 3D image. The possible difference relating to the rest and stress data of the 3D images, in voxels, was calculated, using MATLAB image processing analysis; the quantification and analysis of differences was followed. We tried to determine an index of quantification and define the global quantitative defect size as a fraction of the myocardial volume area in 3D images that will give confidence in cardiac perfusion efficiency recognition by SPECT.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5548486,no
A component based approach for modeling expert knowledge in engine testing automation systems,2010,"Test automation systems used for developing combustion engines comprise hardware components and software functionality they depend on. Such systems usually perform similar tasks; they comprise similar hardware and execute similar software. Regarding their details, however, literally no two systems are exactly the same. In order to support such variations, the automation system has to be customized accordingly. Without a tools that properly supports both, customization as well as standardization of functionality, customization can be time consuming and error-prone. In this paper we describe a modeling driven approach that is based on components with hard- and software view that allows defining standard functionality for physical hardware. We show how this way most of the automation system's standard functionality can be generated automatically, while still allowing to add custom functionality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5549594,no
A hierarchical fault tolerant architecture for component-based service robots,2010,"Due to the benefits of reusability and productivity, component-based approach has become the primary technology in service robot system development. However, because component developer cannot foresee the integration and operating condition of the components, they cannot provide appropriate fault tolerance function, which is crucial for commercial success of service robots. The recently proposed robot software frames such as MSRDS (Microsoft Robotics Developer Studio), RTC (Robot Technology Component), and OPRoS (Open Platform for Robotic Services) are very limited in fault tolerance support. In this paper, we present a hierarchically-structured fault tolerant architecture for component-based robot systems. The framework integrates widely-used, representative fault tolerance measures for fault detection, isolation, and recovery. The system integrators can construct fault tolerance applications from non-fault-aware components, by declaring fault handling rules in configuration descriptors or/and adding simple helper components, considering the constraints of components and the operating environment. To demonstrate the feasibility and benefits, a fault tolerant framework engine and test robot systems are implemented for OPRoS. The experiment results with various simulated fault scenarios validate the feasibility, effectiveness and real-time performance of the proposed approach.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5549693,no
Protection Principles for Future Microgrids,2010,"Realization of future low-voltage (LV) microgrids requires that all technical issues, such as power and energy balance, power quality and protection, are solved. One of the most crucial one is the protection of LV microgrid during normal and island operation. In this paper, protection issues of LV microgrids are presented and extensions to the novel LV-microgrid-protection concept has been developed based on simulations with PSCAD simulation software. Essential in the future-protection concept for LV microgrids will be the utilization of high speed, standard, e.g., IEC-61850-based communication to achieve fast, selective, and reliable operation protection.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5549919,no
A Hybrid Approach for Detection and Correction of Transient Faults in SoCs,2010,"Critical applications based on Systems-on-Chip (SoCs) require suitable techniques that are able to ensure a sufficient level of reliability. Several techniques have been proposed to improve fault detection and correction capabilities of faults affecting SoCs. This paper proposes a hybrid approach able to detect and correct the effects of transient faults in SoC data memories and caches. The proposed solution combines some software modifications, which are easy to automate, with the introduction of a hardware module, which is independent of the specific application. The method is particularly suitable to fit in a typical SoC design flow and is shown to achieve a better trade-off between the achieved results and the required costs than corresponding purely hardware or software techniques. In fact, the proposed approach offers the same fault-detection and -correction capabilities as a purely software-based approach, while it introduces nearly the same low memory and performance overhead of a purely hardware-based one.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5551155,no
A hierarchical mixture model voting system,2010,"It is important to improve voting system in current software fault tolerance research. In this paper, we propose a hierarchical mixture model voting system (HMMVS). This is an application of the hierarchical mixtures of experts (HME) architecture. In HMMVS, individual voting models are used as experts. During the training of HMMVS, an Expectation-Maximizing (EM) algorithm is employed to estimate the parameters for HME architecture. Experiments illustrate that our approach performs quite well after training, and better than single classical voting system. We show that the method can automatically select the most appropriate lower-level model for the data and performances are well in voting procedure.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5552313,no
Combined heading technology in vehicle location and navigation system,2010,"As an important parameter in vehicle location and navigation system, heading measurement requires a higher precision. The paper aims at the research on fusion of heading information obtained through magnetic compass, angular rate gyro and GPS. Firstly, a integrated filter model has been designed to adapt the error characteristics of gyroscope, magnetic compass and GPS. Then, considering loss of GPS, a Preposed Detection Link (PDL) is adopted to inhibit the low-frequency magnetic interference of magnetic compass and to improve the accuracy of information integration. Simulation with gyroscope, magnetic compass and GPS shows that not only the high-frequency magnetic interference and the accumulation of errors can be inhibited, but the low-frequency interference for long time can also be inhibited by the integrated filter, and the PDL-based compensation filter can provide higher accurate heading value for vehicles as an aided plan when GPS signal quality is not good.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5552321,no
Comparative analysis of input parameters using wavelet transform for voltage sag disturbance classification,2010,"Voltage sag is one of the major power quality disturbance today. Significant causes of voltage sag include power system fault, starting of induction motor and energizing of transformer. This paper focus on a comparative analysis carried out to determine the best input parameters to be employed in support vector machine (SVM) method for voltage sag cause classification. Wavelet transform is carried out to extract the feature of these voltage sags which are used as the input to the SVM. Two different types of inputs used, namely the energy level and the min-max values of the wavelet. Comparisons between these input parameters are made to determine the best method which give the best prediction values. Training and testing data based on the standard IEEE 30 bus distribution system are simulated using PSCAD software. The results show that the performance of the min-max wavelet values as the input to the SVM is superior than the energy level input in term of accuracy and computational time.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5552350,no
Queueing models based performance evaluation approach for Video On Demand back office system,2010,"Since queueing is a common behavior in computer systems, analysis of queueing models become a mature theory that is widely applied in performance evaluation. In our work, a queueing model based approach is used to manage the performance attributes of a Video On Demand (VOD) back office system, which has been deployed and put into service on a province-scale cable TV network. Extracted from Software Performance Engineering (SPE) methodology, the approach is more compact and efficient due to its two phases evaluation processes, which are design evaluation used to estimate performance measures for the analysis of design improvement and deployment evaluation used to to determine the hardware requirements for the establishment of high QoS. As a significant research and practice, the approach can be used in any development processes of complex software systems, which have same features of collaboration, distribution and third-party integration as our VOD back office system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5552438,no
Analyzing the Relationships between some Parameters of Web Services Reputation,2010,"In this paper, we provide an analysis of the impacts of some reputation parameters that an agent-based Web service holds while being active in the environment. To this end, we deploy a reputation model that ranks the Web services with respect to their popularity in the network of users. We model and analyze the arrival of requests and study their impacts on the overall reputation. The Web services may be encouraged to handle the peak loads by gathering to a group. Besides theoretical discussions, we also provide significant results, which elaborate more on the details of the system parameters. We extend the details of these results to empirical results and link the observations of the implemented environment to the results that we theoretically obtain.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5552770,no
Software project schedule variance prediction using Bayesian Network,2010,"The major objective of software engineer is to guarantee to deliver high-quality software on time and within budget. But as the development of software technology and the rapid extension of application areas, the size and complexity of software is increasing so quickly that the cost and schedule is often out of control. However, few groups or researchers have proposed an effective method to help project manager make reasonable project plan, resource allocation and improvement actions. This paper proposes Bayesian Network to solve the problem of predicting and controlling of software schedule in order to achieve proactive management. Firstly, we choose factors influencing software schedule and determine some significant cause-effect relationship between factors. Then we analyze data using statistical analysis and deal with data using data discretization. Thirdly, we construct the Bayesian structure of software schedule and learn the condition probability table of the structure. The structure and condition probability table constitute the model for software schedule variance. The model can be used not only to help project manager predict probability of software schedule variance but also guide software developers to make reasonable improvement actions. At last, an application shows how to use the model and the result proves the validity of the model. In addition, a sensitivity analysis is developed with the model to locate the most important factor of software schedule variance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5552847,no
The implement of ROFD model: Logical modeling for financial risk data,2010,"Tata modeling has great significance in software development. To know the quality of data modeling, we need to begin with the data model which is the product of the data modeling. Our analysis shows that there are deficiencies in current financial data model. Today's business dada model only shows the assets amount the enterprise holds and the proprietorship of the stockholders, this framework had built on the traditional accounting system which is regarded as golden rule in the commerce. And it could not show the risks of enterprises' operating. With this problem in mind we have established a new data model in concept level to reconstruct the conceptual schema of business database aimed at forecasting and disclosing future information of enterprise's operating. We added a special information, namely risk information, into the present data model, thus we built up a novel data model we call it â€œrisk-oriented financial data modelâ€?(ROFD). This paper gives out the implement of the ROFD model. In this part of work we firstly presents the new ROFD model structure; secondly we identify the risk category and its relationships with others; thirdly introduce the temporal analysis method in our modeling and finally we illustrate the implement of usage of the ROFD model from accounting view. The risk-oriented financial conceptual data model has an advantage of providing the information about the risk of an enterprise to the users.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5553490,no
The design of alarm and control system for electric fire prevention based on MSP430F149,2010,"This paper introduces an electrical fire control system which is composed of stand-alone electrical fire detector and electrical fire monitoring equipment. The detector with fieldbus communication technologies uses MCU to realize the main route leakage protection for low-voltage 3-phase 4-wire system; at the same time, finish the measure, display and control of voltage, current, power, electricity, temperature and other parameters; through the monitoring equipments have been installed the management software written by VC++, the operation of the scene can be monitored and the alarm information can be detected in time. With RS-485 or Ethernet communication, the system can be associated with kinds of standard power monitoring software, circular monitoring and control more than 250 detects, and record 100 types of faults and data with more than 12 months storage time.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5553985,no
Analysis and suppression for impact fault of hybrid machine tool based on preview control,2010,"The impact fault of the hybrid machine tool were considered as the analysis object in this paper. The kinematics control principle of the hybrid machine tool was given as well as the causes of the impact based on the hybrid structure. The impact suppression method was given, which based on the preview-control algorithm. It was confirmed by experiment that the impact was reduced which was account for the application of preview-control algorithm and the performance of the control strategy was improved.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5554746,no
Design and implementation of MVB protocol analyzer,2010,"The Train Communication Network (TCN) is a high-performance local area network for information transfer of modern train control, state detection, fault diagnosis and passenger service and so on. The Multifunction Vehicle Bus (MVB) is a critical part of the TCN and the design of the MVB's protocol analysis tool plays a very important function. Supported by the friendly Windows graphics user interface, a powerful MVB's protocol analysis tool is developed in this paper and this tool fills in a vacancy of the technical application presently. Starting from the MVB protocol, the frame construction of the MVB protocol analyzer and the function design of each module of up level computer software are introduced. Finally this protocol analyzer is applied to the developing and debugging process of state detection, fault diagnosis and vehicle equipment of the TCN.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5555000,no
Realization of the ground resistance detector based on different frequency signals in DC system,2010,"Several of the conventional ground resistance measurement methods such as AC method, DC leakage method etc. and their shortages are discussed. A new kind of ground resistance detector in DC system is presented. The detector, with C8051F041 as kernel, adopts measurement method basing on different frequency signals. The proposed method can reduce impact of branch distributing capacitor on ground fault location and improve the measurement sensitivity and precision of ground resistance. The principles, hardware and software design are introduced emphatically. The experimental results and practical operations show that the new method for insulation detection and ground fault location are very valid and reliable.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5555070,no
A Service-Oriented Framework for GNU Octave-Based Performance Prediction,2010,"Cloud/Grid environments are characterized by a diverse set of technologies used for communication, execution and management. Service Providers, in this context, need to be equipped with an automated process in order to optimize service provisioning through advanced performance prediction methods. Furthermore, existing software solutions such as GNU Octave offer a wide range of possibilities for implementing these methods. However, their automated use as services in the distributed computing paradigm includes a number of challenges from a design and implementation point of view. In this paper, a loosely coupled service-oriented implementation is presented, for taking advantage of software like Octave in the process of creating and using prediction models during the service lifecycle of a SOI. In this framework, every method is applied as an Octave script in a plug-in fashion. The design and implementation of the approach is validated through a case study application which involves the transcoding of raw video to MPEG4.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5557222,no
Time and Prediction based Software Rejuvenation Policy,2010,"Operational software systems often experience an â€œagingâ€?phenomenon, characterized by progressive performance degradation and a sudden hang/crash failure. Software rejuvenation is a proactive fault-tolerance strategy aimed to prevent unexpected outages due to aging. Existing rejuvenation approaches can be largely classified into two types: Purely Time based software rejuvenation policy (PTSRP) and Purely Prediction based Software rejuvenation policy (PPSRP). In this paper, combining the merits of these two policies, a new rejuvenation policy, Time and Prediction based Software Rejuvenation Policy (TPSRP), is proposed. In this policy, time based rejuvenation policy is performed from the system's start or restart, during which prediction based policy is also employed. To evaluate the effectiveness of this new policy, system availability and Downtime cost is adopted and a stochastic reward net model is built. Numerical results show that under the same conditions, TPSRP can achieve higher availability and lower downtime cost than both the PTSRP and PPSRP.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5557318,no
Computational Intelligent Gait-Phase Detection System to Identify Pathological Gait,2010,"An intelligent gait-phase detection algorithm based on kinematic and kinetic parameters is presented in this paper. The gait parameters do not vary distinctly for each gait phase; therefore, it is complex to differentiate gait phases with respect to a threshold value. To overcome this intricacy, the concept of fuzzy logic was applied to detect gait phases with respect to fuzzy membership values. A real-time data-acquisition system was developed consisting of four force-sensitive resistors and two inertial sensors to obtain foot-pressure patterns and knee flexion/extension angle, respectively. The detected gait phases could be further analyzed to identify abnormality occurrences, and hence, is applicable to determine accurate timing for feedback. The large amount of data required for quality gait analysis necessitates the utilization of information technology to store, manage, and extract required information. Therefore, a software application was developed for real-time acquisition of sensor data, data processing, database management, and a user-friendly graphical-user interface as a tool to simplify the task of clinicians. The experiments carried out to validate the proposed system are presented along with the results analysis for normal and pathological walking patterns.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5557818,no
Wireless Smart Grid Design for Monitoring and Optimizing Electric Transmission in India,2010,"Electricity losses in India during transmission and distribution are extremely high and vary between 30 to 45%. Wireless network based architecture is proposed in this paper, for monitoring and optimizing the electric transmission and distribution system in India. The system consists of multiple smart wireless transformer sensor node, smart controlling station, smart transmission line sensor node, and smart wireless consumer sensor node. The proposed software module also incorporates different data aggregation algorithms needed for the different pathways of the electricity distribution system. This design incorporates effective solutions for multiple problems faced by India's electricity distribution system such as varying voltage levels experienced due to the varying electrical consumption, power theft, manual billing system, and transmission line fault. The proposed architecture is designed for single phase electricity distribution system, and this design can be implemented for three phase system of electricity distribution with minor modifications. The implementation of this system will save large amount of electricity, and thereby electricity will be available for more number of consumers than earlier, in a highly populated country such as India. The proposed smart grid architecture developed for Indian scenario, delivers continuous real-time monitoring of energy utilization, efficient energy consumption, minimum energy loss, power theft detection, line fault detection, and automated billing.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5558101,no
Notice of Retraction<BR>Material inner defect detection by a vibration spectrum analysis,2010,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>In this paper is described possibility of non-destructive diagnostics of solid objects by software analysis of vibration spectrum. With using platform MATLAB, we can process and evaluate information from accelerometer, which is placed on the measured object. The analog signal is digitized by special I/O device dSAPCE CP-1104 and then processed offline with FFT (Fast Fourier Transform). The power spectrum is then examined by developed evaluating procedures and individual results are displayed in bar graph. When we take a look on results, there is an evidently correlation between spectrum and examining object (inner defects).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5558523,no
A novel feature selection technique for highly imbalanced data,2010,"Two challenges often encountered in data mining are the presence of excessive features in a data set and unequal numbers of examples in the two classes in a binary classification problem. In this paper, we propose a novel approach to feature selection for imbalanced data in the context of software quality engineering. This technique consists of a repetitive process of data sampling followed by feature ranking and finally aggregating the results generated during the repetitive process. This repetitive feature selection method is compared with two other approaches: one uses a filter-based feature ranking technique alone on the original data, while the other uses the data sampling and feature ranking techniques together only once. The empirical validation is carried out on two groups of software data sets. The results demonstrate that our proposed repetitive feature selection method performs on average significantly better than the other two approaches, especially when the data set is highly imbalanced.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5558961,no
A Neural network based approach for modeling of severity of defects in function based software systems,2010,"There is lot of work done in prediction of the fault proneness of the software systems. But, it is the severity of the faults that is more important than number of faults existing in the developed system as the major faults matters most for a developer and those major faults needs immediate attention. As, Neural networks, which have been already applied in software engineering applications to build reliability growth models predict the gross change or reusability metrics. Neural networks are non-linear sophisticated modeling techniques that are able to model complex functions. Neural network techniques are used when exact nature of input and outputs is not known. A key feature is that they learn the relationship between input and output through training. In this paper, five Neural Network Based techniques are explored and comparative analysis is performed for the modeling of severity of faults present in function based software systems. The NASA's public domain defect dataset is used for the modeling. The comparison of different algorithms is made on the basis of Mean Absolute Error, Root Mean Square Error and Accuracy Values. It is concluded that out of the five neural network based techniques Resilient Backpropagation algorithm based Neural Network is the best for modeling of the software components into different level of severity of the faults. Hence, the proposed algorithm can be used to identify modules that have major faults and require immediate attention.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5559743,no
A Density Based Clustering approach for early detection of fault prone modules,2010,"Quality of a software component can be measured in terms of fault proneness of data. Quality estimations are made using fault proneness data available from previously developed similar type of projects and the training data consisting of software measurements. To predict fault-proneness of modules different techniques have been proposed which includes statistical methods, machine learning techniques, neural network techniques and clustering techniques. The aim of proposed approach is to investigate that whether metrics available in the early lifecycle (i.e. requirement metrics), metrics available in the late lifecycle (i.e. code metrics) and metrics available in the early lifecycle (i.e. requirement metrics) combined with metrics available in the late lifecycle (i.e. code metrics) can be used to identify fault prone modules by using Density Based Clustering technique. This approach has been tested with real time defect datasets of NASA software projects named as PC1. Predicting faults early in the software life cycle can be used to achieve high software quality. The results show that the fusion of requirement and code metric is the best prediction model for detecting the faults as compared with mostly used code based model.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5559753,no
A software-based self-test methodology for in-system testing of processor cache tag arrays,2010,"Software-Based Self-Test (SBST) has emerged as an effective alternative for processor manufacturing and in-system testing. For small memory arrays that lack BIST circuitry such as cache tag arrays, SBST can be a flexible and low-cost solution for March test application and thus a viable supplement to hardware approaches. In this paper, a generic SBST program development methodology is proposed for periodic in-system (on-line) testing of L1 data and instruction cache memory tag arrays (both for direct mapped and set associative organization) based on contemporary March test algorithms. The proposed SBST methodology utilizes existing special performance instructions and performance monitoring mechanisms of modern processors to overcome cache tag testability challenges. Experimental results on OpenRISC 1200 processor core demonstrate that high test quality of contemporary March test algorithms is preserved while low-cost in-system testing in terms of test duration and test code size is achieved.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5560214,no
Checkpointing virtual machines against transient errors,2010,"This paper proposes VM-Î¼Checkpoint, a lightweight software mechanism for high-frequency checkpointing and rapid recovery of virtual machines. VM-Î¼Checkpoint minimizes checkpoint overhead and speeds up recovery by saving incremental checkpoints in volatile memory and by employing copy-on-write, dirty-page prediction, and in-place recovery. In our approach, knowledge of fault/error latency is used to explicitly address checkpoint corruption, a critical problem, especially when checkpoint frequency is high. We designed and implemented VM-Î¼Checkpoint in the Xen VMM. The evaluation results demonstrate that VM-Î¼Checkpoint incurs an average of 6.3% execution-time overhead for 50ms checkpoint intervals when executing the SPEC CINT 2006 benchmark.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5560226,no
SBST for on-line detection of hard faults in multiprocessor applications under energy constraints,2010,"Software-Based Self-Test (SBST) has emerged as an effective method for on-line testing of processors integrated in non safety-critical systems. However, especially for multi-core processors, the notion of dependability encompasses not only high quality on-line tests with minimum performance overhead but also methods for preventing the generation of excessive power and heat that exacerbate silicon aging mechanisms and can cause long term reliability problems. In this paper, we initially extend the capabilities of a multiprocessor simulator in order to evaluate the overhead in the execution of the useful application load in terms of both performance and energy consumption. We utilize the derived power evaluation framework to assess the overhead of SBST implemented as a test thread in a multiprocessor environment. A range of typical processor configurations is considered. The application load consists of some representative SPEC benchmarks, and various scenarios for the execution of the test thread are studied (sporadic or continuous execution). Finally, we apply in a multiprocessor context an energy optimization methodology that was originally proposed to increase battery life for battery-powered devices. The methodology reduces significantly the energy and performance overhead without affecting the test coverage of the SBST routines.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5560233,no
Identifying effective software engineering (SE) team personality types composition using rough set approach,2010,"This paper presents an application of rough sets in identifying effective personality-type composition in software engineering (SE) teams. Identifying effective personality composition in teams is important for determining software project success. It was shown that a balance of the personality types Sensing (S), Intuitive (N), Thinking (T) and Feeling (F) assisted teams in achieving higher software quality. In addition, Extroverted (E) members also had an impact on team performance. Even though the size of empirical data was too small, the rough-set technique allows the generation of significant personality-type composition rules to assist decision makers in forming effective teams. Future works will include more empirical data in order to develop predicting model of teams' performance based on personality types.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5561479,no
A proposed reusability attribute model for aspect oriented software product line components,2010,"Reusability assessment is vital for software product line due to reusable nature of its core components. Reusability being a high level quality attribute is more relevant to the software product lines as the entire set of individual products depend on the software product line core assets. Recent research proposes the use of aspect oriented techniques for product line development to provide better separation of concerns, treatment of crosscutting concerns and variability management. There are quality models available which relate the software reusability to its attributes. These models are intended to assess the reusability of software or a software component. The assessment of aspect oriented software and a core asset differs from the traditional software or component. There is need to develop a reusability model to relate the reusability attributes of aspect oriented software product line assets. This research work is an effort towards the development of reusability attribute model for software product line development using aspect oriented techniques.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5561503,no
Establishing a defect prediction model using a combination of product metrics as predictors via Six Sigma methodology,2010,"Defect prediction is an important aspect of the Product Development Life Cycle. The rationale in knowing predicted number of functional defects earlier on in the lifecycle, rather than to just find as many defects as possible during testing phase is to determine when to stop testing and ensure all the in-phase defects have been found in-phase before a product is delivered to the intended end user. It also ensures that wider test coverage is put in place to discover the predicted defects. This research is aimed to achieve zero known post release defects of the software delivered to the end user by MIMOS Berhad. To achieve the target, the research effort focuses on establishing a test defect prediction model using Design for Six Sigma methodology in a controlled environment where all the factors contributing to the defects of the product is within MIMOS Berhad. It identifies the requirements for the prediction model and how the model can benefit them. It also outlines the possible predictors associated with defect discovery in the testing phase. Analysis of the repeatability and capability of test engineers in finding defects are demonstrated. This research also describes the process of identifying characteristics of data that need to be collected and how to obtain them. Relationship of customer needs with the technical requirements of the proposed model is then clearly analyzed and explained. Finally, the proposed test defect prediction model is demonstrated via multiple regression analysis. This is achieved by incorporating testing metrics and development-related metrics as the predictors. The achievement of the whole research effort is described at the end of this study together with challenges faced and recommendation for future research work.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5561516,no
Text-based classification incoming maintenance requests to maintenance type,2010,"Classifying maintenance request is one of the important task in the large software system, yet often in large software system are not well classified. This is due to difficult in classifying by software maintainer. The categorization of maintenance type is effect on determine the corrective, adaptive, perfective, and preventive which are important to determine various quality factors of the system. In this paper we found that the requests for maintenance support could be classified correctly into corrective and adaptive. We used two different machine learning techniques alternatively NaiÌˆve Bayesian and Decision tree to classify issues into two type. Machine learning approach used the features that could be effective in increasing the accuracy of the system. We used 10-fold cross validation to evaluate the system performance. 1700 issues from shipment monitoring system were used to asses the accuracy of the system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5561540,no
Improved Adaptation of Web Service Composition Based on Change Impact Probability,2010,"Due to the dynamic nature of environment in service computing, it becomes more important to make the Web service composition able to self-adapt to changes in its environment. Achieving this goal is a challenging task, as the performance of a composite Web service will be decreased if the adaptation happens frequently in runtime. In this paper, we improve adaptation of Web service composition by predicting the probability that a change will actual affect the running composite service, which is named change impact probability (CIP), and provide a methodology based on a comprehensive QoS model of a QoS change and an execution state model of an executing composite service for computing the CIP. To bring the proposed approach to fruition, we develop a prototype system and apply the approach to a loan rate query service for illustration.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562835,no
FTDIS: A Fault Tolerant Dynamic Instruction Scheduling,2010,"In this work, we target the robustness for controller scheduler of type Tomasulo for SEU faults model. The proposed fault-tolerant dynamic scheduling unit is named FTDIS, in which critical control data of scheduler is protected from driving to an unwanted stage using Triple Modular Redundancy and majority voting approaches. Moreover, the feedbacks in voters produce recovery capability for detected faults in the FTDIS, enabling both fault mask and recovery for system. As the results of analytical evaluations demonstrate, the implemented FTDIS unit has over 99% fault detection coverage in the condition of existing less than 4 faults in critical bits. Furthermore, based on experiments, the FTDIS has a 200% hardware overhead comparing to the primitive dynamic scheduling control unit and about 50% overhead in comparision to a full CPU core. The proposed unit also has no performance penalty during simulation. In addition, the experiments show that FTDIS consumes 98% more power than the primitive unit.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562850,no
A Grouping-Based Strategy to Improve the Effectiveness of Fault Localization Techniques,2010,"Fault localization is one of the most expensive activities of program debugging, which is why the recent years have witnessed the development of many different fault localization techniques. This paper proposes a grouping-based strategy that can be applied to various techniques in order to boost their fault localization effectiveness. The applicability of the strategy is assessed over - Tarantula and a radial basis function neural network-based technique; across three different sets of programs (the Siemens suite, grep and gzip). Results are suggestive that the grouping-based strategy is capable of significantly improving the fault localization effectiveness and is not limited to any particular fault localization technique. The proposed strategy does not require any additional information than what was already collected as input to the fault localization technique, and does not require the technique to be modified in any way.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562940,no
Mining Performance Regression Testing Repositories for Automated Performance Analysis,2010,"Performance regression testing detects performance regressions in a system under load. Such regressions refer to situations where software performance degrades compared to previous releases, although the new version behaves correctly. In current practice, performance analysts must manually analyze performance regression testing data to uncover performance regressions. This process is both time-consuming and error-prone due to the large volume of metrics collected, the absence of formal performance objectives and the subjectivity of individual performance analysts. In this paper, we present an automated approach to detect potential performance regressions in a performance regression test. Our approach compares new test results against correlations pre-computed performance metrics extracted from performance regression testing repositories. Case studies show that our approach scales well to large industrial systems, and detects performance problems that are often overlooked by performance analysts.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562942,no
A Simulation Study on Some Search Algorithms for Regression Test Case Prioritization,2010,"Test case prioritization is an approach aiming at increasing the rate of faults detection during the testing phase, by reordering test case execution. Many techniques for regression test case prioritization have been proposed. In this paper, we perform a simulation experiment to study five search algorithms for test case prioritization and compare the performance of these algorithms. The target of the study is to have an in-depth investigation and improve the generality of the comparison results. The simulation study provides two useful guidelines: (1) Two search algorithms, Additional Greedy Algorithm and 2-Optimal Greedy Algorithm, outperform the other three search algorithms in most cases. (2) The performance of the five search algorithms will be affected by the overlap of test cases with regard to test requirements.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562946,no
Software Quality Prediction Models Compared,2010,"Numerous empirical studies confirm that many software metrics aggregated in software quality prediction models are valid predictors for qualities of general interest like maintainability and correctness. Even these general quality models differ quite a bit, which raises the question: Do the differences matter? The goal of our study is to answer this question for a selection of quality models that have previously been published in empirical studies. We compare these quality models statistically by applying them to the same set of software systems, i.e., to altogether 328 versions of 11 open-source software systems. Finally, we draw conclusions from quality assessment using the different quality models, i.e., we calculate a quality trend and compare these conclusions statistically. We identify significant differences among the quality models. Hence, the selection of the quality model has influence on the quality assessment of software based on software metrics.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562947,no
An Improved Regression Test Selection Technique by Clustering Execution Profiles,2010,"In order to improve the efficiency of regression testing, many test selection techniques have been proposed to extract a small subset from a huge test suite, which can approximate the fault detection capability of the original test suite for the modified code. This paper presents a new regression test selection technique by clustering the execution profiles of modification-traversing test cases. Cluster analysis can group program executions that have similar features, so that program behaviors can be well understood and test cases can be selected in a proper way to reduce the test suite effectively. An experiment with some real programs is designed and implemented. The experiment results show that our approach can produce a smaller test suite with most fault-revealing test cases in comparison with existing selection techniques.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562956,no
Data Unpredictability in Software Defect-Fixing Effort Prediction,2010,"The prediction of software defect-fixing effort is important for strategic resource allocation and software quality management. Machine learning techniques have become very popular in addressing this problem and many related prediction models have been proposed. However, almost every model today faces a challenging issue of demonstrating satisfactory prediction accuracy and meaningful prediction results. In this paper, we investigate what makes high-precision prediction of defect-fixing effort so hard from the perspective of the characteristics of defect dataset. We develop a method using a metric to quantitatively analyze the unpredictability of a defect dataset and carry out case studies on two defect datasets. The results show that data unpredictability is a key factor for unsatisfactory prediction accuracy and our approach can explain why high-precision prediction for some defect datasets is hard to achieve inherently. We also provide some suggestions on how to collect highly predictable defect data.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562962,no
A Methodology for Continuos Quality Assessment of Software Artefacts,2010,"Although some methodologies for evaluating the quality of software artifacts exist, all of these are isolated proposals, which focus on specific artifacts and apply specific evaluation techniques. There is no generic and flexible methodology that allows quality evaluation of any kind of software artifact, regardless of type, much less a tool that supports this. To tackle that problem in this paper, we propose the CQA Environment, consisting of a methodology (CQA-Meth) and a tool that implements it (CQA-Tool). We began applying this environment in the evaluation of the quality of UML models (use cases, class and statechart diagrams). To do so, we have connected CQA-Tool to the different tools needed to assess the quality of models, which we also built ourselves. CQA-Tool, apart from implementing the methodology, provides the capacity for building a catalogue of evaluation techniques that integrates the evaluation techniques (e.g. metrics, checklists, modeling conventions, guidelines, etc.) which are available for each software artifact. CQA Environment is suitable for use by companies that offer software quality evaluation services, especially for clients who are software development organizations and who are outsourcing software construction. They will obtain an independent quality evaluation of the software products they acquire. Software development organizations that perform their own evaluation will be able to use it as well.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562967,no
Performance Prediction of WS-CDL Based Service Composition,2010,"In this paper, we propose a translation-based approach for performance prediction of composite service built on WS-CDL. To translate a composite service into a state-transition model for quantitative analysis, we first give a set of translation rules to map WS-CDL elements into general-stochastic-petri-nets (GSPN). Based on the GSPN representation, we introduce the prediction algorithm to calculate the expected-process-normal-completion-time of WS-CDL processes. We also validate the accuracy of the approach in the experimental study by showing 95% confidence intervals obtained from experimental performance results cover corresponding theoretical prediction values.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562973,no
A Fault-Tolerant Strategy for Improving the Reliability of Service Composition,2010,"Service composition is an important means for integrating the individual Web services for creating new value added systems that satisfy complex demands. Since, Web services exist in the heterogeneous environments on the Internet, study on how to guarantee the reliability of service composition in a distributed, dynamic and complex environment becomes more and more important. This paper proposes a service composition net(SCN) and fault-tolerant strategy to improve the reliability of service composition. The strategy consists of static strategy, dynamic strategy and exception handling mechanism, which can be used to dynamically adjust component service for achieving good reliability as well as good overall performance. SCN is adopted to model different components of service composition. The fault detection and fault recovery mechanisms are also considered. Based on the constructed model, theories of Petri nets help prove the consistency of processing states and the effectiveness of the strategy. A case study of Export Service illustrates the feasibility of proposed method.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562976,no
Active Monitoring for Control Systems under Anticipatory Semantics,2010,"As the increment of software complexity, traditional software analysis, verification and testing techniques can not fully guarantee the faultlessness of deployed systems. Therefore, runtime verification has been developed to continuously monitor the running system. Typically, runtime verification can detect property violations but cannot predict them, and consequently cannot prevent the failures from occurring. To remedy this weakness, active monitoring is proposed in this paper. Its purpose is not repairing the faults after failures have occurred, but predicting the possible faults in advance and triggering the necessary steering actions to prevent the software from violating the property. Anticipatory semantics of linear temporal logic is adopted in monitor construction here, and the information of system model is used for successful steering and prevention. The prediction and prevention will form a closed-loop feedback based on control theory. The approach can be regarded as an effective complement of traditional testing and verification techniques.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562977,no
Design of BDI Agent for Adaptive Performance Testing of Web Services,2010,"As services are dynamic discovered and bound in the open Internet environment, testing has to be exercised continuously and online to verify and validate the continuous changes and to ensure the quality of the integrated service-based system. During this process, testing strategies have to be adapted in accordance to the changes in the environment and target systems. Software agents are characterized by context awareness, autonomous decision making and social collaboration capabilities. The paper introduces the design of BDI (Believe-Decision-Intention) agents to facilitate adaptive performance testing of Web Services. The BDI model specifies the necessary test knowledge, test goal and action plan to carry out test and adaptive schedule. Performance testing is defined as a scheduling problem to select the workload and test cases in order to achieve the goal of performance abnormal detection. A two-level control architecture is built. At the TR (Test Runner) level, the BDI agents control the workload of concurrent requests. At the TC (Test Coordinator) level, the BDI agents control the complexity of test cases. Agents communicate and collaborate with each other to share knowledge and test plan. The paper introduces the design of the BDI model, the adaptation rules and the control architecture. Case study is exercised to illustrate the adaptive testing process based on the design of BDI agents.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562998,no
Leveraging Performance and Power Savings for Embedded Systems Using Multiple Target Deadlines,2010,"Tasks running on embedded systems are often associated with deadlines. While it is important to complete tasks before their associated deadlines, performance and energy consumption also play important roles in many usages of embedded systems. To address these issues, we explore the use of Dynamic Voltage and Frequency Scaling (DVFS), a standard feature available on many modern processors for embedded systems. Previous studies often focus on frequency assignment for energy savings and meeting definite task deadlines. In this paper, we present a heuristic algorithm based on convex optimization techniques to compute energy-efficient processor frequencies for soft real-time tasks. Our novel approach provides performance improvements by allowing definitions of multiple target deadlines for each task. We simulate two versions of our algorithm in MATLAB and evaluate their performance and efficiency. The experimental results show that our strategy leverages performance and energy savings, and can be customized to suit practical applications.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5563003,no
Information flow metrics and complexity measurement,2010,"Complexity metrics takes an important role to predict fault density and failure rate in the software deployment. Information flow represents the flow of data in collective procedures in the processes of a concrete system. The present static analysis of source code and the ability of metrics are incompetent to predict the actual amount of information flow complexity in the modules. In this paper, we propose metrics IF-C, F-(I+O), (C-L) and (P-C) focuses on the improved information flow complexity estimation method, which is used to evaluate the static measure of the source code and facilitates the limitation of the existing work. Using the IF-C method, the framework of information flow metrics can be significantly combined with external flow of the active procedure, which depends on various levels of procedure call in the code.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5563667,no
Notice of Retraction<BR>Accurate and efficient reliability Markov model analysis of predictive hybrid m-out-of-n systems,2010,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>In recent years we perceive many researches on new techniques for improving the performance of fault-tolerant control systems. Prediction of correct system output is one of these techniques which can calculate and predict the probable correct system output when the ordinary techniques are incapacitated to make decision. In this paper, a performance model of predictive m-out-of-n hybrid redundancy is introduced and analyzed. The results of equations and mathematical relations based on Markov model demonstrated that this approach can improve the system reliability in comparison with traditional m-out-of-n system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5564803,no
Research on automatic detection for defect on bearing cylindrical surface,2010,"At present, it is still by using manual method to detect the defects on micro bearing surface. The method is laborious and time consuming. Moreover, it has a low efficiency and high miss-detection rate. Due to the fact, an on-line automatic detection system is developed using linear CCD. The proposed system is composed of three subsystems: the detection environment setting subsystem, the automatic detection subsystem, and the data management subsystem. In order to lead the above subsystems to cooperate with each other, control software is developed with LabVIEW 8.5 as a platform. Experimental results indicate that the system realizes the predefined functions and caters to the requirements of stability, real-time performance, and accuracy. Thus it can be applied for actual production.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5565229,no
AM2301-based temperature and humidity tester of solar battery system,2010,"A tester of environmental parameters of solar energy based on ARM microprocessor of S3C2440A was introduced in this paper, the design of software and hardware and implementation methods were detailed expatiated. The environmental parameters which included temperature, humidity and illumination could be measured and displayed real-timely and adjusted the azimuth of cell panel. The data collected by the tester of would be transferred to the host computer through the industry bus of rs485 which would be Analyzed further and storage. A new single all-calibrated warm humidity sensor AM2301 which included a capacitive humidity sensing element and a NTC thermostat was introduced in this article. The sensor was widely used in the automatic control and intelligent detection field because of remarkable quality and fast response and higher performance-price ratio.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5568316,no
An improved method to simplify software metric models constructed with incomplete data samples,2010,"Software metric models are useful in predicting the target software metric(s) for any future software project based on the project's predictor metric(s). Obviously, the construction of such a model makes use of a data sample of such metrics from analogous past projects. However, incomplete data often appear in such data samples. Worse still, the necessity to include a particular continuous predictor metric or a particular category for a certain categorical predictor metric is most likely based on an experience-related intuition that the continuous predictor metric or the category matters to the target metric. However, in the presence of incomplete data, this intuition is traditionally not verifiable â€œretrospectivelyâ€?after the model is constructed, leading to redundant continuous predictor metric(s) and/or excessive categorization for categorical predictor metrics. As an improvement of the author's previous work to solve all these problems, this paper proposes a methodology incorporating the k-nearest neighbors (k-NN) multiple imputation method, kernel smoothing, Monte Carlo simulation, and stepwise regression. This paper documents this methodology and one experiment on it.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5569384,no
The discovery of the fault location in NIGS,2010,"A new method is discovered for calculating the fault distance of the overhead line of the Neutral Indirect Grounded System (NIGS) in power distribution networks, in which the single phase to ground fault point or distance is difficult to detect, because the zero sequence current is in lower value. It is found that the information of the fault distance is kept in the zero sequence voltage vector which may be measured at the tail terminal of the questioned line by digging the data. Then an algorithm to calculate the fault location on the overhead lines is proposed by considering that the zero sequence voltage vector at the tail terminal. The value of the zero sequence voltage is determined by the fault location, and the phase angle also contains the distance traveled by the load current to the fault point. The system analysis for parameters is conducted for the NIGS by considering line is actual and by the two terminals' parameters.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5569691,no
Testing as a Service over Cloud,2010,"Testing-as-a-service (TaaS) is a new model to provide testing capabilities to end users. Users save the cost of complicated maintenance and upgrade effort, and service providers can upgrade their services without impact on the end-users. Due to uneven volumes of concurrent requests, it is important to address the elasticity of TaaS platform in a cloud environment. Scheduling and dispatching algorithms are developed to improve the utilization of computing resources. We develop a prototype of TaaS over cloud, and evaluate the scalability of the platform by increasing the test task load, analyze the distribution of computing time on test task scheduling and test task processing over the cloud, and examine the performance of proposed algorithms by comparing others.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5569908,no
GOS: A Global Optimal Selection Approach for QoS-Aware Web Services Composition,2010,"Services composition technology provides a promising way to create a new service in services-oriented architecture (SOA). However, some challenges are hindering the application of services composition. One of the greatest challenges for composite service developer is how to select a set of services to instantiate composite service with quality of service (QoS) assurance across different autonomous region (e.g. organization or business). To solve QoS-aware Web service composition problem, this paper proposes a global optimization selection (GOS) based on prediction mechanism for QoS values of local services. The GOS includes two parts. First, local service selection algorithm can be used to predict the change of service quality information. Second, GOS aims at enhancing the run-time performance of global selection by reducing to aggregation operation of QoS. The simulation results show that the GOS has lower execution cost than existing approaches.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5569936,no
Fault detection for high availability RAID system,2010,"Designing storage systems to provide high availability in the face of failures needs the use of various data protection techniques, such as dual-controller RAID. The failure of controller may cause data inconsistencies of RAID storage system. Heartbeat is used to detect controllers whether survival. So, the heartbeat cycle's impact on the high availability of a dual-controller hot-standby system has become the key of current research. To address the problem of fixed setting heartbeat in building high availability system currently, an adaptive heartbeat fault detection model of dual controller, which can adjust heartbeat cycle based on the frequency of data read-write request, is designed to improve the high availability of dual-controller RAID storage system. Additionally, this heartbeat mechanism can be used for other applications in distributed settings such as detecting node failures, performance monitoring, and query optimization. Based on this model, the high availability stochastic Petri net model of fault detection was established and used to evaluate the effect of the availability. In addition, we define a AHA (Adaptive Heart Ability) parameter to scale the ability of system heartbeat cycle to adapt to the environment which is changing. The results show that, relatively speaking with fixed configuration, the design is valid and effective, and can enhance dual controller RAID system high availability.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5572373,no
Two-dimensional bar code mobile commerce - Implementation and performance analysis,2010,"Two-dimensional bar code is called in Japan, QR Code (Quick Response Code, quick response codes). Holding a camera phone in front of this small black box according to what, after the phone software to identify, quickly turned into a line on the site, connected to the site, you can get the information you need. Of course, not necessarily a website, small black box may also be represented by e-mail, image data or text data. The study of this system in different two-dimensional bar code Shoujizuoye. The Implementation and performance analysis, in which experiments can see, network quality of service is subject to various factors Ying Xiang, to send Therefore, in the Internet video O'clock, must consider the status of the network, to the decision to adopt the GOP pattern; on streams in the network, the packet error rate will change the packet loss probability, affect the image quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5573247,no
Using search-based metric selection and oversampling to predict fault prone modules,2010,"Predictive models can be used in the detection of fault prone modules using source code metrics as inputs for the classifier. However, there exist numerous structural measures that capture different aspects of size, coupling and complexity. Identifying a metric subset that enhances the performance for the predictive objective would not only improve the model but also provide insights into the structural properties that lead to problematic modules. Another difficulty in building predictive models comes from unbalanced datasets, which are common in empirical software engineering as a majority of the modules are not likely to be faulty. Oversampling attempts to overcome this deficiency by generating new training instances from the faulty modules. We present the results of applying search-based metric selection and oversampling to three NASA datasets. For these datasets, oversampling results in the largest improvement. Metric subset selection was able to reduce up to 52% of the metrics without decreasing the predictive performance gained with oversampling.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5575249,no
A Comparative Study of Threshold-Based Feature Selection Techniques,2010,"Given high-dimensional software measurement data, researchers and practitioners often use feature (metric) selection techniques to improve the performance of software quality classification models. This paper presents our newly proposed threshold-based feature selection techniques, comparing the performance of these techniques by building classification models using five commonly used classifiers. In order to evaluate the effectiveness of different feature selection techniques, the models are evaluated using eight different performance metrics separately since a given performance metric usually captures only one aspect of the classification performance. All experiments are conducted on three Eclipse data sets with different levels of class imbalance. The experiments demonstrate that the choice of a performance metric may significantly influence the results. In this study, we have found four distinct patterns when utilizing eight performance metrics to order 11 threshold-based feature selection techniques. Moreover, performances of the software quality models either improve or remain unchanged despite the removal of over 96% of the software metrics (attributes).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5575976,no
Gait Symmetry Analysis Based on Affinity Propagation Clustering,2010,"When we are conducting an investigation in gait symmetry analysis, we usually grouped the test objects by decade intervals. However, this artificial division method has a large defect that does not truly reflect the human relationship between age and physical condition. Therefore, we found a new grouping method while clustering on the values of the difference in gait symmetry. The reason for using the affinity propagation clustering algorithm was that it can do clustering on data while passing the original information not the random values to processing clustering. And meanwhile this algorithm has good performance and efficiency. It will help us to do gait analysis more effectively, and also more reasonable to explain the relationship of human gait and age, or gait and other characteristics.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5577958,no
Towards Estimating Physical Properties of Embedded Systems using Software Quality Metrics,2010,"The complexity of embedded devices poses new challenges to embedded software development in addition to the traditional physical requirements. Therefore, the evaluation of the quality of embedded software and its impact on these traditional properties becomes increasingly relevant. Concepts such as reuse, abstraction, cohesion, coupling, and other software attributes have been used as quality metrics in the software engineering domain. However, they have not been used in the embedded software domain. In embedded systems development, another set of tools is used to estimate physical properties such as power consumption, memory footprint, and performance. These tools usually require costly synthesis-and-simulation design cycles. In current complex embedded devices, one must rely on tools that can help design space exploration at the highest possible level, identifying a solution that represents the best design strategy in terms of software quality, while simultaneously meeting physical requirements. We present an analysis of the cross-correlation between software quality metrics, which can be extracted before the final system is synthesized, and physical metrics for embedded software. Using a neural network, we investigate the use of these cross-correlations to predict the impact that a given modification on the software solution will have on embedded software physical metrics. This estimation can be used to guide design decisions towards improving physical properties of embedded systems, while maintaining an adequate trade-off regarding software quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5578300,no
Impacts of Inaccurate Information on Resource Allocation for Multi-Core Embedded Systems,2010,"Multi-core technologies are widely used in embedded systems. Stochastic resource allocation can guarantee certain quality of the services (QoS). In heterogeneous embedded system resource allocation, execution time distributions of different tasks on cores are predicted before scheduling. The difference between actual execution time and estimated execution time may lead to allocations which are not robust. In this paper, we present an evaluation of impacts of inaccurate information on resource allocation. We propose a systematic measure of the robustness degradation and evaluate how inaccurate probability parameters affect the robustness of resource allocations. Furthermore, we compare the performance of three widely used greedy heuristics when using the inaccurate information.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5578532,no
Service Level Agreements in a Rental-based System,2010,"In this paper, we investigate how Service Level Agreeements (SLAs) can be incorporated as part of the system's scheduling and rental decisions to satisfy the different performance promises of high performance computing (HPC) applications. Such SLAs are contracts which specify a set of application-driven requirements such as the estimated total load, contract duration, total utility value and the estimated total number of generated jobs. We present several scheduling and rental based policies that make use of these SLA parameters and demonstrate the effectiveness of such policies to accurately predict and plan for resource levels in a rental-based system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5578541,no
Quality Assurance Testing for Magnetization Quality Assessment of BLDC Motors Used in Compressors,2010,"Quality assurance (QA) testing of mass-produced electrical appliances is critical for the reputation of manufacturer since defective units will have a negative impact on safety, reliability, efficiency, and performance of the end product. It has been observed at a brushless dc (BLDC) compressor motor manufacturing facility that improper magnetization of the rotor permanent magnet is one of the leading causes of motor defects. A new technique for postmanufacturing assessment of the magnetization quality of concentrated winding BLDC compressor motors for QA is proposed in this paper. The new method evaluates the data acquired during the test runs performed after motor assembly to observe anomalies in the zero-crossing pattern of the back-EMF voltages for screening motor units with defective magnetization. An experimental study on healthy and defective 250-W BLDC compressor motor units shows that the proposed technique provides sensitive detection of magnetization defects that existing tests were not capable of finding. The proposed algorithm does not require additional hardware for implementation since it can be added to the existing test-run inverter of the QA system as a software algorithm.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5580084,no
Exploring FPGA Routing Architecture Stochastically,2010,"This paper proposes a systematic strategy to efficiently explore the design space of field-programmable gate array (FPGA) routing architectures. The key idea is to use stochastic methods to quickly locate near-optimal solutions in designing FPGA routing architectures without exhaustively enumerating all design points. The main objective of this paper is not as much about the specific numerical results obtained, as it is to show the applicability and effectiveness of the proposed optimization approach. To demonstrate the utility of the proposed stochastic approach, we developed the tool for optimizing routing architecture (TORCH) software based on the versatile place and route tool. Given FPGA architecture parameters and a set of benchmark designs, TORCH simultaneously optimizes the routing channel segmentation and switch box patterns using the performance metric of average interconnect power-delay product estimated from placed and routed benchmark designs. Special techniques - such as incremental routing, infrequent placement, multi-modal move selection, and parallelized metric evaluation - are developed to reduce the overall run time and improve the quality of results. Our experimental results have shown that the stochastic design strategy is quite effective in co-optimizing both routing channel segmentation and switch patterns. With the optimized routing architecture, relative to the performance of our chosen architecture baseline, TORCH can achieve average improvements of 24% and 15% in delay and power consumption for the 20 largest Microelectronics Center of North Carolina benchmark designs, and 27% and 21% for the eight benchmark designs synthesized with the Altera Quartus II University Interface Program tool. Additionally, we found that the average segment length in an FPGA routing channel should decrease with technology scaling. Finally, we demonstrate the versatility of TORCH by illustrating how TORCH can be used to optimize other aspects of the routing arch- - itecture in an FPGA.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5580223,no
Using Content and Text Classification Methods to Characterize Team Performance,2010,"Because of the critical role that communication plays in a team's ability to coordinate action, the measurement and analysis of online transcripts in order to predict team performance is becoming increasingly important in domains such as global software development. Current approaches rely on human experts to classify and compare groups according to some prescribed categories, resulting in a laborious and error-prone process. To address some of these issues, the authors compared and evaluated two methods for analyzing content generated by student groups engaged in a software development project. A content analysis and semi-automated text classification methods were applied to the communication data from a global software student project involving students from the US, Panama, and Turkey. Both methods were evaluated in terms of the ability to predict team performance. Application of the communication analysis' methods revealed that high performing teams develop consistent patterns of communicating which can be contrasted to lower performing teams.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5581509,no
Effect of Replica Placement on the Reliability of Large-Scale Data Storage Systems,2010,"Replication is a widely used method to protect large-scale data storage systems from data loss when storage nodes fail. It is well known that the placement of replicas of the different data blocks across the nodes affects the time to rebuild. Several systems described in the literature are designed based on the premise that minimizing the rebuild times maximizes the system reliability. Our results however indicate that the reliability is essentially unaffected by the replica placement scheme. We show that, for a replication factor of two, all possible placement schemes have mean times to data loss (MTTDLs) within a factor of two for practical values of the failure rate, storage capacity, and rebuild bandwidth of a storage node. The theoretical results are confirmed by means of event-driven simulation. For higher replication factors, an analytical derivation of MTTDL becomes intractable for a general placement scheme. We therefore use one of the alternate measures of reliability that have been proposed in the literature, namely, the probability of data loss during rebuild in the critical mode of the system. Whereas for a replication factor of two this measure can be directly translated into MTTDL, it is only speculative of the MTTDL behavior for higher replication factors. This measure of reliability is shown to lie within a factor of two for all possible placement schemes and any replication factor. We also show that for any replication factor, the clustered placement scheme has the lowest probability of data loss during rebuild in critical mode among all possible placement schemes, whereas the declustered placement scheme has the highest probability. Simulation results reveal however that these properties do not hold for the corresponding MTTDLs for a replication factor greater than two. This indicates that some alternate measures of reliability may not be appropriate for comparing the MTTDL of different placement schemes.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5581603,no
A vector-based approach to digital image scaling,2010,"Image and video capture devices are often designed with different capabilities and end-users in mind. As a result, very often images are captured at one resolution and need to be displayed at another resolution. Resizing thus continues to be an important research area in today's milieu. Well-known pixel-based methods (like bicubic interpolation) have traditionally been used for the purpose. However, these methods are often found lacking in quality from a perceptual standpoint as they tend to soften edges and blur details. Recently, vectorization based approaches have been explored because of their inherent property of artifact free scaling. However, these methods fail to faithfully reproduce textures and fine details in an image. In this work, we present a novel, layered approach of combining image vectorization with a pixel based interpolation technique to achieve high quality scaling of digital images. The proposed method decomposes an image into two layers: a `coarse' layer capturing the visually important structure of the image, and a `fine' layer containing the details. We vectorize only the coarse layer and render at the desired scale, while using classical interpolation on the fine layer. The final scaled image is composed by blending the independently scaled layers. We compare the performance of the proposed method with several state-of-the-art vectorization and scaling methods, and report comparably better performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5582927,no
Intra coding and refresh based on video epitomic analysis,2010,"In video coding, intra coding plays an important role in both coding efficiency and error resilience. In this paper, a new intra coding method based on video epitomic analysis is proposed. Image epitome suitable for coding is extracted through video epitomic analysis, and is used to generate the prediction for each block. The mapping between the original image and image epitome as well as image epitome should be coded and transmitted to the decoder side. Due to the independence of adjacent reconstructed blocks, the proposed intra coding method can also be applied to improve error resilience of intra refresh. Experimental results show that, the proposed intra coding method can significantly improve the performance of intra coding by 0.7dB in terms of PSNR. The simulation results under packet loss environment show that the proposed intra refresh can out-perform random intra refresh (RIR) by up to 1dB, and better subjective quality can also be obtained.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5583074,no
Neural-network-based water quality monitoring for wastewater treatment processes,2010,"Wastewater treatment processes are typical complex dynamic processes. They are characterized by severe random disturbance, strong nonlinearity, time-variant properties and uncertainty. Sensors which have higher reliability and adaptability are needed in such systems. Sensors currently available for wastewater treatment processes are limited both in sorts and reliability, and most of the sensors are expensive. Software sensors can give estimation to unmeasured state variables according to the measured information provided by online measuring instruments available in the system. Soft measurement technology based on artificial neural network has an obvious advantage in solving high nonlinearity and uncertainty. BP neural network was used to construct a soft measurement approach to monitor the effluent COD and BOD in wastewater treatment processes in this paper. Simulation results show that the soft measurement approach based on neural network can estimate the state variables accurately and can be used in real-time monitoring of biochemical wastewater treatment processes.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5584378,no
Predicting mechanical properties of hot-rolling steel by using RBF network method based on complex network theory,2010,"Recently, producing high-precision and high-quality steel products becomes the major aim of the large-scale iron and steel enterprises. Because of the internal multiplex components of products and complex changes in the production process, it is too difficult to achieve precise control in hot rolling production process. In this paper, radial basis function neural network is used to complete performance prediction. It has the advantage of fast training and high accuracy, and overcomes shortcomings of BP neural network used previously, such as local minimum. When determining the center of radial basis function we make use of complex network visualization which can clearly figure out the relationship between input vectors and receive the center and width according to the relationship of the nodes. Experiments show that the method that is combining community discovery algorithm and RBF enjoy high stability, small training time which means to be suitable to analysis large-scale data. More importantly, it can reach high accuracy.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5584387,no
A Quality Framework to check the applicability of engineering and statistical assumptions for automated gauges,2010,"In high-volume part manufacturing, interactions between program data and program flow can depart significantly from the initial statistical assumptions used during software development. This is a particular challenge for industrial gauging systems used in automotive part production where the applicability of statistical models affects system correctness. This paper uses a Quality Framework to track high-level engineering and statistical assumptions during development. Statistical Process Control (SPC) metrics define an â€œin-controlâ€?region where the statistical assumptions apply, and an outlier region where they do not apply. The gauge is monitored on-line to verify that production corresponds to the area of the operation where the gauge algorithms are known to work. If outliers are detected in the on-line manufacturing process, then parts can be quarantined, improved gauging algorithms selected, and/or process improvement activities can be initiated.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5584605,no
Fuzzy clustering and relevance ranking of web search results with differentiating cluster label generation,2010,"This paper introduces a prototype web search results clustering engine that enhances search results by performing fuzzy clustering on web documents returned by conventional search engines, as well as ranking the results and labeling the resulting clusters. This is done using a fuzzy transduction-based clustering algorithm (FTCA), which employs a transduction-based relevance model (TRM) to generate document relevance values. These relevance values are used to cluster similar documents, rank them, and facilitate a term frequency based label generator. The membership degrees of documents to fuzzy clusters also facilitates effective detection and removal of overly similar clusters. FTCA is compared against two other established web document clustering algorithms: Suffix Tree Clustering (STC) and Lingo, which are provided by the free open source Carrot<sup>2</sup> Document Clustering Workbench. To measure cluster quality, an extended version of the classic precision measurement is used to take into account relevance and fuzzy clustering, along with recall and F1 score. Results from testing on five different datasets show a considerable clustering quality and performance advantage over STC and Lingo in most cases.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5584771,no
Cooperative Co-evolution for large scale optimization through more frequent random grouping,2010,"In this paper we propose three techniques to improve the performance of one of the major algorithms for large scale continuous global function optimization. Multilevel Cooperative Co-evolution (MLCC) is based on a Cooperative Co-evolutionary framework and employs a technique called random grouping in order to group interacting variables in one subcomponent. It also uses another technique called adaptive weighting for co-adaptation of subcomponents. We prove that the probability of grouping interacting variables in one subcomponent using random grouping drops significantly as the number of interacting variables increases. This calls for more frequent random grouping of variables. We show how to increase the frequency of random grouping without increasing the number of fitness evaluations. We also show that adaptive weighting is ineffective and in most cases fails to improve the quality of found solution, and hence wastes considerable amount of CPU time by extra evaluations of objective function. Finally we propose a new technique for self-adaptation of the subcomponent sizes in CC. We demonstrate how a substantial improvement can be gained by applying these three techniques.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5586127,no
The jMetal framework for multi-objective optimization: Design and architecture,2010,"jMetal is a Java-based framework for multi-objective optimization using metaheuristics. It is a flexible, extensible, and easy-to-use software package that has been used in a wide range of applications. In this paper, we describe the design issues underlying jMetal, focusing mainly on its internal architecture, with the aim of offering a comprehensive view of its main features to interested researchers. Among the covered topics, we detail the basic components facilitating the implementation of multi-objective metaheuristics (solution representations, operators, problems, density estimators, archives), the included quality indicators to assess the performance of the algorithms, and jMetal's support to carry out full experimental studies.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5586354,no
Achieving high robustness and performance in performing QoS-aware route planning for IPTV networks,2010,QoS-aware network planning becomes increasingly important for network operators and ISP alike as the number and heterogeneity of applications supported by networks continue to increase. We have proposed an architecture and methodology to support QoS-aware route planning for IPTV networks. The route planning problem was formulated as a residual bandwidth optimization problem and solved using GA-VNS hybrid algorithm. The QoS guarantee is provided by performing route allocation for a QoS-aware demand matrix which is generated using efficient and accurate estimation of QoS requirements using empirical effective bandwidth estimations. This paper revisits our formerly proposed approach to perform residual bandwidth optimization with QoS guarantee in IPTV networks and proposes alterations and parameter settings to best suit the problem in terms of robustness and performance. This paper proposes the use of two cost functions to meet the above requirements and the use of a variation of the original VNS algorithm besides recommending the use of specific operators and general parameter settings. The proposals and recommendations are substantiated by the results of performance evaluations and analysis. The results indicate dramatic improvement when the proposed approaches are taken on board.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5586513,no
Enhancing perceptual quality of watermarked high-definition video through composite mask,2010,"This paper proposes a composite mask based watermarking method, which improves perceptual quality of videos watermarked by traditional video watermarking scheme. The composite mask including noise visibility function (NVF) mask, adaptive dithering mask, and contour mask considers human visual system (HVS). The adaptive dithering mask solves the block artifact problem caused by expanding the watermark basic pattern for the robustness against various downscaling attacks. The contour mask makes up for the disadvantage of NVF mask, which cannot handle separately high textured regions and contour or edge regions. Extensive experiments prove that the watermarking method based on the proposed composite mask satisfies imperceptibility, robustness against downscaling as well as common video processing, and real-time performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5586742,no
A design approach for numerical libraries in large scale distributed systems,2010,"Nowadays, large scale distributed systems gather thousands of nodes with hierarchical memory models. They are heterogeneous, volatile and geographically distributed. The efficient exploitation of such systems requires the conception and adaptation of appropriate numerical methods, the definition of new programming paradigms, new metrics for performance prediction, etc. The modern hybrid numerical methods are well adapted to this kind of systems. This is particularly because of their multi-level parallelism and fault tolerance property. However the programming of these methods for these architectures requires concurrent reuse of sequential and parallel code. But the currently existing numerical libraries aren't able to exploit the multi-level parallelism offered by theses methods. A few linear algebra numerical libraries make use of object oriented approach allowing modularity and extensibility. Nevertheless, those which offer modularity,sequential and parallel code reuse are almost non-existent. In this paper, we analyze the lacks in existing libraries and propose a design based on a component approach and the strict separation between computation operations, data management and communication control of an application. We present then an application of this design using YML scientific workflow environment (http://yml.prism.uvsq.fr/) jointly with the object oriented LAKe (Linear Algebra Kernel) library. Some numerical experiments on GRID5000 platform validate our approach and show its efficiency.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5586951,no
A testbed for the evaluation of link quality estimators in wireless sensor networks,2010,"Link quality estimation is a fundamental building block for the design of several different mechanisms and protocols in wireless sensor networks. The accuracy of link quality estimation greatly impacts the efficiency of these protocols. Therefore, a thorough experimental evaluation of link quality estimators (LQEs) is mandatory. This motivated us to build a benchmarking testbed-RadiaLE, that automates LQEs evaluation by analyzing their statistical properties. Our testbed includes (i.) hardware components that represent the WSN under test and (ii.) a software tool for setting up and controlling the experiments and also for analyzing the collected data, allowing for LQEs evaluation. To demonstrate the usefulness of RadiaLE, we carried out a comparative performance study of a set of well-known LQEs.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5587002,no
Performance analysis of distributed software systems: A model-driven approach,2010,"The design of complex software systems is a challenging task because it involves a wide range of quality attributes such as security, performance, reliability, to name a few. Dealing with each of these attributes requires specific set of skills, which quite often, involves making various trade-offs. This paper proposes a novel Model-Driven Software Performance Engineering (MDSPE) process that can be used for performance analysis requirements of distributed software systems. An example assessment is given to illustrate how our MDSPE process can comply with well-known performance models to assess the performance measures.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5589073,no
Power quality analysis of a Synchronous Static Series Compensator (SSSC) connected to a high voltage transmission network,2010,"This paper presents the power quality analysis of a Synchronous Static Series Compensator (SSSC) connected to a high-voltage transmission network. The analysis employs a simplified harmonic equivalent model of the network that is described in the paper. To facilitate the calculations, a software toolset was developed to measure the voltage harmonics content introduced by the SSSC at the Point of Common Coupling (PCC). These harmonics are then evaluated against the stringent power quality legislation set on the Spanish transmission network. Subsequently, the compliance of an SSSC based on two types of Voltage Sourced Converters (VSC): a 48-pulse and a PWM one, usual in this type of applications, is assessed. Finally, a parameter sensitivity analysis is presented, which looks at the influence of parameters such as the line length and short circuit impedances on the PCC voltage harmonics.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5589923,no
A Clustering Algorithm Use SOM and K-Means in Intrusion Detection,2010,"Improving detection definition is a pivotal problem for intrusion detection. Many intelligent algorithms were used to improve the detection rate and reduce the false rate. Traditional SOM cannot provide the precise clustering results to us, while traditional K-Means depends on the initial value serious and it is difficult to find the center of cluster easily. Therefore, in this paper we introduce a new algorithm, first, we use SOM gained roughly clusters and center of clusters, then, using K-Means refine the clustering in the SOM stage. At last of this paper we take KDD CUP-99 dataset to test the performance of the new algorithm. The new algorithm overcomes the defects of traditional algorithms effectively. Experimental results show that the new algorithm has a good stability of efficiency and clustering accuracy.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5590930,no
Research on the Copy Detection Algorithm for Source Code Based on Program Organizational Structure Tree Matching,2010,Code plagiarism is an ubiquitous phenomenon in the teaching of Programming Language. A large number of source code can be automatically detected and uses the similarity value to determine whether the copy is present. It can greatly improve the efficiency of teachers and promote teaching quality. A algorithm is provided that firstly match program organizational structure tree and then process the methods of program to calculate the similarity value. It not only applies to process-oriented programming languages but also applies to object-oriented programming language.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5590938,no
A Study on Defect Density of Open Source Software,2010,"Open source software (OSS) development is considered an effective approach to ensuring acceptable levels of software quality. One facet of quality improvement involves the detection of potential relationship between defect density and other open source software metrics. This paper presents an empirical study of the relationship between defect density and download number, software size and developer number as three popular repository metrics. This relationship is explored by examining forty-four randomly selected open source software projects retrieved from SourceForge.net. By applying simple and multiple linear regression analysis, the results reveal a statistically significant relationship between defect density and number of developers and software size jointly. However, despite theoretical expectations, no significant relationship was found between defect density and number of downloads in OSS projects.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5591023,no
A Structured Framework for Software Metrics Based on Three Primary Metrics,2010,"This paper presents a structured unifying framework for software metrics (numerical software measurements), based on the three ""primary metrics"" of function points (FP), person-months (PM), and lines of code (LOC). The framework is based on a layered model, with the three primary metrics constituting the lowest layer. An important property of the primary metrics, referred to as the ""convertibility property"" is that a primary metric can easily be converted to another primary metric. Time is also included in this layer as a fundamental (not necessarily software) primary metric. The second layer consists of general-purpose metrics such as productivity measures, which are computed from the primary metrics, and the third layer consists of specialpurpose metrics such as reliability and quality measures. This third layer is inherently extensible. The framework readily lends itself for use in both instructional and practitioner environments.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5591049,no
Application and Study on Test Data Analysis in Software Reliability Evaluation Model Selection,2010,"In the software reliability evaluation, selection of model directly affect the reliability evaluation and accuracy of the forecast result. So testing data input to any models, impact of its quality and completeness on the reliability of measurement results is much larger than impact of suppose the quality and evaluation ability of the model. For example, such as actual data set, study on selection and processing of testing data and kinds of data analysis techniques in selection process of reliability model, is useful to help select more effective and rational evaluating models and improve accuracy of the forecast in software reliability evaluation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5591780,no
Cost Efficient Software Review in an E-Business Software Development Project,2010,Many software projects face the trouble to satisfy both cost constraints and high quality requirement. Software review turns out to be an efficient approach to reduce the cost and early remove the defects. This paper classifies the software review methods and proposes a cost efficient framework for different maturity level of software review. Case study in an E-business software development project approved that proposed framework was efficient in cost controlling and benefits the defect removal.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5593181,no
Shape-DNA: Effective Character Restoration and Enhancement for Arabic Text Documents,2010,"We present a novel learning-based image restoration and enhancement technique for improving character recognition performance of OCR products for degraded documents or documents/text captured with mobile devices such as camera-phones. The proposed technique is language independent and can simultaneously increase the effective resolution and restore broken characters with artifacts due to image capturing device such as a low quality/low resolution camera, or due to previous pre-processing such as extracting text region from the document image. The proposed technique develops a predictive relationship between high-resolution training images and their low-resolution/degraded counterparts, and exploits this relationship in a probabilistic scheme to generate a high resolution image from a low quality, low-resolution text image. We present a fast and scalable implementation of the proposed character restoration algorithm to improve the text recognition for document/text images captured by mobile phones. Experimental results demonstrate that the system effectively increases OCR performance for documents captured by mobile imaging devices, from levels of 50% to levels of over 80% for non-latin document/scene text images at 120dpi.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5595910,no
Unifying quality metrics for reservoir networks,2010,"Several metrics for the quality of reservoirs have been proposed and linked to reservoir performance in Echo State Networks and Liquid State Machines. A method to visualize the quality of a reservoir, called the separation ratio graph, is developed from these existing metrics leading to a generalized approach to measuring reservoir quality. Separation ratio provides a method for estimating the components of a reservoir's separation and visualizing the transition from stable to chaotic behavior. This new approach does not require a prior classification of input samples and can be applied to reservoirs trained with unsupervised learning. It can also be used to analyze systems made up of multiple reservoirs to determine performance between any two points in the system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5596307,no
A method to build a representation using a classifier and its use in a K Nearest Neighbors-based deployment,2010,"The K Nearest Neighbors (KNN) is strongly dependent on the quality of the distance metric used. For supervised classification problems, the aim of metric learning is to learn a distance metric for the input data space from a given collection of pair of similar/dissimilar points. A crucial point is the distance metric used to measure the closeness of instances. In the industrial context of this paper the key point is that a very interesting source of knowledge is available : a classifier to be deployed. The knowledge incorporated in this classifier is used to guide the choice (or the construction) of a distance adapted to the situation Then a KNN-based deployment is elaborated to speed up the deployment of the classifier compared to a direct deployment.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5596539,no
Software defect prediction using static code metrics underestimates defect-proneness,2010,"Many studies have been carried out to predict the presence of software code defects using static code metrics. Such studies typically report how a classifier performs with real world data, but usually no analysis of the predictions is carried out. An analysis of this kind may be worthwhile as it can illuminate the motivation behind the predictions and the severity of the misclassifications. This investigation involves a manual analysis of the predictions made by Support Vector Machine classifiers using data from the NASA Metrics Data Program repository. The findings show that the predictions are generally well motivated and that the classifiers were, on average, more â€œconfidentâ€?in the predictions they made which were correct.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5596650,no
Performance Evaluation Tools for Zone Segmentation and Classification (PETS),2010,"This paper describes a set of Performance Evaluation Tools (PETS) for document image zone segmentation and classification. The tools allow researchers and developers to evaluate, optimize and compare their algorithms by providing a variety of quantitative performance metrics. The evaluation of segmentation quality is based on the pixel-based overlaps between two sets of zones proposed by Randriamasy and Vincent. PETS extends the approach by providing a set of metrics for overlap analysis, RLE and polygonal representation of zones and introduces type-matching to evaluate zone classification. The software is available for research use.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5597427,no
Quality Models for Free/Libre Open Source Software Towards the Â“Silver BulletÂ”?,2010,"Selecting the right software is of crucial importance for businesses. Free/Libre Open Source Software (FLOSS) quality models can ease this decision-making. This paper introduces a distinction between first and second generation quality models. The former are based on relatively few metrics, require deep insights into the assessed software, relying strongly on subjective human perception and manual labour. Second generation quality models strive to replace the human factor by relying on tools and a multitude of metrics. The key question this paper addresses is whether the emerging FLOSS quality models provide the â€œsilver bulletâ€?overcoming the shortcomings of first generation models. In order to answer this question, OpenBRR, a first generation quality model, and QualOSS, a second generation quality model, are used for a comparative assessment of Asterisk, a FLOSS implementation of a telephone private branch exchange. Results indicate significant progress, but apparently the â€œsilver bulletâ€?has not yet been found.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5598015,no
Managing Quality Requirements: A Systematic Review,2010,"It is commonly acknowledged that the management of quality requirements is an important and difficult part of the requirements engineering process, which plays a critical role in software product development. In order to identify current research about quality requirements, a systematic literature review was performed. This paper identifies available empirical studies of quality requirements. A database and manual search identified 1,560 studies, of which 18 were found to be empirical research studies of high quality, and relevant to the research questions. The review investigates what is currently known about the benefits and limitations of methods of quality requirements. In addition, the state of research is presented for five identified areas: elicitation, dependencies, quality requirements metrics, cost estimations, and prioritization.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5598106,no
"An Analysis of the ""Inconclusive' Change Report Category in OSS Assisted by a Program Slicing Metric",2010,"In this paper, we investigate the Barcode open-source system (OSS) using one of Weiser's original slice-based metrics (Tightness) as a basis. In previous work, low numerical values of this slice-based metric were found to indicate fault-free (as opposed to fault-prone) functions. In the same work, we deliberately excluded from our analysis a category comprising 221 of the 775 observations representing `inconclusive' log reports extracted from the OSS change logs. These represented OSS change log descriptions where it was not entirely clear whether a fault had occurred or not in a function and, for that reason, could not reasonably be incorporated into our analysis. In this paper we present a methodology through which we can draw conclusions about that category of report.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5598109,no
AI-Based Models for Software Effort Estimation,2010,"Decision making under uncertainty is a critical problem in the field of software engineering. Predicting the software quality or the cost/ effort requires high level expertise. AI based predictor models, on the other hand, are useful decision making tools that learn from past projects' data. In this study, we have built an effort estimation model for a multinational bank to predict the effort prior to projects' development lifecycle. We have collected process, product and resource metrics from past projects together with the effort values distributed among software life cycle phases, i.e. analysis & test, design & development. We have used Clustering approach to form consistent project groups and Support Vector Regression (SVR) to predict the effort. Our results validate the benefits of using AI methods in real life problems. We attain Pred(25) values as high as 78% in predicting future projects.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5598114,no
A survey on Software Reusability,2010,"Software Reusability is primary attribute of software quality. In the literature, there are metrics for identifying the quality of reusable components but there is very less work on the framework that makes use of these metrics to find reusability of software components. The quality of the software if identified in the design phase or even in the coding phase can help us to reduce the rework by improving quality of reuse of the component and hence improve the productivity due to probabilistic increase in the reuse level. In this paper, different models or measures that are discussed in literature for software reusability prediction or evaluation of software components are summarized.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5598467,no
Checkpointing vs. Migration for Post-Petascale Supercomputers,2010,"An alternative to classical fault-tolerant approaches for large-scale clusters is failure avoidance, by which the occurrence of a fault is predicted and a preventive measure is taken. We develop analytical performance models for two types of preventive measures: preventive checkpointing and preventive migration. We also develop an analytical model of the performance of a standard periodic checkpoint fault-tolerant approach. We instantiate these models for platform scenarios representative of current and future technology trends. We find that preventive migration is the better approach in the short term by orders of magnitude. However, in the longer term, both approaches have comparable merit with a marginal advantage for preventive checkpointing. We also find that standard non-prediction-based fault tolerance achieves poor scaling when compared to prediction-based failure avoidance, thereby demonstrating the importance of failure prediction capabilities. Finally, our results show that achieving good utilization in truly large-scale machines (e.g., 2<sup>20</sup> nodes) for parallel workloads will require more than the failure avoidance techniques evaluated in this work.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5599161,no
Use of accelerometers for material inner defect detection,2010,"This paper deals with a possibility of non-destructive diagnostics of solid objects by software analysis of vibration spectrum by accelerometers. By a use of MATLAB platform, a processing and information evaluation from accelerometer is possible. Accelerometer is placed on the measured object. The analog signal needs to be digitized by a special I/O device to be processed offline with FFT (Fast Fourier Transformation). The power spectrum is then examined by developed evaluating procedures.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5599597,no
Application of statistical learning theory to predict corrosion rate of injecting water pipeline,2010,"Support Vector Machines (SVM) represents a new and very promising approach to pattern recognition based on small dataset. The approach is systematic and properly motivated by Statistical Learning Theory (SLT). Training involves separating the classes with a surface that maximizes the margin between them. An interesting property of this approach is that it is an approximate implementation of Structural Risk Minimization (SRM) induction principle, therefore, SVM is more generalized performance and accurate as compared to artificial neural network which embodies the Embodies Risk Minimization (ERM) principle. In this paper, according to corrosion rate complicated reflection relation with influence factors, we studied the theory and method of Support Vector Machines based the statistical learning theory and proposed a pattern recognition method based Support Vector Machine to predict corrosion rate of injecting water pipeline. The outline of the method is as follows: First, we researched the injecting water quality corrosion influence factors in given experimental zones with Gray correlation method; then we used the LibSVM software based Support Vector Machine to study the relationship of those injecting water quality corrosion influence factors, and set up the mode to predict corrosion rate of injecting water pipeline. Application and analysis of the experimental results in Shengli oilfield proved that SVM could achieve greater accuracy than the BP neural network do, which also proved that application of SVM to predict corrosion rate of injecting water pipeline, even to the other theme in petroleum engineering, is reliable, adaptable, precise and easy to operate.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5599754,no
Enforcing SLAs in Scientific Clouds,2010,"Software as a Service (SaaS) providers enable the on-demand use of software, which is an intriguing concept for business and scientific applications. Typically, service level agreements (SLAs) are specified between the provider and the user, defining the required quality of service (QoS). Today SLA aware solutions only exist for business applications. We present a general SaaS architecture for scientific software that offers an easy-to-use web interface. Scientists define their problem description, the QoS requirements and can access the results through this portal. Our algorithms autonomously test the feasibility of the SLA and, if accepted, guarantee its fulfillment. This approach is independent of the underlying cloud infrastructure and successfully deals with performance fluctuations of cloud instances. Experiments are done with a scientific application in private and public clouds and we also present the implementation of a high-performance computing (HPC) cloud dedicated for scientific applications.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5600308,no
AWPS - Simulation Based Automated Web Performance Analysis and Prediction,2010,"The growing demand for quality and performance has become a discriminating factor in the field of web applications. Advances in software technology and project management have increased the amount of web applications which are developed under these aspects. At the same time the research in web performance simulation and prediction is focusing on complex modeling methods and techniques rather than providing easy to use tools to test and ensure the performance requirements of today's and tomorrow's web applications. As a tool for these tasks, the automatic web performance simulation and prediction system (AWPS) will be described. The AWPS is capable of (a) automatically creating an online web performance simulation and (b) conducting trend analysis of the system under test.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5600390,no
Testing the Verticality of Slow-Axis of the Dual Optical Fiber Based on Image Processing,2010,"The polarization dual fiber collimator is one of the important passive devices, which could be used to transform divergent light from the fiber into parallel beam or focus parallel beam into the fiber so as to improve the coupling efficiency of fiber device. However, the quality of double-pigtail fiber used for assembling collimator directly impacts on the performance of fiber collimator. Therefore, it is necessary to detect the quality of double-pigtail fiber before collimators has been packaged to ensure the quality of fiber collimator. The paper has pointed out that the verticality of two slow-axis of the double-pigtail fiber is the major factor to affect the quality of fiber collimator. With ""Panda""-type double-pigtail fiber as the research object, a novel method to detect the verticality of slow-axis is proposed. First, with the red light of LED as background light source, the clear images of the cross-section end surface of double-pigtail fiber has been obtained by micro-imaging systems; Then after a series of image pre-processing operations, the coordinate information of ""cat's eye"" light heart can been extracted with the image centroid algorithm; Finally, the angle between the two slow axes of the double-pigtail fiber can be calculated quickly and accurately according to pre-established mathematical model. The detection system mainly consists of CCD, microscopic imaging device, image board and image processing software developed with VC++. The experimental results prove that the system is both practical and effective and can satisfy the collimator assembly precision demands.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5600654,no
Research on Subjective Trust Routing Algorithm for Mobile Ad Hoc Networks,2010,"The routing behavior of Mobile Ad Hoc Network is a kind of cooperation model in multi mobile nodes. The traditional routing methods cannot solve the problem of malicious behavior and build the trusted transfers route between nodes. Based on the character of trust routing, the subjective confidence for the routing behavior has transform into trust evaluation with the probability model to solve the trust measurement and forecast. For compare the trust routing policy, some trust routing algorithms have proposed to depict the trust characteristic and complete different trust routing. The results of simulation show the trust routing algorithms of mobile ad hoc network is effective, which can guarantee the routing performance and quality of transmission.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5600894,no
Design and Implementation of the Earthquake Precursor Network Running Monitoring Software Based on C/S Structure,2010,"First, we introduce the design principles of the function of Tianjin earthquake precursor network running monitoring software; second, we have developed the software by using the precursor instrument communication technology, the distributed database read-write technology and data quality monitoring methods. The software regularly and automatically monitor network equipment status, data report situation, and abnormal data. It achieves the automatic detection and alarm function for the precursor instruments and observation data. The application of the software improves data quality and work efficiency.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5601468,no
Effective Static Analysis to Find Concurrency Bugs in Java,2010,"Multithreading and concurrency are core features of the Java language. However, writing a correct concurrent program is notoriously difficult and error prone. Therefore, developing effective techniques to find concurrency bugs is very important. Existing static analysis techniques for finding concurrency bugs either sacrifice precision for performance, leading to many false positives, or require sophisticated analysis that incur significant overhead. In this paper, we present a precise and efficient static concurrency bugs detector building upon the Eclipse JDT and the open source WALA toolkit (which provides advanced static analysis capabilities). Our detector uses different implementation strategies to consider different types of concurrency bugs. We either utilize JDT to syntactically examine source code, or leverage WALA to perform interprocedural data flow analysis. We describe a variety of novel heuristics and enhancements to existing analysis techniques which make our detector more practical, in terms of accuracy and performance. We also present an effective approach to create inter-procedural data flow analysis using WALA for complex analysis. Finally we justify our claims by presenting the results of applying our detector to a range of real-world applications and comparing our detector with other tools.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5601820,no
Interval-Based Uncertainty Handling in Model-Based Prediction of System Quality,2010,"Our earlier research indicates feasibility of applying the PREDIQT method for model-based prediction of impacts of architecture design changes on system quality. The PREDIQT method develops and makes use of the so called prediction models, a central part of which are the ""Dependency Views"" (DVs) - weighted trees representing the relationships between system design and the quality notions. The values assigned to the DV parameters originate from domain expert judgments and measurements on the system. However fine grained, the DVs contain a certain degree of uncertainty due to lack and inaccuracy of empirical input. This paper proposes an approach to the representation, propagation and analysis of uncertainties in DVs. Such an approach is essential to facilitate model fitting, identify the kinds of architecture design changes which can be handled by the prediction models, and indicate the value of added information. Based on a set of criteria, we argue analytically and empirically, that our approach is comprehensible, sound, practically useful and better than any other approach we are aware of.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5601889,no
Arranging software test cases through an optimization method,2010,"During the software testing process, the customers would be invited to review or inspect an ongoing software product. This phase is called the â€œin-plantâ€?test, often known as an â€œalphaâ€?test. Typically, this test phase lasts for a very short period of time in which the software test engineers or software quality engineers rush to execute a list of software test cases in the test suite with customers. Because of the time constraint, the test cases have to be arranged in terms of test case severities, estimated test time, and customers' demands. As important as the test case arrangement is, this process is mostly performed manually by the project managers and software test engineers together. As the software systems are getting more sophisticated and complex, a greater volume of test cases have to be generated, and the manual arrangement approach may not be the most efficient way to handle this. In this paper, we propose a framework for automating the process of test case arrangement and management through an optimization method. We believe that this framework will help software test engineers facing with the challenges of prioritizing test cases.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5602131,no
The ATLAS Calorimeter Trigger Commissioning Using Cosmic Ray Data,2010,"While waiting for the first LHC collisions, the ATLAS detector is undergoing integration exercises performing cosmic ray data taking runs. The ATLAS calorimeter trigger software uses a common framework capable of retrieving, decoding and providing data to different particle identification algorithms. This work reports on the results obtained during the first data-taking period of 2009 with the calorimeter trigger. Experts from detector areas such as calorimetry, data flow management, data quality monitoring and the trigger worked together to reach stable data taking conditions. Issues like noise readout channels were addressed and the global trigger robustness was evaluated.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5603485,no
A combined acoustic and optical instrument for fisheries studies,2010,The AOS system was developed through the merging of acoustic and optical technologies to further the quality of fisheries stock assessment. The system has been deployed from several ships (40 m to 75 m in length) over the past three years providing improved accuracy of acoustic target strength measures for commercial fish species and delivering biomass estimates of fish stocks. It has proven to be robust withstanding the harsh environment of pressure and temperature to depths of 1000 meters. The platform must also cope with exposure to significant impacts and vibration during deployment and recovery as the trawl net moves up and down the stern ramp of commercial trawlers. The system continues to be incrementally improved to gain better performance and to streamline post survey data analysis. These improvements come as a direct result of new measurement challenges identified through working with data sets collected by the instrument.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5603536,no
Benchmarking IP blacklists for financial botnet detection,2010,"Every day, hundreds or even thousands of computers are infected with financial malware (i.e. Zeus) that forces them to become zombies or drones, capable of joining massive financial botnets that can be hired by well-organized cyber-criminals in order to steal online banking customers' credentials. Despite the fact that detection and mitigation mechanisms for spam and DDoS-related botnets have been widely researched and developed, it is true that the passive nature (i.e. low network traffic, fewer connections) of financial botnets greatly hinder their countermeasures. Therefore, cyber-criminals are still obtaining high economical profits at relatively low risk with financial botnets. In this paper we propose the use of publicly available IP blacklists to detect both drones and Command & Control nodes that are part of financial botnets. To prove this hypothesis we have developed a formal framework capable of evaluating the quality of a blacklist by comparing it versus a baseline and taking into account different metrics. The contributed framework has been tested with approximately 500 million IP addresses, retrieved during a one-month period from seven different well-known blacklist providers. Our experimental results showed that these IP blacklists are able to detect both drones and C&C related with the Zeus botnet and most important, that it is possible to assign different quality scores to each blacklist based on our metrics. Finally, we introduce the basics of a high-performance IP reputation system that uses the previously obtained blacklists' quality scores, in order to reply almost in real-time whether a certain IP is a member of a financial botnet or not. Our belief is that such a system can be easily integrated into e-banking anti-fraud systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5604040,no
Perceptual Rate-Distortion Optimization Using Structural Similarity Index as Quality Metric,2010,"The rate-distortion optimization (RDO) framework for video coding achieves a tradeoff between bit-rate and quality. However, objective distortion metrics such as mean squared error traditionally used in this framework are poorly correlated with perceptual quality. We address this issue by proposing an approach that incorporates the structural similarity index as a quality metric into the framework. In particular, we develop a predictive Lagrange multiplier estimation method to resolve the chicken and egg dilemma of perceptual-based RDO and apply it to H.264 intra and inter mode decision. Given a perceptual quality level, the resulting video encoder achieves on the average 9% bit-rate reduction for intra-frame coding and 11% for inter-frame coding over the JM reference software. Subjective test further confirms that, at the same bit-rate, the proposed perceptual RDO indeed preserves image details and prevents block artifact better than traditional RDO.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5604279,no
Physical layer monitoring in 8-branched PON-based i-FTTH,2010,This paper reports a novel physical layer monitoring technique called Centralized Failure Detection System (CFDS) based on the use of an optical time domain reflectometry (OTDR) and integrated with inexpensive additional optical devices at the middle of the network system for monitoring the fiber plant of a wide range of passive optical network (PON) based intelligent fiber-to-the-home (i-FTTH) in the background during network operation and determining the faulty branch as well as the fault parameters. Access Control System (ACS) has been developed to handle the centralized line detection for diverting the OTDR pulse signal from central office (CO) to bypass the filter and connect to each corresponding fiber link in drop section. The design and implementation of this integrated hardware/software system enable each lines' status can be observed and monitored at static point where the OTDR injecting the signal. All OTDR measurements are transmitted into remote workstation for centralized monitoring and advanced analyzing.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5604408,no
Click Fraud Prevention via multimodal evidence fusion by Dempster-Shafer theory,2010,"We address the problem of combining information from diversified sources in a coherent fashion. A generalized evidence processing theory and an architecture for data fusion that accommodates diversified sources of information are presented. Different levels at which data fusion may take place such as the level of dynamics, the level of attributes, and the level of evidence are discussed. A multi-level fusion architecture based Collaborative Click Fraud Detection and Prevention (CCFDP) system for real time click fraud detection and prevention is proposed and its performance is compared with a traditional rule based click fraud detection system. Both systems are tested with real world data from an actual ad campaign. Results show that use of multi-level data fusion improves the quality of click fraud analysis.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5604480,no
Network Coding of Rateless Video in Streaming Overlays,2010,"We present a system for collaborative video streaming in wired overlay networks. We propose a scheme that builds on both rateless codes and network coding in order to improve the system throughput and the video quality at clients. Our hybrid coding algorithm permits to efficiently exploit the available source and path diversity without the need for expensive routing nor scheduling algorithms. We consider specifically an architecture where multiple streaming servers simultaneously deliver video information to a set of clients. The servers apply Raptor coding on the video packets for error resiliency, and the overlay nodes selectively combine the Raptor coded video packets in order to increase the packet diversity in the system. We analyze the performance of selective network coding and describe its application to practical video streaming systems. We further compute an effective source and channel rate allocation in our collaborative streaming system. We estimate the expected symbol diversity at clients with respect to the coding choices. Then we cast a minmax quality optimization problem that is solved by a low-cost bisection based method. The experimental evaluation demonstrates that our system typically outperforms Raptor video streaming systems that do not use network coding as well as systems that perform decoding and encoding in the network nodes. Finally, our solution has a low complexity and only requires small buffers in the network coding nodes, which are certainly two important advantages toward deployment in practical streaming systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5604675,no
Cycle accurate simulator generator for NoGap,2010,"Application Specific Instruction-set Processors (ASIPs) are needed to handle the future demand of flexible yet high performance computation in mobile devices. However designing an ASIP is complicated by the fact that not only the processor but, also tools such as assemblers, simulators, and compilers have to be designed. Novel Generator of Accelerators And Processors (NoGap), is a design automation tool for ASIP design that imposes very few limitations on the designer. Yet NoGap supports the designer by automating much of the tedious and error prone tasks associated with ASIP design. This paper will present the techniques used to generate a stand alone software simulator for a processor designed with NoGap. The focus will be on the core algorithms used. Two main problems had to be solved, simulation of a data path graph and simulation of leaf functional units. The concept of sequentialization is introduced and the algorithms used to perform both the leaf unit sequentialization and data path sequentialization is presented. A key component of the sequentialization process is the Micro Architecture Generation Essentials (Mage) dependency graph. The mage dependency graph and the algorithm used for its generation are also presented in this paper. A NoGap simulator was generated for a simple processor and the results were verified.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5604963,no
Multiview Video Coding extension of the H.264/AVC standard,2010,An overview of the new Multiview Video Coding (MVC) extension of the H.264/AVC standard is given in this paper. The benefits of multiview video coding are examined by encoding two data sets captured with multiple cameras. These sequences were encoded using MVC reference software and the results are compared with references obtained by encoding multiple views independently with H.264/AVC reference software. Experimental quality comparison of coding efficiency is given. The peak signal-to-noise ration (PSNR) and Structural Similarity (SSIM) Index are used as objective quality metrics.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5606089,no
Fault Slip Through measurement in software development process,2010,"The pressure to improve software development process is not new, but in today's competitive environment there is even greater emphasis on delivering a better service at lower cost. In market-driven development where time-to-market is of crucial importance, software development companies seek improvements that can decrease the lead-time and improve the delivery precision. One way to achieve this is by analyzing the test process since rework commonly accounts for more than half of the development time. A large reason for high rework costs is fault slippage from earlier phases where they are cheaper to find and remove. As an input to improvements, this article introduces a measure that can quantify this relationship. That is, a measure called faults-slip-through, which determines the faults that would have been more cost-effective to find in an earlier phase.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5606110,no
Fault tolerant amplifier system using evolvable hardware,2010,"This paper proposes the use evolvable hardware (EHW) for providing fault tolerance to an amplifier system in a signal-conditioning environment. The system has to maintain a given gain despite the presence of faults, without direct human intervention. The hardware setup includes a reconfigurable system on chip device and an external computer where a genetic algorithm is running. For detecting a gain fault, we propose a software-based built-in self-test strategy that establishes the actual values of gain achievable by the system. The performance evaluation of the fault tolerance strategy proposed is made by adopting two different types of fault-models. The fault simulation results show that the technique is robust and that the genetic algorithm finds the target gain with low error.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5606375,no
Solution for the power quality improvement in a transportation system,2010,The paper proposes a solution for improving the power quality in a transportation system. The experimental determinations revealed a high level of harmonics and a high consumption of reactive power. The simulation using MATLAB/Simulink software was conceived and validated. A solution using passive filters is proposed and analyzed.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5606902,no
A Measurement Quality Factor for Swath Bathymetry Sounders,2010,"The quality estimation associated with individual soundings measured and computed by swath bathymetry sonars is a paramount issue which is most often imperfectly addressed today by sonar manufacturers. In this paper, a unified definition is proposed for a quality factor usable for all swath bathymetry sonars; the depth-relative error is directly estimated from the signal characteristics used in the sounding computation. The basic algorithms are presented for both phase-difference (oblique incidence) and amplitude (normal incidence) detection, and can be readily implemented in any swath bathymetry system using these detection principles. This approach gives a direct access to the objective bathymetric performance of individual soundings, and hence avoids the explicit estimation of intermediate quantities such as the signal-to-noise ratio (SNR). It makes possible intercomparisons between different systems (multibeam sounders versus interferometric sonars) and processing principles (phase difference versus amplitude); it is expected to provide a useful input to bathymetry postprocessing software using quality estimation of individual soundings.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5607320,no
Live memory migration with matrix bitmap algorithm,2010,"Live migration of OS instance, a powerful instrument to facilitate system maintenance, load balancing, fault tolerance, has become the central issue of virtualization, while the memory Migration has always been the bottleneck. However, exiting algorithms suffer two problems. First, when facing those frequently modified pages, the performance would degrade profoundly. Second, though they do perform more preferable through some certain algorithms, the accessional resources are requisite, which might extraordinary scarce under heavy load conditions. These imperfections will lead to striking performance degradation of virtual machine services. Therefore, based on the â€œProgram Locality Principleâ€? this paper presents the â€œmatrix bitmap algorithmâ€?that collects the dirty page information for many times before deciding whether to transfer the page or not. It provides a more reasonable approach to obtain the determination. Experiments demonstrate that when under heavy load conditions, if sufficient dirty page information is collected, the decrease in total migration time will achieve 50% without increasing the system burden of the original domain.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5607439,no
Low-cost embedded system for the IM fault detection using neural networks,2010,"This paper presents a realization of a low-cost, portable measurement system for induction motor fault diagnosis. The system is composed of a four-channel data acquisition recorder and laptop computer with specialized software. The diagnostic software takes advantage of the motor current signature analysis method (MCSA) for detection of rotor faults, voltage unbalance and mechanical misalignment. For diagnostics purposes the neural network technique is applied.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5607947,no
Assessment and optimization of induction electric motors aiming energy efficiency in industrial applications,2010,"A methodology for assessment and optimization of induction electric motors, devised to achieve energy conservation, is described. This methodology works by means of replacing high-efficiency motors for older ones. The electric motor is an end-use equipment which is strongly present in industry, being subject to replacements that may have satisfactory results when the diagnosis is carried out in a consistent way, in accordance to coherent validation procedures. The adopted motor replacement methodology included an initial study, by means of measurements of electrical parameters. Afterwards, by using a particular simulation software, operating conditions of the electric motor and the expected savings accruing from the use of high-efficiency motors were estimated. When necessary, the motor resizing analysis provides the best rated power for the drive. The motor substitutions carried out in this motor drives efficiency improvement program resulted in yearly savings amounting to 720 MWh.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5608033,no
"Modeling, analysis and detection of rotor field winding faults in synchronous generators",2010,"The paper presents an approach to modeling of shorted turns in rotor field winding of synchronous generator using finite element method. It enables detailed analysis of magnetic field at several operating conditions under healthy and faulty states which are difficult or even impossible to carry out by available measurement methods in industrial environment. Modeling of field winding faults are performed for both typical generator designs - turbo and hydro, and analysis reveals some differences, which are significant for practical use in fault detection procedures. It is confirmed that an extensive analysis should be performed to assure accurate healthy/faulty state predictions, since the level of diagnostic signal is considerably influenced by a combination of many machine and operating parameters. The scheme of developed on-line diagnostic system with its hardware and software concept is also presented and discussed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5608042,no
Performance enhancement of software process models,2010,"Software Process Improvement (SPI) is a systematic approach and continuous improvement of software producing organization's ability to produce and deliver quality software within time and budget constraints [1]. In the course of this project, the researcher developed, verified and validated such a model. This paper work presents a fit for use approach to software process improvement model for improve the process overall. This paper work concentrates on improvement in process as well as organization. This model is based on experience on project of software companies. This paper work is covered assessment, software process improvement, factor that influence on software process improvement. The ultimate aim was to develop a model which would be useful in practice for software development companies. This paper work describes the principle of software process improvement and improvement models. The purposed model is a generic model which is beneficial for small firm as well as large firm.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5608784,no
Software fault detection for reliability using recurrent neural network modeling,2010,"Software fault detection is an important factor for quantitatively characterizing software quality. One of the proposed methods for software fault detection is neural networks. Fault detection is actually a pattern recognition task. Faulty and fault free data are different patterns which must be recognized. In this paper we propose a new framework for modeling software testing and fault detection in applications. Recurrent neural network architecture is used to improve performance of the system. Based on the experiments performed on the software reliability data obtained from middle-sized application software, it is observed that the non-linear RNN can be effective and efficient for software faults detection.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5608831,no
An analytical model for performance evaluation of software architectural styles,2010,"Software architecture is an abstract model that gives syntactic and semantic information about the components of a software system and the relationship among them. The success of the software depends on whether the system can satisfy the quality attributes. One of the most critical aspects of the quality attributes of a software system is its performance. Performance analysis can be useful for assessing whether a proposed architecture can meet the desired performance specifications and whether it can help in making key architectural decisions. An architecture style is a set of principles which an architect uses in designing software architecture. Since software architectural styles have frequently been used by architects, these styles have a specific effect on quality attributes. If this effect is measurable for each existing style, it will enable the architect to evaluate and make architectural decisions more easily and precisely. In this paper an effort has been made to introduce a model for investigating this attributes in architectural styles. So, our approach initially models the system as Discrete Time Markov Chain or DTMC, and then extracts the parameters to predict the response time of the system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5608864,no
Exploratory failure analysis of open source software,2010,"Reliability growth modeling in software system plays an important role in measuring and controlling software quality during software development. One main approach to reliability growth modeling is based on the statistical correlation of observed failure intensities versus estimated ones by the use of statistical models. Although there are a number of statistical models in the literature, this research concentrates on the following seven models: Weibull, Gamma, S-curve, Exponential, Lognormal, Cubic, and Schneidewind. The failure data collected are from five popular open source software (OSS) products. The objective is to determine which of the seven models best fits the failure data of the selected OSS products as well as predicting the future failure pattern based on partial failure history. The outcome reveals that the best model fitting the failure data is not necessarily the best predictor model.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5608887,no
Research on fast mode selection and frame algorithm of H.264,2010,"The H.264 video coding standard adopts many new coding tools, such as variable block size, multiple reference frames, quarter-pixel-accuracy motion estimation, intra prediction, loop filter, etc. Using these coding tools, H.264 achieves significant performance. Especially for reference frame, block size, and predication direction, it can adaptively do the selection. However, the encoding complexity increases tremendously. Among these tools, the macro block modes selection and the motion estimation contributes most to total encoding complexity. This paper focuses on complexity reduction in macro block modes selection. On the basis of analyzing some fast mode selection algorithms, a new algorithm is presented, in order to reduce the time of encoding, in the precondition of reducing the quality of encoding obviously.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5609485,no
Revisiting common bug prediction findings using effort-aware models,2010,"Bug prediction models are often used to help allocate software quality assurance efforts (e.g. testing and code reviews). Mende and Koschke have recently proposed bug prediction models that are effort-aware. These models factor in the effort needed to review or test code when evaluating the effectiveness of prediction models, leading to more realistic performance evaluations. In this paper, we revisit two common findings in the bug prediction literature: 1) Process metrics (e.g., change history) outperform product metrics (e.g., LOC), 2) Package-level predictions outperform file-level predictions. Through a case study on three projects from the Eclipse Foundation, we find that the first finding holds when effort is considered, while the second finding does not hold. These findings validate the practical significance of prior findings in the bug prediction literature and encourage their adoption in practice.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5609530,no
Studying the impact of dependency network measures on software quality,2010,"Dependency network measures capture various facets of the dependencies among software modules. For example, betweenness centrality measures how much information flows through a module compared to the rest of the network. Prior studies have shown that these measures are good predictors of post-release failures. However, these studies did not explore the causes for such good performance and did not provide guidance for practitioners to avoid future bugs. In this paper, we closely examine the causes for such performance by replicating prior studies using data from the Eclipse project. Our study shows that a small subset of dependency network measures have a large impact on post-release failure, while other network measures have a very limited impact. We also analyze the benefit of bug prediction in reducing testing cost. Finally, we explore the practical implications of the important network measures.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5609560,no
SQUANER: A framework for monitoring the quality of software systems,2010,"Despite the large number of quality models and publicly available quality assessment tools like PMD, Checkstyle, or FindBugs, very few studies have investigated the use of quality models by developers in their daily activities. One reason for this lack of studies is the absence of integrated environments for monitoring the evolution of software quality. We propose SQUANER (Software QUality ANalyzER), a framework for monitoring the evolution of the quality of object-oriented systems. SQUANER connects directly to the SVN of a system, extracts the source code, and perform quality evaluations and faults predictions every time a commit is made by a developer. After quality analysis, a feedback is provided to developers with instructions on how to improve their code.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5609684,no
A human study of fault localization accuracy,2010,"Localizing and repairing defects are critical software engineering activities. Not all programs and not all bugs are equally easy to debug, however. We present formal models, backed by a human study involving 65 participants (from both academia and industry) and 1830 total judgments, relating various software- and defect-related features to human accuracy at locating errors. Our study involves example code from Java textbooks, helping us to control for both readability and complexity. We find that certain types of defects are much harder for humans to locate accurately. For example, humans are over five times more accurate at locating â€œextra statementsâ€?than â€œmissing statementsâ€?based on experimental observation. We also find that, independent of the type of defect involved, certain code contexts are harder to debug than others. For example, humans are over three times more accurate at finding defects in code that provides an array abstraction than in code that provides a tree abstraction. We identify and analyze code features that are predictive of human fault localization accuracy. Finally, we present a formal model of debugging accuracy based on those source code features that have a statistically significant correlation with human performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5609691,no
An approach to improving software inspections performance,2010,Software inspections allow finding and removing defects close to their point of injection and are considered a cheap and effective way to detect and remove defects. A lot of research work has focused on understanding the sources of variability and improving software inspections performance. In this paper we studied the impact of inspection review rate in process performance. The study was carried out in an industrial context effort of bridging the gap from CMMI level 3 to level 5. We supported a decision for process change and improvement based on statistical significant information. Study results led us to conclude that review rate is an important factor affecting code inspections performance and that the applicability of statistical methods was useful in modeling and predicting process performance.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5609700,no
Physical and conceptual identifier dispersion: Measures and relation to fault proneness,2010,"Poorly-chosen identifiers have been reported in the literature as misleading and increasing the program comprehension effort. Identifiers are composed of terms, which can be dictionary words, acronyms, contractions, or simple strings. We conjecture that the use of identical terms in different contexts may increase the risk of faults. We investigate our conjecture using a measure combining term entropy and term context coverage to study whether certain terms increase the odds ratios of methods to be fault-prone. Entropy measures the physical dispersion of terms in a program: the higher the entropy, the more scattered across the program the terms. Context coverage measures the conceptual dispersion of terms: the higher their context coverage, the more unrelated the methods using them. We compute term entropy and context coverage of terms extracted from identifiers in Rhino 1.4R3 and ArgoUML 0.16. We show statistically that methods containing terms with high entropy and context coverage are more fault-prone than others.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5609748,no
Influences of different excitation parameters upon PEC testing for deep-layered defect detection with rectangular sensor,2010,"In pulsed eddy current testing, repetitive excitation signals with different parameters: duty-cycle, frequency and amplitude have different response representations. This work studies the influences of different excitation parameters on pulsed eddy current testing for deep-layered defects detection of stratified samples with rectangular sensor. The sensor had been proved to be superior in quantification and classification of defects in multi-layered structures compared with traditional circular ones. Experimental results show necessities to optimize the parameters of pulsed excitation signal, and advantages of obtaining better performances to enhance the POD of PEC testing.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5610253,no
FTA-based assessment methodology for adaptability of system under electromagnetic environment,2010,"In order to develop a methodology for assessing the adaptability of system under electromagnetic environment especially RF environment, a Fault-Tree-Analysis-based method is proposed considering the possible performance degradation, disfunction and fault. A component-level and a system-level assessment are performed. An approach to describing logical and hypotactic relation of nodes in Fault-Tree Analysis is designed and illustrated in detail. A software platform is developed to implement the system-level assessment.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5610368,no
Intelligent and innovative monitoring of water treatment plant in large Gas Refinery,2010,"A PLC can communicate through its inputs and output signals with the plant. The capabilities of PLCs have developed over the years, with reliability, performance, and operational. Developing monitoring in Automation Control System is a major industrial concern since those systems are more and more complex and involved in many safety critical application fields. The Automation system is being widely used in Power, Oil and Gas, and Petrochemicals for monitoring, advance process control and various other functions. Automation system is operated either in single mode and or it is interfaced with other systems such as PLC, SCADA, and Etc for enhanced functionality. Water treatment plants have to provide good water quality and at the same time low operational costs. Owing to various physical, chemical and biological interactions water treatment processes are often difficult to handle and reliable predictions for the course of processes are difficult to obtain. This paper focuses on an innovative and intelligent monitoring system for Water Treatment Plant that has been successfully designed, implemented and commissioned for large Gas Refinery after case study in international standards.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5611116,no
A .NET framework for an integrated fault diagnosis and failure prognosis architecture,2010,"This paper presents a .NET framework as the integrating software platform linking all constituent modules of the fault diagnosis and failure prognosis architecture. The inherent characteristics of the .NET framework provide the proposed system with a generic architecture for fault diagnosis and failure prognosis for a variety of applications. Functioning as data processing, feature extraction, fault diagnosis and failure prognosis, the corresponding modules in the system are built as .NET components that are developed separately and independently in any of the .NET languages. With the use of Bayesian estimation theory, a generic particle-filtering-based framework is integrated in the system for fault diagnosis and failure prognosis. The system is tested in two different applications - bearing spalling fault diagnosis and failure prognosis and brushless DC motor turn-to-turn winding fault diagnosis. The results suggest that the system is capable of meeting performance requirements specified by both the developer and the user for a variety of engineering systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613626,no
Simulation and measurement of back side etched inductors,2010,Deep-silicon etching was applied to an inductor in a 0.25 Î¼m SiGe:C BiCMOS process. Significant increase of quality factor was achieved. Different simulations of the inductor were done with a planar EM simulator. The way how to handle non-homogenous dielectrics in a planar EM simulator is shown. A good agreement between measurement and simulation can be seen. The effect of back side etching is well predicted by EM simulation.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613766,no
Preprocessing of Slovak Blog Articles for Clustering,2010,"Web content clustering is very important part of topic detection and tracking issue. In our paper we focus on pre-processing phase of web content clustering. We focus on blog articles published in Slovak language. We evaluate the impact of different data pre-processing methods on success of blog clustering. We found out that applying various text data manipulation techniques in preprocessing can improve the quality of clusters. The quality of clusters is measured by traditional clustering metrics like precision, recall and F-measure.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5614178,no
An Anomaly Detection Framework for Autonomic Management of Compute Cloud Systems,2010,"In large-scale compute cloud systems, component failures become norms instead of exceptions. Failure occurrence as well as its impact on system performance and operation costs are becoming an increasingly important concern to system designers and administrators. When a system fails to function properly, health-related data are valuable for troubleshooting. However, it is challenging to effectively detect anomalies from the voluminous amount of noisy, high-dimensional data. The traditional manual approach is time-consuming, error-prone, and not scalable. In this paper, we present an autonomic mechanism for anomaly detection in compute cloud systems. A set of techniques is presented to automatically analyze collected data: data transformation to construct a uniform data format for data analysis, feature extraction to reduce data size, and unsupervised learning to detect the nodes acting differently from others. We evaluate our prototype implementation on an institute-wide compute cloud environment. The results show that our mechanism can effectively detect faulty nodes with high accuracy and low computation overhead.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615245,no
System Level Hardening by Computing with Matrices,2010,"Continuous advances in transistor manufacturing have enabled technology scaling along the years, sustaining Moore's law. As transistors sizes rapidly shrink, and voltage scales, the amount of charge in a node also rapidly decreases. A particle hitting the core will probably cause a transient fault to spam over several clock cycles. In this scenario, embedded systems using state-of-the-art technologies will face the challenge of operating in an environment susceptible to multiple errors, but with restricted resources available to deploy fault-tolerance, as these techniques severely increase power consumption. One possible solution to this problem is the adoption of software based fault-tolerance at the system level, aiming at reduced energy levels to ensure reliability and low energy dissipation. In this paper, we claim the detection and correction of errors on generic data structures at system level by using matrices to encode any program and algorithm. With such encoding, it is possible to employ established techniques of detection and correction of errors occurring in matrices, running with inexpressive overhead of power and energy. We evaluated this proposal using two case studies significant for the embedded system domain. Using the proposed approach, we observed in some cases an overhead of only 5% in performance and 8% in program size.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615587,no
Simulation of High-Performance Memory Allocators,2010,"Current general-purpose memory allocators do not provide sufficient speed or flexibility for modern high-performance applications. To optimize metrics like performance, memory usage and energy consumption, software engineers often write custom allocators from scratch, which is a difficult and error-prone process. In this paper, we present a flexible and efficient simulator to study Dynamic Memory Managers (DMMs), a composition of one or more memory allocators. This novel approach allows programmers to simulate custom and general DMMs, which can be composed without incurring any additional runtime overhead or additional programming cost. We show that this infrastructure simplifies DMM construction, mainly because the target application does not need to be compiled every time a new DMM must be evaluated. Within a search procedure, the system designer can choose the ""best"" allocator by simulation for a particular target application. In our evaluation, we show that our scheme will deliver better performance, less memory usage and less energy consumption than single memory allocators.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615664,no
Software Fault Prediction Models for Web Applications,2010,"Our daily life increasingly relies on Web applications. Web applications provide us with abundant services to support our everyday activities. As a result, quality assurance for Web applications is becoming important and has gained much attention from software engineering community. In recent years, in order to enhance software quality, many software fault prediction models have been constructed to predict which software modules are likely to be faulty during operations. Such models can be utilized to raise the effectiveness of software testing activities and reduce project risks. Although current fault prediction models can be applied to predict faulty modules of Web applications, one limitation of them is that they do not consider particular characteristics of Web applications. In this paper, we try to build fault prediction models aiming for Web applications after analyzing major characteristics which may impact on their quality. The experimental study shows that our approach achieves very promising results.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615749,no
The Right Tool for the Right Job: Assessing Model Transformation Quality,2010,"Model-Driven Engineering (MDE) is a software engineering discipline in which models play a central role. One of the key concepts of MDE is model transformations. Because of the crucial role of model transformations in MDE, they have to be treated in a similar way as traditional software artifacts. They have to be used by multiple developers, they have to be maintained according to changing requirements and they should preferably be reused. It is therefore necessary to define and assess their quality. In this paper, we give two definitions for two different views on the quality of model transformations. We will also give some examples of quality assessment techniques for model transformations. The paper concludes with an argument about which type of quality assessment technique is most suitable for either of the views on model transformation quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615754,no
A Parallel Implementation Strategy of Adaptive Testing,2010,"Software testing is an important part of software development and can account for more than 50% of the development cost. In order to shorten the testing time and reduce cost, parallel mechanism is introduced to handle multiple testing tasks on the same or different software under tests (SUTs) simultaneously. On the other hand, advanced testing techniques, such as Adaptive Testing (AT) have been proposed to improve the efficiency of traditional random/partition testing. Inspired by the parallel computing technique, we present a parallel implementation of the Adaptive Testing techniques, namely the AT-P strategy. Experiments on the Space program are conducted and data indicates that the AT-P strategy can notably improve the defect detection efficiency of the original adaptive testing technique, whereas providing stable performance enhancement over multiple testing runs.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615788,no
Influence of Software-Based NIDS on FEC-Protected Uncompressed Video Transmissions,2010,"This study investigates the tradeoff between security and the usability or performance of multimedia services in the case of a software-based network intrusion detection system and uncompressed video transmissions. Uncompressed video transmissions are especially interesting in this context as they are not subjected to jitter and delays caused by compression mechanisms and can therefore be expected to be more robust when it comes to added delay variations and subsequent distortions stemming from security devices. The paper focuses on the special case where video is protected by Forward Error Correction mechanisms, which allow reordering of the original packet sequence after it may have been altered by network impairments. Various Forward Error Correction mechanisms are applied in the presence of a network intrusion detection system and its impact on the resulting video quality is investigated.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5616479,no
Simulation and measurement of back side etched inductors,2010,Deep-silicon etching was applied to an inductor in a 0.25 Î¼m SiGe:C BiCMOS process. Significant increase of quality factor was achieved. Different simulations of the inductor were done with a planar EM simulator. The way how to handle non-homogenous dielectrics in a planar EM simulator is shown. A good agreement between measurement and simulation can be seen. The effect of back side etching is well predicted by EM simulation.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5617155,no
The SQALE Analysis Model: An Analysis Model Compliant with the Representation Condition for Assessing the Quality of Software Source Code,2010,This paper presents the analysis model of the assessment method of software source code SQALE (Software Quality Assessment Based on Lifecycle Expectations). We explain what brought us to develop consolidation rules based in remediation indices. We describe how the analysis model can be implemented in practice.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5617180,no
The study of Harmonic suppression strategies based on permanent-magnet synchronous servo system,2010,"The Harmonics will make the motor generate additional losses, reduce energy efficiency and power factor, affect the normal operation of equipments, and also make the motor generate an electrical fault such as mechanical vibrations and noise, and also interfere with the nearby devices, and reduce their communication quality and EMC, EMI performances. In order to solve the existence of harmonic problems in AC servo motor control system, this paper proposes a low-cost harmonics detection method based on the DSP TMS320F2812, which also uses the corresponding harmonics suppression strategy to achieve purified control voltage and improves the stability and power efficiency of the motor.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5620495,no
An improved tone mapping algorithm for High Dynamic Range images,2010,"Real world scenes contain a large range of light intensities. To adapt to display device, High Dynamic Range (HDR) image should be converted into Low Dynamic Range (LDR) image. A common task of tone mapping algorithms is to reproduce high dynamic range images on low dynamic range display devices. In this paper, a new tone mapping algorithm is proposed for high dynamic range images. Based on the probabilistic model is proposed for high dynamic image's tone reproduction, the proposed method uses a logarithmic normal distribution instead of normal distribution. Therefore, the algorithm can preserve visibility and contrast impression of high dynamic range scenes in the common display devices. Experimental results show the superior performance of the app roach in terms of visual quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5620562,no
Agent-Oriented Designs for a Self Healing Smart Grid,2010,"Electrical grids are highly complex and dynamic systems that can be unreliable, insecure, and inefficient in serving end consumers. The promise of Smart Grids lies in the architecting and developing of intelligent distributed and networked systems for automated monitoring and controlling of the grid to improve performance. We have designed an agent-oriented architecture for a simulation which can help in understanding Smart Grid issues and in identifying ways to improve the electrical grid. We focus primarily on the self-healing problem, which concerns methodologies for activating control solutions to take preventative actions or to handle problems after they occur. We present software design issues that must be considered in producing a system that is flexible, adaptable and scalable. Agent-based systems provide a paradigm for conceptualizing, designing, and implementing software systems. Agents are sophisticated computer programs that can act autonomously and communicate with each other across open and distributed environments. We present design issues that are appropriate in developing a Multi-agent System (MAS) for the grid. Our MAS is implemented in the Java Agent Development Framework (JADE). Our Smart Grid Simulation uses many types of agents to acquire and monitor data, support decision making, and represent devices, controls, alternative power sources, the environment, management functions, and user interfaces.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5622085,no
Comparison research of two typical UML-class-diagram metrics: Experimental software engineering,2010,"Measuring UML class diagram complexity can help developers select one with lowest complexity from a variety of different designs with the same functionality; also provide guidance for developing high quality class diagrams. This paper compared the advantages and disadvantages of two typical class-diagram complexity metrics based on statistics and entropy-distance respectively from the view of newly experimental software engineering. 27 class diagrams related to the banking system were classified and predicted their understandability, analyzability and maintainability by means of algorithm C5.0 in well-known software SPSS Clementine. Results showed that UML class diagrams complexity metric based on statistics has higher classification accuracy than that based on entropy-distance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5622152,no
Development of a real-time machine vision system for detecting defeats of cord fabrics,2010,"Automatic detection techniques based on machine vision can be used in fabric industry for quality control, which constantly pursues intelligent methods to replace human inspections of product. This work introduces the principle components of a real-time machine vision system for defeat detection of cord fabrics, which is usually a challenging task in practice. The work aims at solving some difficulties usually incurring in such kind of tasks. The design and implementation of the algorithm, software and hardware are introduced. Based on the Gabor wavelet techniques, the system can automatically detect regular texture defects. Our experiments show the proposed algorithm is favorably suited for detecting several types of cord fabric defects. The system testing has been carried in both on-line and off-line situations. The corresponding results show the system has good performance with high detection accuracy, quick response and strong robustness.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5622393,no
LMI-based aircraft engine sensor fault diagnosis using a bank of robust H<inf>âˆ?/inf> filters,2010,"This paper proposes a sensor diagnostic approach based on a bank of H<sub>âˆ?/sub> filters for aircraft gas turbine engines, taking into account real-time and anti-disturbance requirements to run on an aircraft on-board computer. First of all, by defining an H<sub>âˆ?/sub> performance index to describe the disturbance attenuation performance of a dynamic system, the robust H<sub>âˆ?/sub> filter design problem has been formulated in the linear matrix inequality (LMI) framework and been efficiently solved using off-the-shelf software. Then we make use of a bank of H<sub>âˆ?/sub> filters for the sensor fault detection and isolation based on the logic status of a group of residuals. Finally, an illustrative simulation has been given to verify the effectiveness of the proposed design method. The benefit of this paper is to bridge the gap between the H<sub>âˆ?/sub> filter theory and engineering applications for reliable diagnostics of aircraft engines.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5622577,no
The study of virtual testing design and analysis method on warhead,2010,"Virtual experiment is an approach to utilize numerical computation to study the complex physics processes, and is used widely in the areas of weapon system development and performance evaluation. The virtual experiment for warhead aims at studying various conditions of damage on target by warhead, analyzing the factors of warhead power, and provide the basis for the warhead design, with high quality cluster systems and high precise FEA software. This paper is based on virtual experiment over the shaped-charge warhead, and study the virtual experiment design and analysis methods.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5623084,no
Practical Aspects in Analyzing and Sharing the Results of Experimental Evaluation,2010,"Dependability evaluation techniques such as the ones based on testing, or on the analysis of field data on computer faults, are a fundamental process in assessing complex and critical systems. Recently a new approach has been proposed consisting in collecting the row data produced in the experimental evaluation and store it in a multidimensional data structure. This paper reports the work in progress activities of the entire process of collecting, storing and analyzing the experimental data in order to perform a sound experimental evaluation. This is done through describing the various steps on a running example.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5623408,no
Quantifying Resiliency of IaaS Cloud,2010,"Cloud based services may experience changes - internal, external, large, small - at any time. Predicting and quantifying the effects on the quality-of-service during and after a change are important in the resiliency assessment of a cloud based service. In this paper, we quantify the resiliency of infrastructure-as-a-service (IaaS) cloud when subject to changes in demand and available capacity. Using a stochastic reward net based model for provisioning and servicing requests in a IaaS cloud, we quantify the resiliency of IaaS cloud w.r.t. two key performance measures - job rejection rate and provisioning response delay.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5623413,no
A Multi-step Simulation Approach toward Secure Fault Tolerant System Evaluation,2010,"As new techniques of fault tolerance and security emerge, so does the need for suitable tools to evaluate them. Generally, the security of a system can be estimated and verified via logical test cases, but the performance overhead of security algorithms on a system needs to be numerically analyzed. The diversity in security methods and design of fault tolerant systems make it impossible for researchers to come up with a standard, affordable and openly available simulation tool, evaluation framework or an experimental test-bed. Therefore, researchers choose from a wide range of available modeling-based, implementation-based or simulation-based approaches in order to evaluate their designs. All of these approaches have certain merits and several drawbacks. For instance, development of a system prototype provides a more accurate system analysis but unlike simulation, it is not highly scalable. This paper presents a multi-step, simulation-based performance evaluation methodology for secure fault tolerant systems. We use a divide-and-conquer approach to model the entire secure system in a way that allows the use of different analytical tools at different levels of granularity. This evaluation procedure tries to strike a balance between the efficiency, effort, cost and accuracy of a system's performance analysis. We demonstrate this approach in a step-by-step manner by analyzing the performance of a secure and fault tolerant system using a JAVA implementation in conjunction with the ARENA simulation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5623417,no
CCDA: Correcting control-flow and data errors automatically,2010,"This paper presents an efficient software technique to detect and correct control-flow errors through addition of redundant codes in a given program. The key innovation performed in the proposed technique is detection and correction of the control-flow errors using both control-flow graph and data-flow graph. Using this technique, most of control-flow errors in the program are detected first, and next corrected, automatically; so, both errors in the control-flow and program data which is caused by control-flow errors can be corrected. In order to evaluate the proposed technique, a post compiler is used, so that the technique can be applied to every 80Ã—86 binaries, transparently. Three benchmarks quick sort, matrix multiplication and linked list are used, and a total of 5000 transient faults are injected on several executable points in each program. The experimental results demonstrate that at least 93% of the control-flow errors can be detected and corrected by the proposed technique automatically without any data error generation. Moreover, the performance and memory overheads of the technique are noticeably less than traditional techniques.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5623537,no
Measuring web service interfaces,2010,"The following short paper describes a tool supported method for measuring web service interfaces. The goal is to assess the complexity and quality of these interfaces as well as to determine their size for estimating evolution and testing effort. Besides the metrics for quantity, quality and complexity, rules are defined for ensuring maintainability. In the end a tool - WSDAudit - is described which the author has developed for the static analysis of web service definitions. The WSDL schemas are automatically audited and measured for quality assurance and cost estimation. Work is underway to verify them against the BPEL procedures from which they are invoked.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5623580,no
Routing metric estimation in multi-hop wireless networks with fading channel,2010,"In this paper we describe a statistical analysis of a typical routing metric, Expected Transmission Time (ETT), used for path evaluation in multi-hop wireless networks. The need of statistics description is linked with the presence of network parameters uncertainty due to both a fading channel and ambiguity of nodes position. This analysis led us to define a statistical estimator, which allows controlling all these effects, in order to improve the quality of route choice during routing decisions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5623697,no
Quality of signaling (QoSg) metrics for evaluating SIP transaction performance,2010,"Service quality and user-perceived experience has been a networking research topic for many years. Whereas most related work concentrates on the quality of media, only very few papers, however, discuss the performance of the signaling for those media. This paper addresses the performance of the Session Initiation Protocol (SIP) which is very popular with Next Generation Networks (NGN). A collection of metrics, the so-called Quality of Signalling (QoSg), is introduced for measuring SIP signaling delays. In contrast to traditional end-to-end performance metrics, QoSg aims at evaluating individual SIP transactions. Therefore, this approach can be applied anywhere in the SIP network and allows for additional insight, for instance regarding congestion detection or SLA monitoring.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5623698,no
Application of a new fast-responding estimation approach in DVR to mitigate voltage sags in harmonic distortion conditions,2010,"The main duty of a Dynamic Voltage Restorer (DVR) is protection of sensitive loads against voltage disturbances, especially voltage sags and swells. DVR should possess necessary response time in restoring load bus voltage to avoid load interruptions. One of the most important factors that affects the performance of DVR is the estimation approach which is used in control unit of this compensator. This paper presents a new fast-responding algorithm for the on-line estimation of symmetrical components in harmonic distortion conditions, which provides adequate response time and accuracy for dynamic voltage restorer. Furthermore, performance of this approach would be compared with some well-known estimation techniques. Application of the proposed estimation approach is realized in DVR by representing a new control unit for compensation of different voltage sags and swells. Finally, the simulation results, which obtained using PSCAD/EMTDC software package confirm efficiency of the suggested control unit.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5624735,no
"Monitoring and unbalance mitigation of harmonic current symmetrical components, using double Complex-ADALINE algorithm",2010,"Harmonics tracking as well as symmetrical components identification are of the great importance in Power Quality (PQ) evaluation and improvement. In this paper, a new approach for fast and accurate estimation of fundamental and harmonic symmetrical components is presented. Firstly, the three-phase input signals are simplified into a real-imaginary system using Park vector. Then, the resulted signal is fed to a Complex-ADALINE (C-ADALINE) structure for the estimation of symmetrical components of both fundamental and harmonics of the input. Also, zero sequences of the harmonics are extracted by another C-ADALINE unit. For simplicity, both of the C-ADALINE structures are organized in one computational system, which is called Double C-ADALINE. Finally, two case studies are conducted on MATLAB software package to investigate the performance of the suggested technique and compare it with conventional ADALINE. The simulation results indicate that the proposed system shows an excellent performance in monitoring the harmonics and symmetrical components in condition of dynamic changes of artificial three-phase signals, as well as control strategy in a D-STATCOM to harmonic cancellation, unbalance mitigation and reactive power compensation in a test distribution network.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5624736,no
"New phasor estimator in the presence of harmonics, dc offset, and interharmonics",2010,"This paper proposes the use of Artificial Neural Networks (ANN) to estimate the magnitude and phase of fundamental component of sinusoidal signals in the presence of harmonics, sub-harmonics and DC offset. The proposed methodology uses a preprocessing that is able to generate a signal that represents the influence of sub-harmonics and DC offset in the fundamental component. This signal is used as input of ANN, which estimate the influence of that signal in quadrature components. Using this information, corrections can be made in quadrature components then the real value of phasor of the fundamental component is estimated. The performance of the proposed algorithm was compared with classical methods such as DFT, using one and two cycles, and LES. The results showed that the proposed method is accurate and fast. The methodology can be used as a phasor estimator of system with poor Power Quality indices for monitoring, control and protection applications.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5625393,no
Motor function assessment using wearable inertial sensors,2010,"We present an approach to wearable sensor-based assessment of motor function in individuals post stroke. We make use of one on-body inertial measurement unit (IMU) to automate the functional ability (FA) scoring of the Wolf Motor Function Test (WMFT). WMFT is an assessment instrument used to determine the functional motor capabilities of individuals post stroke. It is comprised of 17 tasks, 15 of which are rated according to performance time and quality of motion. We present signal processing and machine learning tools to estimate the WMFT FA scores of the 15 tasks using IMU data. We treat this as a classification problem in multidimensional feature space and use a supervised learning approach.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5626156,no
Automatic detection of schwalbe's line in the anterior chamber angle of the eye using HD-OCT images,2010,"Angle-closure glaucoma is a major cause of blindness in Asia and could be detected by measuring the anterior chamber angle (ACA) using gonioscopy, ultrasound biomicroscopy or anterior segment (AS) optical coherence tomography (OCT). The current software in the VisanteTM OCT system by Zeiss is based on manual labeling of the scleral spur, cornea and iris and is a tedious process for ophthalmologists. Furthermore, the scleral spur can not be identified in about 20% to 30% of OCT images and thus measurements of the ACA are not reliable. However, high definition (HD) OCT has identified a more consistent landmark: Schwalbe's line. This paper presents a novel algorithm which automatically detects Schwalbe's line in HD-OCT scans. The average deviation between the values detected using our algorithm and those labeled by the ophthalmologist is less than 0.5% and 0.35% in the horizontal and vertical image dimension, respectively. Furthermore, we propose a new measurement to quantify ACA which is defined as Schwalbe's line bounded area (SLBA).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5626167,no
Quality factors of business value and service level measurement for SOA,2010,"This paper presents the quality factors of web services with definition, classification, and sub-factors for business and service level measurement. Web services, main enabler for SOA, usually have characteristics distinguished from general stand-alone software. They are provided in different ownership domain by network. They can also adopt various binding mechanism and be loosely-coupled, platform independent, and use standard protocols. As a result, a web service system requires its own quality factors unlike installation-based software. For instance, as the quality of web services can be altered in real-time according to the change of the service provider, considering real-time property of web services is very meaningful in describing the web services quality. This paper focuses especially to the two quality factors: business value and service level measurement, affecting real world business activities and a service performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5626604,no
Automatic code generation for solvers of cardiac cellular membrane dynamics in GPUs,2010,"The modeling of the electrical activity of the heart is of great medical and scientific interest, as it provides a way to get a better understanding of the related biophysical phenomena, allows the development of new techniques for diagnoses and serves as a platform for drug tests. However, due to the multi-scale nature of the underlying processes, the simulations of the cardiac bioelectric activity are still a computational challenge. In addition to that, the implementation of these computer models is a time consuming and error prone process. In this work we present a tool for prototyping ordinary differential equations (ODEs) in the area of cardiac modeling that aim to provide the automatic generation of high performance solvers tailored to the new hardware architecture of the graphic processing units (GPUs). The performance of these automatic solvers was evaluated using four different cardiac myocyte models. The GPU version of the solvers were between 75 and 290 times faster than the CPU versions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5626620,no
SeyeS - support system for preventing the development of ocular disabilities in leprosy,2010,"Leprosy is an infectious disease caused by Mycobacterium Leprae, and generally compromises neural fibers, leading to the development of disabilities. These limit daily activities or social life. In leprosy, the study of disability considered functional (physical) and activity limitations; and social participation. These are measured respectively by EHF and SALSA scales; by and PARTICIPATION SCALE. The objective of this work was to propose a support system, SeyeS, to eyes disabilities development and progression identification, applying Bayesians network - BN's. It is expected that the proposed system be applied in monitoring the patient during treatment and after therapeutic cure of leprosy. SeyeS presented specificity 1 and sensitivity 0.6 in the identification of ocular disabilities development. With Seyes was discovered that the presence of trichiasis and lagophthalmos, tend to increase the probability of developing more disabilities. Otherwise, characteristics as cataracts tend to decrease development of other disabilities, considering that medical interventions could reduce it. The more import of this system is to indicate what should be monitored, and which elements needs interventions to not increasing patient's ocular disabilities.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5627769,no
An Adaptive Failure Detection Method with Controllable QoS for Distributed Storage System,2010,"According to the situation that different application asks for different requirement of storage node's performance, an adaptive failure detection method with controllable QoS for distributed storage system is proposed. The delay characteristic of heartbeat message is summarized. The relational expressions of neighbor heartbeat message's arrival time in the different network state are established. The QoS parameters formulas of different network state are proposed. The controllability of QoS parameters is analyzed. The algorithm of the method is designed. The experimental results show that the method realizes the balance control between detection speed and detection accuracy by setting safe remaining time with a certain value, and has adaptability that it can have a high detection speed but fewer mistakes.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5629521,no
Hybrid Probabilistic Relational Models for System Quality Analysis,2010,"The formalism Probabilistic Relational Models (PRM) couples discrete Bayesian Networks with a modeling formalism similar to UML class diagrams and has been used for architecture analysis. PRMs are well-suited to perform architecture analysis with respect to system qualities since they support both modeling and analysis within the same formalism. A particular strength of PRMs is the ability to perform meaningful analysis of domains where there is a high level of uncertainty, as is often the case when performing system quality analysis. However, the use of discrete Bayesian networks in PRMs complicates the analysis of continuous phenomena. The main contribution of this paper is the Hybrid Probabilistic Relational Models (HPRM) formalism which extends PRMs to enable continuous analysis thus extending the applicability for architecture analysis and especially for trade-off analysis of system qualities. HPRMs use hybrid Bayesian networks which allow combinations of discrete and continuous variables. In addition to presenting the HPRM formalism, the paper contains an example which details the use of HPRMs for architecture trade-off analysis.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5630235,no
Novel Quality Measures for Image Fusion Based on Structural Similarity and Visual Attention Mechanism,2010,"A novel objective quality for image fusion based on structural similarity and visual attention mechanism (VAM) is presented. By giving higher weight to the salient areas in the input images, the quality measure can estimate how much visual meaningful information is preserved in the fused image. The correlation analysis between objective measure and subjective evaluation showed that our measures are more consistent with human subjective evaluation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5630969,no
Predicting Faults in High Assurance Software,2010,"Reducing the number of latent software defects is a development goal that is particularly applicable to high assurance software systems. For such systems, the software measurement and defect data is highly skewed toward the not-fault-prone program modules, i.e., the number of fault-prone modules is relatively very small. The skewed data problem, also known as class imbalance, poses a unique challenge when training a software quality estimation model. However, practitioners and researchers often build defect prediction models without regard to the skewed data problem. In high assurance systems, the class imbalance problem must be addressed when building defect predictors. This study investigates the roughly balanced bagging (RBBag) algorithm for building software quality models with data sets that suffer from class imbalance. The algorithm combines bagging and data sampling into one technique. A case study of 15 software measurement data sets from different real-world high assurance systems is used in our investigation of the RBBag algorithm. Two commonly used classification algorithms in the software engineering domain, Naive Bayes and C4.5 decision tree, are combined with RBBag for building the software quality models. The results demonstrate that defect prediction models based on the RBBag algorithm significantly outperform models built without any bagging or data sampling. The RBBag algorithm provides the analyst with a tool for effectively addressing class imbalance when training defect predictors during high assurance software development.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5634306,no
A Framework for Qualitative and Quantitative Formal Model-Based Safety Analysis,2010,"In model-based safety analysis both qualitative aspects i.e. what must go wrong for a system failure) and quantitative aspects (i.e. how probable is a system failure) are very important. For both aspects methods and tools are available. However, until now for each aspect new and independent models must be built for analysis. This paper proposes the SAML framework as a formal foundation for both qualitative and quantitative formal model-based safety analysis. The main advantage of SAML is the combination of qualitative and quantitative formal semantics which allows different analyses on the same model. This increases the confidence in the analysis results, simplifies modeling and is less error-prone. The SAML framework is tool-independent. As proof-of-concept, we present sound transformation of the formalism into two state of the art model-checking notations. Prototypical tool support for the sound transformation of SAML into PRISM and MRMC for probabilistic analysis as well as different variants of the SMV model checker for qualitative analysis is currently being developed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5634319,no
Request Path Driven Model for Performance Fault Diagnoses,2010,"Locating and diagnosing performance faults in distributed systems is crucial but challenging. Distributed systems are increasingly complex, full of various correlation and dependency, and exhibit dramatic dynamics. All these made traditional approaches prone to high false alarms. In this paper, we propose a novel system modeling technique, which encodes component's dynamic dependencies and behavior characteristics into system's meta-model and takes it as a unifying framework to deploy component's sub-models. We propose an automatic analyze approach to distill, from request travel paths, request path signatures, the essential information of component's dynamic behaviors, and use it to induce metamodel with Bayesian network, and then use the model to make fault location and diagnoses. We take up fault-injection experiments with RUBiS, a TPCW alike benchmark, simulating eBay.com. The results indicate that our model approach provides effective problem diagnosis, i.e., Bayesian network technique is effective for fault detecting and pinpointing, in terms of request tracing context. Moreover, meta-model induced with request paths, provides an effective guidance for learning statistical correlations among metrics across the system, which effectively avoid 'false alarms' in fault pinpointing. As a case study, we construct a proactive recovery framework, which integrate our system modeling technique with software rejuvenation technique to guarantee system's quality of services.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5634347,no
Fault Diagnosis System of Locomotive Axle's Track Based on Virtual Instrument,2010,"Railway vehicle is usually working in hazardous environment. The operational status of axle, which is the railway vehicle's main components to run, affects the safe operation of the railway vehicle directly. Over the years, the railway of our country has remained low equipment-rate, high using-rate and high intensity-transport. In addition, because of the need of the rapid development of economy, our country has raised the speed of the railway vehicle, which leads to the railway vehicle's axle wearing and tearing seriously, the life shortened and accidents increasing significantly. So, it is very necessary and urgent to develop the detection system because axle is related to the safety of the railway vehicle. Compared with traditional instrument, the virtual instrument has characteristics of openness, easy using and high cost performance. It has already been widely used in detection systems at present. Based on LabVIEW which is a developing platform software of NI Company, the paper has designed a kind of fault diagnosis system on locomotive axle's track.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5634722,no
Empirical Evaluation of Factors Affecting Distinction between Failing and Passing Executions,2010,"Information captured in software execution profiles can benefit verification activities by supporting more cost-effective fault localization and execution classification. This paper proposes an experimental design which utilizes execution information to quantify the effect of factors such as different programs and fault inclusions on the distinction between passed and failed execution profiles. For this controlled experiment we use well-known, benchmark-like programs. In addition to experimentation, our empirical evaluation includes case studies of open source programs having more complex fault models. The results show that metrics reflecting distinction between failing and passing executions are affected more by program than by faults included.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635050,no
The Impact of Coupling on the Fault-Proneness of Aspect-Oriented Programs: An Empirical Study,2010,"Coupling in software applications is often used as an indicator of external quality attributes such as fault-proneness. In fact, the correlation of coupling metrics and faults in object oriented programs has been widely studied. However, there is very limited knowledge about which coupling properties in aspect-oriented programming (AOP) are effective indicators of faults in modules. Existing coupling metrics do not take into account the specificities of AOP mechanisms. As a result, these metrics are unlikely to provide optimal predictions of pivotal quality attributes such as fault-proneness. This impacts further by restraining the assessments of AOP empirical studies. To address these issues, this paper presents an empirical study to evaluate the impact of coupling sourced from AOP-specific mechanisms. We utilise a novel set of coupling metrics to predict fault occurrences in aspect-oriented programs. We also compare these new metrics against previously proposed metrics for AOP. More specifically, we analyse faults from several releases of three AspectJ applications and perform statistical analyses to reveal the effectiveness of these metrics when predicting faults. Our study shows that a particular set of fine-grained directed coupling metrics have the potential to help create better fault prediction models for AO programs.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635061,no
On the Round Trip Path Testing Strategy,2010,"A number of techniques have been proposed for state-based testing. One well-known technique (criterion) is to traverse the graph representing the state machine and generate a so-called transition tree, in an attempt to exercise round trip paths, i.e., paths that start and end in the same state without any other repeating state. Several hypotheses are made when one uses this criterion: exercising paths in the tree, which do not always trigger complete round trip paths, is equivalent to covering round-trip paths; different traversal algorithms are equivalent. In this paper we investigate whether these assumptions hold in practice, and if they do not, we investigate their consequences. Results also lead us to propose a new transition tree construction algorithm that results in higher efficiency and lower cost.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635073,no
Improving the Precision of Dependence-Based Defect Mining by Supervised Learning of Rule and Violation Graphs,2010,"Previous work has shown that application of graph mining techniques to system dependence graphs improves the precision of automatic defect discovery by revealing subgraphs corresponding to implicit programming rules and to rule violations. However, developers must still confirm, edit, or discard reported rules and violations, which is both costly and error-prone. In order to reduce developer effort and further improve precision, we investigate the use of supervised learning models for classifying and ranking rule and violation subgraphs. In particular, we present and evaluate logistic regression models for rules and violations, respectively, which are based on general dependence-graph features. Our empirical results indicate that (i) use of these models can significantly improve the precision and recall of defect discovery, and (ii) our approach is superior to existing heuristic approaches to rule and violation ranking and to an existing static-warning classifier, and (iii) accurate models can be learned using only a few labeled examples.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635105,no
A Multi-factor Software Reliability Model Based on Logistic Regression,2010,"This paper proposes a multi-factor software reliability model based on logistic regression and its effective statistical parameter estimation method. The proposed parameter estimation algorithm is composed of the algorithm used in the logistic regression and the EM (expectation-maximization) algorithm for discrete-time software reliability models. The multi-factor model deals with the metrics observed in testing phase (testing environmental factors), such as test coverage and the number of test workers, to predict the number of residual faults and other reliability measures. In general, the multi-factor model outperforms the traditional software reliability growth model like discrete-time non-homogeneous models in terms of data-fitting and prediction abilities. However, since it has a number of parameters, there is the problem in estimating model parameters. Our modeling framework and its estimation method are quite simpler than the existing methods, and are promising for expanding the applicability of multi-factor software reliability model. In numerical experiments, we examine data-fitting ability of the proposed model by comparing with the existing multi-factor models. The proposed method provides the similar fitting ability to existing multi-factor models, although the computation effort of parameter estimation is low.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635114,no
"Flexible, Any-Time Fault Tree Analysis with Component Logic Models",2010,"This article presents a novel approach to facilitating fault tree analysis during the development of software-controlled systems. Based on a component-oriented system model, it combines second-order probabilistic analysis and automatically generated default failure models with a level-of-detail concept to ensure early and continuous analysability of system failure behaviour with optimal effort, even in the presence of incomplete information and dissimilar levels of detail in different parts of an evolving system model. The viability and validity of the method are demonstrated by means of an experiment.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635118,no
Towards a Bayesian Approach in Modeling the Disclosure of Unique Security Faults in Open Source Projects,2010,"Software security has both an objective and a subjective component. A lot of the information available about that today is focused on the security vulnerabilities and their disclosure. It is less frequent that security breaches and failures rates are reported, even in open source projects. Disclosure of security problems can take several forms. A disclosure can be accompanied by a release of the fix for the problem, or not. The latter category can be further divided into â€voluntaryâ€?and â€involuntaryâ€?security issues. In widely used software there is also considerable variability in the operational profile under which the software is used. This profile is further modified by attacks on the software that may be triggered by security disclosures. Therefore a comprehensive model of software security qualities of a product needs to incorporate both objective measures, such as security problem disclosure, repair and, failure rates, as well as less objective metrics such as implied variability in the operational profile, influence of attacks, and subjective impressions of exposure and severity of the problems, etc. We show how a classical Bayesian model can be adapted for use in the security context. The model is discussed and assessed using data from three open source software projects. Our results show that the model is suitable for use with a certain subset of disclosed security faults, but that additional work will be needed to identify appropriate shape and scaling functions that would accurately reflect end-user perceptions associated with security problems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635125,no
A Consistency Check Algorithm for Component-Based Refinements of Fault Trees,2010,"The number of embedded systems in our daily lives that are distributed, hidden, and ubiquitous continues to increase. Many of them are safety-critical. To provide additional or better functionalities, they are becoming more and more complex, which makes it difficult to guarantee safety. It is undisputed that safety must be considered before the start of development, continue until decommissioning, and is particularly important during the design of the system and software architecture. An architecture must be able to avoid, detect, or mitigate all dangerous failures to a sufficient degree. For this purpose, the architectural design must be guided and verified by safety analyses. However, state-of-the-art component-oriented or model-based architectural design approaches use different levels of abstraction to handle complexity. So, safety analyses must also be applied on different levels of abstraction, and it must be checked and guaranteed that they are consistent with each other, which is not supported by standard safety analyses. In this paper, we present a consistency check for CFTs that automatically detects commonalities and inconsistencies between fault trees of different levels of abstraction. This facilitates the application of safety analyses in top-down architectural designs and reduces effort.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635142,no
Using Search Methods for Selecting and Combining Software Sensors to Improve Fault Detection in Autonomic Systems,2010,"Fault-detection approaches in autonomic systems typically rely on runtime software sensors to compute metrics for CPU utilization, memory usage, network throughput, and so on. One detection approach uses data collected by the runtime sensors to construct a convex-hull geometric object whose interior represents the normal execution of the monitored application. The approach detects faults by classifying the current application state as being either inside or outside of the convex hull. However, due to the computational complexity of creating a convex hull in multi-dimensional space, the convex-hull approach is limited to a few metrics. Therefore, not all sensors can be used to detect faults and so some must be dropped or combined with others. This paper compares the effectiveness of genetic-programming, genetic-algorithm, and random-search approaches in solving the problem of selecting sensors and combining them into metrics. These techniques are used to find 8 metrics that are derived from a set of 21 available sensors. The metrics are used to detect faults during the execution of a Java-based HTTP web server. The results of the search techniques are compared to two hand-crafted solutions specified by experts.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635154,no
A Quantitative Approach to Software Maintainability Prediction,2010,"Software maintainability is one important aspect in the evaluation of software evolution of a software product. Due to the complexity of tracking maintenance behaviors, it is difficult to accurately predict the cost and risk of maintenance after delivery of software products. In an attempt to address this issue quantitatively, software maintainability is viewed as an inevitable evolution process driven by maintenance behaviors, given a health index at the time when a software product are delivered. A Hidden Markov Model (HMM) is used to simulate the maintenance behaviors shown as their possible occurrence probabilities. And software metrics is the measurement of the quality of a software product and its measurement results of a product being delivered are combined to form the health index of the product. The health index works as a weight on the process of maintenance behavior over time. When the occurrence probabilities of maintenance behaviors reach certain number which is reckoned as the indication of the deterioration status of a software product, the product can be regarded as being obsolete. Longer the time, better the maintainability would be.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635174,no
Search-based Prediction of Fault-slip-through in Large Software Projects,2010,"A large percentage of the cost of rework can be avoided by finding more faults earlier in a software testing process. Therefore, determination of which software testing phases to focus improvements work on, has considerable industrial interest. This paper evaluates the use of five different techniques, namely particle swarm optimization based artificial neural networks (PSO-ANN), artificial immune recognition systems (AIRS), gene expression programming (GEP), genetic programming (GP) and multiple regression (MR), for predicting the number of faults slipping through unit, function, integration and system testing phases. The objective is to quantify improvement potential in different testing phases by striving towards finding the right faults in the right phase. We have conducted an empirical study of two large projects from a telecommunication company developing mobile platforms and wireless semiconductors. The results are compared using simple residuals, goodness of fit and absolute relative error measures. They indicate that the four search-based techniques (PSO-ANN, AIRS, GEP, GP) perform better than multiple regression for predicting the fault-slip-through for each of the four testing phases. At the unit and function testing phases, AIRS and PSO-ANN performed better while GP performed better at integration and system testing phases. The study concludes that a variety of search-based techniques are applicable for predicting the improvement potential in different testing phases with GP showing more consistent performance across two of the four test phases.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635180,no
A Self-Adaptive and Fast Motion Estimation Search Method for H.264/AVC,2010,"H.264/AVC is the outstanding and significant video compression standard developed by ITU-T/ISO/IEC Joint Video Team. Motion estimation (ME) plays a key role in H.264/AVC, it concerns greatly on computational complexity especially when using the full search (FS) algorithm. Although many fast ME algorithms have been proposed to reduce the huge calculation complexity instead of FS, the ME still can not satisfy the critical real-time application's needs. In this paper, a fast integer pixel variable block ME algorithm based on JVT which accepted UMHexagonS algorithm is presented for H.264/AVC encoder. With special considerations on the motion activity of the current macro-block, several adaptive search strategies have been utilized to significantly improve the video coding performance. The simulation results shows that the proposed algorithm maintains an unnoticeable quality loss in terms of PSNR on average compared with FS and reduces nearly 20% ME time compared with UMHexagonS while maintaining coding efficiency.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5636210,no
A Three-Phases Byzantine Fault Tolerance Mechanism for HLA-Based Simulation,2010,"A large scale HLA-based simulation (federation) is composed of a large number of simulation components (federates), which may be developed by different participants and executed at different locations. Byzantine failures, caused by malicious attacks and software/hardware bugs, might happen to federates and propagate in the federation execution. In this paper, a three-phases (i.e., failure detection, failure location, and failure recovery) Byzantine Fault Tolerance (BFT) mechanism is proposed based on the decoupled federate architecture. By combining the replication, check pointing and message logging techniques, some redundant executions of federate replicas are avoided. The BFT mechanism is implemented using both Barrier and No-Barrier federate replication structures. Protocols are also developed to remove the epidemic effect caused by Byzantine failures. As the experiment results show, the BFT mechanism using No-Barrier replication outperforms that using Barrier replication significantly in the case that federate replicas have different runtime performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5636701,no
Rapid design optimisation of microwave structures through automated tuning space mapping,2010,"Tuning space mapping (TSM) is one of the latest developments in space mapping technology. TSM algorithms offer a remarkably fast design optimisation with satisfactory results obtained after one or two iterations, which amounts to just a few electromagnetic simulations of the optimised microwave structure. The TSM algorithms (as exemplified by `Type-1` tuning) could be simply implemented manually. The approach may require interaction between various electromagnetic-based and circuit models, as well as handling different sets of design variables and control parameters. As a result, certain TSM algorithms (especially so-called `Type-0` tuning) may be tedious, thus, error-prone to implement. Here, we present a fully automated tuning space mapping implementation that exploits the functionality of our user-friendly space mapping software, the SMF system. The operation and performance of our new implementation is illustrated through the design of a box-section Chebyshev bandpass filter and a capacitively coupled dual-behaviour resonator filter.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5639189,no
A clustering algorithm for software fault prediction,2010,"Software metrics are used for predicting whether modules of software project are faulty or fault free. Timely prediction of faults especially accuracy or computation faults improve software quality and hence its reliability. As we can apply various distance measures on traditional K-means clustering algorithm to predict faulty or fault free modules. Here, in this paper we have proposed K-Sorensen-means clustering that uses Sorensen distance for calculating cluster distance to predict faults in software projects. Proposed algorithm is then trained and tested using three datasets namely, JM1, PCI and CM1 collected from NASA MDP. From these three datasets requirement metrics, static code metrics and alliance metrics (combining both requirement metrics and static code metrics) have been built and then K-Sorensen-means applied on all datasets to predict results. Alliance metric model is found to be the best prediction model among three models. Results of K-Sorensen-means clustering shown and corresponding ROC curve has been drawn. Results of K-Sorensen-means are then compared with K-Canberra-means clustering that uses other distance measure for evaluating cluster distance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5640474,no
A stochastic model for performance evaluation and bottleneck discovering on SOA-based systems,2010,"The Service-Oriented Architecture (SOA) has become a unifying technical architecture that may be embodied through Web Service technologies. Predicting the variable behavior of SOA systems can mean a way to improve the quality of the business transactions. This paper proposes a simulation modeling approach based on stochastic Petri nets to estimate the performance of SOA-applications. Using the proposed model is possible to predict resource consumption and service levels degradation in scenarios with different compositions and workloads, even before developing the application. A case study was conducted in order to validate our approach, comparing its accuracy with the results from an analytical model existent in the literature.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5641802,no
Designing building automation systems using evolutionary algorithms with semi-directed variations,2010,"In the building automation domain, many prefabricated devices from different manufacturers available in the market realize building automation functions by preprogrammed software components. For given design requirements, the existence of a high number of devices that realize the required functions leads to a combinatorial explosion of design alternatives at different price and quality levels. Finding optimal design alternatives is a hard problem to which we approach with a multi-objective evolutionary algorithm. By integrating problem-specific knowledge into variation operations, a promisingly high optimization performance can be achieved. To realize this, diverse variation operations related to goals are defined upon a classification for the exploration and convergence behavior, and applied in different strategies.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5641996,no
Dynamic parameter control of interactive local search in UML software design,2010,"User-centered Interactive Evolutionary Computation (IEC) has been applied to a wide variety of areas, including UML software design. The performance of evolutionary search is important as user interaction fatigue remains an on-going challenge in IEC. However, to obtain optimal search performance, it is usually necessary to â€œtuneâ€?evolutionary control parameters manually, although tuning control parameters can be time-consuming and error-prone. To address this issue in other fields of evolutionary computation, dynamic parameter control including deterministic, adaptive and self-adaptive mechanisms have been applied extensively to real-valued representations. This paper postulates that dynamic parameter control may be highly beneficial to IEC in general, and UML software design in particular, wherein a novel object-based solution representation is used. Three software design problems from differing design domains and of differing scale have been investigated with mutation probabilities modified by simulated annealing, the Rechenberg â€?/5 success ruleâ€?and self-adaptation within local search. Results indicate that self-adaptation appears to be the most robust and scalable mutation probability modification mechanism. The use of self-adaption with an object-based representation is novel, and results indicate that dynamic parameter control offers great potential within IEC.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5642479,no
Reverse Engineering Utility Functions Using Genetic Programming to Detect Anomalous Behavior in Software,2010,"Recent studies have shown the promise of using utility functions to detect anomalous behavior in software systems at runtime. However, it remains a challenge for software engineers to hand-craft a utility function that achieves both a high precision (i.e., few false alarms) and a high recall (i.e., few undetected faults). This paper describes a technique that uses genetic programming to automatically evolve a utility function for a specific system, set of resource usage metrics, and precision/recall preference. These metrics are computed using sensor values that monitor a variety of system resources (e.g., memory usage, processor usage, thread count). The technique allows users to specify the relative importance of precision and recall, and builds a utility function to meet those requirements. We evaluated the technique on the open source Jigsaw web server using ten resource usage metrics and five anomalous behaviors in the form of injected faults in the Jigsaw code and a security attack. To assess the effectiveness of the technique, the precision and recall of the evolved utility function was compared to that of a hand-crafted utility function that uses a simple thresholding scheme. The results show that the evolved function outperformed the hand-crafted function by 10 percent.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5645446,no
"Design, Modeling, and Evaluation of a Scalable Multi-level Checkpointing System",2010,"High-performance computing (HPC) systems are growing more powerful by utilizing more hardware components. As the system mean-time-before-failure correspondingly drops, applications must checkpoint more frequently to make progress. However, as the system memory sizes grow faster than the bandwidth to the parallel file system, the cost of checkpointing begins to dominate application run times. Multi-level checkpointing potentially solves this problem through multiple types of checkpoints with different costs and different levels of resiliency in a single run. This solution employs lightweight checkpoints to handle the most common failure modes and relies on more expensive checkpoints for less common, but more severe failures. This theoretically promising approach has not been fully evaluated in a large- scale, production system context. We have designed the Scalable Checkpoint/Restart (SCR) library, a multi-level checkpoint system that writes checkpoints to RAM, Flash, or disk on the compute nodes in addition to the parallel file system. We present the performance and reliability properties of SCR as well as a probabilistic Markov model that predicts its performance on current and future systems. We show that multi-level checkpointing improves efficiency on existing large-scale systems and that this benefit increases as the system size grows. In particular, we developed low-cost checkpoint schemes that are 100x-1000x faster than the parallel file system and effective against 85% of our system failures. This leads to a gain in machine efficiency of up to 35%, and it reduces the the load on the parallel file system by a factor of two on current and future systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5645453,no
Linguistic Driven Refactoring of Source Code Identifiers,2010,"Identifiers are an important source of information during program understanding and maintenance. Programmers often use identifiers to build their mental models of the software artifacts. We have performed a preliminary study to examine the relation between the terms in identifiers, their spread in entities, and fault proneness. We introduced term entropy and context-coverage to measure how scattered terms are across program entities and how unrelated are the methods and attributes containing these terms. Our results showed that methods and attributes containing terms with high entropy and context-coverage are more fault-prone. We plan to build on this study by extracting linguistic information form methods and classes. Using this information, we plan to establish traceability link from domain concepts to source code, and to propose linguistic based refactoring.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5645489,no
Improving System Testability and Testing with Microarchitectures,2010,"Testing is essential to ensure software reliability and dependability. however, testing activities are very expensive and complex. Micro architectures, such as design patterns and anti-patterns, widely exist in object oriented systems and are recognized as influencing many software quality attributes. Our goal is to identify their impact on system testability and analyse how they can be leveraged to reduce the testing effort while increasing the system reliability and dependability. The proposed research aims at contributing to reduce complexity and increase testing efficiency by using micro architectures. We will base our work on the rich existing tools of micro architectures detection and code reverse-engineering.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5645492,no
Discrete wavelet and neural network for transmission line fault classification,2010,This paper presents an efficient wavelet and neural network (WNN) based algorithm for fault classification in single circuit transmission line. The first level discrete wavelet transform is applied to decompose the post fault current signals of the transmission line into a series of coefficient components (approximation and detail). The values of the approximation coefficients obtained can accurately discriminate between all types of fault in transmission line and reduce the number of data feeding to the ANN. These coefficients are further used to train an Artificial Neural Network (ANN) fitting function. The trained FFNN clearly distinguishes and classify very accurate and very fast the fault type. A typical generation system connected by single circuit transmission line to many nodes of lodes at the receiving end was simulated using MATLAB simulation software using only one node to do this work. The generated data were used by the MATLAB software to test the performance of the proposed technique. The simulation results obtained show that the new algorithm is more reliable and accurate.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5645842,no
Notice of Violation of IEEE Publication Principles<BR>Network performance management by using stable algorithms,2010,"Notice of Violation of IEEE Publication Principles<BR><BR>""Network Performance Management By Using Stable Algorithms""<BR>by J. Ramkumar and V.B. Kirubanand<BR>in the 2010 2nd International Conference on Computer Technology and Development (ICCTD), 2010, pp. 692 â€?696<BR><BR>After careful and considered review of the content and authorship of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>This paper contains significant portions of original text from the paper cited below. The original text was copied with insufficient attribution (including appropriate references to the original author(s) and/or paper title) and without permission.<BR><BR>Due to the nature of this violation, reasonable effort should be made to remove all past references to this paper, and future references should be made to the following article:<BR><BR>""A Unified Framework for System-level Design: Modeling and Performance Optimization of Scalable Networking Systems""<BR>by Hwisung Jung and Massoud Pedram<BR>in the 8th International Symposium on Quality Electronic Design, 2007 (ISQED 2007), 2007, pp. 198 â€?203<BR><BR>Network Management Systems have played a great important role in information systems. Management is very important and essential in any fields. There are many managements such as configuration management, fault management, performance management, security management, accounting management and etc. Among them, performance management is more important than others. Because it is very essential and useful in any fields. Performance management is to monitor and maintain the whole system. This paper focuses on how the petri net models can be exploited by markov algorithm with adaboost algorithm for performance evaluation of Wireless local area network(WLAN). The main theme of this paper is to increase the performance of various Wireless Local Area Network by using Markov algorithm with A- aboost algorithm. The EQPN model and Markov algorithm with Adaboost algorithm is employed to represent the performance behaviors and to minimize energy consumption of the system under performance constraints through mathematical programming formulations. EQPNs facilitate the integration of both hardware and software aspects of the system behavior in the improved model. In addition to any wireless technology in the systems and using EQPNs one can easily model simultaneous resource possession, synchronization, blocking and contentions for software resources. EQPNs are very powerful as a performance analysis and prediction tool. When comparing the service rates from Wired Local Area Network, it has been found that the service rate from proposed system is very efficient for implementation; we hope to motivate further research in this area.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5646440,no
Dimentional inspection and feature recognition through machine vision system,2010,"Automated computerized machine vision inspection, techniques can be used as a tooling for automated shape inspection in an advanced CIM environment so that real time, reliable, non-contact and non-destructive, and 100% quality measure activities can be achieved fruitfully. This paper develops a generic hardware system, software architecture and a technology-mix for product feature identification and measurement analysis of various machined parts/products. The feature analysis seeks to identify inherent characteristics of feature of an object found within the image frame. These characteristics are used to describe the object or the attributes of the object features, prior to subsequent task for classification of the same. The special features of such an inspection methodology include system which is (i) Micro-computer based; (ii) flexible and reprogrammable; (iii) likely to remain effective at the floor; (iv) ensure the highest possible goes to the customer and (v) very efficient.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5646564,no
Lightning overvoltages on an overhead transmission line during backflashover and shielding failure,2010,Analysis of induced voltages on transmission tower and conductors has been performed when a high voltage line is subjected to propagation of lightning transient. PSCAD/EMTDC software program is used to carry out the modelling and simulation works. Lightning strikes on the tower top or conductors result in large overvoltages appearing between the tower and the conductors. Two cases considered for these effects are: (i) direct strike to a shield wire or tower top; (ii) shielding failure. The probability of a lightning strike terminating on a shield wire or tower top is higher than that of a phase conductor. Voltages produced during shielding failure on conductors are more significant than during back flashovers. The severity of the induced voltages from single stroke and multiple stroke lightning is illustrated using the simulation results. The results demonstrate high magnitude of induced voltages by the multiple stroke lightning compared to those by single strokes. Analytical studies were performed to verify the results obtained from the simulation. Analysis of the performance of the line using IEEE Flash version 1.81 computer programme was also carried out.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5648866,no
A novel method of fingerprint minutiae extraction based on Gabor phase,2010,"In this paper, a novel method of fingerprint minutiae extraction on grey-scale images is proposed based on the Gabor phase field. The novelty of our approach is that the proposed algorithm is performed on the transform domain, i.e. the Gabor phase field of the fingerprint image. This is different from most existing minutiae extraction methods, in which the minutiae are usually extracted from the binarized and thinned fingerprint image. Experimental results on benchmark data sets demonstrate that the proposed algorithm has promising performances.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5648893,no
Notice of Violation of IEEE Publication Principles<BR>Finding the location of switched capacitor banks in distribution systems based on wavelet transform,2010,"Notice of Violation of IEEE Publication Principles<BR><BR>""Finding the Location of Switched Capacitor Banks in Distribution Systems Based on Wavelet Transform""<BR>by B. Noshad, M. Keramatzadeh, M. Saniei<BR>in the Proceedings of the 45th International Universities Power Engineering Conference (UPEC), August 2010<BR><BR>After careful and considered review of the content and authorship of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>This paper contains significant portions of original text from the paper cited below. The original text was copied with insufficient attribution and without permission.<BR><BR>""On Tracking and Finding the Location of Switched Capacitor Banks in Distribution Systems""<BR>by H. Khani, M. Moallem, S. Sadri<BR>in the Proceedings of the Transmission & Distribution Conference & Exposition: Asia and Pacific, October 2009<BR><BR>""A Novel Algorithm for Determining the Exact Location of Switched Capacitor Banks in Distribution Systems ""<BR>by H. Khani, M. Moallem, S. Sadri<BR>Proceedings of the Transmission & Distribution Conference & Exposition: Asia and Pacific, October 2009<BR><BR>In this paper, a method is proposed for determining the exact location of switched capacitor banks in distribution systems. The method consists of two steps. In the first step, the technique is based on measurement of voltage and current transients created by capacitor switching. The hypothesis states that: The slope of voltage and current transient wave forms created by capacitor switching immediately before and after the switching instant will have opposite polarity for monitoring the location at line and branches feeding the capacitor and identical polarity for monitoring the location at other part of the power networks. Wavelet transform techniques are used to determine the exact switching instant effectively. Simulation is done by EMTP. In the second step, using the exis- ting capacitor switching transient data, a method based on linear circuit theory is used to estimate the exact distance of switched capacitor bank from the monitoring location and it is simulated with Matlab Software. The method is valid for both wye and delta configured capacitor banks. This proposed method is applied to the IEEE 37-bus distribution system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5648976,no
Implementation of finite mutual impedances and its influence on earth potential rise estimation along transmission lines,2010,"As the proximity of high fault current power lines to residential areas increases, the need for accurate prediction of earth potential rise (EPR) is of crucial importance for both safety and equipment protection. To date, the most accurate methods for predicting EPR are power system modelling software tools, such as EMTP, or recursive methods that use a span by span approach to model a transmission line. These techniques are generally used in conjunction with impedances and admittances that are derived from the assumption of infinite line length. In this paper a span by span model was created to predict the EPR along a dual circuit transmission line in EMTP-RV, where the mutual impedances were considered to be between finite length conductors. A series of current injection tests were also performed on the system under study in order to establish the accuracy of both the finite and infinite methods.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5650125,no
Global motion temporal filtering for in-loop deblocking,2010,"One of the most severe problems in hybrid video coding is its block-based approach, which leads to distortions called blocking artifacts. These artifacts affect not only the subjective perception at the receiver but also the motion compensated prediction (MCP) that generates a prediction signal from previously decoded pictures. It is therefore directly connected to the amount of data that has to be transmitted. In this paper, we propose a technique called global motion temporal filtering for blocking artifact reduction. Other than common deblocking techniques, this approach does not reduce the blocking artifacts spatially. Filtering is performed temporally using a set of neighboring pictures from the picture buffer. This approach is incorporated into an H.264/AVC reference software. Experimental evaluation shows that the proposed technique significantly improves the quality in terms of rate-distortion performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5650779,no
Submerged aquatic vegetation habitat product development: On-screen digitizing and spatial analysis of Core Sound,2010,"A hydrophyte of high relevance, submerged aquatic vegetation (SAV) is of great importance to estuarine environments. SAV helps improve water quality, provides food and shelter for waterfowl, fish, and shellfish, as well as protects shorelines from erosion. In coastal bays most SAV was eliminated by disease in the 1930's. In the late 1960's and 1970's a dramatic decline of all SAV species was correlated with increasing nutrient and sediment inputs from development of surrounding watersheds (MDNP et. al 2004). Currently state programs work to protect and restore existing wetlands, however, increasing development and population pressure continue to degrade and destroy both tidal and non-tidal wetlands and hinder overall development of SAV growth. The focus of this research was to utilize spatial referencing software in the mapping of healthy submerged aquatic vegetation (SAV) habitats. In cooperation with the United States Fish and Wildlife Service (USFWS), and the National Oceanic and Atmospheric Administration (NOAA), students from Elizabeth City State University (ECSU) developed and applied Geographic Information Systems (GIS) skills to evaluate the distribution and abundance of SAV in North Carolina's estuarine environments. Utilizing ESRI ArcView, which includes ArcMap, ArcCatalog and ArcToolbox, and the applications of on-screen digitizing, an assessment of vegetation cover was made through the delineation of observable SAV beds in Core Sound, North Carolina. Aerial photography of the identified coastal water bodies was taken at 12,000 feet above mean terrain (AMT) scale 1:24,000. The georeferenced aerial photographs were assessed for obscurities and the SAV beds were digitized. Through the adoption of NOAA guidelines and criteria for benthic habitat mapping using aerial photography for image acquisition and analysis, students delineated SAV beds and developed a GIS spatial database relevant to desired results. This newly created database yielded products in - - the form of usable shapefiles of SAV polygonsÎ¹ as well as attribute information with location information, area in hectares, and percent coverage of SAV.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5651718,no
Multi-view prediction structure for free viewpoint video,2010,"In this paper, we described geometric prediction structure for multi-view video coding. By exploiting the geometric relations between each camera pose, we can make prediction pair which maximizes the spatial correlation of each view. To analyze the relationship of each camera pose, we defined the mathematical mean view and view distance in 3D space. We proposed an algorithm for establishing the geometric prediction structure based on view center and view distance. Using this prediction structure, inter-view prediction is performed to camera pair of maximum spatial correlation. In our prediction structure, we also considered the view scalability. Experiments are done using JMVC software on MPEG-FTV test sequences. Overall performance is measured in the PSNR and subjective image quality measure such as PSPNR.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5652174,no
Application-Aware diagnosis of runtime hardware faults,2010,"Extreme technology scaling in silicon devices drastically affects reliability, particularly because of runtime failures induced by transistor wearout. Current online testing mechanisms focus on testing all components in a microprocessor, including hardware that has not been exercised, and thus have high performance penalties. We propose a hybrid hardware/software online testing solution where components that are heavily utilized by the software application are tested more thoroughly and frequently. Thus, our online testing approach focuses on the processor units that affect application correctness the most, and it achieves high coverage while incurring minimal performance overhead. We also introduce a new metric, Application-Aware Fault Coverage, measuring a test's capability to detect faults that might have corrupted the state or the output of an application. Test coverage is further improved through the insertion of observation points that augment the coverage of the testing system. By evaluating our technique on a Sun OpenSPARC T1, we show that our solution maintains high Application-Aware Fault Coverage while reducing the performance overhead of online testing by more than a factor of 2 when compared to solutions oblivious to application's behavior. Specifically, we found that our solution can achieve 95% fault coverage while maintaining a minimal performance overhead (1.3%) and area impact (0.4%).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5653788,no
Definition and Validation of Metrics for ITSM Process Models,2010,"Process metrics can be used to establish baselines, to predict the effort required to go from an â€œas-isâ€?to a â€œto-beâ€?scenario or to pinpoint problematic ITSM process models. Several metrics proposed in the literature for business process models can be used for ITSM process models as well. This paper formalizes some of those metrics and proposes some new ones, using the Metamodel-Driven Measurement (M2DM) approach that provides precision, objectiveness and automatic collection. According to that approach, metrics were specified with the Object Constraint Language (OCL), upon a lightweight BPMN metamodel that is briefly described. That metamodel was instantiated with a case study consisting of two ITSM processes with two scenarios (â€œas-isâ€?and â€œto-beâ€? each. Values collected automatically by executing the OCL metrics definitions, upon the instantiated metamodel, are presented. Using a larger sample with several thousand meta-instances, we analyzed the collinearity of the formalized metrics and were able to identify a smaller set, which will be used to perform further research work on the complexity of ITSM processes.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5654787,no
Model-driven development of ARINC 653 configuration tables,2010,"Model-driven development (MDD) has become a key technique in systems and software engineering, including the aeronautic domain. It facilitates on systematic use of models from a very early phase of the design process and through various model transformation steps (semi-)automatically generates source code and documentation. However, on one hand, the use of model-driven approaches for the development of configuration data is not as widely used as for source code synthesis. On the other hand, we believe that, particular systems that make heavy use of configuration tables like the ARINC 653 standard can benefit from model-driven design by (i) automating error-prone configuration file editing and (ii) using model based validation for early error detection. In this paper, we will present the results of the European project DIANA that investigated the use of MDD in the context of Integrated Modular Avionics (IMA) and the ARINC 653 standard. In the scope of the project, a tool chain was implemented that generates ARINC 653 configuration tables from high-level architecture models. The tool chain was integrated with different target systems (VxWorks 653, SIMA) and evaluated during case studies with real-world and real-sized avionics applications.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5655322,no
Model-driven development of ARINC 653 configuration tables,2010,"Model-driven development (MDD) has become a key technique in systems and software engineering, including the aeronautic domain. It facilitates on systematic use of models from a very early phase of the design process and through various model transformation steps (semi-)automatically generates source code and documentation. However, on one hand, the use of model-driven approaches for the development of configuration data is not as widely used as for source code synthesis. On the other hand, we believe that, particular systems that make heavy use of configuration tables like the ARINC 653 standard can benefit from model-driven design by (i) automating error-prone configuration file editing and (ii) using model based validation for early error detection. In this paper, we will present the results of the European project DIANA that investigated the use of MDD in the context of Integrated Modular Avionics (IMA) and the ARINC 653 standard. In the scope of the project, a tool chain was implemented that generates ARINC 653 configuration tables from high-level architecture models. The tool chain was integrated with different target systems (VxWorks 653, SIMA) and evaluated during case studies with real-world and real-sized avionics applications.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5655451,no
A Tool for Automatic Defect Detection in Models Used in Model-Driven Engineering,2010,"In the Model-Driven Engineering (MDE) field, the quality assurance of the involved models is fundamental for performing correct model transformations and generating final software applications. To evaluate the quality of models, defect detection is usually performed by means of reading techniques that are manually applied. Thus, new approaches to automate the defect detection in models are needed. To fulfill this need, this paper presents a tool that implements a novel approach for automatic defect detection, which is based on a model-based functional size measurement procedure. This tool detects defects related to the correctness and the consistency of the models. Thus, our contribution lays in the new approach presented and its automation for the detection of defects in MDE environments.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5655578,no
Analyzing the Similarity among Software Projects to Improve Software Project Monitoring Processes,2010,"Software project monitoring and control is crucial to detect deviation considering the project plan and to take appropriate actions, when needed. However, to determine which action should be taken is not an easy task, since project managers have to analyze the context of the deviation event and search for actions that were successfully taken on previous similar contexts, trying to repeat the effectiveness of these actions. To do so, usually managers use previous projects data or their own experience, and frequently there is no measure or similarity criteria formally established. Thus, in this paper we present the results of a survey that aimed to identify characteristics that can determine the similarity among software projects and also a measure to indicate the level of similarity among them. A recommendation system to support the execution of corrective actions based on previous projects is also described. We believe these results can support the improvement of software project monitoring process, providing important knowledge to project managers in order to improve monitoring and control activities on the projects.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5655652,no
A Method for Continuous Code Quality Management Using Static Analysis,2010,The quality of source code is a key factor for any software product and its continuous monitoring is an indispensable task for a software development project. We have developed a method for systematic assessing and improving the code quality of ongoing projects by using the results of various static code analysis tools. With different approaches for monitoring the quality (a trend-based one and a benchmarking-based one) and an according tool support we are able to manage the large amount of data that is generated by these static analyses. First experiences when applying the method with software projects in practice have shown the feasibility of our method.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5655663,no
Towards Automated Quality Models for Software Development Communities: The QualOSS and FLOSSMetrics Case,2010,"Quality models for software products and processes help both to developers and users to better understand their characteristics. In the specific case of libre (free, open source) software, the availability of a mature and reliable development community is an important factor to be considered, since in most cases both the evolvability and future fitness of the product depends on it. Up to now, most of the quality models for communities have been based on the manual examination by experts, which is time-consuming, generally inconsistent and often error-prone. In this paper, we propose a methodology, and some examples of how it works in practice, of how a quality model for development communities can be automated. The quality model used is a part of the QualOSS quality model, while the metrics are those collected by the FLOSS Metrics project.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5655666,no
Studying Supply and Demand of Software Maintenance and Evolution Services,2010,"Software maintenance and evolution constitutes an important part of the total cost of the life cycle of software. Some even argue this is the most important fraction of the cost. The added value of software maintenance and evolution is often not fully understood by the customer leading to a perception that software maintenance organizations are costly and inefficient. A common view of maintenance and evolution is that it merely fixes bugs. However, studies over the years have indicated that in many organizations the majority of the maintenance effort is devoted to value-added activities. To improve customer perceptions it is important to provide them with better insight into the activities performed by the maintenance organization and to document such performance with objective measures of software maintenance activities. In this paper, software maintenance trend analysis is used as a basis for improvement. First, the differences between software maintenance activities and the Information System (IS) department development projects are described. Then a trend model is developed as a mean to manage the expectations of customers. To conclude, some remarks are made regarding the application of trend analysis by maintenance work categories for software maintenance managers.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5655668,no
Reducing Subjectivity in Code Smells Detection: Experimenting with the Long Method,2010,"Guidelines for refactoring are meant to improve software systems internal quality and are widely acknowledged as among software's best practices. However, such guidelines remain mostly qualitative in nature. As a result, judgments on how to conduct refactoring processes remain mostly subjective and therefore non-automatable, prone to errors and unrepeatable. The detection of the Long Method code smell is an example. To address this problem, this paper proposes a technique to detect Long Method objectively and automatically, using a Binary Logistic Regression model calibrated by expert's knowledge. The results of an experiment illustrating the use of this technique are reported.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5655669,no
IDS: An Immune-Inspired Approach for the Detection of Software Design Smells,2010,"We propose a parallel between object-oriented system designs and living creatures. We suggest that, like any living creature, system designs are subject to diseases, which are design smells (code smells and anti patterns). Design smells are conjectured in the literature to impact the quality and life of systems and, therefore, their detection has drawn the attention of both researchers and practitioners with various approaches. With our parallel, we propose a novel approach built on models of the immune system responses to pathogenic material. We show that our approach can detect more than one smell at a time. We build and test our approach on Gantt Project v1.10.2 and Xerces v2.7.0, for which manually-validated and publicly available smells exist. The results show a significant improvement in detection time, precision, and recall, in comparison to the state-of-the-art approaches.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5655670,no
Embedded system environment: A framework for TLM-based design and prototyping,2010,"This paper presents Embedded System Environment (ESE), which is a comprehensive set of tools for supporting a model-based design methodology for multi-processor embedded systems. It consists of two parts: ESE Front-End and ESE BackEnd. ESE Front-End provides automatic generation of SystemC transaction level models (TLMs) from graphical capture of system platform and application C/C++ code. ESE generated TLMs can be used either as virtual platforms for SW development or for fast and early timing estimation of system performance. ESE Back-End provides automatic synthesis from TLM to Cycle Accurate Model (CAM) consisting of RTL interfaces, system SW and prototype ready FPGA project files. ESE generated RTL can be synthesized using standard logic synthesis tools and system SW can be compiled along with application code for target processors. ESE automatically creates Xilinx EDK projects for board prototyping. Our experimental results demonstrate that ESE can drastically reduce embedded system design and prototyping time, while maintaining design quality similar to manual design.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5656342,no
A Metrics-Based Approach to Technical Documentation Quality,2010,"Technical documentation is now fully taking the step from stale printed booklets (or electronic versions of these) to interactive and online versions. This provides opportunities to reconsider how we define and assess the quality of technical documentation. This paper suggests an approach based on the Goal-Question-Metric paradigm: predefined quality goals are continuously assessed and visualized by the use of metrics. To test this approach, we perform two experiments. We adopt well known software analysis techniques, e.g., clone detection and test coverage analysis, and assess the quality of two real world documentations, that of a mobile phone and of (parts of) a warship. The experiments show that quality issues can be identified and that the approach is promising.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5656407,no
Algorithm for QOS-aware web service composition based on flow path tree with probabilities,2010,"This paper first presents a novel model for QoS-aware web services composition based on workflow patterns, and an executing path tree with probability. Then a discrete PSO algorithm is proposed to fit our model, which also requires some other preliminary algorithms, such as the generation of all reachable paths and executing path tree. The experiments show the performance of that algorithm and its advantages, compared with genetic algorithm. Finally, we suggest some drawbacks of it and future works.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5657934,no
Adaptive initial quantization parameter selection for H.264/SVC rate control,2010,"Initial quantization parameter (QP) selection is critical for the overall performance of video rate control. In this paper, we propose an adaptive initial QP selection algorithm for H.264/AVC Scalable Extension (SVC) rate control. The proposed algorithm introduces a number of efficient methods, including a coding complexity measure for intra-frames, and a rate-complexity-quantization (R-C-Q) model to accurately determine initial QPs for H.264/SVC rate control. Our experimental results demonstrate that, the proposed initial QP selection algorithm outperforms the one of JVT-W043 rate control scheme, adopted in the H.264/SVC reference software, by providing more accurate initial QP prediction, reducing buffer overflow/underflow, depressing quality fluctuations, and finally, improves the overall coding quality by up to 0.48dB.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5658478,no
Software Defect Prediction Using Dissimilarity Measures,2010,"In order to improve the accuracy of software defect prediction, a novel method based on dissimilarity measures is proposed. Different from traditional predicting methods based on feature space, we solve the problem in dissimilarity space. First the new unit features in dissimilarity space are obtained by measuring the dissimilarity between the initial units and prototypes. Then proper classifier is chosen to complete prediction. By prototype selecting, we can reduce the dimension of units' features and the computational complexity of prediction. The empirical results in the NASA database KC2 and CM1 show that the prediction accuracies of KNN, Bayes, and SVM classifier in dissimilarity space are higher than that of feature space from 1.86% to 9.39%. Also the computational complexities reduce from 18% to 67%.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5659217,no
An Autonomic Performance-Aware Workflow Job Management for Service-Oriented Computing,2010,"Workflow job is composed of several ordered subtasks which invoke different computing services. These computing services are deployed on geographically distributed servers. Towards Workflow Job Management, how to schedule workflow jobs to achieve high server resource utilization and how to ensure Quality of Service (QoS) pose several challenges. In the paper, an autonomic Performance-Aware Workflow Job Management is proposed. It firstly decomposes workflow jobs into subtasks, and then adopts Virtual Allocation Strategy, which utilizes the concept of virtual queue, to allocate them to the servers. We also apply a Detection Adjustment Approach for Virtual Allocation to dynamically adjust workload of each computing server according to the real-time system workload changes. Additionally, it also utilizes Occupy Allocation to ensure the QoS. These capacities enable our Workflow Job Management adaptable and autonomic. Finally we establish simulations to demonstrate system performance and QoS.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5662489,no
Energy-Efficient Clustering Rumor Routing Protocol for Wireless Sensor Networks,2010,"To develop an energy-efficient routing protocol becomes one of the most important and difficult key tasks for wireless sensor networks. Traditional Rumor Routing is effective in random path building but winding or even looped path is prone to be formed, leading to enormous energy wasting. Clustering Rumor Routing proposed in this paper has advantages in energy saving by making full use of three features-clustering structure, selective next-hop scheme and double variable energy thresholds for rotation of cluster-heads. Thus CRR can effectively avoid the problems that occur in RR and a more energy-efficient routing path from event agents to queries can be built. The performance between CRR and traditional RR are evaluated by simulations. The results indicate that compared with traditional RR, the CRR can save more energy consumption, provide better path quality, and improve the delivery rate as well.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5667201,no
The impact of Shift Work Implementation on sleeping quality and job performance: A case study of semi-conductor manufacturing company,2010,"This research is mainly focused on the worker performance of the semi-conductor industry in Taiwan. All we are concerned is how the sleeping quality of the workers on different shifts affects the Job performance. Investigations on workers are undertaken from Silicon Integrated Systems Co-operation, SiS. Pittsburgh Sleep Quality Index, PSQI, and Attitude Towards Shiftwork Scale, ATSS, are used to evaluate workers' sleeping quality and job performance respectively. A questionnaire is designed for collecting relative data. Four hundred well-answered questionnaires are successfully collected. The statistic software package, SPSS11.5, is used to perform t-test, single factor ANOVA analysis and multiple regressions. The evaluation of the correlation between Shift Work Implementation and sleep quality revealed that different work shifts have made significant influences on sleeping quality and job performance. This research also revealed that it would be a good way to reduce accidents resulting from job and to enhance job performance if the employer pays more attention on worker's sleeping quality. For those so called `highly risking group', the company should strengthen worker's health knowledge by implementing necessary courses or practices.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5668351,no
Research of Congestion Control in Wireless Video Transmission,2010,"With the development of the wireless communications technology and the multimedia technology, the wireless internet multimedia transport of real-time becomes an important applications field. We will meet a problem that the distinguish of the status of package loss is not very good which will affect the QoS when we transport multimedia data in wireless internet. Based on this problem the article comes up with a new algorithm which is named LDA(Loss Detection Algorithm). The algorithm utilizes the delay from transmitting end to receiving end to distinguish the status of package loss and use the simulation software NS2 to build a platform for analog video. Through it, we can confirm the performance of the TFRC which is improved has bigger progress.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5668993,no
Attribute Selection and Imbalanced Data: Problems in Software Defect Prediction,2010,"The data mining and machine learning community is often faced with two key problems: working with imbalanced data and selecting the best features for machine learning. This paper presents a process involving a feature selection technique for selecting the important attributes and a data sampling technique for addressing class imbalance. The application domain of this study is software engineering, more specifically, software quality prediction using classification models. When using feature selection and data sampling together, different scenarios should be considered. The four possible scenarios are: (1) feature selection based on original data, and modeling (defect prediction) based on original data; (2) feature selection based on original data, and modeling based on sampled data; (3) feature selection based on sampled data, and modeling based on original data; and (4) feature selection based on sampled data, and modeling based on sampled data. The research objective is to compare the software defect prediction performances of models based on the four scenarios. The case study consists of nine software measurement data sets obtained from the PROMISE software project repository. Empirical results suggest that feature selection based on sampled data performs significantly better than feature selection based on original data, and that defect prediction models perform similarly regardless of whether the training data was formed using sampled or original data.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5670030,no
Requirement based test case prioritization,2010,"Test case prioritization involves scheduling test cases in an order that increases the effectiveness in achieving some performance goals. One of the most important performance goals is the rate of fault detection. Test cases should run in an order that increases the possibility of fault detection and also that detects faults at the earliest in its testing life cycle. In this paper, an algorithm is proposed for system level test case prioritization (TCP) from software requirement specification to improve user satisfaction with quality software and also to improve the rate of severe fault detection. The proposed model prioritizes the system test cases based on the three factors: customer priority, changes in requirement, implementation complexity. The proposed prioritization technique is validated with two different sets of industrial projects and the results show that the proposed prioritization technique improves the rate of severe fault detection.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5670728,no
Selecting High Quality Frames for Super Resolution Reconstruction Using Perceptual Quality Metrics,2010,"Super-resolution involves the use of signal processing techniques to estimate the high-resolution (HR) version of a scene from multiple low-resolution (LR) observations. It follows that the quality of the reconstructed HR image would depend on the quality of the LR observations. The latter depends on multiple factors like the image acquisition process, encoding or compression and transmission. However, not all images are equally affected by a given type of impairment. A proper choice of the LR observations for reconstruction, should yield a better estimate of the HR image, over a naive method using all images. We propose a simple, model-free approach to improve the performance of super-resolution systems based on the use of perceptual quality metrics. Our approach does not require, or even assume, a specific realization of the SR system. Instead, we select the image subset with high perceptual quality from the available set of LR images. Finally, we present the logical extension of our approach to select the perceptually significant regions in a given LR image, for use in SR reconstruction.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5673676,no
Error analysis in indoors localization using ZigBee wireless networks,2010,"This paper describes indoors radio frequency (RF) localization using radio signal strength indication (RSSI) which is available in wireless communications networks. The main advantage of described methodology is taking as localization hardware the communications sub-system and developing dedicated software in order to obtain a location of mobile network nodes using trilateration algorithm. Accuracy is strongly dependent on quality of measured RSSI. Needing of post-processing of raw RSSI values in order to obtain good results in terms of mobile nodes location estimation is shown. In order to apply RSSI values from wireless communications hardware to localization sub-system, filtering is thus an approach to overcome limitations due to RSSI values fluctuations.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5675061,no
Identification and compensation of torque ripples of a PMSM in a haptic context,2010,"A haptic system is an articulated mechanical structure with motors, sensors, embedded electronics as well computer software allowing force feedback. It enables the user to interact with virtual reality through the sense of touch and sight. It has two main operating modes. In the blocked mode the haptic system should be able to reproduce virtual shocks. In the transparent mode, the haptic system should be as transparent as possible. The transparency is defined as the quality of the free movement while displacing outside the blocked regions. A Permanent Magnet Synchronous Motor (PMSM) is used to drive the mechanical structure. Nowadays PMSM connectors use ultra-powerful magnet in order to reduce the motor volume with respect to the delivered electromagnetic torque ratio. Furthermore the use of those magnets, torque ripples phenomena (cogging torque) can be observed and it can deteriorate the haptic interface performances. In this paper a torque ripples identification and compensation method is proposed and evaluated experimentally. The developed least square identification procedure uses the measured currents, while the PMSM is driven at constant speed, in order to reconstruct the cogging signal. The main advantage of the proposed method is that it doesn't use the motor mechanical structure characteristics in order to calculate its parameters.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5675431,no
Visual Indicator Component Software to Show Component Design Quality and Characteristic,2010,"Good design is one of the prerequisites of high quality product. To measure the quality of software design, software metrics are used. Unfortunately in software development practice, there are a lot of software developers who are not concerned with the component's quality and characteristic. Software metrics does not interest them, because to understand the measurement of the metrics, a deep understanding about degree, dimension, and capacity of some attribute of the software product is needed. This event triggers them to build software's whose quality is below the standard. What is more dangerous is that these developers are not aware of quality and do not care with their work product. Of course these occurrences is concerning and a solution needed to be found. Through this paper the researcher is trying to formulate an indicator of component software that shows component design quality and characteristic visually. This indicator can help software developers to make design decision and refactoring decision, detect the design problem more quickly, able to decide which area to apply refactoring, and enable us to do early or final detection of design defects.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5675845,no
Intelligent monitoring and fault tolerance in large-scale distributed systems,2010,"Summary form only. Electronic devices are starting to become widely available for monitoring and controlling large-scale distributed systems. These devices may include sensing capabilities for online measurement, actuators for controlling certain variables, microprocessors for processing information and making realtime decisions based on designed algorithms, and telecommunication units for exchanging information with other electronic devices or possibly with human operators. A collection of such devices may be referred to as a networked intelligent agent system. Such systems have the capability to generate a huge volume of spatial-temporal data that can be used for monitoring and control applications of large-scale distributed systems. One of the most important research challenges in the years ahead is the development of information processing methodologies that can be used to extract meaning and knowledge out of the ever-increasing electronic information that will become available. Even more important is the capability to utilize the information that is being produced to design software and devices that operate seamlessly, autonomously and reliably in some intelligent manner. The ultimate objective is to design networked intelligent agent systems that can make appropriate real-time decisions in the management of large-scale distributed systems, while also providing useful high-level information to human operators. One of the most important classes of large-scale distributed systems deals with the reliable operation and intelligent management of critical infrastructures, such as electric power systems, telecommunication networks, water systems, and transportation systems. The design, control and fault monitoring of critical infrastructure systems is becoming increasingly more challenging as their size, complexity and interactions are steadily growing. Moreover, these critical infrastructures are susceptible to natural disasters, frequent failures, as well as malicio- - us attacks. There is a need to develop a common system-theoretic fault diagnostic framework for critical infrastructure systems and to design architectures and algorithms for intelligent monitoring, control and security of such systems. The goal of this presentation is to motivate the need for health monitoring, fault diagnosis and security of critical infrastructure systems and to provide a fault diagnosis methodology for detecting, isolating and accommodating both abrupt and incipient faults in a class of complex nonlinear dynamic systems. A detection and approximation estimator based on computational intelligence techniques is used for online health monitoring. Various adaptive approximation techniques and learning algorithms will be presented and illustrated, and directions for future research will be discussed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5675940,no
Reliable online water quality monitoring as basis for fault tolerant control,2010,"Clean data are essential for any kind of alarm or control system. To achieve the required level of data quality in online water quality monitoring, a system for fault tolerant control was developed. A modular approach was used, in which a sensor and station management module is combined with a data validation and an event detection module. The station management module assures that all relevant data, including operational data, is available and the state of the monitoring devices is fully documented. The data validation module assures that unreliable data is detected, marked as such, and that the need for sensor maintenance is timely indicated. Finally, the event detection module marks unusual system states and triggers measures and notifications. All these modules were combined into a new software package to be used on water quality monitoring stations.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5675985,no
An Empirical Comparison of Fault-Prone Module Detection Approaches: Complexity Metrics and Text Feature Metrics,2010,"In order to assure the quality of software product, early detection of fault-prone products is necessary. Fault-prone module detection is one of the major and traditional area of software engineering. However, comparative study using the fair environment rarely conducted so far because there is little data publicly available. This paper tries to conduct a comparative study of fault-prone module detection approaches.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676267,no
Design and Analysis of Cost-Cognizant Test Case Prioritization Using Genetic Algorithm with Test History,2010,"During software development, regression testing is usually used to assure the quality of modified software. The techniques of test case prioritization schedule the test cases for regression testing in an order that attempts to increase the effectiveness in accordance with some performance goal. The most general goal is the rate of fault detection. It assumes all test case costs and fault severities are uniform. However, those factors usually vary. In order to produce a more satisfactory order, the cost-cognizant metric that incorporates varying test case costs and fault severities is proposed. In this paper, we propose a cost-cognizant test case prioritization technique based on the use of historical records and a genetic algorithm. We run a controlled experiment to evaluate the proposed technique's effectiveness. Experimental results indicate that our proposed technique frequently yields a higher Average Percentage of Faults Detected per Cost (APFDc). The results also show that our proposed technique is also useful in terms of APFDc when all test case costs and fault severities are uniform.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676289,no
Using Load Tests to Automatically Compare the Subsystems of a Large Enterprise System,2010,"Enterprise systems are load tested for every added feature, software updates and periodic maintenance to ensure that the performance demands on system quality, availability and responsiveness are met. In current practice, performance analysts manually analyze load test data to identify the components that are responsible for performance deviations. This process is time consuming and error prone due to the large volume of performance counter data collected during monitoring, the limited operational knowledge of analyst about all the subsystem involved and their complex interactions and the unavailability of up-to-date documentation in the rapidly evolving enterprise. In this paper, we present an automated approach based on a robust statistical technique, Principal Component Analysis (PCA) to identify subsystems that show performance deviations in load tests. A case study on load test data of a large enterprise application shows that our approach do not require any instrumentation or domain knowledge to operate, scales well to large industrial system, generate few false positives (89% average precision) and detects performance deviations among subsystems in limited time.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676342,no
A Benchmarking Framework for Domain Specific Software,2010,"With the development of the software engineering, finding the ""best-in-class"" practice of a specific domain software and locating its position is an urgent task. This paper proposes a systematic, practical and simplified benchmarking framework for domain specific software. The methodology is useful to assess characteristics, sub-characteristics and attributes that influence the domain specific software product quality qualitative and quantitative. It is helpful for the business managers and the software designers to obtain the competitiveness analysis results of software. Using the domain specific benchmarking framework, the software designers and the business managers can easily recognize the positive and negative gap of their product and locate its position in the specific domain.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676799,no
A Novel Evaluation Method for Defect Prediction in Software Systems,2010,"In this paper, we propose a novel evaluation method for defect prediction in object-oriented software systems. For each metric to evaluate, we start by applying it to the dependency graph extracted from the target software system, and obtain a list of classes ordered by their predicted degree of defect under that metric. By utilizing the actual defect data mined from the subversion database, we evaluate the quality of each metric through means of a weighted reciprocal ranking mechanism. Our method can tell not only the overall quality of each evaluated metric, but also the quality of the prediction result for each class, especially those costly ones. Evaluation results and analysis show the efficiency and rationality of our method.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676810,no
System Identification and Application Based on Parameters Self-Adaptive SMO,2010,"This paper studies the identification algorithm of parameters self adaptive SMO based on linear kernel function, and analyses its performance and advantages. For ARX model and long-term prediction model, the method is used to identify the model of main steam pressure of thermal system and dual-lane gas turbine engine of aero system. The simulation results show that the algorithm can effectively identify model parameters and has a higher accuracy, reducing the requirements of training data including quantity and quality, so that its engineering applications and implementation are easier.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676952,no
On Engineering Challenges of Applying Relevance Feedback to Fingerprint Identification Systems,2010,"Effective fingerprint identification is critical in crime detection and many security related operations. This article investigates the use of Relevance Feedback with automatic fingerprint identification systems. It also summarises how several unique engineering challenges faced when applying relevance feedback are being addressed. Relevance feedback is a process for acquiring and using knowledge from a human user to improve the quality of results from an information retrieval system. It has been applied extensively to both text-based and images-based information retrieval systems, but not to fingerprint identification systems. Compared to automatic processes, relevance feedback has the potential to assist in faster convergence towards correct fingerprint identification and removes the reliance on black box fingerprint matching algorithm for the system's performance. Experimental results collected from a prototype software implementation confirm that relevance feedback can improve the quality of fingerprint identification queries significantly.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676959,no
Naive Bayes Software Defect Prediction Model,2010,"Although the value of using static code attributes to learn defect predictor has been widely debated, there is no doubt that software defect predictions can effectively improve software quality and testing efficiency. Many data mining methods have already been introduced into defect predictions. We noted there have several versions of defect predictor based on Naive Bayes theory, and analyzed their difference estimation method and algorithm complexity. We found the best one which is Multi- variants Gauss Naive Bayes (MvGNB) by performing prediction performance evaluation, and we compared this model with decision tree learner J48. Experiment results on the benchmarking data sets of MDP made us believe that MvGNB would be useful for defect predictions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5677057,no
Artificial Neural Network Model Application on Long Term Water Level Predictions of South Florida Coastal Waters,2010,"Increasing the number and quality of water level data is very important to the hurricane and surge research. Long-term water level data of local stations in estuaries and inland waterway can be used to validate the performance of traditional storm surge models in complex coastal environments. However, field data collections are expensive and often limited by available research budget. Only a few water level stations operated by NOAA provide long-term observations close to 100 years. In this study, a Feed-forward backpropagation ANN Model has been applied to establish quantitative relationship between short-term water level measurements at Naples station and long-term water measurements at Cedar Key station. Using water levels at NOAA stations, the neural network model can be used to derive reliable long-term historical water level data at other stations along south Florida coast by model training and verification. Long-term water level data derived from the ANN model can be used analyze historic hurricane surge hydrograph in Florida coast after removing tidal signals. The data can also be used as boundaries for modeling hurricane storm surges in bays, estuaries, and coastal waterways.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5677263,no
"Crystals and Snowflakes: Building Computation from Stochastically-Assembled, Defect- and Variation-prone Nanowire Crossbars",2010,"Continued Moore's Law capacity scaling is threatened by the capabilities and rising costs of optical lithography. Bottom-up synthesis and assembly of nanoscale structures such as nanowires provide an alternative to top-down lithography. However, bottom-up synthesis demands high regularlity (crystals) creating challenges for differentiation and comes with high rates of defects and variation (snowflakes). With suitable architectures---crossbar-based PLAs, memories, and interconnect---and paradigm shifts in our assembly and usage models---stochastic assembly and post-fabrication, component-specific mapping---we can accommodate these requirements. This allows us to exploit the compactness and energy benefits of single-nanometer dimension devices and to extend these structures into the third dimension without depending on top-down lithography to define the smallest feature sizes in the system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5677506,no
Seamless high speed simulation of VHDL components in the context of comprehensive computing systems using the virtual machine faumachine,2010,"Testing the interaction between hard- and software is only possible once prototype implementations of the hardware exist. HDL simulations of hardware models can help to find defects in the hardware design. To predict the behavior of entire software stacks in the environment of a complete system, virtual machines can be used. Combining a virtual machine with HDL-simulation enables to project the interaction between hard- and software implementations, even if no prototype was created yet. Hence it allows for software development to begin at an earlier stage of the manufacturing process and helps to decrease the time to market. In this paper we present the virtual machine FAUmachine that offers high speed emulation. It can co-simulate VHDL components in a transparent manner while still offering good overall performance. As an example application, a PCI sound card was simulated using the presented environment.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5679102,no
An efficient negotiation based algorithm for resources advanced reservation using hill climbing in grid computing system,2010,"Ensuring quality of services and reducing the blocking probability is one of the important cases in the environment of grid computing. In this paper, we present a deadline aware algorithm to give solution to ensuring the end-to-end QoS and improvement of the efficiency of grid resources. Investigating requests as a group in the start of time slot and trying to accept the highest number of them. Totally can cause to increase the possibility of acceptance of requests and also increase efficiency of grid resources. Deadline aware algorithm reaches us to this goal. Simulations show that the deadline aware algorithm improves efficiency of advance reservation resources in both case. Possibility of acceptance of requests and optimizing resources in short time slot and also in rate of high entrance of requests in each time.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5679601,no
â€œBeautiful picture of an ugly placeâ€? Exploring photo collections using opinion and sentiment analysis of user comments,2010,"User generated content in the form of customer reviews, feedbacks and comments plays an important role in all types of Internet services and activities like news, shopping, forums and blogs. Therefore, the analysis of user opinions is potentially beneficial for the understanding of user attitudes or the improvement of various Internet services. In this paper, we propose a practical unsupervised approach to improve user experience when exploring photo collections by using opinions and sentiments expressed in user comments on the uploaded photos. While most existing techniques concentrate on binary (negative or positive) opinion orientation, we use a real-valued scale for modeling opinion and sentiment strengths. We extract two types of sentiments: opinions that relate to the photo quality and general sentiments targeted towards objects depicted on the photo. Our approach combines linguistic features for part of speech tagging, traditional statistical methods for modeling word importance in the photo comment corpora (in a real-valued scale), and a predefined sentiment lexicon for detecting negative and positive opinion orientation. In addition, a semi-automatic photo feature detection method is applied and a set of syntactic patterns is introduced to resolve opinion references. We implemented a prototype system that incorporates the proposed approach and evaluates it on several regions in the World using real data extracted from Flickr.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5679726,no
An Upper Bound on the Probability of Instability of a DVB-T/H Repeater with a Digital Echo Canceller,2010,"The architecture of a digital On-Channel Repeater (OCR) for DVB-T/H signals is described in this paper. The presence of a coupling channel between the transmitting and the receiving antennas gives origin to one or more echoes, having detrimental effects on the quality of the repeated signal and critically affecting the overall system stability. A low-complexity echo canceller unit is then proposed, performing a coupling channel estimation based on the local transmission of low-power training signals. In particular, in this paper we focus on the stability issues which arise due to the non perfect echo cancellation. An upper bound on the probability of instability of the system is analytically found, providing useful guidelines for conservative OCR design, and some performance figures concerning different propagation scenarios are provided.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5683631,no
Reconstructing control flow graph for control flow checking,2010,"In the space radiation environment, a large number of cosmic rays often lead to transient faults on the on-board computer. These transient faults result in data flow errors or control flow errors during program running. The present software implemented hardware fault tolerant technology mainly uses the signature analysis method to realize the control flow checking, namely, through assigning signature for each basic block and inserting some instructions into every basic block to realize the control flow checking. Because the size of different basic blocks in one program usually exist obvious difference, applying unified checking method for these basic blocks will reduce the protection efficiency. To solve this problem, this paper has proposed a control flow checking optimization method named RCFG by reconstructing control flow graph. RCFG firstly merges basic blocks into larger logic blocks, then cuts the logic blocks into basic logic blocks with similar size. At last, control flow detection algorithm can be applied based on the control flow graph composed with the basic logic blocks. RCFG can effectively improve the protection efficiency of algorithm, and user can regulate the balance between performance and reliability by configuring the size of basic logic block. This paper has finished the fault injection experiment for a typical signature analysis algorithm named CFCSS. According to the experiment result, compared with the original CFCSS algorithm, the average performance expense of the CFCSS algorithm implemented based on RCFG increased by 16.6%, and the average memory expense increased by 13.5%, but the number of the faults resulting in the program outputting wrong result reduced by 47.67% equally.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5687403,no
A comprehensive evaluation methodology for domain specific software benchmarking,2010,"With the wide using of information system, software users are demanding higher quality software and the software developers are pursue the â€œbest-in-classâ€?practice in a specific domain. But till now it is difficult to get an evaluation result due to lack of benchmark method. This paper proposes a systematic, practical and simplified evaluation methodology for domain specific software benchmarking. In this methodology, an evaluation model with software test characteristics (elementary set) and domain characteristics (extended set) is described; in order to weaken the uncertainty of subjective and objective, the rough set theory is applied to gain the attributes weights, and the gray analysis method is introduced to remedy the incomplete and inadequate information. The method is useful to assess characteristics, sub-characteristics and attribute that influence the domain specific software product quality qualitative and quantitative. The evaluation and benchmarking results prove that the model is practical and effective.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5688006,no
Software documents quality measurement- a fuzzy approach,2010,"The quality of software documents is important to software developers and those who testing and evaluating software. Good documents reduce the risk around schedule and cost. Effective documents quality measurement is one challenging activity in software development that software industries are facing. The goal of this research work is to put forward a method to measure documents quality by a fuzzy approach. Quality measurement model and synthetically comparison model is presented to estimate the quality of software documents. By using this method, we can comparison software documents impersonal and get the shortage of documents, and improve the documents quality accordingly.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5689620,no
Dependency-aware fault diagnosis with metric-correlation models in enterprise software systems,2010,"The normal operation of enterprise software systems can be modeled by stable correlations between various system metrics; errors are detected when some of these correlations fail to hold. The typical approach to diagnosis (i.e., pinpoint the faulty component) based on the correlation models is to use the Jaccard coefficient or some variant thereof, without reference to system structure, dependency data, or prior fault data. In this paper we demonstrate the intrinsic limitations of this approach, and propose a solution that mitigates these limitations. We assume knowledge of dependencies between components in the system, and take this information into account when analyzing the correlation models. We also propose the use of the Tanimoto coefficient instead of the Jaccard coefficient to assign anomaly scores to components. We evaluate our new algorithm with a Trade6-based test-bed. We show that we can find the faulty components within top-3 components with the highest anomaly score in four out of nine cases, while the prior method can only find one.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5691319,no
Similarity metric for risk assessment in IT change plans,2010,"The proper management of IT infrastructures is essential for organizations that aim to deliver high quality services. Given the dynamics of these infrastructures, changes become imminent. In some cases, these changes might raise failures, causing disruption to provided services and consequently affecting the business continuity. Therefore, it is strongly recommended to evaluate the risks associated with changes before their actual execution. Learning from information of past deployed changes it is possible to estimate the risks for recently planned ones. Thereby, in this paper, we propose a solution to weigh the information available from past executed plans by the similarity calculated in relation with the analyzed change plan. A prototype system has been developed in order to evaluate the effectiveness of the solution over an emulated IT infrastructure. The results obtained show that the solution is capable of capturing similarity among activities in change plans, improving the accuracy of risk assessment for IT change planning.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5691340,no
A Quasi-best Random Testing,2010,"Random testing, having been employed in both hardware and software for a long time, is well known for its simplicity and straightforwardness, in which each test is selected randomly regardless of the tests previously generated. However, traditionally, it seems to be inefficient for its random selection of test patterns. Therefore, a new concept of quasi-best distance random testing is proposed in the paper to make it more effective in testing. The new idea is based on the fact that the distance between two adjacent selected test vectors in a test sequence would greatly influence the efficiency of fault testing. Procedures of constructing such a testing sequence are presented and discussed in detail. The new approach has shown its remarkable advantage of fitting in most circuits. Experimental results and mathematical analysis of efficiency are also given to assess the performances of the proposed approach.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5692214,no
Quantitative Analysis of Requirements Evolution across Multiple Versions of an Industrial Software Product,2010,"Requirements evolution is one of critical problems influencing software engineering activities. Despite there is much research on requirements evolution, there still lacks quantitative understanding of requirements evolution. In this paper, we quantitatively analyze requirements evolution across multiple versions of an industrial software product. Based on data of requirements evolution and defects, we analyze the relationship between requirements evolution and requirements as well as between defects and requirements evolution. We also analyze the evolution characteristics about requirements modification. Our findings include estimation of the number of defects using evolved requirements may increase accuracy of defect estimation and business rule is the most volatile part in requirements. These findings deepen our understanding of requirements evolution and can help software organizations manage requirements evolution.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5693179,no
Quality Attributes Assessment for Feature-Based Product Configuration in Software Product Line,2010,"Product configuration based on a feature model in software product lines is the process of selecting the desired features based on customers' requirements. In most cases, application engineers focus on the functionalities of the target product during product configuration process whereas the quality attributes are handled until the final product is produced. However, it is costly to fix the problem if the quality attributes have not been considered in the product configuration stage. The key issue of assessing a quality attribute of a product configuration is to measure the impact on a quality attribute made by the set of functional variable features selected in a configuration. Current existing approaches have several limitations, such as no quantitative measurements provided or requiring existing valid products and heavy human effort for the assessment. To overcome theses limitations, we propose an Analytic Hierarchical Process (AHP) based approach to estimate the relative importance of each functional variable feature on a quality attribute. Based on the relative importance value of each functional variable feature on a quality attribute, the level of quality attributes of a product configuration in software product lines can be assessed. An illustrative example based on the Computer Aided Dispatch (CAD) software product line is presented to demonstrate how the proposed approach works.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5693189,no
An Interaction-Pattern-Based Approach to Prevent Performance Degradation of Fault Detection in Service Robot Software,2010,"In component-based robot software, it is crucial to monitor software faults and deal with them on time before they lead to critical failures. The main causes of software failures include limited resources, component-interoperation mismatches, and internal errors of components. Message-sniffing is one of the popular methods to monitor black-box components and handle these types of faults during runtime. However, this method normally causes some performance problems of the target software system because the fault monitoring and detection process consumes a significant amount of resources of the target system. There are three types of overheads that cause the performance degradation problems: frequent monitoring, transmission of a large amount of monitoring-data, and the processing time for fault analysis. In this paper, we propose an interaction-pattern-based approach to reduce the performance degradation caused by fault monitoring and detection in component-based service robot software. The core idea of this approach is to minimize the number of messages to monitor and analyze in detecting faults. Message exchanges are formalized as interaction patterns which are commonly observed in robot software. In addition, important messages that need to be monitored are identified in each of the interaction patterns. An automatic interaction pattern-identification method is also developed. To prove the effectiveness of our approach, we have conducted a performance simulation. We are also currently applying our approach to silver-care robot systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5693201,no
Quantitative Analysis for Non-linear System Performance Data Using Case-Based Reasoning,2010,"Effective software architecture evaluation methods are essential in today's system development for mission critical systems. We have previously developed MEMS and a set of test statistics for evaluating middleware architectures, which proven an effective assessment of important quality attributes and their characterizations. We have observed it is common that many system performance response data are not of linear nature, where using linear modeling is not feasible in these scenarios for system performance predictions. To provide an alternative quantitative assessment on the system performance using actual runtime datasets, we developed a set of non-linear analysis procedure based on Case-based Reasoning (CBR), a machine learning method widely used in another disciplines of Software Engineering. Experiments were carried out based on actual runtime performance datasets. Results confirm that our non-linear analysis method CBR4MEMS produced accurate performance predictions and outperformed linear approaches. Our approach utilizing CBR to enable performance assessments on non-linear datasets, a major step forward to support software architecture evaluation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5693211,no
Using Faults-Slip-Through Metric as a Predictor of Fault-Proneness,2010,"Background: The majority of software faults are present in small number of modules, therefore accurate prediction of fault-prone modules helps improve software quality by focusing testing efforts on a subset of modules. Aims: This paper evaluates the use of the faults-slip-through (FST) metric as a potential predictor of fault-prone modules. Rather than predicting the fault-prone modules for the complete test phase, the prediction is done at the specific test levels of integration and system test. Method: We applied eight classification techniques, to the task of identifying fault prone modules, representing a variety of approaches, including a standard statistical technique for classification (logistic regression), tree-structured classifiers (C4.5 and random forests), a Bayesian technique (NaiÌˆve Bayes), machine-learning techniques (support vector machines and back-propagation artificial neural networks) and search-based techniques (genetic programming and artificial immune recognition systems) on FST data collected from two large industrial projects from the telecommunication domain. Results: Using area under the receiver operating characteristic (ROC) curve and the location of (PF, PD) pairs in the ROC space, the faults slip-through metric showed impressive results with the majority of the techniques for predicting fault-prone modules at both integration and system test levels. There were, however, no statistically significant differences between the performance of different techniques based on AUC, even though certain techniques were more consistent in the classification performance at the two test levels. Conclusions: We can conclude that the faults-slip-through metric is a potentially strong predictor of fault-proneness at integration and system test levels. The faults-slip-through measurements interact in ways that is conveniently accounted for by majority of the data mining techniques.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5693218,no
"Distributed, Scalable Clustering for Detecting Halos in Terascale Astronomy Datasets",2010,"Terascale astronomical datasets have the potential to provide unprecedented insights into the origins of our universe. However, automated techniques for determining regions of interest are a must if domain experts are to cope with the intractable amounts of simulation data. This paper addresses the important problem of locating and tracking high density regions in space that generally correspond to halos and sub-halos and host galaxies. A density based, mode following clustering method called Automated Hierarchical Density Shaving (Auto-HDS) is adapted for this application. Auto-HDS can detect clusters of different densities while discarding the vast majority of background data. Two alternative parallel implementations of the algorithm, based respectively on the dataflow computational model and on Hadoop/ MapReduce functional programming constructs, are realized and compared. Based on runtime performance, scalability across compute cores and across increasing data volumes, we demonstrate the benefits of fine grain parallelism. The proposed distributed and multithreaded AutoHDS clustering algorithm is shown to produce high quality clusters, be computationally efficient, and scalable from 1 through 1024 compute-cores.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5693293,no
Clustering Performance on Evolving Data Streams: Assessing Algorithms and Evaluation Measures within MOA,2010,"In today's applications, evolving data streams are ubiquitous. Stream clustering algorithms were introduced to gain useful knowledge from these streams in real-time. The quality of the obtained clusterings, i.e. how good they reflect the data, can be assessed by evaluation measures. A multitude of stream clustering algorithms and evaluation measures for clusterings were introduced in the literature, however, until now there is no general tool for a direct comparison of the different algorithms or the evaluation measures. In our demo, we present a novel experimental framework for both tasks. It offers the means for extensive evaluation and visualization and is an extension of the Massive Online Analysis (MOA) software environment released under the GNU GPL License.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5693462,no
Multi process real-time network applications in Distribution Management System,2010,"The heart of every Power System Control Center (PSCC) - Distribution Management System (DMS) software is sophisticated calculation real-time core of Network Applications. Crucial applications of the control center such as State Estimator - Power Flow, Volt VAr Control, Fault isolation and service restoration, must operate in real-time, providing high performance calculation, communication and data processing. This paper presents implementation of real-time multi-process Distribution State Estimator and Volt VAr control and those important performance considerations specific to the scheduling processes of these applications, by leveraging multi-core processor technology. Real-time Scheduler as one of the crucial performance wise components is analyzed. The scheduling process and algorithm are analyzed in detail with its dependence on the nature of Power Networks. Tests are done and results for multi-process Distribution State Estimator scheduling executions on multi-core processors are presented.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5697155,no
How dynamic is the Grid? Towards a quality metric for Grid information systems,2010,"Grid information systems play a core role in today's production Grid Infrastructures. They provide a coherent view of the Grid services in the infrastructure while addressing the performance, robustness and scalability issues that occur in dynamic, large-scale, distributed systems. Quality metrics for Grid information systems are required in order to compare different implementations and to evaluate suggested improvements. This paper proposes the adoption of a quality metric, first used in the domain of Web search, to measure the quality of Grid information systems with respect to their information content. The application of this metric requires an understanding of the dynamic nature of Grid information. An empirical study based on information from the EGEE Grid infrastructure is carried out to estimate the frequency of change for different types of Grid information. Using this data, the proposed metric is assessed with regards to its applicability to measuring the quality of Grid information systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5697957,no
Analysis and modeling of time-correlated failures in large-scale distributed systems,2010,"The analysis and modeling of the failures bound to occur in today's large-scale production systems is invaluable in providing the understanding needed to make these systems fault-tolerant yet efficient. Many previous studies have modeled failures without taking into account the time-varying behavior of failures, under the assumption that failures are identically, but independently distributed. However, the presence of time correlations between failures (such as peak periods with increased failure rate) refutes this assumption and can have a significant impact on the effectiveness of fault-tolerance mechanisms. For example, the performance of a proactive fault-tolerance mechanism is more effective if the failures are periodic or predictable; similarly, the performance of checkpointing, redundancy, and scheduling solutions depends on the frequency of failures. In this study we analyze and model the time-varying behavior of failures in large-scale distributed systems. Our study is based on nineteen failure traces obtained from (mostly) production large-scale distributed systems, including grids, P2P systems, DNS servers, web servers, and desktop grids. We first investigate the time correlation of failures, and find that many of the studied traces exhibit strong daily patterns and high autocorrelation. Then, we derive a model that focuses on the peak failure periods occurring in real large-scale distributed systems. Our model characterizes the duration of peaks, the peak inter-arrival time, the inter-arrival time of failures during the peaks, and the duration of failures during peaks; we determine for each the best-fitting probability distribution from a set of several candidate distributions, and present the parameters of the (best) fit. Last, we validate our model against the nineteen real failure traces, and find that the failures it characterizes are responsible on average for over 50% and up to 95% of the downtime of these systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5697961,no
Adaptively detecting changes in Autonomic Grid Computing,2010,"Detecting the changes is the common issue in many application fields due to the non-stationary distribution of the applicative data, e.g., sensor network signals, web logs and grid-running logs. Toward Autonomic Grid Computing, adaptively detecting the changes in a grid system can help to alarm the anomalies, clean the noises, and report the new patterns. In this paper, we proposed an approach of self-adaptive change detection based on the Page-Hinkley statistic test. It handles the non-stationary distribution without the assumption of data distribution and the empirical setting of parameters. We validate the approach on the EGEE streaming jobs, and report its better performance on achieving higher accuracy comparing to the other change detection methods. Meanwhile this change detection process could help to discover the device fault which was not claimed in the system logs.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5698017,no
Validation of CK Metrics for Object Oriented Design Measurement,2010,"Since object oriented system is becoming more pervasive, it is necessary that software engineers have quantitative measurements for accessing the quality of designs at both the architectural and components level. These measures allow the designer to access the software early in the process, making changes that will reduce complexity and improve the continuing capability of the product. Object oriented design metrics is an essential part of software engineering. This paper presents a case study of applying design measures to assess software quality. Six Java based open source software systems are analyzed using CK metrics suite to find out quality of the system and possible design faults that will reversely affect different quality parameters such as reusability, understandability, testability, maintainability. This paper also presents general guidelines for interpretation of reusability, understandability, testability, maintainability in the context of selected projects.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5698406,no
Sensors integration in embedded systems,2010,"Sensors/actuators are critical components of several Complex Embedded Systems (CES). In these CES a fault in the sensors or actuators can trigger the incidence of catastrophic events. Sensor/actuator faults detection is difficult and impacts critically the system performance. Subsystem or Embedded Processing Component (EPC), integration testing with sensor or actuator is required to assure that the `Subsystem under test (SUT)' meets the specifications. Several researchers have addressed software integration issues only; however sensors/actuators integration issues were not addressed adequately. This paper focuses on the integration testing of sensors/actuators in ES. The sensor integration with EPC is viewed as identification of faulty communication channel within SUT. The Sensor and EPC models are developed based on and are modified for incorporating faults in the communication channel between them. A faulty communication channel is modelled by a â€œfaulty process Î¨â€?that changes the operations or possibly modifies events between the fault-free processes, while the components are assumed to be fault free. The functional tests are important in verifying system and their reuse in integration testing correctly identifies the known and tested system requirements. Comparing the observed output in the communication channel with expected response identifies the fault(s) in the channel and provides integration test verdict: pass or fail. An example illustrates the application of functional testing for integration of sensor.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5698681,no
Many Task Computing for modeling the fate of oil discharged from the Deep Water Horizon well blowout,2010,"The Deep Water Horizon well blowout on April 20th 2010 discharged between 40,000-1.2 million tons of crude oil into the Gulf of Mexico. In order to understand the fate and impact of the discharged oil, particularly on the environmentally sensitive Florida Keys region, we have implemented a multi-component application which consists of many individual tasks that utilize a distributed set of computational and data management resources. The application consists of two 3D ocean circulation models of the Gulf and South Florida and a 3D oil spill model. The ocean models used here resolve the Gulf at 2 km and the South Florida region at 900 m. This high resolution information on the ocean state is then integrated with the oil model to track the fate of approximately 10 million oil particles. These individual components execute as MPI based parallel applications on a 576 core IBM Power 5 cluster and a 5040 core Linux cluster, both operated by the Center for Computational Science, University of Miami. The data and workflow between is handled by means of a custom distributed software framework built around the Open Project for Networked Data Access Protocol (OPeNDAP). In this paper, we present this application as an example of Many Task Computing, report on the execution characteristics of this application, and discuss the challenges presented by the many task distributed workflow involving heterogeneous components. The application is a typical example from the ocean modeling and forecasting field and imposes soft timeliness and output quality constraints on top of the traditional performance requirements.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5699424,no
The Equipment Development of Detecting the Beer Bottles' Thickness in Real Time,2010,"In recent years, the requirement of beer bottle quality becomes ever-strict, so the annual detection can't satisfy the requirement of a modern industry, in this paper, we design a equipment which can detect the beer bottles' thickness in real time. The measurement system takes the chip computer W77E58 as the core, and takes the theory of ultrasonic measurement as the basis, we can observe directly the thickness of the detected beer bottle and judge the bottle is qualified or not by the visualization software. Through the analysis of experimental data, ultrasonic testing is relatively stable, it can meet the requirement of modern industry.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5701499,no
A Bayesian Based Method for Agile Software Development Release Planning and Project Health Monitoring,2010,"Agile software development (ASD) techniques are iteration based powerful methodologies to deliver high quality software. To ensure on time high quality software, the impact of factors affecting the development cycle should be evaluated constantly. Quick and precise factor evaluation results in better risk assessment, on time delivery and optimal use of resources. Such an assessment is easy to carry out for a small number of factors. However, with the increase of factors, it becomes extremely difficult to assess in short time periods. We have designed and developed a project health measurement model to evaluate the factors affecting software development of the project. We used Bayesian networks (BNs) as an approach that gives such an estimation. We present a quantitative model for project health evaluation that helps decision makers make the right decision early to amend any discrepancy that may hinder on time and high quality software delivery.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5702095,no
Two Efficient Software Techniques to Detect and Correct Control-Flow Errors,2010,"This paper proposes two efficient software techniques, Control-flow and Data Errors Correction using Data-flow Graph Consideration (CDCC) and Miniaturized Check-Pointing (MCP), to detect and correct control-flow errors. These techniques have been implemented based on addition of redundant codes in a given program. The creativity applied in the methods for online detection and correction of the control-flow errors is using data-flow graph alongside of using control-flow graph. These techniques can detect most of the control-flow errors in the program firstly, and next can correct them, automatically. Therefore, both errors in the control-flow and program data which is caused by control-flow errors can be corrected, efficiently. In order to evaluate the proposed techniques, a post compiler is used, so that the techniques can be applied to every 80Ã—86 binaries, transparently. Three benchmarks quick sort, matrix multiplication and linked list are used, and a total of 5000 transient faults are injected on several executable points in each program. The experimental results demonstrate that at least 93% and 89% of the control-flow errors can be detected and corrected without any data error generation by the CDCC and MCP, respectively. Moreover, the strength of these techniques is significant reduction in the performance and memory overheads in compare to traditional methods, for as much as remarkable correction abilities.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5703238,no
Phase Characterization and Classification for Micro-architecture Soft Error,2010,"Transient faults have become a key challenge to modern processor design. Processor designers take Architectural Vulnerability Factor (AVF) as an estimation method of micro-architectures soft error rate. Dynamic, phase-based system reliability management, which tunes system hardware and software parameters at runtime for different phases, has become a focus in the field of processor design. Phase characterization technique (PCT) and phase classification algorithm (PCA) determine the accuracy of phase identification, which is the foundation of dynamic, phase-based system management. To our knowledge, this paper is the first to give a comprehensive evaluation and comparison of PCTs and PCAs for micro-architecture soft error. We first compare the efficiency of basic block vectors (BBV) and performance metric counters (PMC) based PCTs in reliability-oriented phase characterization on three micro-architectural structures (i.e. instruction queue, function unit and reorder buffer). Experimental results show that PMC based PCT performs better than BBV based PCT for most programs studied. Also, we compare the accuracy of three clustering algorithms (i.e. hierarchical clustering, k-means clustering and regression tree) in reliability-oriented phase classification. Regression tree method is demonstrated to improve the accuracy of classification by 30% compared with other two PCAs on average. Furthermore, based on the comparisons of PCTs and PCAs, we propose the optimal combination of PCT and PCA for soft error reliability-oriented phase identification - the combination of PMC and regression tree. In addition, we quantify the upper bound of predictability of AVF using BBV/PMC. Overall, an average of 82% AVF can be explained by PMC, while BBV can explain 78% AVF averagely.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5703596,no
PBTrust: A Priority-Based Trust Model for Service Selection in General Service-Oriented Environments,2010,"How to choose the best service provider (agent), which a service consumer can trust in terms of the quality and success rate of the service in an open and dynamic environment, is a challenging problem in many service-oriented applications such as Internet-based grid systems, e-trading systems, as well as service-oriented computing systems. This paper presents a Priority-Based Trust (PBTrust) model for service selection in general service-oriented environments. The PBTrust is robust and novel from several perspectives. (1) The reputation of a service provider is derived from referees who are third parties and had interactions with the provider in a rich context format, including attributes of the service, the priority distribution on attributes and a rating value for each attribute from a third party, (2) The concept of 'Similarity' is introduced to measure the difference in terms of distributions of priorities on attributes between requested service and a refereed service in order to precisely predict the performance of a potential provider on the requested service, (3) The concept of general performance of a service provider on a service in history is also introduced to improve the success rate on the requested service. The experimental results can prove that PBtrust has a better performance than that of the CR model in a service-oriented environment.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5703618,no
A Novel Method to Measure Comprehensive Complexity of Software Based on the Metrics Statistical Model,2010,"Calculating software complexity is one of the most challenging problems in the Software Engineering due to using them in estimating errors, having a landscape of software reliability, approximating costs of software implementation and maintenance, and delivering software with better quality. Most of the recent researches on calculating the software's complexity focus on special directions and goals. This paper presents a novel method for measuring comprehensive complexity of software based on Statistical model evaluation of the existing complexity metrics through modules. To reach this purpose, the amount of comprehensive complexity is achieved for every module by identifying statistical distribution of complexity metric quantities, normalization and their combination. Afterward, the comprehensive complexity of the software is calculated by composition of the module's complexity amounts. This method is applied on some samples of the ""NASA Software Engineering laboratory"" and some of its positive results are presented.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5703738,no
Context-aware autonomic systems in real-time applications: Performance evaluation,2010,"Nowadays one can observe rapid development of various technologies supporting autonomic features and context-awareness of pervasive systems. And this is perfectly understandable as these features effectively increase the systems autonomicity. However, in the case of safety-critical realtime systems it becomes crucial to determine how the entire system performance is affected by the growth in complexity of its autonomic / dynamic decision making engine which is processing the context information for control purposes. In this paper we consider policy-based computing as a candidate technology to support selected autonomic features and context-awareness in safety-critical systems. We conduct performance analysis in a generic policy-supervised system in order to determine the relation between policy complexity (reflecting decision making system complexity) and the decision evaluation time (affecting the entire system performance). We present a coherent characteristic polynomial method as a means of pre-estimation of policy evaluation time depending on its complexity. The results we present can be potentially used for pre-evaluation of whether a policy of a certain complexity level is of any applicability in a safety critical system that has to meet fixed real-time deadlines.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5704090,no
Research on load balance of Service Capability Interaction Management,2010,"With the evolution of the IP Multimedia Subsystem (IMS), service interaction and management has received an increasing amount of attention. Service Capability Interaction Management (SCIM) is introduced as a component of Service Delivery Platform (SDP) for providing service interaction of IMS. Although there is much work on the standardization of SCIM, architectures based on industry standards like IMS may still have problems, especially the load balance issue, which has been seldom mentioned and discussed. Limited application servers become the bottleneck that affects the quality of service (QoS) due to the rising traffic levels. SCIM should provide load balance function among homogenous application servers to improve the performance of the whole system. We propose a module called Load Balancer that handles service messages from Call Control Network and sends each message to the optimal application server (AS). Four components of this module Load estimator, Load analyzer, Load Policies Center and Dispatcher cooperate together to implement the load balance function. This paper introduces load balance problem of SCIM from the network architecture viewpoint, then describes a load balance model and the mechanism of load balance. It discusses the concept and content of Load Policies which provide the necessary information for deciding the optimal AS to implement the load balance. Then it provides a load balance algorithm and the effectiveness is validated through simulation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5705082,no
Registry support for core component evolution,2010,"The United Nations Centre for Trade Facilitation and Electronic Business (UN/CEFACT) provides a conceptual approach, named the Core Components Technical Specification (CCTS), for creating business document models. These business document models are essential for defining service interfaces of service-oriented systems and are typically stored in a registry for enabling easy storage, search, and retrieval of these artifacts. However, in such a highly dynamic environment with ever-changing market demands, business partners are confronted with the need to constantly adapt their systems. This implies revising conceptual business document models as well as adapting the corresponding service interface definitions resulting in a tedious and often error-prone task when done manually. In this paper, we present a framework for dealing with these types of evolution in service-oriented systems. Having such a framework at hand, supports business partners in coping with the evolution of business document models, as well as in adapting the corresponding service interface definitions automatically. Furthermore, we present a prototypical implementation and an evaluation of the framework proposed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5707135,no
A Comparative Study of Ensemble Feature Selection Techniques for Software Defect Prediction,2010,"Feature selection has become the essential step in many data mining applications. Using a single feature subset selection method may generate local optima. Ensembles of feature selection methods attempt to combine multiple feature selection methods instead of using a single one. We present a comprehensive empirical study examining 17 different ensembles of feature ranking techniques (rankers) including six commonly-used feature ranking techniques, the signal-to-noise filter technique, and 11 threshold-based feature ranking techniques. This study utilized 16 real-world software measurement data sets of different sizes and built 13,600 classification models. Experimental results indicate that ensembles of very few rankers are very effective and even better than ensembles of many or all rankers.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5708824,no
Efficient packet classification on FPGAs also targeting at manageable memory consumption,2010,"Packet classification involving multiple fields is used in the area of network intrusion detection, as well as to provide quality of service and value-added network services. With the ever-increasing growth of the Internet and packet transfer rates, the number of rules needed to be handled simultaneously in support of these services has also increased. Field-Programmable Gate Arrays (FPGAs) provide good platforms for hardware-software co-designs that can yield high processing efficiency for highly complex applications. However, since FPGAs contain rather limited user-programmable resources, it becomes necessary for any FPGA-based packet classification algorithm to compress the rules as much as possible in order to achieve a widely acceptable on-chip solution in terms of performance and resource consumption; otherwise, a much slower external memory will become compulsory. We present a novel FPGA-oriented method for packet classification that can deal with rules involving multiple fields. This method first groups the rules based on their number of important fields, then attempts to match two fields at a time and finally combines the constituent results to identify longer matches. Our design with a single FPGA yields a larger than 9.68 Gbps throughput and has a small memory consumption of around 256Kbytes for more than 10,000 rules. This memory consumption is the lowest among all previously proposed FPGA-based designs for packet classification. Our scalable design can easily be extended to achieve a higher than 10Gbps throughput by employing multiple FPGAs running in parallel.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5709753,no
Rule Engine based on improvement Rete algorithm,2010,"Rete algorithm is the mainstream of the algorithm in the rules engine; it provides efficient local entities data and rules pattern matching method. But Rete algorithm applied in rules engine has defects in performance and demand aspect, this paper applies three methods to improve the Rete algorithm in the rule engine: rule decomposition, Alpha-Node-Hashing and Beta-Node-Indexing. Rules engine is enterprise applications, business logic framework for extracting rules from software application, and make it more flexible. This paper describes the principle of Rule Engine at first, secondly show the Rete algorithm, and finally we do some improvement to Rete algorithm. It makes Rule Engine widely meet the application demand and be greatly improved the efficiency.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5709916,no
QAM: QoS-assured management for interoperability and manageability for SOA,2010,"This paper presents the quality factors of web services with definition, classification, and sub-factors for interoperability and manageability. In SOA systems, service consumers and service providers are usually placed in different ownership domains. They are also developed independently on various platforms and loosely-coupled by network. Services are consumed generally without direct management by service consumers. As a result, interoperability and manageability for SOA systems are core issues for enabling SOA services. This paper addresses the new concept of sub-quality factors for interoperability and manageability in the view point of management. This is a result of SOA project designed for implementing and evaluating Korea e-government system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5711035,no
Quality factors of business value and service level measurement for SOA,2010,"This paper presents the quality factors of web services with definition, classification, and sub-factors for business and service level measurement. Web services, main enabler for SOA, usually have characteristics distinguished from general stand-alone software. They are provided in different ownership domain by network. They can also adopt various binding mechanism and be loosely-coupled, platform independent, and use standard protocols. As a result, a web service system requires its own quality factors unlike installation-based software. For instance, as the quality of web services can be altered in real-time according to the change of the service provider, considering real-time property of web services is very meaningful in describing the web services quality. This paper focuses especially to the two quality factors: business value and service level measurement, affecting real world business activities and a service performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5711036,no
Wide area measurement based out-of-step detection technique,2010,"The electrical power systems function as a huge interconnected network dispersed over a large area. A balance exist between generated and consumed power, any disturbance to this balance in the system caused due to change in load as well as faults and their clearance often results in electromechanical oscillations. As a result there is variation in power flow between two areas. This phenomenon is referred as Power Swing. This paper uses PMU data to measure the currents and voltages of the three phases of two buses connected to a 400 kV line. The measured data is then used for differentiating between a swing or fault condition, and if a swing is detected, to predict whether the swing is a stable or an unstable one. The performance of the method has been tested on a simulated system using PSCAD and MATLAB software.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5712569,no
The prediction of software aging trend based on user intention,2010,"Owing to the limitation of traditional software aging trend prediction method that based on time and based on measurement in dealing with sudden large scale concurrent questions, this paper proposes a new software aging trend prediction method which is based on user intention. This method predicts the trend of software aging according to the quantity of user requests for each components during the moment of system operation, and the software aging damage with each component is requested once.The experiment indicates, compared with the measurement method, this method has highter accuracy in dealing with sudden large scale concurrent questions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5713081,no
Diagnosing the root-causes of failures from cluster log files,2010,"System event logs are often the primary source of information for diagnosing (and predicting) the causes of failures for cluster systems. Due to interactions among the system hardware and software components, the system event logs for large cluster systems are comprised of streams of interleaved events, and only a small fraction of the events over a small time span are relevant to the diagnosis of a given failure. Furthermore, the process of troubleshooting the causes of failures is largely manual and ad-hoc. In this paper, we present a systematic methodology for reconstructing event order and establishing correlations among events which indicate the root-causes of a given failure from very large syslogs. We developed a diagnostics tool, FDiag, to extract the log entries as structured message templates and uses statistical correlation analysis to establish probable cause and effect relationships for the fault being analyzed. We applied FDiag to analyze failures due to breakdowns in interactions between the Lustre file system and its clients on the Ranger supercomputer at the Texas Advanced Computing Center (TACC). The results are positive. FDiag is able to identify the dates and the time periods that contain the significant events which eventually led to the occurrence of compute node soft lockups.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5713159,no
Fault-tolerant communication runtime support for data-centric programming models,2010,"The largest supercomputers in the world today consist of hundreds of thousands of processing cores and many more other hardware components. At such scales, hardware faults are a commonplace, necessitating fault-resilient software systems. While different fault-resilient models are available, most focus on allowing the computational processes to survive faults. On the other hand, we have recently started investigating fault resilience techniques for data-centric programming models such as the partitioned global address space (PGAS) models. The primary difference in data-centric models is the decoupling of computation and data locality. That is, data placement is decoupled from the executing processes, allowing us to view process failure (a physical node hosting a process is dead) separately from data failure (a physical node hosting data is dead). In this paper, we take a first step toward data-centric fault resilience by designing and implementing a fault-resilient, one-sided communication runtime framework using Global Arrays and its communication system, ARMCI. The framework consists of a fault-resilient process manager; low-overhead and network-assisted remote-node fault detection module; non-data-moving collective communication primitives; and failure semantics and err or codes for one-sided communication runtime systems. Our performance evaluation indicates that the framework incurs little overhead compared to state-of-the-art designs and provides a fundamental framework of fault resiliency for PGAS models.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5713195,no
QoS metrics for service level measurement for SOA environment,2010,"This paper presents the quality factors of web services with definition, classification, and sub-factors for business and service level measurement. Web services, main enabler for SOA, usually have characteristics distinguished from general stand-alone software. They are provided in different ownership domain by network. They can also adopt various binding mechanism and be loosely-coupled, platform independent, and use standard protocols. As a result, a web service system requires its own quality factors unlike installation-based software. For instance, as the quality of web services can be altered in real-time according to the change of the service provider, considering real-time property of web services is very meaningful in describing the web services quality. This paper establishes the QoS metrics for service level measurement and focuses especially to the two quality factors: business value and service level measurement, affecting real world business activities and a service performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5713503,no
Automating Coverage Metrics for Dynamic Web Applications,2010,"Building comprehensive test suites for web applications poses new challenges in software testing. Coverage criteria used for traditional systems to assess the quality of test cases are simply not sufficient for complex dynamic applications. As a result, faults in web applications can often be traced to insufficient testing coverage of the complex interactions between the components. This paper presents a new set of coverage criteria for web applications, based on page access, use of server variables, and interactions with the database. Following an instrumentation transformation to insert dynamic tracking of these aspects, a static analysis is used to automatically create a coverage database by extracting and executing only the instrumentation statements of the program. The database is then updated dynamically during execution by the instrumentation calls themselves. We demonstrate the usefulness of our coverage criteria and the precision of our approach on the analysis of the popular internet bulletin board system PhpBB 2.0.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5714417,no
Effort-Aware Defect Prediction Models,2010,"Defect Prediction Models aim at identifying error-prone modules of a software system to guide quality assurance activities such as tests or code reviews. Such models have been actively researched for more than a decade, with more than 100 published research papers. However, most of the models proposed so far have assumed that the cost of applying quality assurance activities is the same for each module. In a recent paper, we have shown that this fact can be exploited by a trivial classifier ordering files just by their size: such a classifier performs surprisingly good, at least when effort is ignored during the evaluation. When effort is considered, many classifiers perform not significantly better than a random selection of modules. In this paper, we compare two different strategies to include treatment effort into the prediction process, and evaluate the predictive power of such models. Both models perform significantly better when the evaluation measure takes the effort into account.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5714425,no
Using Architecturally Significant Requirements for Guiding System Evolution,2010,"Rapidly changing technology is one of the key triggers of system evolution. Some examples are: physically relocating a data center, replacement of infrastructure such as migrating from an in-house broker to CORBA, moving to a new architectural approach such as migrating from client-server to a service-oriented architecture. At a high level, the goals of such an evolution are easy to describe. While the end goals of the evolution are typically captured and known, the key architecturally significant requirements that guide the actual evolution tasks are often unexplored. At best, they are tucked under maintainability and/or modifiability concerns. In this paper, we argue that eliciting and using architecturally significant requirements of an evolution has a potential to significantly improve the quality of the evolution effort. We focus on elicitation and representation techniques of architecturally significant evolution requirements, and demonstrate their use in analysis for evolution planning.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5714427,no
Reverse Engineering Component Models for Quality Predictions,2010,"Legacy applications are still widely spread. If a need to change deployment or update its functionality arises, it becomes difficult to estimate the performance impact of such modifications due to absence of corresponding models. In this paper, we present an extendable integrated environment based on Eclipse developed in the scope of the Q-Impress project for reverse engineering of legacy applications (in C/C++/Java). The Q-Impress project aims at modeling quality attributes (performance, reliability, maintainability) at an architectural level and allows for choosing the most suitable variant for implementation of a desired modification. The main contributions of the project include i) a high integration of all steps of the entire process into a single tool, a beta version of which has been already successfully tested on a case study, ii) integration of multiple research approaches to performance modeling, and iii) an extendable underlying meta-model for different quality dimensions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5714435,no
InCode: Continuous Quality Assessment and Improvement,2010,"While significant progress has been made over the last ten years in the research field of quality assessment, developers still can't take full advantage of the benefits of these new tools and technique. We believe that there at least two main causes for this lack of adoption: (i) the lack of integration in mainstream IDEs and (ii) the lack of support for a continuous (daily) usage of QA tools. In this context we created INCODE as an Eclipe plug in that would transform quality assessment and code inspections from a standalone activity, into a continuous, agile process, fully integrated in the development life-cycle. But INCODE not only assesses continuously the quality of Java systems, it also assists developers in taking restructuring decisions, and even supports them in triggering refactorings.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5714452,no
Multi resolution analysis for bearing fault diagnosis,2010,A wavelet-based vibration analysis was done for Defense Applications. Vibration measurements were carried out in MSL Engine test bed in order to verify that it meets the specifications at different load conditions. Such measurements are often carried out in connection with troubleshooting in order to determine whether vibration levels are within acceptable limits at given engine speeds and maneuvers. A State-of-the-art portable Vibration Data Recorder is used for data acquisition of real-world signals. This paper is intended to take the reader through the various stages in a signal processing of the vibration data using modern digital technology. Vibration signals are post-analyzed using Wavelet Transform for data mining of the vibration signal observed from accelerometers. Wavelet Transform (WT) techniques are applied to decipher vibration characteristics due to Engine excitation to diagnose the faulty bearing. The Time-Scale analysis by WT achieves a comparable accuracy than Fast Fourier Transform (FFT) while having a lower computational cost with fast predictive capability. The result from wavelet analysis is validated using the LabVIEW software.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5714855,no
An Online Performance Anomaly Detector in Cluster File Systems,2010,"Performance problems, which can stem from different system components, such as network, memory, and storage devices, are difficult to diagnose and isolate in a cluster file system. In this paper, we present an online performance anomaly detector which is able to efficiently detect performance anomaly and accurately identify the faulty sources in a system node of a cluster file system. Our method exploits the stable relationship between workloads and system resource statistics to detect the performance anomaly and identify faulty sources which cause the performance anomaly in the system. Our preliminary experimental results demonstrate the efficiency and accuracy of the proposed performance anomaly detector.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5715083,no
A Software-Implemented Configurable Control Flow Checking Method,2010,"In space radiation environment, a large number of cosmic ray often results in transient faults on on-board computer. These transient faults lead to data flow errors or control flow errors during program running. For the control flow errors caused by transient faults, this paper proposes a control flow checking method based on classifying basic blocks CFCCB. CFCCB firstly classifies the basic blocks based on the control flow graph that has been inserted the abstract blocks. CFCCB then designs formatted signatures for the basic blocks and inserts the instructions for comparing and update signatures into every basic block for the purpose of checking the control flow errors that are inter-blocks, intra-blocks or inter-procedures. Compared to existing algorithms, CFCCB not only has high label express ability, but also can be configured flexibly. The fault injection experiment results of CFCCB and other similar algorithms have shown that, the average fail rate of programs with CFCCB has decreased to 19.9% at the cost of increasing the executing time by 34% and increasing the memory overhead by 41.5% in average. CFCCB has lower performance and memory overhead, and has highest reliability among the similar algorithms.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5715084,no
The Q-ImPrESS Method -- An Overview,2010,"The difficulty in evolving service-oriented architectures with extra-functional requirements seriously hinders the spread of this paradigm in critical application domains. The Q-ImPrESS method offsets this disadvantage by introducing a quality impact prediction, which allows software engineers to predict the consequences of alternative design decisions on the quality of software services and select the optimal architecture without having to resort to costly prototyping. The method takes a wider perspective on software quality by explicitly considering multiple quality attributes (performance, reliability and maintainability), and the typical trade-offs between these attributes. The benefit of using this approach is that it enables the creation of service-oriented systems with predictable end-to-end quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5715317,no
Estimating design quality of digital systems via machine learning,2010,"Although the term design quality of digital systems can be assessed from many aspects, the distribution and density of bugs are two decisive factors. This paper presents the application of machine learning techniques to model the relationship between specified metrics of high-level design and its associated bug information. By employing the project repository (i.e., high level design and bug repository), the resultant models can be used to estimate the quality of associated designs, which is very beneficial for design, verification and even maintenance processes of digital systems. A real industrial microprocessor is employed to validate our approach. We hope that our work can shed some light on the application of software techniques to help improve the reliability of various digital designs.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5724589,no
Novel fault diagnostic technique for permanent Magnet Synchronous Machines using electromagnetic signature analysis,2010,"This paper proposes a novel alternative scheme for permanent magnet synchronous machine (PMSM) health monitoring and multi-faults detection using direct flux measurement with search coils. Phase current spectrum is not used for analysis and therefore it is not influenced by power supply introduced harmonics any more. In addition, it is also not necessary to specify load condition for accurate diagnosis. In this study, numerical models of a healthy machine and of machines with various faults are developed and examined. Simulation by means of a two-dimensional finite-element analysis (FEA) software package is presented to verify the application of the proposed method over different motor operation conditions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5729130,no
Using pattern detection techniques and refactoring to improve the performance of ASMOV,2010,"One of the most important challenges in semantic Web is ontology matching. Ontology matching is a technology that enables semantic interoperability between structurally and semantically heterogeneous resources on the Web. Despite serious research efforts on ontology matching, matchers still suffers from severe problems with respect to the quality of matching results. Furthermore, Most of them take a lot of time for finding the correspondences. The aim of this paper is improving ontology matching results by adding the preprocessing phase for analyzing the input ontologies. This phase is added in order to solve problems caused by ontology diversity. We select one of the best matchers of Ontology Alignment Evaluation Initiative (OAEI) which is Automated Semantic Matching of Ontologies with Verification, called ASMOV. In preprocessing phase, some new patterns of ontologies are detected and then refactoring operations are used for reaching assimilated ontologies. Afterward, we applied ASMOV for testing our approach on both the original ontologies and their refactored counterparts. Experimental results show that these refactored ontologies are more efficient than the original unrepaired ones with respect to the standard evaluation measures i.e. Precision, Recall, and F-Measure.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5734164,no
Measuring testability of aspect oriented programs,2010,"Testability design is an effective way to realize the fault detection and isolation. It becomes crucial in the case of Aspect Oriented designs where control flows are generally not hierarchical, but are diffuse and distributed over the whole architecture. In this paper, we concentrate on detecting, pinpointing and suppressing potential testability weaknesses of a UML Aspect-class diagram. The attribute significant from design testability is called â€œclass interactionâ€? it appears when potentially concurrent client/supplier relationships between classes exist in the system. These interactions point out parts of the design that need to be improved, driving structural modifications or constraints specifications, to reduce the final testing effort. This paper does an extensive review on testability of aspect oriented software, and put forth some relevant information about class-level testability.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5735101,no
The research and development of comprehensive evaluation and management system for harmonic and negative-sequence,2010,"Power quality interference sources generate a large number of harmonics and negative-sequence current into power grid in the process of using electricity. Harmonics and negative-sequence current not only affect power grid's safety and economical operation also cause interference to other normal users, which brings large threat and danger to the power grid and users. As an effective method to prevent and control power quality interference source, it is important to evaluate the harmonic and negative-sequence current at the installation part of power quality interference source, guide the user taking measures to control power quality in the design of current electricity-consummation. This paper describes the research and development of comprehensive evaluation and management system for harmonic and negative-sequence current. This software has the following functions: harmonic computation, comprehensive assessment of user harmonic and negative-sequence, filter design and checking, SVC measurement, harmonic source database management. This software models the system components, load and typical harmonic source under the CIGRE standard with graphical interface. It models the power supply network by the means of one-line diagram, completes the harmonic calculation by Monte-Carlo or Laguerre polynomial, and simulates the distribution of power system harmonics by statistical moment with harmonics power flow techniques. The evaluation of user's harmonic and negative-sequence based mainly on the GB, combined with the access point short-circuit capacity, power capacity and protocol capacity to calculate the limit value of harmonic and negative-sequence current that assigned to user. According to the typical user's harmonic emission level or measured value, we can calculate the value of harmonic current generated by user and execute assessment by comparing them. One way to design filter and carry on SVC evaluation is calculating by custom method. Another way is to cut the ex- - isting data into calculation and checking, and then the bus voltage waveform before and after inputting filter or SVC by graphical virtual operation can be obtained, and a rich and intuitive results reports and graphics can be generated at the same time. This software is easy to operate, and its calculation results are accurate. It is an effective tool to assess and manage power quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5736040,no
An evolutionary algorithm for regression test suite reduction,2010,"As the software is modified and new test cases are added to the test-suite, the size of the test-suite grows and the cost of regression testing increases. In order to decrease the cost of regression testing, researchers have focused on the use of test-suite reduction techniques, which identify a subset of test cases that provides the same coverage of the software, according to some criterion, as the original test-suite. This paper investigates the use of an evolutionary approach, called genetic algorithms, for test-suite reduction. The proposed model builds the initial population based on test history, calculates the fitness value using coverage and run time of test case, and then selectively breeds the successive generations using genetic operations and allows only the fit tests to the reduced suite. This generational process is repeated until an optimized test-suite is found.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5738781,no
The development of monitoring and analyzing system of voltage sag,2010,"It is necessary to monitor and analyze the power quality firstly in order to solve the power quality problems in power system, and then the possible reforming project can be put forward for improving power supply quality. According to the actual situation in the region, a system approach to voltage sag monitoring and analyzing has been designed and developed. The system and its composition, software design, network architecture and application effects, etc, are detail introduced, analyzed and discussed in this paper. System software, including front-desk program, the background-desk program, analysis software, adopt homemade advanced unary multi-point voltage sag criterion used for giving more voltage sag characteristics, improves the monitoring accuracyÂ¿second-order Butterworth low-pass filters designed to reduce the impact of the voltage harmonics on the voltage sag detection algorithm, so reduces misjudgment rate during the voltage sag detection. The system has taken a series of measures, including signal acquisition, signal isolation module, anti-jamming, that is why it has a good reliability and stability. In the May 1 2007, a load rejection and voltage sag incident occurred in a 110kV substation, the device accurate records provided significant assistance to the accident analysis. The system meets the design requirements, that is, can be online accurately, high-density monitors the measurement system's three-phase bus voltage. Measurement system can accurately record the system operation state, the voltage sag occurs in the system will be accurate and timely recorded, and be able to regularly analyze the overall operation of the system voltage, it is beneficial to make detailed, scientific judgments and analysis for the system voltage sag and incidents.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5739654,no
Research and Design for Infrared Diagnostic System of Aviation Electronic Components,2010,"The civil airports now mostly use the imported equipment, the maintenance of electronic circuit board becomes the important segment of aviation electronic equipment maintenance. In the rising and development of component level maintenance, as a new nondestructive testing technology, infrared fault diagnosis gets more research and application. The paper utilizes the Ti50 thermal infrared imager to design a set of electronic circuit of infrared diagnostic system. Considering the real-time and complexity of electronic circuit board, by applying different signal sources, we first group the electronic components of electronic circuit board, then measure, which can rapidly narrow the scope of failure and prevent undetected. Through testing laboratories and the actual use, the speed of the test system is high, diagnostic efficiency is good, with a strong practical value.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5743395,no
The alice data quality monitoring system,2010,"ALICE (A Large Ion Collider Experiment) is a heavy-ion detector designed to study the physics of strongly interacting matter and the quark-gluon plasma at the CERN Large Hadron Collider (LHC). Due to the complexity of ALICE in terms of number of detectors and performance requirements, data quality monitoring (DQM) plays an essential role in providing an online feedback on the data being recorded. It intends to provide operators with precise and complete information to quickly identify and overcome problems, and, as a consequence, to ensure acquisition of high quality data. DQM typically involves the online gathering of data samples, their analysis by user-defined algorithms and the visualization of the monitoring results. In this paper, we illustrate the final design of the DQM software framework of ALICE, AMORE (Automatic Monitoring Environment), and its latest features and developments. We describe how this system is used to monitor the event data coming from the ALICE detectors allowing operators and experts to access a view of monitoring elements and to detect potential problems. Important features include the integration with the offline analysis and reconstruction framework and the interface with the electronic logbook that makes the monitoring results available everywhere through a web browser. Furthermore, we show the advantage of using multi-core processors through a parallel images/results production and the flexibility of the graphic user interface that gives to the user the possibility to apply filters and customize the visualization. We finally review the wide range of usage people make of this framework, from the basic monitoring of a single sub-detector to the most complex ones within the High Level Trigger farm or using the Prompt Reconstruction. We also describe the various ways of accessing the monitoring results. We conclude with our experience, after the LHC restart, when monitoring the data quality in a realworld and challenging environment.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5750364,no
The ATLAS trigger monitoring and operation in proton-proton collisions,2010,"With the first proton-proton collisions at âˆšs = 900 GeV in December 2009, the full chain of the ATLAS Trigger and Data Acquisition system could be tested under real conditions for the first time. Monitoring the performance and operation of these systems required a smooth and parallel running of many complex software tools depending on each other. They are the basis of rate measurements, data quality determination of selected objects and supervision of the system during the data taking. Based on the successful data taking experience with first collisions, the ATLAS trigger monitoring and operations performance are described.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5750450,no
Hardware/software optimization of error detection implementation for real-time embedded systems,2010,"This paper presents an approach to system-level optimization of error detection implementation in the context of fault-tolerant real-time distributed embedded systems used for safety-critical applications. An application is modeled as a set of processes communicating by messages. Processes are mapped on computation nodes connected to the communication infrastructure. To provide resiliency against transient faults, efficient error detection and recovery techniques have to be employed. Our main focus in this paper is on the efficient implementation of the error detection mechanisms. We have developed techniques to optimize the hardware/software implementation of error detection, in order to minimize the global worst-case schedule length, while meeting the imposed hardware cost constraints and tolerating multiple transient faults. We present two design optimization algorithms which are able to find feasible solutions given a limited amount of resources: the first one assumes that, when implemented in hardware, error detection is deployed on static reconfigurable FPGAs, while the second one considers partial dynamic reconfiguration capabilities of the FPGAs.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5751522,no
Structural Aware Quantitative Interprocedural Dataflow Analysis,2010,"A key challenge in system design whether for high performance computing or in embedded systems is to automatically partition software on the thread level for target architectures like multi-core, heterogeneous, or even hardware/software co-design systems. Similar to the techniques employed on the instruction level, the inter-procedural data flow in a software system is an essential property for identifying global data dependencies and hence extracting tasks or exploiting coarse grained parallelism. Also, detailed insight into the global data flow in quantity and its dynamics is vital to precisely estimate interconnect resources demands (e.g. bandwidth, latency) in distributed or embedded, heterogeneous target environments. This paper presents a novel approach to quantitative, global dataflow analysis based on a virtual instruction set architecture, which allows precise investigation of interprocedural dataflow over time of complex software system from the atomic to the transactional level. The introduced analysis system LLILA is part of the Synphony HW/SW co-design framework and can be used to evaluate the quality of system partitioning.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5758995,no
Step Up Transformer online monitoring experience in TucuruÃ­ Power Plant,2010,"The Step Up Transformers at the TucuruiÌ Hydroelectric Power Plant are very important for the National Interconnected System (SIN). Due to that, and due to the severe work conditions, EletrobraÌs Eletronorte has always kept a rigorous preventive maintenance program for these equipments. However, transformer failure history in the first powerhouse (older ones) led to the implantation of the online monitoring system, in order to detect the defects when they start, and mitigate the risks even more. System installation started in 2006, with sensors and software. Four transformers which were already operating began to be monitored and implantation for three more was in progress, taking advantage of the modularity and expandability features of the decentralized architecture used. The Architecture and the solutions applied in system implantation, as well as the results obtained, will be described in this paper. Some of the goals successfully attained were easier insurance negotiation for some equipment and more safety for the personnel, the equipment and the facility.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5762910,no
Vision based cross sectional area estimator for industrial rubber profile extrusion process controlling,2010,"This paper presents a vision based cross sectional area measurement system suitable for in-process quality controlling of rubber profile extrusions via cross section measurements. The system is basically developed for solid trapezoidal shaped rubber profiles commonly used in the rubber tack manufacturing industry. Since the extrusion continuously comes out the extruder, it is impossible to view the cross section directly. So in the proposed system, a laser projected at an angle on to the profile is used which is finally observed by a camera. Laser projections seen in each camera image is processed in a general purpose note book computer by the use of a vision software to estimate the cross section area of the actual profile. The paper contains the development approach of the method, experimental results and further improvement tips. Several test results are also presented in order to give an idea of the system accuracy and repeatability. The technology with further developments can be used as closed loop feedbacks for efficient process controlling of rubber profile extruders.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5767511,no
Vision based cross sectional area estimator for industrial rubber profile extrusion process controlling,2010,"This paper presents a vision based cross sectional area measurement system suitable for in-process quality controlling of rubber profile extrusions via cross section measurements. The system is basically developed for solid trapezoidal shaped rubber profiles commonly used in the rubber tack manufacturing industry. Since the extrusion continuously comes out the extruder, it is impossible to view the cross section directly. So in the proposed system, a laser projected at an angle on to the profile is used which is finally observed by a camera. Laser projections seen in each camera image is processed in a general purpose note book computer by the use of a vision software to estimate the cross section area of the actual profile. The paper contains the development approach of the method, experimental results and further improvement tips. Several test results are also presented in order to give an idea of the system accuracy and repeatability. The technology with further developments can be used as closed loop feedbacks for efficient process controlling of rubber profile extruders.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770280,no
PINCETTE â€?Validating changes and upgrades in networked software,2010,"Summary form only given. PINCETTE is a STREP project under the European Community's 7th Framework Programme [FP7/2007-2013]. The project focuses on detecting failures resulting from software changes, thus improving the reliability of networked software systems. The goal of the project is to produce technology for efficient and scalable verification of complex evolving networked software systems, based on integration of static and dynamic analysis and verification algorithms, and the accompanying methodology. The resulting technology will also provide quality metrics to measure the thoroughness of verification. The PINCETTE consortium is composed of the following partners: IBM Israel, University of Oxford, Universita della Svizzera Italiana (USI), Universita degli Studi di Milano-Bicocca (UniMiB), Technical Research Center of Finland (VTT), ABB, and Israeli Aerospace Industries (IAI).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770962,no
Monitoring and fault diagnosis of photovoltaic panels,2010,"Solar irradiance and temperature affect the performance of systems using photovoltaic generator. In the same way, it is essential to insure good performances of the installation so that its profitability won't be reduced. The objective of this work consists in diagnosing the panels faults and in certain cases in locating the faults using a model, the temperatures, the luminous flow, the speed of the wind, as well as the currents and the tensions. The development of software fault detection on a real installation is performed under the Matlab/Simulink environment.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5771711,no
Analyzing personality types to predict team performance,2010,"This paper presents an approach in analyzing personality types, temperament and team diversity to determine software engineering (SE) teams performance. The benefits of understanding personality types and its relationships amongst team members are crucial for project success. Rough set analysis was used to analyze Myers-Briggs Type Indicator (MBTI) personality types, Keirsey temperament, team diversity, and team performance. The result shows positive relationships between these attributes.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5773856,no
Test effort optimization by prediction and ranking of fault-prone software modules,2010,"Identification of fault-prone or not fault-prone modules is very essential to improve the reliability and quality of a software system. Once modules are categorized as fault-prone or not fault-prone, test effort are allocated accordingly. Testing effort and efficiency are primary concern and can be optimized by prediction and ranking of fault-prone modules. This paper discusses a new model for prediction and ranking of fault-prone software modules for test effort optimization. Model utilizes the classification capability of data mining techniques and knowledge stored in software metrics to classify the software module as fault-prone or not fault-prone. A decision tree is constructed using ID3 algorithm for the existing project data. Rules are derived form the decision tree and integrated with fuzzy inference system to classify the modules as either fault-prone or not fault-prone for the target data. The model is also able to rank the fault-prone module on the basis of its degree of fault-proneness. The model accuracy are validated and compared with some other models by using the NASA projects data set of PROMOSE repository.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5779531,no
Reliability comparison of computer based core temperature monitoring system with two and three thermocouples per sub-assembly for Fast Breeder Reactors,2010,"Prototype Fast Breeder Reactor (PFBR) is a mixed oxide fuelled, sodium cooled, 500 MWe, pool type fast breeder reactor under construction at Kalpakkam, India. The reactor core consists of fuel pins assembled in a number of hexagonal shaped, vertically stacked SubAssemblies (SA). Sodium flows from the bottom of the SAs, takes heat from the fission reaction, comes out through the top. Reactor protection systems are provided to trip the reactor in case of design basis events which may cause the safety parameters (like clad, fuel and coolant temperature) to cross their limits. Computer based Core temperature monitoring system (CTMS) is one of the protection systems. CTMS for PFBR has two thermocouples (TC) at the outlet of each SA(other than central SA) to measure coolant outlet temperature, three TC at central SA outlet and six thermocouples to measure coolant inlet temperature. Each thermocouple at SA outlet is electronically triplicated and fed to three computer systems for further processing and generate reactor trip signal whenever necessary. Since the system has two sensors per SA and three processing units the redundancy provided is not independent. A study is done to analyze the reliability implications of providing three thermocouples at the outlet of each SA and thereby feed independent thermocouple signals to three computer systems. Failure data derived from fast reactor experiences and from reliability prediction methods provided by handbooks are used. Fault trees are built for the existing CTMS system with two TC per SA and for the proposed system with three TC per SA. Failure probability upon demand and spurious trip rates are estimated as reliability indicators. Since the computer systems have software intelligence to sense invalid field inputs, not all sensor failures would directly affect the system probability to fail upon a demand. For instance, the coolant outlet temperature cannot be lower than the coolant inlet temperature. This intelligence is ta- en into account by assuming different â€œfault coverage percentageâ€?and comparing the results. A 100% fault coverage means the software algorithm could detect all of the possible thermocouple faults. It was found that the system probability to fail upon demand is reduced in the new independent system but the spurious trip rate is slightly worse. The diagnostic capability is marginally affected due to complete independence. The paper highlights how an intelligent computer based safety system poses difficulties in modeling and the checks and balances between an interlinked and independent redundancy.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5779593,no
Regulatory review of computer based systems: Indian perspectives,2010,"The use of state of art digital instrumentation and control (I&C) in safety and safety related systems in nuclear power plants has become prevalent due to the performance in terms of accuracy, computational capabilities and data archiving capability for future diagnosis. Added advantages in computer based systems are fault tolerance, self-testing, signal validation capability and process system diagnostics. But, uncertainty exists about the quality, reliability and performance of such software based nuclear instrumentation which poses new challenges for the industry and regulators in using them for safety and safety related systems. To obtain adequate confidence in licensing them for use in NPPs, CBS were deployed gradually from monitoring system to control system (i.e, non-safety, safety related & lastly safety systems). Based upon the experience over a decade, AERB safety guide AERB/SGID-25 was prepared to prescribe the criteria and requirements to assess the qualitative reliability of such software based nuclear instrumentation. This paper describes the regulatory review and audit process as required by the above guide. Further, Software Configuration Management (SCM) is an important item during life cycle of CBS, whether it is design phase or operating phase. Configuration control becomes necessary due to operation feedback, introduction of additional features and due to obsolescence. Therefore configuration control during operating phase for CBS becomes all the more important. This paper elaborates on the regulatory approach adopted by AERB for regulatory review and control of design modifications in operating phase of NPPs. This paper also covers a case study of AERB audit on verification & validation activities for software based safety and safety related systems used in an Indian plant.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5779596,no
Optimal testing resource allocation for modular software considering imperfect debugging and change point using genetic algorithm,2010,"Optimal decision making is of utmost importance for planning, controlling, and working of any industry. One of the fields where mathematical modeling and optimization have been vastly applied is software reliability, which is the most significant quality metric for commercial software. Such softwares are generally modular in structure. Testing provides a mathematical measure of software reliability and is a process of raising the confidence that the software is free of flaws. However, due to the complexity of software, the testing team may not be able to remove/correct the fault perfectly on observation/detection of a failure and the original fault may remain resulting in a phenomenon known as imperfect debugging, or get replaced by another fault causing error generation. Moreover, fault detection/correction rate may not be same throughout testing; it may change at any time moment called as change point. Also, testing cannot be done indefinitely due to the availability of limited testing resources. In this paper we formulate an optimization problem to allocate the resources among different modules such that the total fault removal is maximized while incorporating the effect of both types of imperfect debugging and change point. The relative importance of each module is obtained using Analytical Hierarchy Process. We have also considered the problem of determining minimum requirement of the testing resources so that a desired proportion of faults are removed from each module. The non linear optimization problems are solved using genetic algorithm. Numerical example has been discussed to illustrate the solution of the formulated optimal resource allocation problems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5779607,no
"Clone detection: Why, what and how?",2010,"Excessive code duplication is a bane of modern software development. Several experimental studies show that on average 15 percent of a software system can contain source code clones - repeatedly reused fragments of similar code. While code duplication may increase the speed of initial software development, it undoubtedly leads to problems during software maintenance and support. That is why many developers agree that software clones should be detected and dealt with at every stage of software development life cycle. This paper is a brief survey of current state-of-the-art in clone detection. First, we highlight main sources of code cloning such as copy-and-paste programming, mental code patterns and performance optimizations. We discuss reasons behind the use of these techniques from the developer's point of view and possible alternatives to them. Second, we outline major negative effects that clones have on software development. The most serious drawback duplicated code have on software maintenance is increasing the cost of modifications - any modification that changes cloned code must be propagated to every clone instance in the program. Software clones may also create new software bugs when a programmer makes some mistakes during code copying and modification. Increase of source code size due to duplication leads to additional difficulty of code comprehension. Third, we review existing clone detection techniques. Classification based on used source code representation model is given in this work. We also describe and analyze some concrete examples of clone detection techniques highlighting main distinctive features and problems that are present in practical clone detection. Finally, we point out some open problems in the area of clone detection. Currently questions like ""What is a code clone?"", ""Can we predict the impact clones have on software quality"" and ""How can we increase both clone detection precision and recall at the same time? "" stay open to further re- - search. We list the most important questions in modern clone detection and explain why they continue to remain unanswered despite all the progress in clone detection research.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5783148,no
Source code modification technology based on parameterized code patterns,2010,"Source code modification is one of the most frequent operations which developers perform in software life cycle. Such operation can be performed in order to add new functionality, fix bugs or bad code style, optimize performance, increase readability, etc. During the modification of existing source code developer needs to find parts of code, which meet to some conditions, and change it according to some rules. Usually developers perform such operations repeatedly by hand using primitive search/replace mechanisms and ""copy and paste programming"", and that is why manual modification of large-scale software systems is a very error-prone and time-consuming process. Automating source code modifications is one of the possible ways of coping with this problem because it can considerably decrease both the amount of errors and the time needed in the modification process. Automated source code modification technique based on parameterized source code patterns is considered in this article. Intuitive modification description that does not require any knowledge of complex transformation description languages is the main advantage of our technique. We achieve this feature by using a special source code pattern description language which is closely tied with the programming language we're modifying. This allows developers to express the modification at hand as simple ""before ""/""after"" source code patterns very similar to source code. Regexp-like elements are added to the language to increase its expressionalpower. The source code modification is carried out using difference abstract syntax trees. We build a set of transformation operations based on ""before""/""after"" patterns (using algorithm for change detection in hierarchically structured information) and apply them to those parts of source code that match with the search ""before "" pattern. After abstract syntax tree transformation is completed we pretty-print them back to source code. A prototype of source code modification sy- - stem based on this technique has been implemented for the Java programming language. Experimental results show that this technique in some cases can increase the speed of source code modifications by several orders of magnitude, at the same time completely avoiding ""copy-and paste "" errors. In future we are planning to integrate prototype with existing development environments such as Eclipse and NetBeans.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5783177,no
Simulation strategies for the LHC ATLAS experiment,2010,"The ATLAS experiment, operational at the new LHC collider, is fully simulated using a Geant 4-based simulation program within the ATLAS Athena framework. This simulation software is used for large-scale production of events on the LHC Computing Grid as well as for smaller-scale studies. Simulation of ATLAS requires many components, from the generators that simulate particle collisions to packages simulating the response of the various detectors and triggers. The latest developments in the saga of ATLAS simulation have been focussed on better representation of the real detector, based on first LHC data, and on performance improvements to ensure that event simulation is not a limiting factor in ATLAS physics analyses. The full process is constantly monitored and profiled to guarantee the best use of available resources without any degradation in the quality and accuracy of the simulation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5873849,no
Diagnostic systems and resource utilization of the ATLAS high level trigger,2010,"With the first year of successful data taking of proton-proton collisions at LHC, the full chain of the ATLAS Trigger and Data Acquisition System could be tested under real conditions. The trigger monitoring and data quality infrastructure was essential to this success. We describe the software tools used to monitor the trigger system performance, assess the overall quality of the trigger selection and analyze the resource utilization during collision runs. Monitoring the performance and operation of these systems required smooth and parallel running of many complex software tools depending on each other. These are the basis for rate measurements, data quality determination of selected objects and supervision of the system during the data taking. Based on the data taking experience with first collisions, we describe the ATLAS trigger monitoring and operations performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5873867,no
New ultra high resolution LYSO pentagon detector blocks for lower-cost murine PET-CT (MuPET/CT),2010,"We developed and built a solid LYSO detector ring for a new MD Anderson Murine PET (MUPET) camera using regular Ã˜ 19 mm photomultipliers in quadrant-sharing (PQS) configuration. 180 PQS pentagon blocks distributed in 6 sub-rings (30 blocks each) constitute the detector system Each block has nominal dimensions of 19 Ã—x 19 Ã— 10 mm<sup>3</sup> in a 13 Ã— 13 crystal array. To form a zero gap solid ring, PQS-blocks were ground into pentagon shapes with a taper angle of 6Â° on the edge crystals and optical surfaces. Two crystal widths were used for the blocks; a largest width for two edge crystals and, smallest for inner crystals. We explored the possibility of increasing the detectors performance by implementing new design, materials and techniques, testing the detectors response and, measuring the timing resolution. List-mode data were acquired using Ga-68 source with our high yield pileup-event recovery (HYPER) electronics and data acquisition software. Four PQS-blocks were selected to evaluate our detector quality and our mass-production method. The performances of these blocks were quite similar. A typical block is presented in this study. This block has a packing fraction of 95%. All 169 crystal detectors are clearly decoded, peak-to-valley ratio of 2.4, light collection efficiency of 78% and, energy resolution of 14% at 511 keV. Average timing resolution of 430-ps (FWHM) was achieved by single crystal with PQS block-to-block coincidence of 530-ps. This PQS-pentagon block design shows the feasibility of high resolution (~1.2 mm) MuPET-CT detector ring using the lower cost Ã˜ 19 mm PMTs as an alternative to the more expensive position sensitive PMTs or solid state detectors.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5874430,no
Simulation of high quality ultrasound imaging,2010,"This paper investigates if the influence on image quality using physical transducers can be simulated with an sufficient accuracy to reveal system performance. The influence is investigated in a comparative study between Synthetic Aperture Sequential Beamformation (SASB) and Dynamic Receive Focus (DRF). The study is performed as a series of simulations and validated by measurements. The influence from individual element impulse response, phase, and amplitude deviations are quantized by the lateral resolution (LR) at Full Width at Half Maximum (FWHM), Full Width at One-Tenth Maximum (FWOTM), and at Full Width at One-Hundredth Maximum (FWOHM) of 9 points spread functions resulting from evenly distributed point targets at depths ranging from 10 mm to 90 mm. The results are documented for a 64 channel system, using a 192 element linear array transducer model. A physical BK Medical 8804 transducer is modeled by incorporating measured element pulse echo responses into the simulation software. Validation is performed through measurements on a water phantom with three metal wires, each with a diameter of 0.07 mm. Results show that when comparing measurement and simulation, the lateral beam profile using SASB can be estimated with a correlation coefficient of 0.97. Further, it is shown that SASB successfully maintains a constant LR though depth at FWHM, and is a factor of 2.3 better than DRF at 80 mm. However, when using SASB the LR at FWOHM is affected by non-ideal element responses. Introducing amplitude and phase compensation, the LR at FWOHM improves from 6.3 mm to 4.7 mm and is a factor of 2.2 better than DRF. This study has shown that individual element impulse response, phase, and amplitude deviations are important to include in simulated system performance evaluations. Furthermore, it is shown that SASB provides a constant LR through depth and has improved resolution and contrast compared to DRF.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5935765,no
Design considerations for a high voltage compact power transformer,2010,"This paper presents a new topology for a high voltage 50kV, high frequency (HF) 20kHz, multi-cored transformer. The transformer is suitable for use in pulsed power application systems. The main requirements are: high voltage capability, small size and weight. The HV, HF transformer is a main critical block of a high frequency power converter system. The transformer must have high electrical efficiency and in the proposed approach has to be optimized by the number of the cores. The transformer concept has been investigated analytically and through software simulations and experiments. This paper introduces the transformer topology and discusses the design procedure. Experimental measurements to predict core losses are also presented.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5958305,no
Advanced frequency estimation technique using gain compensation,2010,"The frequency is an important operating parameter for protection, control, and stability of a power system. Due to the sudden change in generation and loads or faults in power system, the frequency is supposed to deviate from its nominal value. It is essential that the frequency be maintained very close to its nominal frequency. An accurate monitoring of the power frequency is essential to optimum operation and prevention for wide area blackout. As most conventional frequency estimation schemes are based on DFT filter, it has been pointed out that the gain error could cause the defects when the frequency is deviated from nominal value. This paper presents an advanced frequency estimation technique using gain compensation to enhance the DFT filter based technique. The proposed technique can reduce the gain error caused when the frequency deviates from nominal value. Simulation studies using both the data from EMTP-RV software and user defined arbitrary signals are performed to demonstrate the effectiveness of the proposed algorithm. The simulation results show that the proposed algorithm achieves good performance under both steady state tests and dynamic conditions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6007231,no
The Network Interdisciplinary Computing Environment,2010,"The study of mobile networking modeling for ad hoc networks is still in its infancy, a full analysis environment of common utilities does not exist. While some very important capability currently exists, a means of integrating them together into a production quality environment does not. The Network Interdisciplinary Computing Environment (NiCE) is designed to accomplish this task. NiCE is built around a new common data model and format for mobile networking experiments and codes. NiCE provides network researchers a cross-platform setup, runtime, analysis and visualization environment. Since this computational discipline is not as mature as some other physics based modeling efforts, it is difficult to exchange models and results between experimental and computational resources to develop high-fidelity, accurate predictions of how various networks will perform under realistic military conditions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6018046,no
Maturity model for process of academic management,2010,"The segment of education in Brazil, especially in higher education, has undergone major changes. The search for professionalization, for cost reduction and process standardization has led many institutions to associate through partnerships or acquisitions. On the other hand, maturity models have been used very successfully in several areas of knowledge especially in software development, as an approach for quality models. This paper presents a methodology for assessing the maturity of academic process management in private institutions of higher education that, initially, aims the Brazilian market, but its idea can be applied to a global maturity model of academic process management.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6018745,no
"Measuring complexity, effectiveness and efficiency in software course projects",2010,"This paper discusses results achieved in measuring complexity, effectiveness and efficiency, in a series of related software course projects, spanning a period of seven years. We focus on how the complexity of those projects was measured, and how the success of the students in effectively and efficiently taming that complexity was assessed. This required defining, collecting, validating and analyzing several indicators of size, effort and quality; their rationales, advantages and limitations are discussed. The resulting findings helped to improve the process itself.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062058,no
A discriminative model approach for accurate duplicate bug report retrieval,2010,"Bug repositories are usually maintained in software projects. Testers or users submit bug reports to identify various issues with systems. Sometimes two or more bug reports correspond to the same defect. To address the problem with duplicate bug reports, a person called a triager needs to manually label these bug reports as duplicates, and link them to their ""master"" reports for subsequent maintenance work. However, in practice there are considerable duplicate bug reports sent daily; requesting triagers to manually label these bugs could be highly time consuming. To address this issue, recently, several techniques have be proposed using various similarity based metrics to detect candidate duplicate bug reports for manual verification. Automating triaging has been proved challenging as two reports of the same bug could be written in various ways. There is still much room for improvement in terms of accuracy of duplicate detection process. In this paper, we leverage recent advances on using discriminative models for information retrieval to detect duplicate bug reports more accurately. We have validated our approach on three large software bug repositories from Firefox, Eclipse, and OpenOffice. We show that our technique could result in 17-31%, 22-26%, and 35-43% relative improvement over state-of-the-art techniques in OpenOffice, Firefox, and Eclipse datasets respectively using commonly available natural language information only.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062072,no
A machine learning approach for tracing regulatory codes to product specific requirements,2010,"Regulatory standards, designed to protect the safety, security, and privacy of the public, govern numerous areas of software intensive systems. Project personnel must therefore demonstrate that an as-built system meets all relevant regulatory codes. Current methods for demonstrating compliance rely either on after-the-fact audits, which can lead to significant refactoring when regulations are not met, or else require analysts to construct and use traceability matrices to demonstrate compliance. Manual tracing can be prohibitively time-consuming; however automated trace retrieval methods are not very effective due to the vocabulary mismatches that often occur between regulatory codes and product level requirements. This paper introduces and evaluates two machine-learning methods, designed to improve the quality of traces generated between regulatory codes and product level requirements. The first approach uses manually created traceability matrices to train a trace classifier, while the second approach uses web-mining techniques to reconstruct the original trace query. The techniques were evaluated against security regulations from the USA government's Health Insurance Privacy and Portability Act (HIPAA) traced against ten healthcare related requirements specifications. Results demonstrated improvements for the subset of HIPAA regulations that exhibited high fan-out behavior across the requirements datasets.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062083,no
Predicting build outcome with developer interaction in Jazz,2010,Investigating the human aspect of software development is becoming prominent in current research. Studies found that the misalignment between the social and technical dimensions of software work leads to losses in developer productivity and defects. We use the technical and social dependencies among pairs of developers to predict the success of a software build. Using the IBM Jazzâ„?data we found information about developers and their social and technical relation can build a powerful predictor for the success of a software build. Investigating human aspects of software development is becoming prominent in current research. High misalignment between the social and technical dimensions of software work lowers productivity and quality.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062122,no
Transparent combination of expert and measurement data for defect prediction: an industrial case study,2010,"Defining strategies on how to perform quality assurance (QA) and how to control such activities is a challenging task for organizations developing or maintaining software and software-intensive systems. Planning and adjusting QA activities could benefit from accurate estimations of the expected defect content of relevant artifacts and the effectiveness of important quality assurance activities. Combining expert opinion with commonly available measurement data in a hybrid way promises to overcome the weaknesses of purely data-driven or purely expert-based estimation methods. This article presents a case study of the hybrid estimation method HyDEEP for estimating defect content and QA effectiveness in the telecommunication domain. The specific focus of this case study is the use of the method for gaining quantitative predictions. This aspect has not been empirically analyzed in previous work. Among other things, the results show that for defect content estimation, the method performs significantly better statistically than purely data-based methods, with a relative error of 0.3 on average (MMRE).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062145,no
A cost-benefit framework for making architectural decisions in a business context,2010,"In any IT-intensive organization, it is useful to have a model to associate a value with software and system architecture decisions. More generally, any effort-a project undertaken by a team-needs to have an associated value to offset its labor and capital costs. Unfortunately, it is extremely difficult to precisely evaluate the benefit of ""architecture projects""-those that aim to improve one or more quality attributes of a system via a structural transformation without (generally) changing its behavior. We often resort to anecdotal and informal ""hand-waving"" arguments of risk reduction or increased developer productivity. These arguments are typically unsatisfying to the management of organizations accustomed to decision-making based on concrete metrics. This paper will discuss research done to address this long-standing dilemma. Specifically, we will present a model derived from analyzing actual projects undertaken at Vistaprint Corporation. The model presented is derived from an analysis of effort tracked against modifications to specific software components before and after a significant architectural transformation to the subsystem housing those components. In this paper, we will discuss the development, implementation, and iteration of the model and the results that we have obtained.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062148,no
JDF: detecting duplicate bug reports in Jazz,2010,"Both developers and users submit bug reports to a bug repository. These reports can help reveal defects and improve software quality. As the number of bug reports in a bug repository increases, the number of the potential duplicate bug reports increases. Detecting duplicate bug reports helps reduce development efforts in fixing defects. However, it is challenging to manually detect all potential duplicates because of the large number of existing bug reports. This paper presents JDF (representing Jazz Duplicate Finder), a tool that helps users to find potential duplicates of bug reports on Jazz, which is a team collaboration platform for software development and process management. JDF finds potential duplicates for a given bug report using natural language and execution information.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062192,no
Exploratory study of a UML metric for fault prediction,2010,"This paper describes the use of a UML metric, an approximation of the CK-RFC metric, for predicting faulty classes before their implementation. We built a code-based prediction model of faulty classes using Logistic Regression. Then, we tested it in different projects, using on the one hand their UML metrics, and on the other hand their code metrics. To decrease the difference of values between UML and code measures, we normalized them using Linear Scaling to Unit Variance. Our results indicate that the proposed UML RFC metric can predict faulty code as well as its corresponding code metric does. Moreover, the normalization procedure used was of great utility, not just for enabling our UML metric to predict faulty code, using a code-based prediction model, but also for improving the prediction results across different packages and projects, using the same model.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062214,no
Capturing the long-term impact of changes,2010,"Developers change source code to add new functionality, fix bugs, or refactor their code. Many of these changes have immediate impact on quality or stability. However, some impact of changes may become evident only in the long term. The goal of this thesis is to explore the long-term impact of changes by detecting dependencies between code changes and by measuring their influence on software quality, software maintainability, and development effort. Being able to identify the changes with the greatest long-term impact will strengthen our understanding of a project's history and thus shape future code changes and decisions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062222,no
First International Workshop on Quantitative Stochastic Models in the Verification and Design of Software Systems (QUOVADIS 2010),2010,"Nowadays requirements related to quality attributes such as performance, reliability, safety and security are often considered the most important requirements for software development projects. To reason about these quality attributes different stochastic models can be used. These models enable probabilistic verification as well as quantitative prediction at design time. On the other hand, these models could be also used to perform runtime adaptation in order to achieve certain quality goals. This workshop aims to provide a forum for researchers in these areas that should help with the adoption of quantitative stochastic models into general software development processes.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062243,no
Spatial error concealment technique for losslessly compressed images using data hiding in error-prone channels,2010,"Error concealment techniques are significant due to the growing interest in imagery transmission over error-prone channels. This paper presents a spatial error concealment technique for losslessly compressed images using least significant bit (LSB)-based data hiding to reconstruct a close approximation after the loss of image blocks during image transmission. Before transmission, block description information (BDI) is generated by applying quantization following discrete wavelet transform. This is then embedded into the LSB plane of the original image itself at the encoder. At the decoder, this BDI is used to conceal blocks that may have been dropped during the transmission. Although the original image is modified slightly by the message embedding process, no perceptible artifacts are introduced and the visual quality is sufficient for analysis and diagnosis. In comparisons with previous methods at various loss rates, the proposed technique is shown to be promising due to its good performance in the case of a loss of isolated and continuous blocks.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6391373,no
Automated Derivation of Application-Aware Error Detectors Using Static Analysis: The Trusted Illiac Approach,2011,"This paper presents a technique to derive and implement error detectors to protect an application from data errors. The error detectors are derived automatically using compiler-based static analysis from the backward program slice of critical variables in the program. Critical variables are defined as those that are highly sensitive to errors, and deriving error detectors for these variables provides high coverage for errors in any data value used in the program. The error detectors take the form of checking expressions and are optimized for each control-flow path followed at runtime. The derived detectors are implemented using a combination of hardware and software and continuously monitor the application at runtime. If an error is detected at runtime, the application is stopped so as to prevent error propagation and enable a clean recovery. Experiments show that the derived detectors achieve low-overhead error detection while providing high coverage for errors that matter to the application.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5089331,no
Nondetection Zone Assessment of an Active Islanding Detection Method and its Experimental Evaluation,2011,"This paper analytically determines the nondetection zone (NDZ) of an active islanding detection method, and proposes a solution to obviate the NDZ. The method actively injects a negative-sequence current through the interface voltage-sourced converter (VSC) of a distributed generation (DG) unit, as a disturbance signal for islanding detection. The estimated magnitude of the corresponding negative-sequence voltage at the PCC is used as the islanding detection signal. In this paper, based on a laboratory test system, the performance of the islanding detection method under UL1741 anti-islanding test conditions is evaluated. Then, determining the NDZ of the method and proposing the countermeasure, the existence of the NDZ and the performance of the modified method to eliminate the NDZ is verified based on simulation results in PSCAD/EMTDC software environment and experimental tests.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5350388,no
Self-Supervising BPEL Processes,2011,"Service compositions suffer changes in their partner services. Even if the composition does not change, its behavior may evolve over time and become incorrect. Such changes cannot be fully foreseen through prerelease validation, but impose a shift in the quality assessment activities. Provided functionality and quality of service must be continuously probed while the application executes, and the application itself must be able to take corrective actions to preserve its dependability and robustness. We propose the idea of self-supervising BPEL processes, that is, special-purpose compositions that assess their behavior and react through user-defined rules. Supervision consists of monitoring and recovery. The former checks the system's execution to see whether everything is proceeding as planned, while the latter attempts to fix any anomalies. The paper introduces two languages for defining monitoring and recovery and explains how to use them to enrich BPEL processes with self-supervision capabilities. Supervision is treated as a cross-cutting concern that is only blended at runtime, allowing different stakeholders to adopt different strategies with no impact on the actual business logic. The paper also presents a supervision-aware runtime framework for executing the enriched processes, and briefly discusses the results of in-lab experiments and of a first evaluation with industrial partners.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5432226,no
GUI Interaction Testing: Incorporating Event Context,2011,"Graphical user interfaces (GUIs), due to their event-driven nature, present an enormous and potentially unbounded way for users to interact with software. During testing, it is important to â€œadequately coverâ€?this interaction space. In this paper, we develop a new family of coverage criteria for GUI testing grounded in combinatorial interaction testing. The key motivation of using combinatorial techniques is that they enable us to incorporate â€œcontextâ€?into the criteria in terms of event combinations, sequence length, and by including all possible positions for each event. Our new criteria range in both efficiency (measured by the size of the test suite) and effectiveness (the ability of the test suites to detect faults). In a case study on eight applications, we automatically generate test cases and systematically explore the impact of context, as captured by our new criteria. Our study shows that by increasing the event combinations tested and by controlling the relative positions of events defined by the new criteria, we can detect a large number of faults that were undetectable by earlier techniques.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5444885,no
Recovery Device for Real-Time Dual-Redundant Computer Systems,2011,"This paper proposes the design of specialized hardware, called Recovery Device, for a dual-redundant computer system that operates in real-time. Recovery Device executes all fault-tolerant services including fault detection, fault type determination, fault localization, recovery of system after temporary (transient) fault, and reconfiguration of system after permanent fault. The paper also proposes the algorithms for determination of fault type (whether the fault is temporary or permanent) and localization of faulty computer without using self-testing techniques and diagnosis routines. Determination of fault type allows us to eliminate only the computer with a permanent fault. In other words, the determination of fault type prevents the elimination of nonfaulty computer because of short temporary fault. On the other hand, localization of faulty computer without using self-testing techniques and diagnosis routines shortens the recovery point time period and reduces the probability that a fault will occur during the execution of fault-tolerant procedure. This is very important for real-time fault-tolerant systems. These contributions bring both an increase in system performance and an increase in the degree of system reliability.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5444888,no
hiCUDA: High-Level GPGPU Programming,2011,"Graphics Processing Units (GPUs) have become a competitive accelerator for applications outside the graphics domain, mainly driven by the improvements in GPU programmability. Although the Compute Unified Device Architecture (CUDA) is a simple C-like interface for programming NVIDIA GPUs, porting applications to CUDA remains a challenge to average programmers. In particular, CUDA places on the programmer the burden of packaging GPU code in separate functions, of explicitly managing data transfer between the host and GPU memories, and of manually optimizing the utilization of the GPU memory. Practical experience shows that the programmer needs to make significant code changes, often tedious and error-prone, before getting an optimized program. We have designed hiCUDA}, a high-level directive-based language for CUDA programming. It allows programmers to perform these tedious tasks in a simpler manner and directly to the sequential code, thus speeding up the porting process. In this paper, we describe the hiCUDA} directives as well as the design and implementation of a prototype compiler that translates a hiCUDA} program to a CUDA program. Our compiler is able to support real-world applications that span multiple procedures and use dynamically allocated arrays. Experiments using nine CUDA benchmarks show that the simplicity hiCUDA} provides comes at no expense to performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5445082,no
Automatic Generation of Multicore Chemical Kernels,2011,"Abstract-This work presents the Kinetics Preprocessor: Accelerated (KPPA), a general analysis and code generation tool that achieves significantly reduced time-to-solution for chemical kinetics kernels on three multicore platforms: NVIDIA GPUs using CUDA, the Cell Broadband Engine, and Intel Quad-Core Xeon CPUs. A comparative performance analysis of chemical kernels from WRFChem and the Community Multiscale Air Quality Model (CMAQ) is presented for each platform in double and single precision on coarse and fine grids. We introduce the multicore architecture parameterization that KPPA uses to generate a chemical kernel for these platforms and describe a code generation system that produces highly tuned platform-specific code. Compared to state-of-the-art serial implementations, speedups exceeding 25x are regularly observed, with a maximum observed speedup of 41.1x in single precision.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5473221,no
Software-Based Self-Test of Set-Associative Cache Memories,2011,"Embedded microprocessor cache memories suffer from limited observability and controllability creating problems during in-system tests. This paper presents a procedure to transform traditional march tests into software-based self-test programs for set-associative cache memories with LRU replacement. Among all the different cache blocks in a microprocessor, testing instruction caches represents a major challenge due to limitations in two areas: 1) test patterns which must be composed of valid instruction opcodes and 2) test result observability: the results can only be observed through the results of executed instructions. For these reasons, the proposed methodology will concentrate on the implementation of test programs for instruction caches. The main contribution of this work lies in the possibility of applying state-of-the-art memory test algorithms to embedded cache memories without introducing any hardware or performance overheads and guaranteeing the detection of typical faults arising in nanometer CMOS technologies.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5499464,no
An Adaptive Filter to Approximate the Bayesian Strategy for Sonographic Beamforming,2011,"A first-principles task-based approach to the design of medical ultrasonic imaging systems for breast lesion discrimination is described. This study explores a new approximation to the ideal Bayesian observer strategy that allows for object heterogeneity. The new method, called iterative Wiener filtering, is implemented using echo data simulations and a phantom study. We studied five lesion features closely associated with visual discrimination for clinical diagnosis. A series of human observer measurements for the same image data allowed us to quantitatively compare alternative beamforming strategies through measurements of visual discrimination efficiency. Employing the Smith-Wagner model observer, we were able to breakdown efficiency estimates and identify the processing stage at which performance losses occur. The methods were implemented using a commercial scanner and a cyst phantom to explore development of spatial filters for systems with shift-variant impulse response functions. Overall we found that significant improvements were realized over standard B-mode images using a delay-and-sum beamformer but at the cost of higher complexity and computational load.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5512633,no
"Evaluating Complexity, Code Churn, and Developer Activity Metrics as Indicators of Software Vulnerabilities",2011,"Security inspection and testing require experts in security who think like an attacker. Security experts need to know code locations on which to focus their testing and inspection efforts. Since vulnerabilities are rare occurrences, locating vulnerable code locations can be a challenging task. We investigated whether software metrics obtained from source code and development history are discriminative and predictive of vulnerable code locations. If so, security experts can use this prediction to prioritize security inspection and testing efforts. The metrics we investigated fall into three categories: complexity, code churn, and developer activity metrics. We performed two empirical case studies on large, widely used open-source projects: the Mozilla Firefox web browser and the Red Hat Enterprise Linux kernel. The results indicate that 24 of the 28 metrics collected are discriminative of vulnerabilities for both projects. The models using all three types of metrics together predicted over 80 percent of the known vulnerable files with less than 25 percent false positives for both projects. Compared to a random selection of files for inspection and testing, these models would have reduced the number of files and the number of lines of code to inspect or test by over 71 and 28 percent, respectively, for both projects.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5560680,no
Simple Fault Diagnosis Based on Operating Characteristic of Brushless Direct-Current Motor Drives,2011,"In this paper, a simple fault diagnosis scheme for brushless direct-current motor drives is proposed to maintain control performance under an open-circuit fault. The proposed scheme consists of a simple algorithm using the measured phase current information and detects open-circuit faults based on the operating characteristic of motors. It requires no additional sensors or electrical devices to detect open-circuit faults and can be embedded into the existing drive software as a subroutine without excessive computation effort. The feasibility of the proposed fault diagnosis algorithm is proven by simulation and experimental results.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5560811,no
Wireless Sensor Networks for Distributed Chemical Sensing: Addressing Power Consumption Limits With On-Board Intelligence,2011,"Chemicals detection and quantification is extremely important for ensuring safety and security in multiple application domains like smart environments, building automation, etc. Characteristics of chemical signal propagation make single point of measure approach mostly inefficient. Distributed chemical sensing with wireless platforms may be the key for reconstructing chemical images of sensed environment but its development is currently hampered by technological limits on solid-state sensors power management. We present the implementation of power saving sensor censoring strategies on a novel wireless electronic nose platform specifically designed for cooperative chemical sensing and based on TinyOS. An on-board sensor fusion component complements its software architecture with the capability of locally estimate air quality and chemicals concentrations. Each node is hence capable to decide the informative content of sampled data extending the operative lifespan of the entire network. Actual power savings are modeled and estimated with a measurement approach in experimental scenarios.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5585658,no
An Automatic Approach for Radial Lens Distortion Correction From a Single Image,2011,"Radial lens distortion correction is a fundamental task in photogrammetry and computer vision disciplines, mainly when metric and quality results are required. Radial lens distortion estimation plays an essential role in several photogrammetric tasks such as: registration of sensors, 3-D reconstruction, and mapping of textures. This paper presents an iterative numerical approach for the automatic estimation and correction of radial lens distortion using only a single image. To this end, several geometric constraints (rectilinear elements and vanishing points) are considered. The reported experimental tests indicate that within certain limits, results from a single image can stand the comparison with those from multiple image bundle adjustment.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5585662,no
A General Software Defect-Proneness Prediction Framework,2011,"BACKGROUND - Predicting defect-prone software components is an economically important activity and so has received a good deal of attention. However, making sense of the many, and sometimes seemingly inconsistent, results is difficult. OBJECTIVE - We propose and evaluate a general framework for software defect prediction that supports 1) unbiased and 2) comprehensive comparison between competing prediction systems. METHOD - The framework is comprised of 1) scheme evaluation and 2) defect prediction components. The scheme evaluation analyzes the prediction performance of competing learning schemes for given historical data sets. The defect predictor builds models according to the evaluated learning scheme and predicts software defects with new data according to the constructed model. In order to demonstrate the performance of the proposed framework, we use both simulation and publicly available software defect data sets. RESULTS - The results show that we should choose different learning schemes for different data sets (i.e., no scheme dominates), that small details in conducting how evaluations are conducted can completely reverse findings, and last, that our proposed framework is more effective and less prone to bias than previous approaches. CONCLUSIONS - Failure to properly or fully evaluate a learning scheme can be misleading; however, these problems may be overcome by our proposed framework.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5611551,no
Dynamic QoS Management and Optimization in Service-Based Systems,2011,"Service-based systems that are dynamically composed at runtime to provide complex, adaptive functionality are currently one of the main development paradigms in software engineering. However, the Quality of Service (QoS) delivered by these systems remains an important concern, and needs to be managed in an equally adaptive and predictable way. To address this need, we introduce a novel, tool-supported framework for the development of adaptive service-based systems called QoSMOS (QoS Management and Optimization of Service-based systems). QoSMOS can be used to develop service-based systems that achieve their QoS requirements through dynamically adapting to changes in the system state, environment, and workload. QoSMOS service-based systems translate high-level QoS requirements specified by their administrators into probabilistic temporal logic formulae, which are then formally and automatically analyzed to identify and enforce optimal system configurations. The QoSMOS self-adaptation mechanism can handle reliability and performance-related QoS requirements, and can be integrated into newly developed solutions or legacy systems. The effectiveness and scalability of the approach are validated using simulations and a set of experiments based on an implementation of an adaptive service-based system for remote medical assistance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5611553,no
Optimizing the Performance of Virtual Machine Synchronization for Fault Tolerance,2011,"Hypervisor-based fault tolerance (HBFT), which synchronizes the state between the primary VM and the backup VM at a high frequency of tens to hundreds of milliseconds, is an emerging approach to sustaining mission-critical applications. Based on virtualization technology, HBFT provides an economic and transparent fault tolerant solution. However, the advantages currently come at the cost of substantial performance overhead during failure-free, especially for memory intensive applications. This paper presents an in-depth examination of HBFT and options to improve its performance. Based on the behavior of memory accesses among checkpointing epochs, we introduce two optimizations, read-fault reduction and write-fault prediction, for the memory tracking mechanism. These two optimizations improve the performance by 31 percent and 21 percent, respectively, for some applications. Then, we present software superpage which efficiently maps large memory regions between virtual machines (VM). Our optimization improves the performance of HBFT by a factor of 1.4 to 2.2 and achieves about 60 percent of that of the native VM.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5629326,no
High Dynamic Range Image Display With Halo and Clipping Prevention,2011,"The dynamic range of an image is defined as the ratio between the highest and the lowest luminance level. In a high dynamic range (HDR) image, this value exceeds the capabilities of conventional display devices; as a consequence, dedicated visualization techniques are required. In particular, it is possible to process an HDR image in order to reduce its dynamic range without producing a significant change in the visual sensation experienced by the observer. In this paper, we propose a dynamic range reduction algorithm that produces high-quality results with a low computational cost and a limited number of parameters. The algorithm belongs to the category of methods based upon the Retinex theory of vision and was specifically designed in order to prevent the formation of common artifacts, such as halos around the sharp edges and clipping of the highlights, that often affect methods of this kind. After a detailed analysis of the state of the art, we shall describe the method and compare the results and performance with those of two techniques recently proposed in the literature and one commercial software.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635332,no
Protector: A Probabilistic Failure Detector for Cost-Effective Peer-to-Peer Storage,2011,"Maintaining a given level of data redundancy is a fundamental requirement of peer-to-peer (P2P) storage systems-to ensure desired data availability, additional replicas must be created when peers fail. Since the majority of failures in P2P networks are transient (i.e., peers return with data intact), an intelligent system can reduce significant replication costs by not replicating data following transient failures. Reliably distinguishing permanent and transient failures, however, is a challenging task, because peers are unresponsive to probes in both cases. In this paper, we propose Protector, an algorithm that enables efficient replication policies by estimating the number of â€œremaining replicasâ€?for each object, including those temporarily unavailable due to transient failures. Protector dramatically improves detection accuracy by exploiting two opportunities. First, it leverages failure patterns to predict the likelihood that a peer (and the data it hosts) has permanently failed given its current downtime. Second, it detects replication level across groups of replicas (or fragments), thereby balancing false positives for some peers against false negatives for others. Extensive simulations based on both synthetic and real traces show that Protector closely approximates the performance of a perfect â€œoracleâ€?failure detector, and significantly outperforms time-out-based detectors using a wide range of parameters. Finally, we design, implement and deploy an efficient P2P storage system called AmazingStore by combining Protector with structured P2P overlays. Our experience proves that Protector enables efficient long-term data maintenance in P2P storage systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5639009,no
Islanding Detection for Inverter-Based DG Coupled With Frequency-Dependent Static Loads,2011,"Islanding detection is an essential protection requirement for distribution generation (DG). Antiislanding techniques for inverter-based DG are typically designed and tested on constant RLC loads and, hence, do not take into account frequency-dependent loads. In this paper, a new antiislanding technique, based on simultaneous <i>P</i> -<i>f</i> and <i>Q</i>- <i>V</i> droops, is designed and tested under different islanding conditions considering different load types. The behavior of frequency-dependent static loads during islanding operation is discussed. The developed antiislanding technique is designed to fully address the critical islanding cases with frequency-dependent static loads and RLC-based loads. The performance of the proposed method is tested to accommodate load switching disturbances and grid voltage distortions. Theoretical investigation coupled with intensive simulation results using the Matlab/Simulink software is presented to validate the performance of the proposed antiislanding technique. Unlike previously proposed methods, the results show that the proposed islanding detection method is robust to frequency-dependent loads and system disturbances.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5639070,no
Preventing Temporal Violations in Scientific Workflows: Where and How,2011,"Due to the dynamic nature of the underlying high-performance infrastructures for scientific workflows such as grid and cloud computing, failures of timely completion of important scientific activities, namely, temporal violations, often take place. Unlike conventional exception handling on functional failures, nonfunctional QoS failures such as temporal violations cannot be passively recovered. They need to be proactively prevented through dynamically monitoring and adjusting the temporal consistency states of scientific workflows at runtime. However, current research on workflow temporal verification mainly focuses on runtime monitoring, while the adjusting strategy for temporal consistency states, namely, temporal adjustment, has so far not been thoroughly investigated. For this issue, two fundamental problems of temporal adjustment, namely, where and how, are systematically analyzed and addressed in this paper. Specifically, a novel minimum probability time redundancy-based necessary and sufficient adjustment point selection strategy is proposed to address the problem of where and an innovative genetic-algorithm-based effective and efficient local rescheduling strategy is proposed to tackle the problem of how. The results of large-scale simulation experiments with generic workflows and specific real-world applications demonstrate that our temporal adjustment strategy can remarkably prevent the violations of both local and global temporal constraints in scientific workflows.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5645643,no
A Refactoring Approach to Parallelism,2011,"In the multicore era, a major programming task will be to make programs more parallel. This is tedious because it requires changing many lines of code; it's also error-prone and nontrivial because programmers need to ensure noninterference of parallel operations. Fortunately, interactive refactoring tools can help reduce the analysis and transformation burden. The author describes how refactoring tools can improve programmer productivity, program performance, and program portability. The article also describes a toolset that supports several refactorings for making programs thread-safe, threading sequential programs for throughput, and improving scalability of parallel programs.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5672516,no
Density-Aware Routing in Highly Dynamic DTNs: The RollerNet Case,2011,"We analyze the dynamics of a mobility data set collected in a pipelined disruption-tolerant network (DTN), a particular class of intermittently-connected wireless networks characterized by a 1-D topology. First, we collected and investigated traces of contact times among thousands of participants of a rollerblading tour in Paris. The data set shows extreme dynamics in the mobility pattern of a large number of nodes. Most strikingly, fluctuations in the motion of the rollerbladers cause a typical accordion phenomenon - the topology expands and shrinks with time, thus influencing connection times and opportunities between participants. Second, we show through an analytical model that the accordion phenomenon, through the variation of the average node degree, has a major impact on the performance of epidemic dissemination. Finally, we test epidemic dissemination and other existing forwarding schemes on our traces, and conclude that routing should adapt to the varying, though predictable, nature of the network. To this end, we propose DA-SW (Density-Aware Spray-and-Wait), a measurement-oriented variant of the spray-and-wait algorithm that tunes, in a dynamic fashion, the number of a message copies to be disseminated in the network. The particularity of DA-SW is that it relies on a set of abaci that represents the three phases of the accordion phenomenon: aggregation, expansion, and stabilization. We show that DA-SW leads to performance results that are close to the best case (obtained with an oracle).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5677536,no
Fault Detection for Tâ€“S Fuzzy Discrete Systems in Finite-Frequency Domain,2011,"This paper investigates the problem of fault detection for Takagi-Sugeno (T-S) fuzzy discrete systems in finite-frequency domain. By means of the T-S fuzzy model, both a fuzzy fault detection filter system and the dynamics of filtering error generator are constructed. Two finite-frequency performance indices are introduced to measure fault sensitivity and disturbance robustness. Faults are considered in a middle frequency domain, while disturbances are considered in another certain finite-frequency domain interval. By using the generalized Kalman-Yakubovic-Popov Lemma in a local linear system model, the design methods are presented in terms of solutions to a set of linear matrix inequalities, which can be readily solved via standard numerical software. The design problem is formulated as a two-objective optimization algorithm. A numerical example is given to illustrate the effectiveness and potential of the developed techniques.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5696767,no
Athanasia: A User-Transparent and Fault-Tolerant System for Parallel Applications,2011,"This article presents Athanasia, a user-transparent and fault-tolerant system, for parallel applications running on large-scale cluster systems. Cluster systems have been regarded as a de facto standard to achieve multitera-flop computing power. These cluster systems, as we know, have an inherent failure factor that can cause computation failure. The reliability issue in parallel computing systems, therefore, has been studied for a relatively long time in the literature, and we have seen many theoretical promises arise from the extensive research. However, despite the rigorous studies, practical and easily deployable fault-tolerant systems have not been successfully adopted commercially. Athanasia is a user-transparent checkpointing system for a fault-tolerant Message Passing Interface (MPI) implementation that is primarily based on the sync-and-stop protocol. Athanasia supports three critical functionalities that are necessary for fault tolerance: a light-weight failure detection mechanism, dynamic process management that includes process migration, and a consistent checkpoint and recovery mechanism. The main features of Athanasia are that it does not require any modifications to the application code and that it preserves many of the high performance characteristics of high-speed networks. Experimental results show that Athanasia can be a good candidate for practically deployable fault-tolerant systems in very-large and high-performance clusters and that its protocol can be applied to a variety of parallel communication libraries easily.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5710900,no
A recursive Otsu thresholding method for scanned document binarization,2011,"The use of digital images of scanned handwritten historical documents has increased in recent years, especially with the online availability of large document collections. However, the sheer number of images in some of these collections makes them cumbersome to manually read and process, making the need for automated processing of increased importance. A key step in the recognition and retrieval of such documents is binarization, the separation of document text from the page's background. Binarization of images of historical documents that have been affected by degradation or are otherwise of poor image quality is difficult and continues to be a topic of research in the field of image processing. This paper presents a novel approach to this problem, including two primary variations. One combines a recursive extension of Otsu thresholding and selective bilateral filtering to allow automatic binarization and segmentation of handwritten text images. The other also builds on the recursive Otsu method and adds improved background normalization and a post-processing step to the algorithm to make it more robust and to perform adequately even for images that present bleed-through artifacts. Our results show that these techniques segment the text in historical documents comparable to and in some cases better than many state-of-the-art approaches based on their performance as evaluated using the dataset from the recent ICDAR 2009 Document Image Binarization Contest.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5711519,no
A Game Platform for Treatment of Amblyopia,2011,"We have developed a prototype device for take-home use that can be used in the treatment of amblyopia. The therapeutic scenario we envision involves patients first visiting a clinic, where their vision parameters are assessed and suitable parameters are determined for therapy. Patients then proceed with the actual therapeutic treatment on their own, using our device, which consists of an Apple iPod Touch running a specially modified game application. Our rationale for choosing to develop the prototype around a game stems from multiple requirements that such an application satisfies. First, system operation must be sufficiently straight-forward that ease-of-use is not an obstacle. Second, the application itself should be compelling and motivate use more so than a traditional therapeutic task if it is to be used regularly outside of the clinic. This is particularly relevant for children, as compliance is a major issue for current treatments of childhood amblyopia. However, despite the traditional opinion that treatment of amblyopia is only effective in children, our initial results add to the growing body of evidence that improvements in visual function can be achieved in adults with amblyopia.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5713843,no
Efficient Fault Detection and Diagnosis in Complex Software Systems with Information-Theoretic Monitoring,2011,"Management metrics of complex software systems exhibit stable correlations which can enable fault detection and diagnosis. Current approaches use specific analytic forms, typically linear, for modeling correlations. In practice, more complex nonlinear relationships exist between metrics. Moreover, most intermetric correlations form clusters rather than simple pairwise correlations. These clusters provide additional information and offer the possibility for optimization. In this paper, we address these issues by using Normalized Mutual Information (NMI) as a similarity measure to identify clusters of correlated metrics, without assuming any specific form for the metric relationships. We show how to apply the Wilcoxon Rank-Sum test on the entropy measures to detect errors in the system. We also present three diagnosis algorithms to locate faulty components: RatioScore, based on the Jaccard coefficient, SigScore, which incorporates knowledge of component dependencies, and BayesianScore, which uses Bayesian inference to assign a fault probability to each component. We evaluate our approach in the context of a complex enterprise application, and show that 1) stable, nonlinear correlations exist and can be captured with our approach; 2) we can detect a large fraction of faults with a low false positive rate (we detect up to 18 of the 22 faults we injected); and 3) we improve the diagnosis with our new diagnosis algorithms.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5714701,no
Hardware/Software Codesign Architecture for Online Testing in Chip Multiprocessors,2011,"As the semiconductor industry continues its relentless push for nano-CMOS technologies, long-term device reliability and occurrence of hard errors have emerged as a major concern. Long-term device reliability includes parametric degradation that results in loss of performance as well as hard failures that result in loss of functionality. It has been reported in the ITRS roadmap that effectiveness of traditional burn-in test in product life acceleration is eroding. Thus, to assure sufficient product reliability, fault detection and system reconfiguration must be performed in the field at runtime. Although regular memory structures are protected against hard errors using error-correcting codes, many structures within cores are left unprotected. Several proposed online testing techniques either rely on concurrent testing or periodically check for correctness. These techniques are attractive, but limited due to significant design effort and hardware cost. Furthermore, lack of observability and controllability of microarchitectural states result in long latency, long test sequences, and large storage of golden patterns. In this paper, we propose a low-cost scheme for detecting and debugging hard errors with a fine granularity within cores and keeping the faulty cores functional, with potentially reduced capability and performance. The solution includes both hardware and runtime software based on codesigned virtual machine concept. It has the ability to detect, debug, and isolate hard errors in small noncache array structures, execution units, and combinational logic within cores. Hardware signature registers are used to capture the footprint of execution at the output of functional modules within the cores. A runtime layer of software (microvisor) initiates functional tests concurrently on multiple cores to capture the signature footprints across cores to detect, debug, and isolate hard errors. Results show that using targeted set of functional test sequences, faults ca- be debugged to a fine-granular level within cores. The hardware cost of the scheme is less than three percent, while the software tasks are performed at a high-level, resulting in a relatively low design effort and cost.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5714704,no
Experimental Validation of Channel State Prediction Considering Delays in Practical Cognitive Radio,2011,"As part of the effort toward building a cognitive radio (CR) network testbed, we have demonstrated real-time spectrum sensing. Spectrum sensing is the cornerstone of CR. However, current hardware platforms for CR introduce time delays that undermine the accuracy of spectrum sensing. The time delay named response delay incurred by hardware and software can be measured at two antennas colocated at a secondary user (SU), the receiving antenna, and the transmitting antenna. In this paper, minimum response delays are experimentally quantified and reported based on two hardware platforms, i.e., the universal software radio peripheral 2 (USRP2) and the small-form-factor software-defined-radio development platform (SFF SDR DP). The response delay has a negative impact on the accuracy of spectrum sensing. A modified hidden Markov model (HMM)-based single-secondary-user (single-SU) prediction is proposed and examined. When multiple SUs exist and their channel qualities are diverse, cooperative prediction can benefit the SUs as a whole. A prediction scheme with two stages is proposed, where the first stage includes individual predictions conducted by all the involved SUs, and the second stage further performs cooperative prediction using individual single-SU prediction results obtained at the first stage. In addition, a soft-combining decision rule for cooperative prediction is proposed. To have convincing performance evaluation results, real-world Wi-Fi signals are used to test the proposed approaches, where the Wi-Fi signals are simultaneously recorded at four different locations. Experimental results show that the proposed single-SU prediction outperforms the 1-nearest neighbor (1-NN) prediction, which uses current detected state as an estimate of future states. Moreover, even with just a few SUs, cooperative prediction leads to overall performance improvement.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5714762,no
Measuring the Effectiveness of the Defect-Fixing Process in Open Source Software Projects,2011,"The defect-fixing process is a key process in which an open source software (OSS) project team responds to customer needs in terms of detecting and resolving software defects, hence the dimension of defect-fixing effectiveness corresponds nicely to adopters' concerns regarding OSS products. Although researchers have been studying the defect fixing process in OSS projects for almost a decade, the literature still lacks rigorous ways to measure the effectiveness of this process. Thus, this paper aims to create a valid and reliable instrument to measure the defect-fixing effectiveness construct in an open source environment through the scale development methodology proposed by Churchill [4]. This paper examines the validity and reliability of an initial list of indicators through two rounds of data collection and analysis. Finally four indicators are suggested to measure defect-fixing effectiveness. The implication for practitioners is explained through a hypothetical example followed by implications for the research community.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5718854,no
Quality Market: Design and Field Study of Prediction Market for Software Quality Control,2011,"Given the increasing competition in the software industry and the critical consequences of software errors, it has become important for companies to achieve high levels of software quality. Generating early forecasts of potential quality problems can have significant benefits to quality improvement. In our research, we utilized a novel approach, called prediction markets, for generating early forecasts of confidence in software quality for an ongoing project in a firm. Analogous to financial market, in a quality market, a security was defined that represented the quality requirement to be predicted. Participants traded on the security to provide their predictions. The market equilibrium price represented the probability of occurrence of the quality being measured. The results suggest that forecasts generated using the prediction markets are closer to the actual project outcomes than polls. We suggest that a suitably designed prediction market may have a useful role in software development domain.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5718861,no
Facilitating Performance Predictions Using Software Components,2011,"Component-based software engineering (CBSE) poses challenges for predicting and evaluating software performance but also offers several advantages. Software performance engineering can benefit from CBSE ideas and concepts. The MediaStore, a fictional system, demonstrates how to achieve compositional reasoning about software performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5719590,no
Detecting SEEs in Microprocessors Through a Non-Intrusive Hybrid Technique,2011,"This paper presents a hybrid technique based on software signatures and a hardware module with watchdog and decoder characteristics to detect SEU and SET faults in microprocessors. These types of faults have a major influence in the microprocessor's control-flow, affecting the basic blocks and the transitions between them. In order to protect the transitions between basic blocks a light hardware module is implemented in order to spoof the data exchanged between the microprocessor and its memory. Since the hardware alone is not capable of detecting errors inside the basic blocks, it is enhanced to support the new technique and then provide full control-flow protection. A fault injection campaign is performed using a MIPS microprocessor. Simulation results show high detection rates with a small amount of performance degradation and area overhead.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5720529,no
Defects Detection of Cold-roll Steel Surface Based on MATLAB,2011,"The measure of detecting the surface edge information of cold-roll steel sheets has been investigated rely on the digital image processing toolbox of the mathematic software MATLAB, and the edge detecting experiment of surface grayscale image has been conducted on the computer. The method of detecting defects such as black flecks and scratching has been realized in the research, the different effect of operators with the disturbance of noise has been compared, the performance of the LOG's edge detection ability various due to the change of the parameter Sigma. Conclusions have been made that LOG operator maintains the satisfying performance with the disturbance of noise, smaller Sigma is, less satisfying the smoothing ability is, which maintains more details, whereas the smoothing ability is better with loss of more details, defects detection of cold-roll steel surface based on MATLAB has a quite satisfying performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5720911,no
Fault Detection Devices of Rotating Machinery Based on SOPC,2011,"For the common failure of motor rotation, a reliable acquisition and detection device of motor rotation signal is designed, and the general design scheme of the test system and the implementation of hardware and software are given. The system consists of high-speed frequency counter chip FPGA module and the SOPC system structure, using NIOS II as the system control unit to control the work of the counter, and completing high-precision frequency meter design at the core of FPGA with appropriate software and hardware resources. The Counting result is sent by serial port to the host computer for further signal analysis such as FFT and harmonic wavelet packet, to get more detailed distribution of signal spectrum as the basis to judge the fault signal. The frequency meter system developed with Nios technology can simplify the external measurement hardware circuits, stabilize the performance and flexibly achieve custom application. Experimental results show that the test system can well be used to fault detection of mechanical system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5721138,no
Fault Diagnosis of Rolling Element Bearing Based on Vibration Frequency Analysis,2011,"Element bearing is one of the widely used universal parts in machine. Its running condition and failure rate influence the performance, service life and efficiency of the equipment directly. So it is significant to research on bearing condition monitoring and fault diagnosis techniques. The paper describes the use of vibration measurements by a periodic monitoring system for monitoring the condition of rolling element bearing of the centrifugal machine. Vibration data is collected using accelerometers, which are placed at the 12 o'clock position at both the drive end and the driven end of the centrifugal machine. Vibration signals are collected using a 16 channel DAT recorder, and are post processed by vibration signals analysis software in personal computer. Simple diagnosis by vibration is based on frequency analysis. Each element of rolling bearing is of its fault characteristic frequency. This paper introduces frequency analysis method of using low and high frequency bands in conjunction with time domain waveform. Fault position of drive end bearing in the centrifugal machine is detected successfully by using this method.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5721155,no
Online PD Monitoring and Analysis for Step-up Transformers of Hydropower Plant,2011,"As we known, to evaluate and diagnosis the insulation faults of large-size power transformers, partial discharges (PDs) can be detected via ultra high frequency (UHF) technique. In this paper, an UHF PD online monitoring system was developed for large-size step-up transformers. The principle of UHF PD monitoring method was introduced., and the hardware structure and software key techniques in the system were described. In order to achieve the integrated PD monitoring and ensure the accuracy of PD analysis, the operating environments and operating conditions of the transformers have been acquired synchronously. Meanwhile, the correlative analysis of PDs with respect to operating conditions can be performed and the characteristics of PD activities with respect to relevant states can be studied. The association analysis of PDs is performed as follows: (a) periodical PDs caused by power frequency voltage under stable operating conditions, (b) stochastic PDs caused by transient over-voltages under unstable operating conditions. At present, the system has been applied into several large-size step-up transforms and achieved large mount of on-site monitoring results, and the reliability of PD analyzing results is verified by the analysis of the onsite data.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5721292,no
Multi-view coding and view synthesis for 3DTV,2011,"In this paper we analyze the current developments in 3DTV technologies concerning multi-view video coding and free-viewpoint rendering. Multi-View Coding (MVC) can be performed in several ways that influence the performance and the complexity of the system. We underline the connection between coding and free-viewpoint rendering techniques. Since the latest research shows that a better multi-view coding can be reached by using interview prediction based on view synthesis, we present our view synthesis algorithm and compare it with the MPEG standardization software. The first quality measurements, indicating the difference between our view synthesis and the MPEG algorithm, are provided and discussed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5722809,no
Selecting Oligonucleotide Probes for Whole-Genome Tiling Arrays with a Cross-Hybridization Potential,2011,"For designing oligonucleotide tiling arrays popular, current methods still rely on simple criteria like Hamming distance or longest common factors, neglecting base stacking effects which strongly contribute to binding energies. Consequently, probes are often prone to cross-hybridization which reduces the signal-to-noise ratio and complicates downstream analysis. We propose the first computationally efficient method using hybridization energy to identify specific oligonucleotide probes. Our Cross-Hybridization Potential (CHP) is computed with a Nearest Neighbor Alignment, which efficiently estimates a lower bound for the Gibbs free energy of the duplex formed by two DNA sequences of bounded length. It is derived from our simplified reformulation of t-gap insertion-deletion-like metrics. The computations are accelerated by a filter using weighted ungapped q-grams to arrive at seeds. The computation of the CHP is implemented in our software OSProbes, available under the GPL, which computes sets of viable probe candidates. The user can choose a trade-off between running time and quality of probes selected. We obtain very favorable results in comparison with prior approaches with respect to specificity and sensitivity for cross-hybridization and genome coverage with high-specificity probes. The combination of OSProbes and our Tileomatic method, which computes optimal tiling paths from candidate sets, yields globally optimal tiling arrays, balancing probe distance, hybridization conditions, and uniqueness of hybridization.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5722951,no
Monitoring metal fill profile in lost foam casting process using capacitive sensors and metal fill time estimation,2011,"Lost foam casting (LFC) is one of the most important casting methods used in the production of complex metal castings. Several casting defects may result due to an improper metal fill process during LFC; hence, monitoring the metal fill profile is important so that failures in the process can be quickly discovered. This paper presents a monitoring process based on capacitive sensors developed by the research team. In this application, an array of metallic electrodes is mounted around a target area and a set of capacitance measuring circuits are used to measure the mutual capacitances between these electrodes. Measuring the change in capacitance as grounded molten metal displaces the foam pattern provide a simple nondestructive method of acquiring metal fill information. An iterative algorithm for the estimation of metal fill time is also presented. This estimation utilizes better filtration and outlier detection techniques to provide a good prediction of the filling time. The proposed system also makes use of a wireless sensor network for transmitting data between the sensor boards and a computer running our metal fill monitoring application software; thus making this system suitable for the harsh foundry environment.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5739811,no
Does Socio-Technical Congruence Have an Effect on Software Build Success? A Study of Coordination in a Software Project,2011,"Socio-technical congruence is an approach that measures coordination by examining the alignment between the technical dependencies and the social coordination in the project. We conduct a case study of coordination in the IBM Rational Team Concert project, which consists of 151 developers over seven geographically distributed sites, and expect that high congruence leads to a high probability of successful builds. We examine this relationship by applying two congruence measurements: an unweighted congruence measure from previous literature, and a weighted measure that overcomes limitations of the existing measure. We discover that there is a relationship between socio-technical congruence and build success probability, but only for certain build types, and observe that in some situations, higher congruence actually leads to lower build success rates. We also observe that a large proportion of zero-congruence builds are successful, and that socio-technical gaps in successful builds are larger than gaps in failed builds. Analysis of the social and technical aspects in IBM Rational Team Concert allows us to discuss the effects of congruence on build success. Our findings provide implications with respect to the limits of applicability of socio-technical congruence and suggest further improvements of socio-technical congruence to study coordination.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5740929,no
Using Multivariate Split Analysis for an Improved Maintenance of Automotive Diagnosis Functions,2011,"The amount of automotive software functions is continuously growing. With their interactions and dependencies increasing, the diagnosis' task of differencing between symptoms indicating a fault, the fault cause itself and uncorrelated data gets enormously difficult and complex. For instance, up to 40% of automotive software functions are contributable to diagnostic functions, resulting in approximately three million lines of diagnostic code. The diagnosis' complexity is additionally increased by legal requirements forcing automotive manufacturers maintaining the diagnosis of their cars for 15 years after the end of the car's series production. Clearly, maintaining these complex functions over such an extend time span is a difficult and tedious task. Since data from diagnosis incidents has been transferred back to the OEMs for some years, analysing this data with statistic techniques promises a huge facilitation of the diagnosis' maintenance. In this paper we use multivariate split analysis to filter diagnosis data for symptoms having real impact on faults and their repair measures, thus detecting diagnosis functions which have to be updated as they contain irrelevant or erroneous observations and/or repair measurements. A key factor for performing an unbiased split analysis is to determine an ideally representative control data set for a given test data set showing some property whose influence is to be studied. In this paper, we present a performant algorithm for creating such a representative control data set out of a very large initial data collection. This approach facilitates the analysis and maintenance of diagnosis functions. It has been successfully evaluated on case studies and is part of BMW's continuous improvement process for automotive diagnosis.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5741278,no
Design Defect Detection Rules Generation: A Music Metaphor,2011,"We propose an automated approach for design defect detection. It exploits an algorithm that automatically finds rules for the detection of possible design defects, thus relieving the designer from doing so manually. Our algorithm derives rules in the form of metric/threshold combinations, from known instances of design defects (defect examples). Due to the large number of possible combinations, we use a music-inspired heuristic that finds the best harmony when combining metrics. We evaluated our approach on finding potential defects in three open-source systems (Xerces-J, Quick UML and Gantt). For all of them, we found more than 80% of known defects, a better result when compared to a state-of-the-art approach, where the detection rules are manually specified.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5741331,no
On the Utility of a Defect Prediction Model during HW/SW Integration Testing: A Retrospective Case Study,2011,"Testing is an important and cost-intensive part of the software development life cycle. Defect prediction models try to identify error-prone components, so that these can be tested earlier or more in-depth, and thus improve the cost-effectiveness during testing. Such models have been researched extensively, but whether and when they are applicable in practice is still debated. The applicability depends on many factors, and we argue that it cannot be analyzed without a specific scenario in mind. In this paper, we therefore present an analysis of the utility for one case study, based on data collected during the hardware/software integration test of a system from the avionic domain. An analysis of all defects found during this phase reveals that more than half of them are not identifiable by a code-based defect prediction model. We then investigate the predictive performance of different prediction models for the remaining defects. The small ratio of defective instances results in relatively poor performance. Our analysis of the cost-effectiveness then shows that the prediction model is not able to outperform simple models, which order files either randomly or by lines of code. Hence, in our setup, the application of defect prediction models does not offer any advantage in practice.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5741333,no
Prioritizing Requirements-Based Regression Test Cases: A Goal-Driven Practice,2011,"Any changes for maintenance or evolution purposes may break existing working features, or may violate the requirements established in the previous software releases. Regression testing is essential to avoid these problems, but it may be ended up with executing many time-consuming test cases. This paper tries to address prioritizing requirements-based regression test cases. To this end, system-level testing is focused on two practical issues in industrial environments: i) addressing multiple goals regarding quality, cost and effort in a project, and ii) using non-code metrics due to the lack of detailed code metrics in some situations. This paper reports a goal-driven practice at Research In Motion (RIM) towards prioritizing requirements-based test cases regarding these issues. Goal-Question-Metric (GQM) is adopted in identifying metrics for prioritization. Two sample goals are discussed to demonstrate the approach: detecting bugs earlier and maintaining testing effort. We use two releases of a prototype Web-based email client to conduct a set of experiments based on the two mentioned goals. Finally, we discuss lessons learned from applying the goal-driven approach and experiments, and we propose few directions for future research.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5741354,no
Using many-core processors to improve the performance of space computing platforms,2011,"Taking into consideration space applications' demands for higher bandwidth and processing power, we propose to efficiently apply upcoming many-core processors and other Commercial-Off-The-Shelf (COTS) products to improve the on-board processing power.<sup>1 2</sup> A combination of traditional hardware and software-implemented fault-tolerant techniques, addresses the reliability of the system. We first describe common requirements and design challenges presented by future space applications like the High Resolution Wide Swath Synthetic Aperture Radar (HRWS SAR). After proposing the High Performance Computing (HPC) architecture, we compare between most suitable hardware technologies and give some rough performance estimations based on their features. For benchmarking purposes we have manually converted the Scalable Synthetic Compact Application (SSCA#3) from Matlab to C and parallelized it using OpenMP. It turns out that this application scales very well on many-core processors especially on a distributed Shared Memory Architecture (SMA).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5747445,no
SCIPS: An emulation methodology for fault injection in processor caches,2011,"Due to the high level of radiation endured by space systems, fault-tolerant verification is a critical design step for these systems. Space-system designers use fault-injection tools to introduce system faults and observe the system's response to these faults. Since a processor's cache accounts for a large percentage of total chip area and is thus more likely to be affected by radiation, the cache represents a key system component for fault-tolerant verification. Unfortunately, processor architectures limit cache accessibility, making direct fault injection into cache blocks impossible. Therefore, cache faults can be emulated by injecting faults into data accessed by load instructions. In this paper, we introduce SPFI-TILE, a software-based fault-injection tool for many-core devices. SPFI-TILE emulates cache fault injections by randomly injecting faults into load instructions. In order to provide unbiased fault injections, we present the cache fault-injection methodology SCIPS (Smooth Cache Injection Per Skipping). Results from MATLAB simulation and integration with SPFI-TILE reveal that SCIPS successfully distributes fault-injection probabilities across load instructions, providing an unbiased evaluation and thus more accurate verification of fault tolerance in cache memories.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5747450,no
V-22 ground station enhancements and vibration diagnostics field experience,2011,"The V-22 tiltrotor aircraft's primary tool for aircraft maintenance is the Comprehensive Automated Maintenance Environment - Optimized (CAMEO). CAMEO is a ground based system for aircraft download processing. Under the CAMEO umbrella are several tools with distinct functionality, linked together and sharing common data to support aircraft maintenance operations. This paper provides a general overview of the CAMEO system along with selected components that support mechanical systems (rotors, engines, interconnect shafting system and nacelle blower) monitoring with an emphasis on vibration diagnostics. Also covered are ongoing activities associated with improved performance and integration of maintenance tools associated with vibration monitoring. Vibration diagnostics are automated, directly linking with the associated V-22 Integrated Electronic Technical Manual (IETM) procedures. Fleet Support Team (FST) and industry original equipment manufacturer (OEM) engineers use additional tools like the V-22 Data Visualization Toolset (VDVT) to review and confirm existing diagnostic indications, or determine new ones as necessary. Recent enhancements and anticipated further improvements of the on-board Vibration, Structural Life and Engine Diagnostics (VSLED) unit are addressed. Some examples of fault detection of degraded mechanical components using vibration data are presented, with related lessons learned.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5747557,no
Hardware/software-based diagnosis of load-store queues using expandable activity logs,2011,"The increasing device count and design complexity are posing significant challenges to post-silicon validation. Bug diagnosis is the most difficult step during post-silicon validation. Limited reproducibility and low testing speeds are common limitations in current testing techniques. Moreover, low observability defies full-speed testing approaches. Modern solutions like on-chip trace buffers alleviate these issues, but are unable to store long activity traces. As a consequence, the cost of post-Si validation now represents a large fraction of the total design cost. This work describes a hybrid post-Si approach to validate a modern load-store queue. We use an effective error detection mechanism and an expandable logging mechanism to observe the microarchitectural activity for long periods of time, at processor full-speed. Validation is performed by analyzing the log activity by means of a diagnosis algorithm. Correct memory ordering is checked to root the cause of errors.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5749740,no
Architectural framework for supporting operating system survivability,2011,"The ever increasing size and complexity of Operating System (OS) kernel code bring an inevitable increase in the number of security vulnerabilities that can be exploited by attackers. A successful security attack on the kernel has a profound impact that may affect all processes running on it. In this paper we propose an architectural framework that provides survivability to the OS kernel, i.e. able to keep normal system operation despite security faults. It consists of three components that work together: (1) security attack detection, (2) security fault isolation, and (3) a recovery mechanism that resumes normal system operation. Through simple but carefully-designed architecture support, we provide OS kernel survivability with low performance overheads (<; 5% for kernel intensive benchmarks). When tested with real world security attacks, our survivability mechanism automatically prevents the security faults from corrupting the kernel state or affecting other processes, recovers the kernel state and resumes execution.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5749751,no
Implementing a safe embedded computing system in SRAM-based FPGAs using IP cores: A case study based on the Altera NIOS-II soft processor,2011,"Reconfigurable Field Programmable Gate Arrays (FPGAs) are growing the attention of developers of mission- and safety-critical applications (e.g., aerospace ones), as they allow unprecedented levels of performance, which are making these devices particularly attractive as ASICs replacement, and as they offer the unique feature of in-the-field reconfiguration. However, the sensitivity of reconfigurable FPGAs to ionizing radiation mandates the adoption of fault tolerant mitigation techniques that may impact heavily the FPGA resource usage. In this paper we consider time redundancy, that allows avoiding the high overhead that more traditional approaches like N-modular redundancy introduce, at an affordable cost in terms of application execution-time overhead. A single processor executes two instances of the same software sequentially; the two instances are segregated in their own memory space through a soft IP core that monitors the processor/memory interface for any violations. Moreover, the IP core checks for any processor functional interruption by means of a watchdog timer. Fault injection results are reported showing the characteristics of the proposed approach.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5750278,no
Design of Electric Power Parameter Monitoring System Based on DSP and CPLD,2011,"In this article, we introduce a set of online power quality monitor who is based on DSP (Digital Signal Processor). The monitor makes full use of DSP chip's strong operation capability. It can monitor power quality online, display real-time measure data and save super scalar data, which can provide accurate data to power quality evaluation and improvement. On the hardware side, the system takes DSP and CPLD as core of power parameter detection circuit, and complex programmable logic device (CPLD) and phase-locked loop (PLL) as synchronous sampling circuit, TO the software, designed relative software arithmetic. At the same time, the main program of electric power parameter detection was introduced. Experiments show the proposed system is of fast response, high accuracy, and real-time processing.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5750699,no
Predicting the software performance during feasibility study,2011,"Performance is an important non-functional attribute to be considered for producing quality software. Software performance engineering (SPE) is a methodology having significant role in software engineering to assess the performance of software systems early in the lifecycle. Gathering performance data is an essential aspect of SPE approach. The authors have proposed a methodology to gather data during feasibility study by exploiting the use case point approach, gearing factor and COCOMO model. The proposed methodology is used to estimate the performance data required for performance assessment in the integrated performance prediction process (IP<sup>3</sup>) model. The gathered data is used as the input for solving the two models, (i) use case performance model and (ii) system model. The methodology is illustrated with a case study of airline reservation application. A regression analysis is carried out to validate the response time obtained in the use case performance model. The analysis shows the proposed estimation can be used along with performance walkthrough in data gathering. The performance metrics are obtained by solving the system model, and the behaviour of the hardware resources is observed. Bottleneck resources are identified and the performance parameters are optimised using sensitivity analysis.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5751770,no
Investigation of automatic prediction of software quality,2011,"The subjective nature of software code quality makes it a complex topic. Most software managers and companies rely on the subjective evaluations of experts to determine software code quality. Software companies can save time and money by utilizing a model that could accurately predict different code quality factors during and after the production of software. Previous research builds a model predicting the difference between bad and excellent software. This paper expands this to a larger range of bad, poor, fair, good, and excellent, and builds a model predicting these classes. This research investigates decision trees and ensemble learning from the machine learning tool Weka as primary classifier models predicting reusability, flexibility, understandability, functionality, extendibility, effectiveness, and total quality of software code.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5751946,no
Ultra-High Resolution LYSO PQS-SSS Heptahedron Blocks for Low-Cost MuPET,2011,"We developed and built a solid detector ring for a new murine positron emission tomography (MuPET) system. We use cerium-doped lutetium yttrium orthosilicate (LYSO) crystals and regular round 19 mm photomultipliers (PMTs) arranged in a quadrant-sharing (PQS) configuration. The detector system comprised 180 PQS-SSS heptahedron-shaped blocks distributed in 6 subrings. Each block comprised a 13 Ã— 13 crystal array with nominal dimensions of 19 Ã— 19 Ã— 10 mm<sup>3</sup>. To form a zero-gap solid ring, the rectangular blocks were ground into heptahedron-shaped blocks with a taper angle of 6<sup>Â°</sup> on the edge crystals and optical surfaces. The two edge crystals were 1.76 mm wide, and the inner crystals were 1.32 mm wide. We explored the possibility of increasing the detector's performance by implementing new design, materials, and production techniques; testing the detector's performance; and measuring the detector's timing resolution. List-mode data were acquired using a Ga-68 source, in-house high-yield pileup-event recovery electronics, and data-acquisition software. Four randomly selected blocks were used to evaluate the quality of the detector and our mass-production method. The four blocks' performances were quite similar. A typical block had a packing fraction of 95%, a peak-to-valley ratio of 2.4, a light collection efficiency of 78%, and an energy resolution of 14% at 511 keV, and all 169 of the block's crystal detectors were clearly decoded. Using a single crystal in coincidence with a block, the average coincidence timing resolution was found to be 430 ps (full width at half maximum). A block-to-block coincidence timing resolution of 530 ps is expected. Our PQS-SSS heptahedron block design indicates that it is feasible to construct a high resolution (~1.2 mm) MuPET detector ring using round 19 mm PMTs instead of the more expensive position-sensitive PMTs or solid-state detectors.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5752884,no
Intelligent Trend Indices in Detecting Changes of Operating Conditions,2011,"Temporal reasoning is a very valuable tool to diagnose and control slow processes. Identified trends are also used in data compression and fault diagnosis. Although humans are very good at visually detecting such patterns, for control system software it is a difficult problem including trend extraction and similarity analysis. In this paper, an intelligent trend index is developed from scaled measurements. The scaling is based on monotonously increasing, nonlinear functions, which are generated with generalised norms and moments. The monotonous increase is ensured with constraint handling. Triangular episodes are classified with the trend index and the derivative of it. Severity of the situations is evaluated by a deviation index which takes into account the scaled values of the measurements.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5754208,no
Software reliability model with bathtub-shaped fault detection rate,2011,"This paper proposes a software reliability model with a bathtub-shaped fault detection rate. We discuss how the inherent characteristics of the software testing process support the three phases of the bathtub; the first phase with a decreasing fault detection rate arises from the removal of simple, yet frequent faults like syntax errors and typos; the second phase possesses a constant fault detection rate marking the beginning of functional requirements testing; the third and final code comprehension stage exhibits an increasing fault detection rate because testers are now familiar with the system and can focus their attention on the outstanding and as yet untested portions of code. We also discuss how eliminating one of the testing phases gives rise to the burn-in model, which is a special case of the bathtub model. We compare the performance of the bathtub and burn-in models with the three classical software reliability models using the Predictive Mean Square Error and Akaike Information Criterion, by applying these models to a data set in the literature. Our results suggest that the bathtub model best describes the observed data and also most precisely predicts the future data points compared to the other popular software reliability models. The bathtub model can thus be used to provide accurate predictions during the testing process and guide optimal release time decisions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5754490,no
Nationwide real-time monitoring system for electrical quantities and power quality of the electricity transmission system,2011,"In this study, a nationwide real-time monitoring system has been developed to monitor all electrical quantities and power quality (PQ) parameters of the electricity transmission network including its interfaces with the generation and distribution systems. The implemented system is constituted of extended PQ analysers (PQ<sup>+</sup> analysers), a national monitoring center for power quality (NMCPQ), and the PQ retrieval and analysis software suite. The PQ<sup>+</sup> analyser is specifically designed for multipurpose usage such as event recording and raw data collection, in addition to the usual PQ analysis functions. The system has been designed to support up to 1000 PQ<sup>+</sup> analysers, to make the electricity transmission system and its interfaces completely observable in the future. The remote monitoring, analysis and reporting interface, the map-based interface, and the real-time monitoring interface, as well as the Web applications run on the NMCPQ servers, can be accessed by the utility, large industrial plants, distribution companies and researchers, to the extent of the authorisation given by the transmission system operator. By activating sufficient number of analogue and digital outputs of PQ<sup>+</sup> analysers, and equipping the NMCPQ with necessary hardware, analysis and decision making software, the proposed system can serve as an advanced supervisory control and data acquisition system in the future. The proposed monitoring concept can be extended to serve the needs of the modern electricity markets, by making various regulations, codes and instructions implementable.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5755155,no
Blind Image Quality Assessment: From Natural Scene Statistics to Perceptual Quality,2011,"Our approach to blind image quality assessment (IQA) is based on the hypothesis that natural scenes possess certain statistical properties which are altered in the presence of distortion, rendering them un-natural; and that by characterizing this un-naturalness using scene statistics, one can identify the distortion afflicting the image and perform no-reference (NR) IQA. Based on this theory, we propose an (NR)/blind algorithm-the Distortion Identification-based Image Verity and INtegrity Evaluation (DIIVINE) index-that assesses the quality of a distorted image without need for a reference image. DIIVINE is based on a 2-stage framework involving distortion identification followed by distortion-specific quality assessment. DIIVINE is capable of assessing the quality of a distorted image across multiple distortion categories, as against most NR IQA algorithms that are distortion-specific in nature. DIIVINE is based on natural scene statistics which govern the behavior of natural images. In this paper, we detail the principles underlying DIIVINE, the statistical features extracted and their relevance to perception and thoroughly evaluate the algorithm on the popular LIVE IQA database. Further, we compare the performance of DIIVINE against leading full-reference (FR) IQA algorithms and demonstrate that DIIVINE is statistically superior to the often used measure of peak signal-to-noise ratio (PSNR) and statistically equivalent to the popular structural similarity index (SSIM). A software release of DIIVINE has been made available online: http://live.ece.utexas.edu/research/quality/DIIVINE_release.zip for public use and evaluation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5756237,no
Assessment of WERA long-range HF-radar performance from the user's perspective,2011,"Since April 2006, long range (8.3MHz) WERA HF radars have been operated on the Southeastern United States coastline, as part of the U.S. Integrated Ocean Observing System (IOOS) and in particular the national HF Radar network. These radars measure currents operationally, and waves and winds experimentally across the wide continental shelf of Georgia (GA) and South Carolina (SC). Half-hourly data at 3km horizontal resolution are acquired to a range of approximately 200 km, providing measurements across the wide continental shelf and into the adjacent Gulf Stream at the shelf edge. Radar performance in range and quality is discussed. Ease in siting of these space and cable intensive systems along populated coastlines, and the feasibility of their operation by non-radar specialists is also briefly discussed. Long term in situ measurements of currents, waves and winds concurrent with the long-term radar measurements were available. These measurements were also acquired as part of the U.S. national coastal ocean observatory network, under the evolving auspices of SABSOON, SEACOOS, and SECOORA. Wind, wave and ADCP measurements from several instrumented Navy towers at the 27, 33, and 44 m isobaths are available, and winds and wave information also exist at the 18m isobath from an NDBC buoy. Comparisons between radar-derived estimates and the in situ measurements are examined for a variety of parameters, including (near) surface currents, significant wave heights, directional wave spectra, and wind direction and speed. Radar estimates of surface velocity compare quite well with in situ ADCP near surface current data, with complex vector correlation magnitude of 0.95, and phase angle of -0.2 degrees. The negative sign is consistent with an expected counterclockwise rotation with depth between the radar surface current estimates and the subsurface upper water column ADCP measurements. Tidal amplitudes, which are large and predominantly semidiurnal on the GA/SC coast are extre- > - > mely well reproduced by the radar estimates. Radar significant wave height estimates from manufacturer-supplied software are much noisier than measurements from in situ pressure sensors or wave buoys, but capture several-hour low-passed variability fairly well. The spectra from which the radar significant wave heights are estimated have been examined, also exhibiting higher variability than indicated by the in situ estimates. Wind direction estimates from radar using manufacturer-supplied software were evaluated for a range of wind speeds and direction (fetch limited or not) and by differentiating between conditions where the predominant water waves satisfied the long-wave assumption or not. For non-fetch limited wind speeds in excess of the Bragg wave propagation speed, and wave fields for which the long-wave assumption is relevant, correlations between radar and in situ anemometer wind directions were good, at approximately 0.7. HF-radar directional wave spectra estimates are also derived with aftermarket software from SeaView Sensing Ltd, and are compared to in situ estimates from a five beam ADCP. These novel in situ measurements use the ADCP slant beam horizontal velocities combined with vertical velocities from the vertically oriented 5th beam to measure wave orbital velocities and derive directional wave spectra. Wave buoy accelerometer estimates of directional wave spectra are also available for comparison. Radar significant wave heights and wind direction from SeaView software are also being examined.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5759520,no
Transmission line fault detection and classification,2011,"Transmission line protection is an important issue in power system engineering because 85-87% of power system faults are occurring in transmission lines. This paper presents a technique to detect and classify the different shunt faults on a transmission lines for quick and reliable operation of protection schemes. Discrimination among different types of faults on the transmission lines is achieved by application of evolutionary programming tools. PSCAD/EMTDC software is used to simulate different operating and fault conditions on high voltage transmission line, namely single phase to ground fault, line to line fault, double line to ground and three phase short circuit. The discrete wavelet transform (DWT) is applied for decomposition of fault transients, because of its ability to extract information from the transient signal, simultaneously both in time and frequency domain. The data sets which are obtained from the DWT are used for training and testing the SVM architecture. After extracting useful features from the measured signals, a decision of fault or no fault on any phase or multiple phases of a transmission line is carried out using three SVM classifiers. The ground detection task is carried out by a proposed ground index. Gaussian radial basis kernel function (RBF) has been used, and performances of classifiers have been evaluated based on fault classification accuracy. In order to determine the optimal parametric settings of an SVM classifier (such as the type of kernel function, its associated parameter, and the regularization parameter c), fivefold cross-validation has been applied to the training set. It is observed that an SVM with an RBF kernel provides better fault classification accuracy than that of an SVM with polynomial kernel. It has been found that the proposed scheme is very fast and accurate and it proved to be a robust classifier for digital distance protection.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5760084,no
Experimental study report on Opto-electronic sensor based gaze tracker system,2011,The paper presents a smart assistive technology to improve the life quality of the people with severe mobility disorders by giving them independence of motion despite the difficulties in moving their limbs. The objective of this paper is to develop an Opto-electronic sensor based gaze tracker apparatus to detect the eye gaze direction based on movement of iris. Eventually this detection helps the user to control and steer the wheel chair by themselves through their eye gaze. The principle of operation of this system is based on the reflection of incidental light in the iris and sclera regions of eye. The user's gaze tracker is in the form of eye-goggle embedded with infrared source and detectors. The performance of various optical sources and detectors are tested and the results are graphically represented. A template database and also an algorithm is generated to provide real time computational analysis.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5760182,no
Network architecture for smart grids,2011,"Smart grid is designed to make the existing power grid system function spontaneously and independently without human intervention. Sensors are deployed to detect faults in the flow of power. In the proposed work, smart monitoring and controls are done by intelligent electronic devices. IED's (Intelligent electronic devices) monitors and records the value of power generated, and its corresponding voltage and frequency, which in turn is fed in to the demand-supply chain. Network architecture is designed to determine the flow of power from the generation end to the consumers. Demand-supply curve is embedded in architecture to map the power generated with the supply. If the demand at a particular instant of time is higher than the supply, then tariff is fixed and warning is given to the users regarding the rate of payment, to meet the higher tariff. Trust based authencity is provided for security.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5762480,no
A single-specification principle for functional-to-timing simulator interface design,2011,"Microarchitectural simulators are often partitioned into separate, but interacting, functional and timing simulators. These simulators interact through some interface whose level of detail depends upon the needs of the timing simulator. The level of detail supported by the interface profoundly affects the speed of the functional simulator, therefore, it is desirable to provide only the detail that is actually required. However, as the microarchitectural design space is explored, these needs may change, requiring corresponding time-consuming and error-prone changes to the interface. Thus simulator developers are tempted to include extra detail in the interface ""just in case"" it is needed later, trading off simulator speed for development time. We show that this tradeoff is unnecessary if a single-specification design principle is practiced: write the simulator once with an extremely detailed interface and then derive less-detailed interfaces from this detailed simulator. We further show that the use of an Architectural Description Language (ADL) with constructs for interface specification makes it possible to synthesize simulators with less-detailed interfaces from a highly-detailed specification with only a few lines of code and minimal effort. The speed of the resulting low-detail simulators is up to 14.4 times the speed of high-detail simulators.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5762735,no
Architectures for online error detection and recovery in multicore processors,2011,"The huge investment in the design and production of multicore processors may be put at risk because the emerging highly miniaturized but unreliable fabrication technologies will impose significant barriers to the life-long reliable operation of future chips. Extremely complex, massively parallel, multi-core processor chips fabricated in these technologies will become more vulnerable to: (a) environmental disturbances that produce transient (or soft) errors, (b) latent manufacturing defects as well as aging/wearout phenomena that produce permanent (or hard) errors, and (c) verification inefficiencies that allow important design bugs to escape in the system. In an effort to cope with these reliability threats, several research teams have recently proposed multicore processor architectures that provide low-cost dependability guarantees against hardware errors and design bugs. This paper focuses on dependable multicore processor architectures that integrate solutions for online error detection, diagnosis, recovery, and repair during field operation. It discusses taxonomy of representative approaches and presents a qualitative comparison based on: hardware cost, performance overhead, types of faults detected, and detection latency. It also describes in more detail three recently proposed effective architectural approaches: a software-anomaly detection technique (SWAT), a dynamic verification technique (Argus), and a core salvaging methodology.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763096,no
ScTMR: A scan chain-based error recovery technique for TMR systems in safety-critical applications,2011,"We propose a roll-forward error recovery technique based on multiple scan chains for TMR systems, called Scan chained TMR (ScTMR). ScTMR reuses the scan chain flip-flops employed for testability purposes to restore the correct state of a TMR system in the presence of transient or permanent errors. In the proposed ScTMR technique, we present a voter circuitry to locate the faulty module and a controller circuitry to restore the system to the fault-free state. As a case study, we have implemented the proposed ScTMR technique on an embedded processor, suited for safety-critical applications. Exhaustive fault injection experiments reveal that the proposed architecture has the error detection and recovery coverage of 100% with respect to Single Event Upset (SEU) while imposing a negligible area and performance overhead as compared to traditional TMR-based techniques.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763277,no
Multi-level attacks: An emerging security concern for cryptographic hardware,2011,"Modern hardware and software implementations of cryptographic algorithms are subject to multiple sophisticated attacks, such as differential power analysis (DPA) and fault-based attacks. In addition, modern integrated circuit (IC) design and manufacturing follows a horizontal business model where different third-party vendors provide hardware, software and manufacturing services, thus making it difficult to ensure the trustworthiness of the entire process. Such business practices make the designs vulnerable to hard-to-detect malicious modifications by an adversary, termed as â€œHardware Trojansâ€? In this paper, we show that malicious nexus between multiple parties at different stages of the design, manufacturing and deployment makes the attacks on cryptographic hardware more potent. We describe the general model of such an attack, which we refer to as Multi-level Attack, and provide an example of it on the hardware implementation of the Advanced Encryption Standard (AES) algorithm, where a hardware Trojan is embedded in the design. We then analytically show that the resultant attack poses a significantly stronger threat than that from a Trojan attack by a single adversary. We validate our theoretical analysis using power simulation results as well as hardware measurement and emulation on a FPGA platform.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763307,no
Effects of Soft Error to System Reliability,2011,"Soft errors on hardware could affect the reliability of computer system. To estimate system reliability, it is important to know the effects of soft errors to system reliability. This paper explores the effects of soft errors to computer system reliability. We propose a new approach to measure system reliability for soft error factor. In our approach, hardware components reliability is concerned first. Then, system reliability which shows the ability to perform required function is concerned. We equal system reliability to software reliability based on the mechanism that soft errors affect system reliability. We build a software reliability model under soft errors condition. In our software model, we analyze the state of software combining with the state of hardware. For program errors which are resulted from soft errors, we give an analysis of error mask. These real errors which could lead to software failure are distinguished. Finally, our experiments illustrate our analyses and validate our approach.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763400,no
A Facility Framework for Distributed Application,2011,"The domain of distributed applications is developing rapidly. Facilities to support distributed applications have till now been designed on a case by case basis for each specialized user application. A systematic study and a generic facility framework for executing distributed applications are currently nonexistent and progress towards their development would have a significant impact in the seamless use of distributed applications. In line with these objectives, we have developed a component-based facility framework for distributed applications. The facility is defined as a high level conceptual facility which serves as an infrastructure providing services for distributed applications. The conceptual facility includes hardware and software (e.g. sensors, actuators and controllers) as sub-layers, which are dispersed physically. In this paper, we describe the framework, and examples are used to illustrate potential implementations of important concepts. The workflow of a distributed application is also provided. An implementation of a distributed application, with a partial version of the designed facility framework, for the water level control system of a nuclear power plant steam generator is used for evaluation of the concepts. Results convince us that the implementation as well as the facility is feasible. Application performance is shown to be affected by the time delay of data transmission and reasons for such delay are examined. A network quality test provides statistical estimates of data transmission delays due to the network. The results demonstrate that network quality is not a bottleneck for implementation for implementation of the component-based distributed application and support the hypothesis that a feasible implementation of such a conceptual facility framework is possible.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763544,no
Adaptive Genetic Algorithm for QoS-aware Service Selection,2011,"An adaptive Genetic Algorithm is presented to select optimal web service composite plan from a lot of composite plans on the basis of global Quality-of-Service (QoS) constraints. In this Genetic Algorithm, a population diversity measurement and an adaptive crossover strategy are proposed to further improve the efficiency and convergence of Genetic Algorithm. The probability value of the crossover operation can be set according to the combination of population diversity and individual fitness. The algorithm can get more excellent composite service plan because it accords with the characteristic of web service selection very well. Some simulation results on web service selection with global QoS constraints have shown that the adaptive Genetic Algorithm can gain quickly better composition service plan that satisfies the global QoS requirements.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763674,no
Software faults prediction using multiple classifiers,2011,"In recent years, the use of machine learning algorithms (classifiers) has proven to be of great value in solving a variety of problems in software engineering including software faults prediction. This paper extends the idea of predicting software faults by using an ensemble of classifiers which has been shown to improve classification performance in other research fields. Benchmarking results on two NASA public datasets show all the ensembles achieving higher accuracy rates compared with individual classifiers. In addition, boosting with AR and DT as components of an ensemble is more robust for predicting software faults.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763845,no
Phase-based tuning for better utilization of performance-asymmetric multicore processors,2011,"The latest trend towards performance asymmetry among cores on a single chip of a multicore processor is posing new challenges. For effective utilization of these performance-asymmetric multicore processors, code sections of a program must be assigned to cores such that the resource needs of code sections closely matches resource availability at the assigned core. Determining this assignment manually is tedious, error prone, and significantly complicates software development. To solve this problem, we contribute a transparent and fully-automatic process that we call phase-based tuning which adapts an application to effectively utilize performance-asymmetric multicores. Compared to the stock Linux scheduler we see a 36% average process speedup, while maintaining fairness and with negligible overheads.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5764670,no
Using machines to learn method-specific compilation strategies,2011,"Support Vector Machines (SVMs) are used to discover method-specific compilation strategies in Testarossa, a commercial Just-in-Time (JiT) compiler employed in the IBM<sup>Â®</sup> J9 Javaâ„?Virtual Machine. The learning process explores a large number of different compilation strategies to generate the data needed for training models. The trained machine-learned model is integrated with the compiler to predict a compilation plan that balances code quality and compilation effort on a per-method basis. The machine-learned plans outperform the original Testarossa for start-up performance, but not for throughput performance, for which Testarossa has been highly hand-tuned for many years.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5764693,no
A comparison study of automatic speech quality assessors sensitive to packet loss burstiness,2011,"The paper delves the behavior rating of new emerging automatic quality assessors of VoIP calls subject to bursty packet loss process. The examined speech quality assessment (SQA) algorithms are able to estimate speech quality of live VoIP calls at run-time using control information extracted from header content of received packets. They are especially designed to be sensitive to packet loss burstiness. The performance evaluation study is performed using a dedicated set-up software-based SQA framework. It offers a personalized packet killer and includes implementation of four SQA algorithms. A speech quality database, which covers a wide range of bursty packet loss conditions, has been created then thoroughly analyzed. Our important findings are the following: (1) all examined automatic bursty-loss aware speech quality assessors achieve a satisfactory correlation under upper (>;20%) and lower (<;10%) ranges of packet loss process (2) They exhibit a clear weakness to assess speech quality under a moderated packet loss process (3) The accuracy of sequence-by-sequence basis of examined SQA algorithms should be addressed in details for further precision.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5766503,no
Software reliability prediction model based on PSO and SVM,2011,"Software reliability prediction classifies software modules as fault-prone modules and less fault-prone modules at the early age of software development. As to a difficult problem of choosing parameters for Support Vector Machine (SVM), this paper introduces Particle Swarm Optimization (PSO) to automatically optimize the parameters of SVM, and constructs a software reliability prediction model based on PSO and SVM. Finally, the paper introduces Principal Component Analysis (PCA) method to reduce the dimension of experimental data, and inputs these reduced data into software reliability prediction model to implement a simulation. The results show that the proposed prediction model surpasses the traditional SVM in prediction performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5768285,no
Numerical simulation of FLD based on leaner strain path for fracture on auto panel surface,2011,"In order to emerge the traditional measurement's shortage of auto body panel, we proposed the corrected FLD based on the linear path method and calculation methods. According to the above mentioned programs and the fracture defect diagnosis, we developed a CAE module for the fracture defect analysis based on VC++ environment, and solved the problems which the traditional sheet metal forming CAE software can not accurately predict. And then a forming process of a hood is simulated by applying the proposed method and AUTOFORM. Comparison between the two simulation results is done which shows the method we proposed is better, and then we introduced some methods to optimize the adjustment amount of metal flow and the stamping dies for fracture. Some suggestions are given by investigating the adjustment amount and modification of the stamping die.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5768473,no
Application of GPR and Rayleigh wave prospecting in blasting compaction of ore terminal,2011,"The applications of both ground penetrating radar (GPR) and transient Rayleigh waves techniques were introduced in blasting compaction of ore terminal project in Jiaonan bay. Considered the geological structure and special detection conditions of seawall, a 25MHz GPR antenna was chosen to ensure the adequate penetrating depth, and a field test was carried out to obtain exact propagation velocity of electromagnetic waves in different layers. In transient Rayleigh wave detection, the GeopenRWA software which contains an effective inversion scheme was used to process the data. The result of two methods showed good agreement for determining the thickness of riprap layer, and therefore, has potential for quality control of seawall construction and accurate calculation of earthwork volume.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5769289,no
Assessing Oracle Quality with Checked Coverage,2011,"A known problem of traditional coverage metrics is that they do not assess oracle quality - that is, whether the computation result is actually checked against expectations. In this paper, we introduce the concept of checked coverage - the dynamic slice of covered statements that actually influence an oracle. Our experiments on seven open-source projects show that checked coverage is a sure indicator for oracle quality - and even more sensitive than mutation testing, its much more demanding alternative.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770598,no
An Empirical Evaluation of Assertions as Oracles,2011,"In software testing, an oracle determines whether a test case passes or fails by comparing output from the program under test with the expected output. Since the identification of faults through testing requires that the bug is both exercised and the resulting failure is recognized, it follows that oracles are critical to the efficacy of the testing process. Despite this, there are few rigorous empirical studies of the impact of oracles on effectiveness. In this paper, we report the results of one such experiment in which we exercise seven core Java classes and two sample programs with branch-adequate, input only(i.e., no oracle) test suites and collect the failures observed by different oracles. For faults, we use synthetic bugs created by the muJava mutation testing tool. In this study we evaluate two oracles: (1) the implicit oracle (or ""null oracle"") provided by the runtime system, and (2) runtime assertions embedded in the implementation (by others) using the Java Modeling Language. The null oracle establishes a baseline measurement of the potential benefit of rigorous oracles, while the assertions represent a more rigorous approach that is sometimes used in practice. The results of our experiments are interesting. First, on a per-method basis, we observe that the null oracle catches less than 11% of the faults, leaving more than 89% uncaught. Second, we observe that the runtime assertions in our subjects are effective at catching about 53% of the faults not caught by null oracle. Finally, by analyzing the data using data mining techniques, we observe that simple, code-based metrics can be used to predict which methods are amenable to the use of assertion-based oracles with a high degree of accuracy.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770600,no
An Empirical Study on the Relation between Dependency Neighborhoods and Failures,2011,"Changing source code in large software systems is complex and requires a good understanding of dependencies between software components. Modification to components with little regard to dependencies may have an adverse impact on the quality of the latter, i.e., increase their risk to fail. We conduct an empirical study to understand the relationship between the quality of components and the characteristics of their dependencies such as their frequency of change, their complexity, number of past failures and the like. Our study has been conducted on two large software systems: Microsoft VISTA and ECLIPSE. Our results show that components that have outgoing dependencies to components with higher object-oriented complexity tend to have fewer field failures for VISTA, but the opposite relation holds for ECLIPSE. Likewise, other notable observations have been made through our study that (a) confirm that certain characteristics of components increase the risk of their dependencies to fail and (b) some of the characteristics are project specific while some were also found to be common. We expect that such results can be leveraged for use to provide new directions for research in defect prediction, test prioritization and related research fields that utilize code dependencies in their empirical analysis. Additionally, these results provide insights to engineers on the potential reliability impacts of new component dependencies based upon the characteristics of the component.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770624,no
Cost Optimizations in Runtime Testing and Diagnosis of Systems of Systems,2011,"In practically all development processes tests are used to detect the presence of faults. This is not an exception for critical and high-availability systems. However, these systems cannot be taken offline or duplicated for testing in some cases. This makes runtime testing necessary. This paper presents work aimed at optimizing the three main sources of testing cost: preparation, execution and diagnosis. First, preparation cost is optimized by defining a metric of the runtime testability of the system, used to elaborate an implementation plan of preparative work for runtime testing. Second, the interrelated nature of test execution cost and diagnostic cost is highlighted and a new diagnostic test prioritization is introduced.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770638,no
Tracking hardware evolution,2011,"Software evolution is the term used to describe the process of developing and updating software systems. Software repositories such as versioning systems and bug tracking systems are used to manage the evolution of software projects. The mining of this information is used to support predictions and improve design and reuse. Integrated circuit development can also benefit from these techniques. Nowadays, both software and hardware development use repositories and bug tracking systems. There are many hardware open source projects such as SUN's OpenSparc and designs at Opencores.org. We propose a methodology to track specific HDL metrics in order to improve design quality. Our results present a case study that correlates HDL metrics and bug proneness of Verilog HDL modules. We also present EyesOn, an open source framework designed to automate historical and complexity metrics tracking of HDL projects.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770764,no
Occurrence probability analysis of a path at the architectural level,2011,"In this paper, we propose an algorithm to compute the occurrence probability for a given path precisely in an acyclic synthesizable VHDL or software code. This can be useful for the ranking of critical paths and in a variety of problems that include compiler-level architectural optimization and static timing analysis for improved performance. Functions that represent condition statements at the basic blocks are manipulated using Binary Decision Diagrams (BDDs). Experimental results show that the proposed method outperforms the traditional Monte Carlo simulation approach. The later is shown to be non-scalable as the number of inputs increases.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770768,no
CT Saturation Detection Based on Waveform Analysis Using a Variable-Length Window,2011,"Saturation of current transformers (CTs) can lead to maloperation of protective relays. Using the waveshape differences between the distorted and undistorted sections of fault current, this paper introduces a novel method to quickly detect CT saturation. First, a symmetrical variable-length window is defined for the current waveform. The least error squares technique is employed to process the current inside this window and make two estimations for the current samples exactly before and after the window. CT saturation can be identified based on the difference between these two estimations. The accurate performance of this method is independent of the CT parameters, such as CT remanence and its magnetization curve. Moreover, the proposed method is not influenced by the fault current characteristics, noise, etc., since it is based on the significant differences between the distorted and undistorted fault currents. Extensive simulation studies were performed using PSCAD/EMTDC software and the fast and reliable response of the proposed method for various conditions, including very fast and mild saturation events, was demonstrated.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5771582,no
Simulation Based Functional and Performance Evaluation of Robot Components and Modules,2011,"This paper presents a simulation based test method for functional and performance evaluation of robotic components and modules. In the proposed test method, function test procedure consists of unit, state, and interface tests which assess if the functional specifications of robot component 1 or module 2 are met. As for performance test, simulation environment provides a down scaled virtual work space for performance test of robot module accommodating virtual devices in conformity with the detailed performance specifications of real robot components. The proposed method can be used for verification of reliability of robot modules and components which prevents faults of them before their usage in real applications. In addition, the developed test system can be extended to support various test conditions implying possible cost saving for additional tests.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5772438,no
Qualification and Selection of Off-the-Shelf Components for Safety Critical Systems: A Systematic Approach,2011,"Mission critical systems are increasingly been developed by means of Off-The-Shelf (OTS) items since this allows reducing development costs. Crucial issues to be properly treated are (i) to assess the quality of each potential OTSitem to be used and (ii) to select the one that better fits the system requirements. Despite the importance of these issues, the current literature lacks a systematic approach to perform the previous two operations. The aim of this paper is to present a framework that can overcome this lack. Reasoning from the available product assurance standards for certifying mission critical systems, the proposed approach is based on the customized quality model that describes the quality attributes. Such quality model will guide a proper evaluation of OTS products, and the choice of which product to use is based on the outcomes of such an evaluation process. This framework represents a key solution to have a dominant role in the market of mission critical systems due to the demanding request by manufactures of such systems for an efficient qualification/certification process.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5773415,no
Discussion on questions about using artificial neural network for predicting of concrete property,2011,"Several questions about predicting concrete property using BP artificial neural network have been discussed, including the selection of network structure, the determination of sample capacity and grouping method, the protection from over-fitting, and the comparison on precision of prediction. For the network-structure, it has been found that directly apply the consumption of raw-material and other crucial quality indices as the units of input can bring about a satisfactory result of prediction, in which a single hide layer holds 10 units and the workability along with the strength and durability formed two sub-networks simultaneously. For the sample capacity and grouping method, at least 100 sets of samples are necessary to find the intrinsic regularity, among them 1/3-1/4 should be taken as test samples. A new tactics for error-tracking has been proposed which is verified effective to avoid the over-fitting. The comparison of effectiveness and feasibility between BP neural networks and linear regression algorithm showed that BP neural networks have better performance in accuracy of prediction. Finally, an applicable software has been developed and used as examples to predict 163 sets of mixes for a ready-mixed concrete plant, to show its application in detail.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5775143,no
Simulation of sensing field for electromagnetic tomography system,2011,"Electromagnetic tomography has potential value in process measurement. The frontier of electromagnetic tomography system is the sensor array. Owing to the excited signal acting on the sensor array directly, the excited strategy and the frequency and amplitude of the signal affect the quality of the information that the detected coil acquired from the object space. Furthermore, it would affect the accuracy of the information post extracted from the object field. To improve the sensitivity of the sensor array on the changes of the object field distribution, upgrade the sensitivity and accuracy of the system and guarantee high precision and high stability of the experimental data, use the finite element simulation software COMSOL Multiphysics to analyze the excited strategy and the characteristic of the excitation frequency of electromagnetic tomography. Establish the foundation in optimal using of electromagnetic tomography system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5777631,no
A distributed cable harness tester based on CAN bus,2011,"This paper discusses a distributed cable harness tester based on CAN bus, which has a few functions such as connection detecting of a wire, diode orientation testing and resistor's impedance testing. In this article, application layer protocal design is researched on in particular in order to improve the tester's performance, and the software design of upper computer and the framework of handware of tester nodes are introduced in details. The tester is designed to ensure quality and reliability of cable harness. Through detecting , early failure products such as breakage circuit, short circuit and wrong conductor arrangement can be rejected. So it improves the efficiency of detection.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5777720,no
The design of circular saw blade dynamic performance detection of high-speed data acquisition system,2011,"The dynamic properties of the circular saw blade when cutting at a high speed influence the cutting quality at a large extent. This paper gives a brief introduction about the employment of advanced sensors and detection methods together with self-detection software system detect circular saw blade vibration quickly and accurately, non-destructively at the condition of high-speed rotating of the circular saw blade. The results of the actual measurement of the different carbide circular saw blades show that this detection system has the quality of high accuracy and high reliability. It is fitful for the production process of circular saw blade body for quality control.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5777815,no
Design of a Novel Noninvasive Blood Glucose Monitor Based on Improved Holography Concave Grating NIR-Spectrometer,2011,"Real-time monitoring of blood glucose concentration (BGC) is an efficient way to prevent diabetes. And the noninvasive detection of BGC has been becoming a popular study point. In order to overcome some limitations and improve the performances of the noninvasive BGC detection based on spectroscopy, a novel blood glucose monitor (BGM) based on an improved NIR-spectrometer is designed in this paper. Meanwhile, a custom-built spectrometer for BGM based on the holography concave grating is developed. In order to eliminate the stray-light and improve the performances of the BGM, the grooves are delineated and black-painted on the inner-wall of the spectrometer. In addition, the linear CCD with combined data acquisition (DAQ) card and the virtual-spectrometer based on Lab VIEW are used as the spectrum acquisition hardware and software-platform unit. Experimental results show that the wavelength range, resolution and spectral quality of the BGM are improved. The wavelength range of the BGM is 300-1000nm; its resolution can reach 1nm. Therefore, the BGM has the potential values in the research of the diabetes and BGC detection.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5780288,no
A new methodology for realistic open defect detection probability evaluation under process variations,2011,"CMOS IC scaling has provided significant improvements in electronic circuit performance. Advances in test methodologies to deal with new failure mechanisms and nanometer issues are required. Interconnect opens are an important defect mechanism that requires detailed knowledge of its physical properties. In nanometer process, variability is predominant and considering only nominal value of parameters is not realistic. In this work, a model for computing a realistic coverage of via open defect that takes into account the process variability is proposed. Correlation between parameters of the affected gates is considered. Furthermore, spatial correlation of the parameters for those gates tied to the defective floating node can also influence the detectability of the defect. The proposed methodology is implemented in a software tool to determine the probability of detection of via opens for some ISCAS benchmark circuits. The proposed detection probability evaluation together with a test methodology to generate favorable logic conditions at the coupling lines can allow a better test quality leading to higher product reliability.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5783781,no
CEDA: Control-Flow Error Detection Using Assertions,2011,"This paper presents an efficient software technique, control-flow error detection through assertions (CEDA), for online detection of control-flow errors. Extra instructions are automatically embedded into the program at compile time to continuously update runtime signatures and to compare them against preassigned values. The novel method of computing runtime signatures results in a huge reduction in the performance overhead, as well as the ability to deal with complex programs and the capability to detect subtle control-flow errors. The widely used C compiler, GCC, has been modified to implement CEDA, and the SPEC benchmark programs were used as the target to compare with earlier techniques. Fault injection experiments were used to demonstrate the effect of control-flow errors on software and to evaluate the fault detection capabilities of CEDA. Based on a new comparison metric, method efficiency, which takes into account both error coverage and performance overhead, CEDA is found to be much better than previously proposed methods.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5871589,no
Towards a software quality assessment model based on open-source statical code analyzers,2011,"In the context of software engineering, quality assessment is not straightforward. Generally, quality assessment is important since it can cut costs in the product life-cycle. A software quality assessment model based on open-source analyzers and quality factors facilitates quality measurement. Our quality assessment model is based on three principles: mapping rules to quality factors, computing scores based on the percentage of rule violating entities (classes, methods, instructions), assessing quality as a weighted mean of the rule attached scores.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5873026,no
A suite of java specific metrics for software quality assessment based on statical code analyzers,2011,"Software quality assessment can cut costs significantly from early development stages. On the other hand quality assessment helps in taking development decisions, checking the effect of fault corrections, estimating maintenance effort. Fault density based quality assessment relying on statical source code analyzers need language specific metrics in order to quantify quality as a number. Thus, we identified and defined informally a suite of Java metrics in order to accomplish our quality assessment goal.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5873027,no
Accurate location of phase-to-earth and phase-to-phase faults on power transmission lines using two-end synchronized measurements,2011,This paper presents a new non-iterative algorithm for accurate location of phase-to-earth and phase-to-phase faults on power transmission lines. Input signals of the fault locator are considered as obtained from two-end synchronized measurements of three-phase currents and voltages. High accuracy of fault location is assured by considering a distributed-parameter line model and processing the signals from the fault interval only. The developed algorithm has been thoroughly tested with use of signals generated by ATP-EMTP software for wide range of simulations. Selected results from such testing are presented and discussed.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5874671,no
Employing S-transform for fault location in three terminal lines,2011,"This paper has proposed a new fault location method for three terminal transmission lines. Once a fault occurs in a transmission line, high frequency transient components through travelling of the waves are appeared in all terminals. In this paper, S-transform is employed to detect the arrival time of these waves to terminals. For simulating various conditions, ATP-EMTP software and to process the proposed fault location method MATLAB software are used. The results have shown the good performance of the algorithm in different conditions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5874769,no
Evaluation and performance comparison of power swing detection algorithms in presence of series compensation on transmission lines,2011,"Any sudden change in the configuration or the loading of an electrical network causes power swing between the load concentrations of the network. In order to prevent the distance protection from tripping during such conditions, a power swing blocking is utilized. This paper compares and evaluates the performance of several power swing detection algorithms in the presence of series compensation and without it. The decreasing impedance, the VcosÏ† and the power derivation methods are ordinary algorithms for power swing detection. A precise model for power swing conditions has been generated by EMTDC/PSCAD software. In this paper this model is used to simulate the algorithms behavior under different operational conditions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5874850,no
A real-time coarse-to-fine multiview capture system for all-in-focus rendering on a light-field display,2011,"We present an end-to-end system capable of real-time capturing and displaying with full horizontal parallax high-quality 3D video contents on a cluster-driven multiprojector light-field display. The capture component is an array of low-cost USB cameras connected to a single PC. Raw M-JPEG data coming from the software-synchronized cameras are multicast over Gigabit Ethernet to the back-end nodes of the rendering cluster, where they are decompressed and rendered. For all-in-focus rendering, view-dependent depth is estimated on the GPU using a customized multiview space-sweeping approach based on fast Census-based area matching implemented in CUDA. Realtime performance is demonstrated on a system with 18 VGA cameras and 72 SVGA rendering projectors.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5877176,no
An AMI based measurement and control system in smart distribution grid,2011,"To realize some of the smart grid goals, for the distribution system of the rural area, with GPRS communication network, a feeder automation based on AMI is proposed. The three parts of the system are introduced. Integrated with the advanced communication and measurement technology, the proposed system can monitor the operating situation and status of breakers, detect and locate the fault of the feeders. The information from the system will help realizing the advanced distribution operation, such as improve power quality, loss detection, state estimation and so on. The application case in Qingdao utilities in Shandong Province, PR China shows the effectiveness of the proposed system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5890876,no
Simulation and experimental analysis of a ZigBee sensor network with fault detection and reconfiguration mechanism,2011,"In this paper, the networking architecture of wireless sensor system is studied and proposed by simulation analysis and experimental testing to implement the fault detection and reconfiguration mechanism on ZigBee network. The tool software NS2 is used to simulate and analyze the networking topologies and transmission characteristics for ZigBee network system. By way of simulation analysis of different networking topologies, the system performance of ZigBee sensor network is discussed and recognized individually for the mesh and tree routing algorithms, respectively. Moreover, the CC2430 development kit of Texas Instrument (TI) is used to develop the ZigBee nodes including the coordinator, router, and end-devices to form a specific application scenario for disaster prevention and warning system. By applying the Inter-PAN conception and algorithm, the networking architecture of ZigBee sensor system is proposed and exercised to carry out the fault detection and reconfiguration functions which may increase the reliability of data transmission on ZigBee network. Finally, through the various and practical experiments, the proposed networking topologies are implemented and tested to illustrate and evaluate the system feasibility and performance of proposed ZigBee network for wireless sensor system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5899150,no
Empirical study of an intelligent argumentation system in MCDM,2011,"Intelligent argumentation based collaborative decision making system assists stakeholders in a decision making group to assess various alternatives under different criterion based on the argumentation. A performance score of each alternative under every criterion in Multi-Criteria Decision Making (MCDM) is represented in a decision matrix and it denotes satisfaction of the criteria by that alternative. The process of determining the performance scores of alternatives in a decision matrix for criterion could be controversial sometimes because of the subjective nature of criterion. We developed a framework for acquiring performance scores in a decision matrix for multi-criteria decision making using an intelligent argumentation and collaborative decision support system we developed in the past [1]. To validate the framework empirically, we have conducted a study in a group of stakeholders by providing them an access to use the intelligent argumentation based collaborative decision making tool over the Web. The objectives of the study are: 1) to validate the intelligent argumentation system for deriving performance scores in multi-criteria decision making, and 2) to validate the overall effectiveness of the intelligent argumentation system in capturing rationale of stakeholders. The results of the empirical study are analyzed in depth and they show that the system is effective in terms of collaborative decision support and rationale capturing. In this paper, we present how the study was carried out and its empirical results.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5928674,no
Software and communications architecture for Prognosis and Health Monitoring of ocean-based power generator,2011,"This paper presents a communications and software architecture in support of Prognosis and Health Monitoring (PHM) applications for renewable ocean-based power generation. The generator/turbine platform is instrumented with various sensors (e.g. vibration, temperature) that generate periodic measurements used to assess the current system health and to project its future performance. The power generator platform is anchored miles offshore and uses a pair of wireless data links for monitoring and control. Since the link is expected to be variable and unreliable, being subject to challenging environmental conditions, the main functions of the PHM system are performed on a computing system located on the surface platform. The PHM system architecture is implemented using web services technologies following MIMOSA OSA-CBM standards. To provide sufficient Quality of Service for mission-critical traffic, the communications system employs application-level queue management with semantic-based filtering for the XML PHM messages, combined with IP packet traffic control and link quality monitoring at the network layer.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5929092,no
The possibility of application the optical wavelength division multiplexing network for streaming multimedia distribution,2011,"In this paper, simulation is used to investigate the possibility of streaming multimedia distribution over optical wavelength division multiplexing (WDM) network with optical burst switching (OBS) nodes. Simulation model is developed using software tool Delsi for Delfi 4.0. Pareto generator is used to model real-time multimedia stream. The wavelength allocation (WA) method and the deflection routing are implemented in OBS nodes. Performance measures, packet loss and delay, are estimated in order to investigate the quality of multimedia service. Statistical analysis of simulation output is performed by estimating the confidence intervals for a given degree according to the Student distribution. Obtained results indicate that optical WDM network manages multimedia contents distribution in efficient way to give a high quality of service.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5929183,no
Applying source code analysis techniques: A case study for a large mission-critical software system,2011,"Source code analysis has been and still is extensively researched topic with various applications to the modern software industry. In this paper we share our experience in applying various source code analysis techniques for assessing the quality of and detecting potential defects in a large mission-critical software system. The case study is about the maintenance of a software system of a Bulgarian government agency. The system has been developed by a third-party software vendor over a period of four years. The development produced over 4 million LOC using more than 20 technologies. Musala Soft won a tender for maintaining this system in 2008. Although the system was operational, there were various issues that were known to its users. So, a decision was made to assess the system's quality with various source code analysis tools. The expectation was that the findings will reveal some of the problems' cause, allowing us to correct the issues and thus improve the quality and focus on functional enhancements. Musala Soft had already established a special unit - Applied Research and Development Center - dealing with research and advancements in the area of software system analysis. Thus, a natural next step was for this unit to use the know-how and in-house developed tools to do the assessment. The team used various techniques that had been subject to intense research, more precisely: software metrics, code clone detection, defect and â€œcode smellsâ€?detection through flow-sensitive and points-to analysis, software visualization and graph drawing. In addition to the open-source and free commercial tools, the team used internally developed ones that complement or improve what was available. The internally developed Smart Source Analyzer platform that was used is focused on several analysis areas: source code modeling, allowing easy navigation through the code elements and relations for different programming languages; quality audit through software metri- s by aggregating various metrics into a more meaningful quality characteristic (e.g. â€œmaintainabilityâ€?; source code pattern recognition - to detect various security issues and â€œcode smellsâ€? The produced results presented information about both the structure of the system and its quality. As the analysis was executed in the beginning of the maintenance tenure, it was vital for the team members to quickly grasp the architecture and the business logic. On the other hand, it was important to review the detected quality problems as this guided the team to quick solutions for the existing issues and also highlighted areas that would impede future improvements. The tool IPlasma and its System Complexity View (Fig. 1) revealed where the business logic is concentrated, which are the most important and which are the most complex elements of the system. The analysis with our internal metrics framework (Fig. 2) pointed out places that need refactoring because the code is hard to modify on request or testing is practically impossible. The code clone detection tools showed places where copy and paste programming has been applied. PMD, Find Bugs and Klockwork Solo tools were used to detect various â€œcode smellsâ€?(Fig. 3). There were a number of occurrences that were indeed bugs in the system. Although these results were productive for the successful execution of the project, there were some challenges that should be addressed in the future through more extensive research. The two aspects we consider the most important are usability and integration. As most of the tools require very deep understanding of the underlying analysis, the whole process requires tight cooperation between the analysis team and the maintenance team. For example, most of the metrics tools available provide specific values for a given metric without any indication what the value means and what is the threshold. Our internal metrics framework aggregates the met",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5929241,no
GPU-Based Fast Iterative Reconstruction of Fully 3-D PET Sinograms,2011,"This work presents a graphics processing unit (GPU)-based implementation of a fully 3-D PET iterative reconstruction code, FIRST (Fast Iterative Reconstruction Software for [PET] Tomography), which was developed by our group. We describe the main steps followed to convert the FIRST code (which can run on several CPUs using the message passing interface [MPI] protocol) into a code where the main time-consuming parts of the reconstruction process (forward and backward projection) are massively parallelized on a GPU. Our objective was to obtain significant acceleration of the reconstruction without compromising the image quality or the flexibility of the CPU implementation. Therefore, we implemented a GPU version using an abstraction layer for the GPU, namely, CUDA C. The code reconstructs images from sinogram data, and with the same System Response Matrix obtained from Monte Carlo simulations than the CPU version. The use of memory was optimized to ensure good performance in the GPU. The code was adapted for the VrPET small-animal PET scanner. The CUDA version is more than 70 times faster than the original code running in a single core of a high-end CPU, with no loss of accuracy.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5929498,no
An approach for source code classification to enhance maintainability,2011,"The importance of software development is a design and programming to create a code quality, especially maintainability factor. An approach for source code classification to enhance maintainability is a method to classify and improve code. This approach can increase High cohesion and Low coupling that effect to the internal and the external of software. It can reduce degraded integrity. First of all, we classify source code with software metrics and fuzzy logic and then improve bad smell, ambiguous code with refactoring to be a clean code. The last step, we evaluate the quality of clean code with maintainability measurement.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5930141,no
Bad-smell prediction from software design model using machine learning techniques,2011,Bad-smell prediction significantly impacts on software quality. It is beneficial if bad-smell prediction can be performed as early as possible in the development life cycle. We present methodology for predicting bad-smells from software design model. We collect 7 data sets from the previous literatures which offer 27 design model metrics and 7 bad-smells. They are learnt and tested to predict bad-smells using seven machine learning algorithms. We use cross-validation for assessing the performance and for preventing over-fitting. Statistical significance tests are used to evaluate and compare the prediction performance. We conclude that our methodology have proximity to actual values.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5930143,no
Apad: A QoS Guarantee System for Virtualized Enterprise Servers,2011,"Today's data center often employs virtualization to allow multiple enterprise applications to share a common hosting platform in order to improve resource and space utilization. It could be a greater challenge when the shared server becomes overloaded or enforces priority of the common resource among applications in such an environment. In this paper, we propose Apad, a feedback-based resource allocation system which can dynamically adjust the resource shares to multiple virtual machine nodes in order to meet performance target on shared virtualized infrastructure. To evaluate our system design, we built a test bed hosting several virtual machines which employed Xen Virtual Machine Monitor (VMM), and using Apache server along with its workload-generated tool. Our experiment results indicate that our system is able to detect and adapt to resource requirement that changes over time and allocate virtualized resources accordingly to achieve application-level Quality of Service (QoS).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5931129,no
Identity attack and anonymity protection for P2P-VoD systems,2011,"As P2P multimedia streaming service is becoming more popular, it is important for P2P-VoD content providers to protect their servers identity. In this paper, we first show that it is possible to launch an ""identity attack"": exposing and identifying servers of peer-to-peer video-on-demand (P2P-VoD) systems. The conventional wisdom of the P2P-VoD providers is that identity attack is very difficult because peers cannot distinguish between regular peers and servers in the P2P streaming process. We are the first to show that it is otherwise, and present an efficient and systematic methodology to perform P2P-VoD servers detection. Furthermore, we present an analytical framework to quantify the probability that an endhost is indeed a P2P-VoD server. In the second part of this paper, we present a novel architecture that can hide the identity and provide anonymity protection for servers in P2P-VoD systems. To quantify the protective capability of this architecture, we use the ""fundamental matrix theory"" to show the high complexity of discovering all protective nodes so as to disrupt the P2P-VoD service. We not only validate the model via extensive simulation, but also implement this protective architecture on PlanetLab and carry out measurements to reveal its robustness against identity attack.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5931313,no
Finding Interaction Faults Adaptively Using Distance-Based Strategies,2011,"Software systems are typically large and exhaustive testing of all possible input parameters is usually not feasible. Testers select tests that they anticipate may catch faults, yet many unanticipated faults may be overlooked. This work complements current testing methodologies by adaptively dispensing one-test-at-a-time, where each test is as ""distant"" as possible from previous tests. Two types of distance measures are explored: (1) distance defined in relation to combinations of parameter-values not previously tested together and (2) distance computed as the maximum minimal Hamming distance from previous tests. Experiments compare the effectiveness of these two types of distance-based tests and random tests. Experiments include simulations, as well as examination of instrumented data from an actual system, the Traffic Collision Avoidance System (TCAS). Results demonstrate that the two instantiations of distance-based tests often find more faults sooner and in fewer tests than randomly generated tests.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5934799,no
Self-diagnosis for large scale wireless sensor networks,2011,"Existing approaches to diagnosing sensor networks are generally sink-based, which rely on actively pulling state information from all sensor nodes so as to conduct centralized analysis. However, the sink-based diagnosis tools incur huge communication overhead to the traffic sensitive sensor networks. Also, due to the unreliable wireless communications, sink often obtains incomplete and sometimes suspicious information, leading to highly inaccurate judgments. Even worse, we observe that it is always more difficult to obtain state information from the problematic or critical regions. To address the above issues, we present the concept of self-diagnosis, which encourages each single sensor to join the fault decision process. We design a series of novel fault detectors through which multiple nodes can cooperate with each other in a diagnosis task. The fault detectors encode the diagnosis process to state transitions. Each sensor can participate in the fault diagnosis by transiting the detector's current state to a new one based on local evidences and then pass the fault detector to other nodes. Having sufficient evidences, the fault detector achieves the Accept state and outputs the final diagnosis report. We examine the performance of our self-diagnosis tool called TinyD2 on a 100 nodes testbed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5934944,no
Multi-field range encoding for packet classification in TCAM,2011,"Packet classification has wide applications such as unauthorized access prevention in firewalls and Quality of Service supported in Internet routers. The classifier containing pre-defined rules is processed by the router for finding the best matching rule for each incoming packet and for taking appropriate actions. Although many software-based solutions had been proposed, high search speed required for Internet backbone routers is not easy to achieve. To accelerate the packet classification, the state-of-the-art ternary content-addressable memory (TCAM) is a promising solution. In this paper, we propose an efficient multi-field range encoding scheme to solve the problem of storing ranges in TCAM and to decrease TCAM usage. Existing range encoding schemes are usually single-field schemes that perform range encoding processes in the range fields independently. Our performance experiments on real-life classifiers show that the proposed multi-field range encoding scheme uses less TCAM memory than the existing single field schemes. Compared with existing notable single-field encoding schemes, the proposed scheme uses 12% ~ 33% of TCAM memory needed in DRIPE or SRGE and 56% ~ 86% of TCAM memory needed in PPC for the classifiers of up to 10k rules.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5935001,no
QoF: Towards comprehensive path quality measurement in wireless sensor networks,2011,"Due to its large scale and constrained communication radius, a wireless sensor network mostly relies on multi-hop transmissions to deliver a data packet along a sequence of nodes. It is of essential importance to measure the forwarding quality of multi-hop paths and such information shall be utilized in designing efficient routing strategies. Existing metrics like ETX, ETF mainly focus on quantifying the link performance in between the nodes while overlooking the forwarding capabilities inside the sensor nodes. The experience on manipulating GreenOrbs, a large-scale sensor network with 330 nodes, reveals that the quality of forwarding inside each sensor node is at least an equally important factor that contributes to the path quality in data delivery. In this paper we propose QoF, Quality of Forwarding, a new metric which explores the performance in the gray zone inside a node left unattended in previous studies. By combining the QoF measurements within a node and over a link, we are able to comprehensively measure the intact path quality in designing efficient multi-hop routing protocols. We implement QoF and build a modified Collection Tree Protocol (CTP). We evaluate the data collection performance in a test-bed consisting of 50 TelosB nodes, and compare it with the original CTP protocol. The experimental results show that our approach takes both transmission cost and forwarding reliability into consideration, thus achieving a high throughput for data collection.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5935299,no
Case-based reasoning and real-time systems: Exploiting successfully poorer solutions,2011,"In the literature of real-time software applications, case-based reasoning (CBR) techniques have been successfully used in order to develop systems able to carry on with their temporal restrictions. This paper presents a mathematical technique for modelling the generation of solutions by a RealTime (RT) system employing a CBR that allows their response times to be bounded. Speaking in general, a system that tries to be adapted to highly dynamic environment needs an efficient integration of high-level processes (deliberative and time-costly, but close-fitting) within low-level (reactive, faster but poorer in quality) processes is necessary. The most relevant aspect of our current approach is that, unexpectedly, the performance of the system do not get worse any time that it retrieves worse cases in situations even when it has enough time to generate better solutions. We concentrate on formal aspects of the proposed integrated CBR-RT system without establishing which should be the most adequate procedure in a subsequent implementation stage. The advantage of the presented scheme is that it does not depend on neither the particular problem nor a concrete environment. It consists in a formal approach that only requires, on one hand, local information about the averaged-time spent by the system in obtaining a solution and, on the other hand, an estimation about their temporal restrictions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5936305,no
Motion estimation with Second Order Prediction,2011,"To exploit both spatial and temporal correlation of video signal, a Second Order Prediction (SOP) scheme has been presented to eliminate the spatial correlation existing in motion compensated residue. Although SOP achieves better RD performance by adopting residue prediction in Mode Decision (MD) stage, it can be further improved by combining residue prediction into Motion Estimation (ME). Our analysis demonstrates that a more accurate motion vector could be obtained if both ME and MD take residue prediction into account. Implementation of the proposed algorithm into the H.264/AVC reference software demonstrates that the enhanced SOP can achieve up to 18.46% of bit-rate saving at equivalent objective quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5938143,no
Test case prioritization for regression testing based on fault dependency,2011,"Test case prioritization techniques involve scheduling test cases for regression testing in an order that increases their effectiveness at meeting some performance goal. This is inefficient to re execute all the test cases in regression testing following the software modifications. Using information obtained from previous test case execution, prioritization techniques order the test cases for regression testing so that most beneficial are executed first thus allows an improved effectiveness of testing. One performance goal, rate of dependency detected among faults, measures how quickly dependency among faults are detected within the regression testing process. An improved rate of fault dependency can provide faster feedback on software and let developers start debugging on the severe faults that cause other faults to appear later. This paper presents the new metric for assessing rate of fault dependency detection and an algorithm to prioritize test cases. Using the new metric the effectiveness of this prioritization is shown comparing it with non-prioritized test case. Analysis proves that prioritized test cases are more effective in detecting dependency among faults.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5941954,no
Analysis of quality of object oriented systems using object oriented metrics,2011,Measurement is fundamental to any engineering discipline. There is considerable evidence that object-oriented design metrics can be used to make quality management decisions. This leads to substantial cost savings in allocation of resources for testing or estimation of maintenance effort for a project. C++ has always been the most preferred language of choice for many object oriented systems and many object oriented metrics have been proposed for it. This paper focuses on an empirical evaluation of object oriented metrics in C++. Two projects have been considered as inputs for the study - the first project is a Library management system for a college and the second is a graphical editor which can be used to describe and create a scene. The metric values have been calculated using a semi automated tool. The resulting values have been analyzed to provide significant insight about the object oriented characteristics of the projects.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5941986,no
MngRisk â€?A decisional framework to measure managerial dimensions of legacy application for rejuvenation through reengineering,2011,"Nowadays legacy system reengineering has emerged as a well-known system evolution technique. The goal of reengineering is to increase productivity and quality of legacy system through fundamental rethinking and radical redesigning of system. A broad range of risk issues and concerns must be addressed to understand and model reengineering process. Overall success of reengineering effort requires to considering three distinctive but connected areas of interest i.e. system domain, managerial domain and technical domain. We present a hierarchical managerial domain risk framework MngRisk to analyze managerial dimensions of legacy system. The fundamental premise of framework is to observe, extract and categories the contextual perspective models and risk clusters of managerial domain. This work contributes for a decision driven framework to identify and assess risk components of managerial domain. Proposed framework provides guidance on interpreting the results obtained from assessment to take decision about when evolution of a legacy system through reengineering is successful.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5942048,no
Service selection based on status identification in SOA,2011,"Service Oriented Architecture (SOA) has become a new software development paradigm because it provides a flexible framework that can help to reduce development cost and time. SOA promises loosely coupled interoperable and composable services. Service selection in dynamic environment is the usage of techniques in selecting and providing quality of services (QoS) to consumers. The dynamic nature of web services is realized by an environment for collecting runtime information of web services. Based on the collected runtime information, several characteristics of service failures and successes are identified and four service statuses are defined. Based on these statuses, an approach to dynamic availability estimation is proposed along with multiple QoS criteria's such as cost and performance which is called Status Identification based service selection (SISS).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5942062,no
Type a uncertainty in jitter measurements in communication networks,2011,"Reliable quality of service measurements are crucial issues to monitor and evaluate the performance of computer and communication networks. To these aims, depending on the applications, several figures of merit have to be measured. They mainly concern with: throughput, available bandwidth, packet jitter, one way delay, round trip delay, and packet loss. To estimate these quantities two general classes of measurement systems (also known as â€œprotocol analyzersâ€? can be considered: Special Purpose Architecture based Protocol Analyzer (SPAPA) and General Purpose Architecture based Protocol Analyzer (GPAPA). Both measurement solutions can be thought as composed by a reference data traffic generator and one or more measurement nodes. All these components are constituted by hardware and software sections which cooperate to achieve the measurement results. Focusing the attention on the uncertainty involved in the QoS indexes measurement, no information is provided by the manufactures for both SPAPA and GPAPA. In addition, the analysis of the literature in the field of computer networks testing, has highlighted that this important aspect is not adequately dealt with or that methodological studies are not yet available. This paper proposes a suitable approach to evaluate the type A measurement uncertainty of SPAPA and GPAPA systems. A general model to study the effect of the involved quantities of influence is presented and a method to quantify their effects is provided. The experimental results show the suitability and the effectiveness of the proposal.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5944205,no
Diagnosis method for suspension-errors detection in electro-dynamic loud-speakers,2011,"In this paper, the wear of the suspensions in a speaker. This phenomenon can be characterized and seen as a variation of half inductance seen from the amplifier, because that value depends directly on the minimum and maximum excursion in the displacement of the moving coil. Having analyzed the possible cases of involvement of the shift from worn suspensions, has raised a board decision by which it is possible to diagnose the state of the suspension by measuring currents and voltages and calculation of half inductance seen from the amplifier.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5944220,no
Pattern and Phonetic Based Street Name Misspelling Correction,2011,"With increasing information reproduced each year, the quality of the data is subject to damage. Variations and errors in the address fields of databases make address-matching strategies problematic. In this paper, first the sources of the error in street names are investigated. Then, a hybrid approach, which is a combination of pattern and phonetic-matching algorithms, is used to fix the problems in the street names. The algorithm is evaluated in terms of its efficiency and correction rates. The newly introduced technique shows an improvement of at least 7% in correction rates compared to the well-known spelling correction algorithms.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5945296,no
Early Availability Requirements Modeling Using Use Case Maps,2011,"The design and implementation of distributed real-time systems is often dominated by non-functional considerations like timing, distribution and fault tolerance. As a result, it is increasingly recognized that non-functional requirements should be considered at the earliest stages of system development life cycle. The ability to model non-functional properties (such as timing constraints, availability, performance and security) at the system requirement level not only facilitates the task of moving towards real-time design, but ultimately supports the early detection of errors through automated validation and verification. This paper introduces a novel approach to describe availability features in Use Case Maps (UCM) specifications. The proposed approach relies on a mapping of availability architectural tactics to UCM components. We illustrate the applicability of our approach using the ISSU (In Service Software Upgrade) feature on IP routers.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5945331,no
Large Scale Monitoring and Online Analysis in a Distributed Virtualized Environment,2011,"Due to increase in number and complexity of the large scale systems, performance monitoring and multidimensional quality of service (QoS) management has become a difficult and error prone task for system administrators. Recently, the trend has been to use virtualization technology, which facilitates hosting of multiple distributed systems with minimum infrastructure cost via sharing of computational and memory resources among multiple instances, and allows dynamic creation of even bigger clusters. An effective monitoring technique should not only be fine grained with respect to the measured variables, but also should be able to provide a high level overview of the distributed systems to the administrator of all variables that can affect the QoS requirements. At the same time, the technique should not add performance burden to the system. Finally, it should be integrated with a control methodology that manages performance of the enterprise system. In this paper, a systematic distributed event based (DEB) performance monitoring approach is presented for distributed systems by measuring system variables (physical/virtual CPU utilization and memory utilization), application variables (application queue size, queue waiting time, and service time), and performance variables (response time, throughput, and power consumption) accurately with minimum latency at a specified rate. Furthermore, we have shown that proposed monitoring approach can be utilized to provide input to an application monitoring utility to understand the underlying performance model of the system for a successful on-line control of the distributed systems for achieving predefined QoS parameters.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5946180,no
Error detection scheme based on fragile watermarking for H.264/AVC,2011,"Compressed video bitstreams are very sensitive to transmission errors. If we lose packets or receive them with errors during transmission, not only the current frame will be corrupted, but also the error will propagate to succeeding frames due to the spatiotemporal predictive coding structure. Error detection and concealment is a good approach to reduce the bad influence on the reconstructed visual quality. To increase concealment efficiency, we need to get some more accurate error detection algorithm. In this paper, we present a new error detection scheme based on a fragile watermarking algorithm to increase the error detection ratio. We verified that the pro posed algorithm generates good performances in PSNR and objective visual quality through the computer simulation by H.324M mobile simulation set.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5946590,no
Failure Avoidance through Fault Prediction Based on Synthetic Transactions,2011,"System logs are an important tool in studying the conditions (e.g., environment misconfigurations, resource status, erroneous user input) that cause failures. However, production system logs are complex, verbose, and lack structural stability over time. These traits make them hard to use, and make solutions that rely on them susceptible to high maintenance costs. Additionally, logs record failures after they occur: by the time logs are investigated, users have already experienced the failures' consequences. To detect the environment conditions that are correlated with failures without dealing with the complexities associated with processing production logs, and to prevent failure-causing conditions from occurring before the system goes live, this research suggests a three step methodology: (i) using synthetic transactions, i.e., simplified workloads, in pre-production environments that emulate user behavior, (ii) recording the result of executing these transactions in logs that are compact, simple to analyze, stable over time, and specifically tailored to the fault metrics of interest, and (iii) mining these specialized logs to understand the conditions that correlate to failures. This allows system administrators to configure the system to prevent these conditions from happening. We evaluate the effectiveness of this approach by replicating the behavior of a service used in production at Microsoft, and testing the ability to predict failures using a synthetic workload on a 650 million events production trace. The synthetic prediction system is able to predict 91% of real production failures using 50-fold fewer transactions and logs that are 10,000-fold more compact than their production counterparts.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5948623,no
Autonomic SLA-Driven Provisioning for Cloud Applications,2011,"Significant achievements have been made for automated allocation of cloud resources. However, the performance of applications may be poor in peak load periods, unless their cloud resources are dynamically adjusted. Moreover, although cloud resources dedicated to different applications are virtually isolated, performance fluctuations do occur because of resource sharing, and software or hardware failures (e.g. unstable virtual machines, power outages, etc.). In this paper, we propose a decentralized economic approach for dynamically adapting the cloud resources of various applications, so as to statistically meet their SLA performance and availability goals in the presence of varying loads or failures. According to our approach, the dynamic economic fitness of a Web service determines whether it is replicated or migrated to another server, or deleted. The economic fitness of a Web service depends on its individual performance constraints, its load, and the utilization of the resources where it resides. Cascading performance objectives are dynamically calculated for individual tasks in the application workflow according to the user requirements. By fully implementing our framework, we experimentally proved that our adaptive approach statistically meets the performance objectives under peak load periods or failures, as opposed to static resource settings.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5948634,no
An evolutionary multiobjective optimization approach to component-based software architecture design,2011,"The design of software architecture is one of the difficult tasks in the modern component-based software development which is based on the idea that develop software systems by assembling appropriate off-the-shelf components with a well-defined software architecture. Component-based software development has achieved great success and been extensively applied to a large range of application domains from realtime embedded systems to online web-based applications. In contrast to traditional approaches, it requires software architects to address a large number of non-functional requirements that can be used to quantify the operation of system. Moreover, these quality attributes can be in conflict with each other. In practice, software designers try to come up with a set of different architectural designs and then identify good architectures among them. With the increasing scale of architecture, this process becomes time-consuming and error-prone. Consequently architects could easily end up with some suboptimal designs because of large and combinatorial search space. In this paper, we introduce AQOSA (Automated Quality-driven Optimization of Software Architecture) toolkit, which integrates modeling technologies, performance analysis techniques, and advanced evolutionary multiobjective optimization algorithms (i.e. NSGA-II, SPEA2, and SMS-EMOA) to improve non-functional properties of systems in an automated manner.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5949650,no
A Configurable Approach to Tolerate Soft Errors via Partial Software Protection,2011,"Compared with hardware-based methods, software-based methods which need not additional hardware costs are regarded as efficient methods to tolerate soft errors. Software-based methods which are implemented by software protection have performance sacrifice. This paper proposes a new configurable approach whose purpose is to balance system reliability and performance, to tolerate soft errors via partial software protection. Those unprotected software regions which are motivated by soft error mask on software level are related to statically dead codes, those codes whose probabilities to be executed are low and some partially dead codes. For those protected codes, we copy every data and operate every operation twice to ensure those data stored into memory are right. Additionally, we ensure every branch instruction can jump to the right address by checking condition and destination address. Finally, our approach is implemented by modification of compiler. System reliability and performance are evaluated with different configurations. Experimental results demonstrate our purpose to balance system reliability and performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5951985,no
Exploiting Module Locality to Improve Software Fault Prediction,2011,"Receiving bug reports, developers usually need to spend significant amount of time resolving where to fix the faults. Although previous studies have shown that the revision frequency of a file location is an important measure to reflect the possibility of containing bugs, the frequency-based approaches achieve limited prediction accuracy for file locations having low revision frequencies. Our empirical observations show that the files of low revision frequencies in the same file directory or package of the files of high revision frequencies may be potential bug-fixing candidates for future bug reports. In this paper, we present a novel enhancement by exploiting module locality to improve the frequency-based approaches. Our experiments on three open source projects reveal that module locality can be employed to consistently improve the hit rate of a frequency-based approach and achieve the highest improvement of about 14%.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5951999,no
Guaranteed Seamless Transmission Technique for NGEO Satellite Networks,2011,"Non-geostationary (NGEO) satellite communication systems are able to provide global communication with reasonable latency and low terminal power requirements. However the highly topological dynamics, large delay and error prone links have been a matter of fact in the satellite network studies. This paper proposes a novel Guaranteed Seamless Transmission Technique(GST), which is a Hop-by-Hop scheme enhanced with the End-to-End scheme and associated with a link algorithm, which updates the link load explicitly and sends it back to the sources that use the link. We analyze GST theoretically by adopting a simple fluid model. The good performance of GST, in terms of bandwidth utilization, effective transmission ratio and fairness, is verified via a set of simulations.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5952753,no
A new ontology for fault tolerance in QoS-enabled service oriented systems,2011,Semantic web today is considered to be the extended limit of the World Wide Web and in recent years there has been promising advancements with great potential in the field of semantic computing. We see semantic applications emerging not only in web environment but in different industries to realize its utility in explicit expression of data in enterprises. This proved to have a rather excellent outcome in semantic web implementations but the true power of semantic web lies in service composition. Semantic web service composition could take the most advantage from the semantic data for finding the optimum combination of web services. Quality of Service (QoS) parameters are known as critical factors for selecting the web service with the best attributes but yet remain a statistic and metric form of data. QoS attributes are usually estimated trough formulas and mostly inaccurate which leads to composition failure and raises the need for fault tolerance and its mechanisms. So far there has been little work done to make the fault tolerance more compatible with the semantic web. In this paper we propose a more profound solution for this problem with the use of semantic web technologies for fault tolerance. A new ontology for fault tolerance is introduced which essentially improves the web service composition performance with providing a fault detection and recovery guidance system.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5952811,no
Journey: A Massively Multiplayer Online Game Middleware,2011,"The design of massively multiplayer online games (MMOGs) is challenging because scalability, consistency, reliability, and fairness must be achieved while providing good performance and enjoyable gameplay. This article presents Journey, an MMOG middleware that hides the complexity of dealing with the aforementioned issues from the game programmer. Journey builds on top of a peer-to-peer network infrastructure to provide load-balancing, fault-tolerance, and cheat-detection capabilities centered on object-oriented technology. Experimental results show performance measurements obtained by running Journey on more than 200 machines.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5953579,no
A middleware for reliable soft real-time communication over IEEE 802.11 WLANs,2011,"This paper describes a middleware layer for soft real-time communication in wireless networks devised and realized using standard WLAN hardware and software. The proposed middleware relies on a simple network architecture comprising a number of stations that generate real-time traffic and a particular station, called Scheduler, that coordinates the transmission of the real-time packets using a polling mechanism. The middleware combines EDF scheduling with a dynamic adjustment of the maximum number of transmission attempts, so as to adapt the performance to fluctuations of the link quality, thus increasing the communication reliability, while taking deadlines into account. After describing the basic communication paradigm and the underlying concepts of the proposed middleware, the paper describes the target network configuration and the software architecture. Finally, the paper assesses the effectiveness of the proposed middleware in terms of PER, On-Time Throughput, and Deadline Miss Rate, presenting the results of measurements performed on real testbeds.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5953653,no
Control-flow error detection using combining basic and program-level checking in commodity multi-core architectures,2011,"This paper presents a software-based technique to detect control-flow errors using basic level control-flow checking and inherent redundancy in commodity multi-core processors. The proposed detection technique is composed of two phases of basic and program-level control-flow checking. Basic-level control-flow error detection is achieved through inserting additional instructions into program at design time regarding to control-flow graph. Previous research shows that modern superscalar microprocessors already contain significant amounts of redundancy. Program-level control-flow checking can detect CFEs by leveraging existing microprocessors redundancy. Therefore, the cost of adding extra redundancy for fault tolerance is eliminated. In order to evaluate the proposed technique, three workloads quick sort, matrix multiplication and linked list utilized to run on a multi-core processor, and a total of 6000 transient faults have been injected on the processor. The advantage of the proposed technique in terms of performance and memory overheads and detection capability compared with conventional control-flow error detection techniques.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5953691,no
A Random search based effective algorithm for pairwise test data generation,2011,"Testing is a very important task to build error free software. As the resources and time to market is limited for a software product, it is impossible to perform exhaustive test i.e., to test all combinations of input data. To reduce the number of test cases in an acceptable level, it is preferable to use higher interaction level (t way, where t â‰?2). Pairwise (2-way or t = 2) interaction can find most of the software faults. This paper proposes an effective random search based pairwise test data generation algorithm named R2Way to optimize the number of test cases. Java program has been used to test the performance of the algorithm. The algorithm is able to support both uniform and non-uniform values effectively with performance better than the existing algorithms/tools in terms of number of generated test cases and time consumption.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5953894,no
Regression Test Selection Techniques for Test-Driven Development,2011,"Test-Driven Development (TDD) is characterized by repeated execution of a test suite, enabling developers to change code with confidence. However, running an entire test suite after every small code change is not always cost effective. Therefore, regression test selection (RTS) techniques are important for TDD. Particularly challenging for TDD is the task of selecting a small subset of tests that are most likely to detect a regression fault in a given small and localized code change. We present cost-bounded RTS techniques based on both dynamic program analysis and natural-language analysis. We implemented our techniques in a tool called Test Rank, and evaluated its effectiveness on two open-source projects. We show that using these techniques, developers can accelerate their development cycle, while maintaining a high bug detection rate, whether actually following TDD, or in any methodology that combines testing during development.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954400,no
A Principled Evaluation of the Effect of Directed Mutation on Search-Based Statistical Testing,2011,"Statistical testing generates test inputs by sampling from a probability distribution that is carefully chosen so that the inputs exercise all parts of the software being tested. Sets of such inputs have been shown to detect more faults than test sets generated using traditional random and structural testing techniques. Search-based statistical testing employs a metaheuristic search algorithm to automate the otherwise labour-intensive process of deriving the probability distribution. This paper proposes an enhancement to this search algorithm: information obtained during fitness evaluation is used to direct the mutation operator to those parts of the representation where changes may be most beneficial. A principled empirical evaluation demonstrates that this enhancement leads to a significant improvement in algorithm performance, and so increases both the cost-effectiveness and scalability of search-based statistical testing. As part of the empirical approach, we demonstrate the use of response surface methodology as an effective and objective method of tuning algorithm parameters, and suggest innovative refinements to this methodology.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954408,no
An Evaluation of Mutation and Data-Flow Testing: A Meta-analysis,2011,"Mutation testing is a fault-based testing technique for assessing the adequacy of test cases in detecting synthetic faulty versions injected to the original program. The empirical studies report the effectiveness of mutation testing. However, the inefficiency of mutation testing has been the major drawback of this testing technique. Though a number of studies compare mutation to data flow testing, the summary statistics for measuring the magnitude order of effectiveness and efficiency of these two testing techniques has not been discussed in literature. In addition, the validity of each individual study is subject to external threats making it hard to draw any general conclusion based solely on a single study. This paper introduces a novel meta-analytical approach to quantify and compare mutation and data flow testing techniques based on findings reported in research articles. We report the results of two statistical meta-analyses performed on comparing and measuring the effectiveness as well as efficiency of mutation and data-flow testing based on relevant empirical studies. We focus on the results of three empirical research articles selected from the premier venues with their focus on comparing these two testing techniques. The results show that mutation is at least two times more effective than data-flow testing, i.e., odds ratio= 2.27. However, mutation is three times less efficient than data-flow testing, i.e., odds ratio= 2.94.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954435,no
An Experience Report on Using Code Smells Detection Tools,2011,"Detecting code smells in the code and consequently applying the right refactoring steps when necessary is very important to improve the quality of the code. Different tools have been proposed for code smell detection, each one characterized by particular features. The aim of this paper is to describe our experience on using different tools for code smell detection. We outline the main differences among them and the different results we obtained.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954446,no
On Investigating Code Smells Correlations,2011,"Code smells are characteristics of the software that may indicate a code or design problem that can make software hard to evolve and maintain. Detecting and removing code smells, when necessary, improves the quality and maintainability of a system. Usually detection techniques are based on the computation of a particular set of combined metrics, or standard object-oriented metrics or metrics defined ad hoc for the smell detection. The paper investigates the direct and indirect correlations existing between smells. If one code smell exists, this can imply the existence of another code smell, or if one smell exists, another one cannot be there, or perhaps it could observe that some code smells tend to go together.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954451,no
Assessing the Impact of Using Fault Prediction in Industry,2011,"Software developers and testers need realistic ways to measure the practical effects of using fault prediction models to guide software quality improvement methods such as testing, code reviews, and refactoring. Will the availability of fault predictions lead to discovery of different faults, or to more efficient means of finding the same faults? Or do fault predictions have no practical impact at all? In this challenge paper we describe the difficulties of answering these questions, and the issues involved in devising meaningful ways to assess the impact of using prediction models. We present several experimental design options and discuss the pros and cons of each.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954465,no
An Empirical Study on Object-Oriented Metrics and Software Evolution in Order to Reduce Testing Costs by Predicting Change-Prone Classes,2011,"Software maintenance cost is typically more than fifty percent of the cost of the total software life cycle and software testing plays a critical role in reducing it. Determining the critical parts of a software system is an important issue, because they are the best place to start testing in order to reduce cost and duration of tests. Software quality is an important key factor to determine critical parts since high quality parts of software are less error prone and easy to maintain. As object oriented software metrics give important evidence about design quality, they can help software engineers to choose critical parts, which should be tested firstly and intensely. In this paper, we present an empirical study about the relation between object oriented metrics and changes in software. In order to obtain the results, we analyze modifications in software across the historical sequence of open source projects. Empirical results of the study indicate that the low level quality parts of a software change frequently during the development and management process. Using this relation we propose a method that can be used to estimate change-prone classes and to determine parts which should be tested first and more deeply.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954466,yes
Modeling the Diagnostic Efficiency of Regression Test Suites,2011,"Diagnostic performance, measured in terms of the manual effort developers have to spend after faults are detected, is not the only important quality of a diagnosis. Efficiency, i.e., the number of tests and the rate of convergence to the final diagnosis is a very important quality of a diagnosis as well. In this paper we present an analytical model and a simulation model to predict the diagnostic efficiency of test suites when prioritized with the information gain algorithm. We show that, besides the size of the system itself, an optimal coverage density and uniform coverage distribution are needed to achieve an efficient diagnosis. Our models allow us to decide whether using IG with our current test suite will provide a good diagnostic efficiency, and enable us to define criteria for the generation or improvement of test suites.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954476,no
A Diagnostic Point of View for the Optimization of Preparation Costs in Runtime Testing,2011,"Runtime testing is emerging as the solution for the validation and acceptance testing of service-oriented systems, where many services are external to the organization, and duplicating the system's components and their context is too complex, if possible at all. In order to perform runtime tests, an additional expense in the test preparation phase is required, both in software development and in hardware. Preparation cost prioritization methods have been based on runtime testability (i.e, coverage) and do not consider whether a good runtime testability is sufficient for a good runtime diagnosis quality in case faults are detected, and whether this diagnosis will be obtained efficiently (i.e., with a low number of test cases). In this paper we show (1) the direct relationship between testability and diagnosis quality, that (2) these two properties do not guarantee an efficient diagnosis, and (3) a measurement that ensures better prediction of efficiency.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954478,no
Efficient FPGA implementation of a high-quality super-resolution algorithm with real-time performance,2011,"Nowadays, most of the image contents are recorded in high resolution (HR). Nevertheless, there is still a need of low resolution (LR) content presentation when recapturing is unviable. As high resolution content becomes commonplace (with HDTV penetration in US estimated at 65% for 2010) the viewers expectations on quality and presentation become higher and higher. Thus, in order to meet todayÂ¿s consumer expectations the LR content quality has to be enhanced before being displayed. Content pre-processing/upscaling methods include interpolation and Super-Resolution Image Reconstruction (SRIR). SRIR hardware implementations are scarce, mainly due to high memory and performance requirements. This is the challenge we address. In this work an efficient super-resolution core implementation with high quality of the super-resolved outcome is presented. In order to provide high outcome quality the implementation is based on state-of-the-art software. The software algorithm is presented and its output quality compared with other state-of-the-art solutions. The results of the comparison show that the software provides superior output quality. When implemented in targeted FPGA device, the system operating frequency was estimated at 109 MHz. This resulted in a performance that allows dynamic 2x super resolution of QCIF YUV 4:2:0p sequences at a frame rate of 25 fps, leading to realtime execution using only on-chip device memory. Post-layout simulations with back-annotated time proved that the hardware implementation is capable of producing the same output quality as the base software.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5955206,no
A real-time 1080p 2D-to-3D video conversion system,2011,"In this paper, we demonstrate a 2D-to-3D video conversion system capable of real-time 1920Ã—1080p conversion. The proposed system generates 3D depth information by fusing cues from edge feature-based global scene depth gradient and texture-based local depth refinement. By combining the global depth gradient and local depth refinement, generated 3D images have comfortable and vivid quality, and algorithm has very low computational complexity. Software is based on a system with a multi-core CPU and a GPU. To optimize performance, we use several techniques including unified streaming dataflow, multi-thread schedule synchronization, and GPU acceleration for depth image-based rendering (DIBR). With proposed method, real-time 1920Ã—1080p 2Dto- 3D video conversion running at 30fps is then achieved.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5955240,no
Detection of Sleeping Cells in LTE Networks Using Diffusion Maps,2011,"In mobile networks emergence of failures is caused by various breakdowns of hardware and software elements. One of the serious failures in radio networks is a Sleeping Cell. In our work one of the possible root causes for appearance of this network failure is simulated in a dynamic network simulator. The main aim of the research is to detect the presence of a Sleeping Cell in the network and to define its location. For this purpose Diffusion Maps data mining technique is employed. The developed fault identification framework is using the performance characteristics of the network, collected during its regular operation, and for that reason it can be implemented in real Long Term Evolution (LTE) networks within the Self-Organizing Networks (SON) concept.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5956626,no
Towards Service Level Engineering for IT Services: Defining IT Services from a Line of Business Perspective,2011,"Today, the management of service quality is posing a major challenge for many service systems formed by providers and their business customers. We argue that the trade-off between service costs and benefits incurred by both of these parties is not sufficiently considered when service quality is stipulated. Many Service Level Agreements are tailored to fine-grained IT services. The impact of service levels defined for these technical services on customers' business processes, however, is difficult to estimate. Thus, it is a major objective to identify IT services that directly affect the performance of customers' business departments. In this research-in-progress paper we present first results of an empirical study aiming at the definition of IT services and corresponding service level indicators from a customer business department perspective. Based on an initial literature research and a number of semi-structured interviews - with users working in different departments of a public IT customer and having different backgrounds and IT knowledge - we have identified a set of common, ""directly business-relevant"" IT services. Thus, we take an important first step towards the application of Service Level Engineering, i.e. the derivation of business-relevant performance metrics and associated cost-efficient target values to precisely identify efficient service quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5958157,no
Boundless memory allocations for memory safety and high availability,2011,"Spatial memory errors (like buffer overflows) are still a major threat for applications written in C. Most recent work focuses on memory safety - when a memory error is detected at runtime, the application is aborted. Our goal is not only to increase the memory safety of applications but also to increase the application's availability. Therefore, we need to tolerate spatial memory errors at runtime. We have implemented a compiler extension, Boundless, that automatically adds the tolerance feature to C applications at compile time. We show that this can increase the availability of applications. Our measurements also indicate that Boundless has a lower performance overhead than SoftBound, a state-of-the-art approach to detect spatial memory errors. Our performance gains result from a novel way to represent pointers. Nevertheless, Boundless is compatible with existing C code. Additionally, Boundless provides a trade-off to reduce the runtime overhead even further: We introduce vulnerability specific patching for spatial memory errors to tolerate only known vulnerabilities. Vulnerability specific patching has an even lower runtime overhead than full tolerance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5958203,no
A methodology for the generation of efficient error detection mechanisms,2011,"A dependable software system must contain error detection mechanisms and error recovery mechanisms. Software components for the detection of errors are typically designed based on a system specification or the experience of software engineers, with their efficiency typically being measured using fault injection and metrics such as coverage and latency. In this paper, we introduce a methodology for the design of highly efficient error detection mechanisms. The proposed methodology combines fault injection analysis and data mining techniques in order to generate predicates for efficient error detection mechanisms. The results presented demonstrate the viability of the methodology as an approach for the development of efficient error detection mechanisms, as the predicates generated yield a true positive rate of almost 100% and a false positive rate very close to 0% for the detection of failure-inducing states. The main advantage of the proposed methodology over current state-of-the-art approaches is that efficient detectors are obtained by design, rather than by using specification-based detector design or the experience of software engineers.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5958204,no
Aaron: An adaptable execution environment,2011,"Software bugs and hardware errors are the largest contributors to downtime, and can be permanent (e.g. deterministic memory violations, broken memory modules) or transient (e.g. race conditions, bitflips). Although a large variety of dependability mechanisms exist, only few are used in practice. The existing techniques do not prevail for several reasons: (1) the introduced performance overhead is often not negligible, (2) the gained coverage is not sufficient, and (3) users cannot control and adapt the mechanism. Aaron tackles these challenges by detecting hardware and software errors using automatically diversified software components. It uses these software variants only if CPU spare cycles are present in the system. In this way, Aaron increases fault coverage without incurring a perceivable performance penalty. Our evaluation shows that Aaron provides the same throughput as an execution of the original application while checking a large percentage of requests - whenever load permits.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5958254,no
A new framework for call admission control in wireless cellular network,2011,"Managing the limited amount of the radio spectrum is an important issue with increasing demand of the same. In recent work, we have introduced MAS (Multi-agent System) for channel assignment problem in wireless cellular networks. Instead of using a base station directly for negotiation, a multi-agent system comprising of software agents was designed to work at base station. The system consists of a collection of layers to take care of local and global scenario. In this paper we propose the combination of MAS with a new call admission control (CAC) mechanism based on fuzzy control. This paper aims to provide improvement in QoS parameters using fuzzy control at call admission level. From the simulation studies it is observed that combined approach of Multi-agent system and fuzzy control at initial level improves channel allocation and other QoS factors in an effective and efficient manner. The simulation results are presented on a benchmark 49 cell environment with 70 channels that validate the performance of this approach.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5958510,no
An epidemic approach to dependable key-value substrates,2011,"The sheer volumes of data handled by today's Internet services demand uncompromising scalability from the persistence substrates. Such demands have been successfully addressed by highly decentralized key-value stores invariably governed by a distributed hash table. The availability of these structured overlays rests on the assumption of a moderately stable environment. However, as scale grows with unprecedented numbers of nodes the occurrence of faults and churn becomes the norm rather than the exception, precluding the adoption of rigid control over the network's organization. In this position paper we outline the major ideas of a novel architecture designed to handle today's very large scale demand and its inherent dynamism. The approach rests on the well-known reliability and scalability properties of epidemic protocols to minimize the impact of churn. We identify several challenges that such an approach implies and speculate on possible solutions to ensure data availability and adequate access performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5958794,no
Automated vulnerability discovery in distributed systems,2011,"In this paper we present a technique for automatically assessing the amount of damage a small number of participant nodes can inflict on the overall performance of a large distributed system. We propose a feedback-driven tool that synthesizes malicious nodes in distributed systems, aiming to maximize the performance impact on the overall behavior of the distributed system. Our approach focuses on the interface of interaction between correct and faulty nodes, clearly differentiating the two categories. We build and evaluate a prototype of our approach and show that it is able to discover vulnerabilities in real systems, such as PBFT, a Byzantine Fault Tolerant system. We describe a scenario generated by our tool, where even a single malicious client can bring a BFT system of over 250 nodes down to zero throughput.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5958811,no
DynaPlan: Resource placement for application-level clustering,2011,"Creating a reliable computing environment from an unreliable infrastructure is a common challenge. Application-Level High Availability (HA) clustering addresses this problem by relocating and restarting applications when failures are detected. Current methods of determining the relocation target(s) of an application are rudimentary in that they do not take into account the myriad factors that influence an optimal placement. This paper presents DynaPlan, a method that improves the quality of failover planning by allowing the expression of a wide and extensible range of considerations, such as multidimensional resource consumption and availability, architectural compatibility, security constraints, location constraints, and policy considerations, such as energy-favoring versus performance-favoring. DynaPlan has been implemented by extending the IBM PowerHA clustering solution running on a group of IBM System P servers. In this paper, we describe the design, implementation, and preliminary performance evaluation of DynaPlan.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5958825,no
SOFAS: A Lightweight Architecture for Software Analysis as a Service,2011,"Access to data stored in software repositories by systems such as version control, bug and issue tracking, or mailing lists is essential for assessing the quality of a software system. A myriad of analyses exploiting that data have been proposed throughout the years: source code analysis, code duplication analysis, co-change analysis, bug prediction, or detection of bug fixing patterns. However, easy and straight forward synergies between these analyses rarely exist. To tackle this problem we have developed SOFAS, a distributed and collaborative software analysis platform to enable a seamless interoperation of such analyses. In particular, software analyses are offered as Restful web services that can be accessed and composed over the Internet. SOFAS services are accessible through a software analysis catalog where any project stakeholder can, depending on the needs or interests, pick specific analyses, combine them, let them run remotely and then fetch the final results. That way, software developers, testers, architects, or quality assurance experts are given access to quality analysis services. They are shielded from many peculiarities of tool installations and configurations, but SOFAS offers them sophisticated and easy-to-use analyses. This paper describes in detail our SOFAS architecture, its considerations and implementation aspects, and the current set of implemented and offered Restful analysis services.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5959723,no
Assessing Suitability of Cloud Oriented Platforms for Application Development,2011,"The enterprise data centers and software development teams are increasingly embracing the cloud oriented and virtualized computing platforms and technologies. As a result it is no longer straight forward to choose the most suitable platform which may satisfy a given set of Non-Functional Quality Attributes (NFQA) criteria that is significant for an application. Existing methods such as Serial Evaluation and Consequential Choice etc. are inadequate as they fail to capture the objective measurement of various criteria that are important for evaluating the platform alternatives. In practice, these methods are applied in an ad-hoc fashion. In this paper we introduce three application development platforms: 1) Traditional non-cloud 2) Virtualized and 3) Cloud Aware. We propose a systematic method that allows the stakeholders to evaluate these platforms so as to select the optimal one by considering important criteria. We apply our evaluation method to these platforms by considering a certain (non-business) set of NFQAs. We show that the pure cloud oriented platforms fare no better than the traditional non-cloud and vanilla virtualized platforms in case of most NFQAs.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5959767,no
A Low-Power High-Performance Concurrent Fault Detection Approach for the Composite Field S-Box and Inverse S-Box,2011,"The high level of security and the fast hardware and software implementations of the Advanced Encryption Standard have made it the first choice for many critical applications. Nevertheless, the transient and permanent internal faults or malicious faults aiming at revealing the secret key may reduce its reliability. In this paper, we present a concurrent fault detection scheme for the S-box and the inverse S-box as the only two nonlinear operations within the Advanced Encryption Standard. The proposed parity-based fault detection approach is based on the low-cost composite field implementations of the S-box and the inverse S-box. We divide the structures of these operations into three blocks and find the predicted parities of these blocks. Our simulations show that except for the redundant units approach which has the hardware and time overheads of close to 100 percent, the fault detection capabilities of the proposed scheme for the burst and random multiple faults are higher than the previously reported ones. Finally, through ASIC implementations, it is shown that for the maximum target frequency, the proposed fault detection S-box and inverse S-box in this paper have the least areas, critical path delays, and power consumptions compared to their counterparts with similar fault detection capabilities.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5962403,no
10-Gbps IP Network Measurement System Based on Application-Generated Packets Using Hardware Assistance and Off-the-Shelf PC,2011,"Targeting high-bandwidth applications such as video streaming services, we discuss advanced measurement systems for high-speed 10-Gbps networks. To verify service stability in such high-speed networks, we need to detect network quality under real environmental conditions. For example, test traffic injected into networks under-test for measurements should have the same complex characteristics as the video streaming traffic. For such measurements, we have built Internet protocol (IP) stream measurement systems by using our 10-Gbps network interface card with hardware-assisted active/passive monitor extensions based on low-cost off-the-shelf personal computers (PCs). After showing hardware requirements and our implementation of each hardware-assisted extensions, we report how we build pre-service and in-service network measurement systems to verify the feasibility of our hardware architecture. A traffic-playback system captures packets and stores traffic characteristics data without sampling any packets and then sends them precisely emulating the complex characteristics of the original traffic by using our hardware assistance. The generated traffic is useful as test traffic in pre-service measurement. A distributed in-service network monitoring system collects traffic characteristics at multiple sites by utilizing synchronized precise timestamps embedded in video streaming traffic. The results are presented on the operator's display. We report on their effectiveness by measuring 1.5-Gbps uncompressed high-definition television traffic flowing in the high-speed testbed IP network in Japan.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5962735,no
AMBA to SoCWire network on Chip bridge as a backbone for a Dynamic Reconfigurable Processing unit,2011,"Instruments on spacecrafts or even complete payload data handling units are typically controlled by a dedicated data processing unit. This data processing unit exercises control of subsystems and telecommand processing as well as processing of acquired science data. With increasing detector resolutions and measurement speeds the processing demands are rising rapidly. To fulfill these increasing demands under the constraints of limited power budgets, a dedicated hardware design as a System on Chip (SoC), e.g. in a FPGA, has been shown to be a viable solution. In previous papers we have presented our Network on Chip (NoC) solution SoCWire as a higly efficient and reliable approach for a dynamic reconfigurable architectures. However, the control task still requires a sequential processor which is able to execute software. The LEON processor is a processor that is available in a fault tolerant version suitable for space applications and is accessible as an open source design. This paper presents an efficient solution for a combined NoC and classic processor bus-based communication architecture, i.e. the AHB2SOCW bridge as an efficient connection between a SoCWire network and a LEON processor bus systems. Direct memory access enables the AHB2SOCW bridge to operate efficiently. The resource utilization and obtainable data rates are presented as well as an outlook for the intended target application, which is an efficient SoC controller for a reconfigurable processing platform based on FPGAs.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5963941,no
Earthquake response of an arch bridge under near-fault ground motions,2011,"During the Wenchuan earthquake in 2008, some of the arch bridges in the seismic zone were damaged. To assess the performance of arch bridge under near-fault ground motions, both experimental and numerical studies are conducted in this paper. Firstly, an ambient vibration test of a real typical arch bridge is made to measure the dynamic characteristics of the structure and calibrate the finite-element model. Then the earthquake response of the bridge is made using FE software MIDAS based on recorded ground motion in the Wenchuan earthquake. The study shows that the joint between deck and tie at 1/4 span and 3/4 span, the joint between beam and arch and middle of the beam are weak links under near-fault ground motions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5964478,no
Study on the relationship between aerosol anthropogenic component and air quality in the city of Wuhan,2011,"Problems with air pollution in urban areas have been known for a long time. The air quality has an enormous impact for people's daily life. The Wuhan Environmental Protection Bureau currently uses ground sensors to monitor air quality in the city of Wuhan. While this method provides accurate information at specific locations, it does not provide a clear indication of conditions over the whole region. Measurements from satellite imagery have the potential to provide timely air quality data for large swaths of land. Satellite remote sensing of air quality has evolved dramatically over the last decade. Global observations are now available for a wide range of species including aerosols, O<sub>3</sub>, NO<sub>2</sub>, CO and SO<sub>2</sub>. Previous studies show strong correlations between MODIS-derived Aerosol Optical Depth (AOD) and surface PM (particulate matter) measurements, which is an important indicator for the air-quality. However, this relationship is not so significant from region to region. The reason is that, although the basic transfer theory in the atmosphere is well understood, its application in urban environments is less than adequate when the domain controlling parameters and their coupling mechanisms are not well defined. Satellite instruments do not measure the aerosol chemical composition needed to discriminate anthropogenic from natural aerosol components. But the ability of new satellite instruments to distinguish fine (submicron) from coarse (supermicron) aerosols over the oceans, serves as a signature of the anthropogenic component and can be used to estimate the fraction of anthropogenic aerosols. In this paper, we developed this method to continental area, the city of Wuhan. By collecting the MOD04 production over the city of Wuhan in 2005, we extracted the aerosol optical thickness(AOT) at the 550nm (Ï„<sub>550nm</sub>) and the fine mode fraction (FMF), which were used to calculate out the aerosol anthropogenic component (Ï„<s- - ub>ant</sub>). The ground-based air quality indexes, such as PM<sub>10</sub>, NO<sub>2</sub> and SO<sub>2</sub>, were also collected synchronously. The pre-processing of the MOD04 data were performed using external development of the IDL language. Statistical analysis was conducted in MATLAB (version 7.8) software to establish linear regression model of the Ï„<sub>ant</sub> and the index of air-quality relationships. Correspondingly, the relationships between the Ï„<sub>550nm</sub> and the index of air-quality were also explored. It showed that the relationships between the former were much stronger than the latter. And the main factor of the air quality in the city of Wuhan was changing from season to season.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5965103,no
H.264/AVC rate control with enhanced ratequantisation model and bit allocation,2011,"Rate control algorithms aim to achieve perceivable video quality for real-time video communication applications under some real-world constraints, such as bandwidth, delay, buffer sizes etc. This study presents an adaptive and efficient rate control framework for low-delay H.264/AVC video communication. At the basic unit level, to realise quantisation decision improvement, a novel rate-qantisation (<i>R</i>-<i>Q</i>) model is proposed for P frame. This P frame <i>R</i>-<i>Q</i> model is established through the Ï domain rate control theory to compute quantisation according to target bit assignment. In addition, an I frame <i>R</i>-<i>Q</i> model is presented to measure frame level coding activity for quantisation decision without performing computationally intensive intra-prediction, which helps alleviating the frame skipping problem. When compared with Joint Video Team (JVT)-G012, a rate control scheme adopted by JVT H.264/AVC reference software JM12.3, the proposed rate control framework shows good performance in terms of improved average luminance peak signal noise ratio (PSNR), reduced number of skipped frames and smaller mismatch between target and actual bit rates.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5966796,no
A cost-effective Software Development and Validation environment and approach for LEON based satellite & payload subsystems,2011,"This paper describes a configurable Software Development and Validation Facility (SDVF), designed to support the lifecycle of spacecraft equipment containing on board software (OBSW) in our case LEON-based units with a cost effective approach. The environment fulfils three main purposes: 1. Software Development and Verification Facility (SDVF) for OBSW 2. Simulation for performance and design feasibility assessment 3. Special Check-Out Equipment (SCOE) for hardware integration and testing With a modular design, it can be modified into different configurations, from a full-virtual environment containing a processor emulator and a set of simulation models, to a processor in the loop (PIL) or complete hardware in the loop (HIL) configuration. As the use case is not limited to the SDVF only, we called the environment ""EGSE-Lite"". The key design characteristics of the system are: 1) Easy configurability to use virtual models or real hardware versions of system components or a mix of them: A central ""Packet Switch"" interconnects the modules and allows easy switch from one configuration to another via a configuration file. This is possible as the modules, either virtual or hardware via their interface driver, shares a common interface protocol. 2) Simple coupling of interfaces among the equipment modules, based on TCP/IP sockets. The solution allows distributing the modules on different host machines if needed. 3) Use of ESA's SCOS-2000 software as Central Check out System (CCS): The integration of SCOS allows easy system level testing of the software, to edit and send commands, verify telemetry results on displays, archive telemetry for offline analysis and enable automated regression testing. It also enforces compatibility of the equipment under development with the final system EGSE or mission control system, if based on SCOS2000. 4) Packet sniffing and recording capability at different levels with capability to inject corrupted packets for robustness validation of the- - system. 5) Use of Eclipse and LEON Integrated Development Environment (LIDE) available from Gaisler Research, providing a development environment with GDB interface for debugging. 6) Possibility to use either a virtual LEON emulator, such as Gaisler Research TSIM, or a hybrid hardware/software emulator, or the final LEON processor board. Use of COTS I/O cards such as PCI-SpaceWire interface card and other standard interfaces to connect to the hardware equipment. The facility has been successfully deployed to CESR, an institute in France for astrophysics and payload development and integration, providing emulation of a LEON3 processor, high fidelity models for X-ray detectors and a NAND based fault tolerant mass memory unit, each replaceable with its hardware counterpart (the processor board was based on Aeroflex LEON3 UT699). The environment is currently being integrated with a LEON2 hybrid hardware/software emulator developed by ASTRIUM under ESA contract (LeonSvf): The objective is to perform a characterization study of the emulator card using computational intensive flight representative software developed for CESR The EGSE-Lite takes full advantage from COTS products (such as Gaisler TSIM), and ESA software products (such as SCOS 2000) to provide a light weight, scalable and cost-efficient solution, for integration and validation of satellite subsystems and payload equipment containing embedded software.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5966889,no
Software component quality-finite mixture component model using Weibull and other mathematical distributions,2011,"Software component quality has a major influence in software development project performances such as lead-time, time to market and cost. It also affects the other projects within the organization, the people assigned into the projects and the organization in general. In this study a finite mixture of several mathematical distributions is used to describe the fault occurrence in the system based on individual software component contribution. Several examples are selected to demonstrate model fitting and comparison between the models. Four case studies are presented and evaluated for modeling software quality in very large development projects within the AXE platform, BICC as a call control protocol in the Ericsson Nikola Tesla R&D.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5967122,no
Identification method research of invalid questionnaire based on partial least squares regression,2011,"The researchers often apply the survey questionnaire as a measurement tool to verify hypothetical propositions and structural models in humanities and social science, therefore, the data quality of survey questionnaire can effect the scientificalness of research propositions and models directly. The data quality of survey questionnaire can be divided into 3 important components in research procedure. Firstly, the quality of survey questionnaire is the foundation for theory research; secondly, the reliability of survey data plays an important role in practical application; thirdly, the scientificalness of analyze data is a guarantee for empirical research. These components are inseparable and conformable, the paper make classification to the data which describes data identification in quantitative and qualitative as classification criterion. Thus, identification method of invalid questionnaire will be divided into 2 steps which are qualitative identification and quantitative one. Identification method for invalid questionnaire based on partial least squares(PLS) regression may apply the basis theory of PLS and SIMCA-P software to form ellipses graphs or ellipsoid ones, which will display specific points outside graphs. In order to delete data of invalid questionnaire, deleting specific points in graphs is needed. The paper uses empirical data to verify the feasibility and scientificness of the method in Customer Satisfaction Index Model. The practice shows the popular value of the method in theoretical research and practical application.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5968370,no
Design of Three Phase Network Parameter Monitoring System based on 71M6513,2011,"In this paper, the design process of Three Phase Network Parameter Monitoring System is analyzed and the overall research scheme and design idea of the system are expounded. This paper mainly studies how to improve the accuracy of input voltage and current. The front-end voltage decreasing, current dropping circuit and remote data transmission circuit are designed. The programs of the data acquisition and calculation are made, real-time detection of power network parameters, energy calculation and the remote data transmission are achieved. Based on .Net platform, the host computer software is designed to receive the data and store them in the database, the mean value of power and energy parameters are calculated by the day, the week and the month, which are available to the user for inquiry. Many parameters such as three-phase voltage, three-phase current, frequency, active power, reactive power and voltage harmonics, can be detected by the system, which will provide a reliable basis for power quality analysis.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5968842,no
Reasoning about Faults in Aspect-Oriented Programs: A Metrics-Based Evaluation,2011,"Aspect-oriented programming (AOP) aims at facilitating program comprehension and maintenance in the presence of crosscutting concerns. Aspect code is often introduced and extended as the software projects evolve. Unfortunately, we still lack a good understanding of how faults are introduced in evolving aspect-oriented programs. More importantly, there is little knowledge whether existing metrics are related to typical fault introduction processes in evolving aspect-oriented code. This paper presents an exploratory study focused on the analysis of how faults are introduced during maintenance tasks involving aspects. The results indicate a recurring set of fault patterns in this context, which can better inform the design of future metrics for AOP. We also pinpoint AOP-specific fault categories which are difficult to detect with popular metrics for fault-proneness, such as coupling and code churn.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5970147,no
Adding Process Metrics to Enhance Modification Complexity Prediction,2011,"Software estimation is used in various contexts including cost, maintainability or defect prediction. To make the estimate, different models are usually applied based on attributes of the development process and the product itself. However, often only one type of attributes is used, like historical process data or product metrics, and rarely their combination is employed. In this report, we present a project in which we started to develop a framework for such complex measurement of software projects, which can be used to build combined models for different estimations related to software maintenance and comprehension. First, we performed an experiment to predict modification complexity (cost of a unity change) based on a combination of process and product metrics. We observed promising results that confirm the hypothesis that a combined model performs significantly better than any of the individual measurements.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5970157,no
Design Defects Detection and Correction by Example,2011,"Detecting and fixing defects make programs easier to understand by developers. We propose an automated approach for the detection and correction of various types of design defects in source code. Our approach allows to automatically find detection rules, thus relieving the designer from doing so manually. Rules are defined as combinations of metrics/thresholds that better conform to known instances of design defects (defect examples). The correction solutions, a combination of refactoring operations, should minimize, as much as possible, the number of defects detected using the detection rules. In our setting, we use genetic programming for rule extraction. For the correction step, we use genetic algorithm. We evaluate our approach by finding and fixing potential defects in four open-source systems. For all these systems, we found, in average, more than 80% of known defects, a better result when compared to a state-of-the-art approach, where the detection rules are manually or semi-automatically specified. The proposed corrections fix, in average, more than 78%of detected defects.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5970166,no
An Empirical Study of the Impacts of Clones in Software Maintenance,2011,"The impacts of clones on software maintenance is a long-lived debate on whether clones are beneficial or not. Some researchers argue that clones lead to additional changes during the maintenance phase and thus increase the overall maintenance effort. Moreover, they note that inconsistent changes to clones may introduce faults during evolution. On the other hand, other researchers argue that cloned code exhibits more stability than non-cloned code. Studies resulting in such contradictory outcomes may be a consequence of using different methodologies, using different clone detection tools, defining different impact assessment metrics, and evaluating different subject systems. In order to understand the conflicting results from the studies, we plan to conduct a comprehensive empirical study using a common framework incorporating nine existing methods that yielded mostly contradictory findings. Our research strategy involves implementing each of these methods using four clone detection tools and evaluating the methods on more than fifteen subject systems of different languages and of a diverse nature. We believe that our study will help eliminate tool and study biases to resolve conflicts regarding the impacts of clones on software maintenance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5970172,no
Towards accurate and agile link quality estimation in wireless sensor networks,2011,"Link quality estimation has been an important research topic in the wireless sensor networking community and researchers have developed a large number of different methods to estimate link quality. The commonly used CC2420 radio provides simple signal quality indicators. These are agile in that they react fast to changing link quality but they are inaccurate under complicated channel conditions. More sophisticated link quality estimators combine these simple metrics with packet reception statistics collected by the network stack. These approaches compensate the hardware-based metrics to a limited degree but they compromise agility and incur extra overhead. In this paper, we take a novel approach and develop a number of link quality metrics using a software defined radio. We evaluate our metrics under several channel conditions. The results show that they have good accuracy and can handle complicated channel conditions if combined properly.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5970486,no
An integrated apparatus to measure Mallampati score for the characterization of Obstructive Sleep Apnea (OSA),2011,"Obstructive Sleep Apnea (OSA) affects as many as 1 in every 5 adults and has the potential to cause serious long-term health complications such as, cardiovascular disease, stroke, hypertension and the consequent reduced quality of life. Studies have shown that the probability of having OSA increases with a higher BMI irrespective of gender and that there is a definite link concerning the race of the patient and having OSA. This paper describes the design of an integrated apparatus to collect Mallampati scores with little human intervention and perform automatic processing of various parameters. The system permits life-cycle studies on patients with OSA and other sleep disorders.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5972398,no
NMSIS - SCEN,2011,"The network laboratories management and maintenance processes consist of several tasks which may interfere with the normal network operation. The NMSIS - Network Management System with Imaging Support is a management tool that enables the remote installation of software images, without compromising the network performance. This article describes a new NMSIS module, the SCEN. By integrating the Simple Event Correlator (SEC) and extending the alarms correlation, SCEN enables the detection and isolation of faults in a network. With SCEN, the NMSIS proved effectiveness in alarms processing, fault identification and clearing, in scenarios where link, systems and services failures occur.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5974272,no
Building the pillars for the definition of a data quality model to be applied to the artifacts used in the Planning Process of a Software Development Project,2011,"The success of a software development project is mainly dependent on the quality of the used artifacts along the project; this quality is reliant on the contents of the artifacts, as well as the level of quality of the data values corresponding to the metadata that describe the artifacts. In order to assess both kind of qualities, it should be therefore taken into account the artifacts' structure and metadata. This paper proposes a DQ model that can be used as a reference by project managers to assess and, if necessary, improve the quality of the data values corresponding to the metadata describing the artifacts used in the process of planning a software development project. For our research, we have identified the corresponding artifacts from those described as part of the Planning Process defined in international standard ISO / IEC 12207:2008. We have aligned these found artifacts with those proposed by PMBOK, in order to better depict their structure; and finally, we are to build our data quality model upon the DQ dimensions proposed by Strong, D. M., Y. W. Lee and Wang, R. in â€œData Quality in Context.â€?Comm. of the ACM 1997 40 (5): 103-110. We all of these elements, we intend to optimize the performance of the software development process by improving the project management process.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5974325,no
The design of software process management and evaluation system in small and medium software enterprises,2011,"Software process improvement is the necessary choice through which small software enterprises improve their software quality and productivity. Aiming at the practical circumstances of the enterprises, the key technologies of software process management and estimation were studied in this paper and a process control platform was built based on the technologies. It is beneficial to improve overall integral level of software and promote the steady and healthy development of software industry.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5974788,no
Design of a highly adaptive auto-test software platform based on common data interface,2011,"In military and industrial sectors, automatic test system remarkably reduces maintenance cost of certain equipments. As is depicted in the thesis, the highly adaptive automatic test software architecture is presented with multiple common data interfaces, which not only enables rapid prototyping of automatic test procedure, but also collectively facilitate information sharing and data exchange in the whole lifecycle.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5975590,no
Linking a physical arc model with a black box arc model and verification,2011,"The arc behavior in the current zero region is critical in the case of very steep rising TRV, such as after clearing a short-line fault. Therefore, intensive and abundant short-line fault tests (L90) of a 245 kV SF6 circuit breaker were performed at the KEMA High Power Laboratory. For the purpose of a comparative analysis three different sets of data were obtained during the tests: 1) High-resolution measurements of near current-zero arc current and voltage were carried out. The current zero measurement system (CZM) works as a standalone system in addition to the standard laboratory data acquisition system. The arc conductance shortly before current zero and the arc voltage extinction peak give a clear indication of the interrupting capability of the breaker under test. 2) From the measured traces of every individual test, arc parameters (3 time constants and 3 cooling-power constants) were extracted for the composite black box arc model, which has been developed by KEMA High Power Laboratory and is based on more than 1000 high-resolution measurements during tests of commercial high-voltage circuit breakers. Its aim is to simulate interruption phenomenon in SF6 gas, evaluate performance of HV SF6 circuit breakers in testing and enable the prediction of the performance under conditions other than those tested. 3) After each test, using specially developed computer software, based on a simplified physical enthalpy flow arc model, the values of the arcing contact distance, gas mass flow through the nozzle throat and pressure inside the compression cylinder were calculated. The values of these characteristic quantities at the current zero are relevant indicators for successful interruption. In the comparative analysis, mathematical relations and statistical correlations between the evaluated parameters of the composite black box arc model and the characteristic output quantities are established and discussed. The link has been verified by MatLAB simulation of every indi- - vidual test. This approach enables acceptable prediction of interruption success in a similar circuit and with a similar interrupter without SLF tests and CZM.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5976092,no
An Evaluation of QoE in Cloud Gaming Based on Subjective Tests,2011,"Cloud Gaming is a new kind of service, which combines the successful concepts of Cloud Computing and Online Gaming. It provides the entire game experience to the users remotely from a data center. The player is no longer dependent on a specific type or quality of gaming hardware, but is able to use common devices. The end device only needs a broadband internet connection and the ability to display High Definition (HD) video. While this may reduce hardware costs for users and increase the revenue for developers by leaving out the retail chain, it also raises new challenges for service quality in terms of bandwidth and latency for the underlying network. In this paper we present the results of a subjective user study we conducted into the user-perceived quality of experience (QoE) in Cloud Gaming. We design a measurement environment, that emulates this new type of service, define tests for users to assess the QoE, derive Key Influence Factors (KFI) and influences of content and perception from our results.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5976180,no
Continuous software reliability models,2011,"For large scale software systems with a huge number of codes, it may be often useful to regard the fault-counting processes observed in testing phase as continuous-state stochastic processes. In this paper we introduce two types of continuous-state software reliability models; diffusion processes with time-dependent drift and non-stationary gamma wear processes, and compare them with the usual non-homogeneous Poisson processes. We expect that our models can provide the better goodness-of-fit performance than the existing software reliability models in many cases and provide convenient assessment tools to estimate the software reliability.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5976641,no
Research on key techniques of simulation and detection for fire control systemon the basis of database,2011,"Fire control system is the core component of the x-type artillery weapon system. Because of the position distribution, work control process and signal transfer relationship of the units in the fire control system, the traditional detective devices are limited when detecting it. This paper describe how to simulate the work environment of the fire control system on the basis of which the equipment units are detected. Realizing the management of sending and reception of serial port information by building the datagram management model and communication protocol management model adopting database techniques in simulation system. On the basis of the purpose that it is used to detect the units, it is hardly necessary to realize the ballistic curve solving and manipulation & aiming solving according to the equipment principle. Also, database technique is employed to build up the database of ballistic curve solving and manipulation & aiming solving in line with which the functions of ballistic curve solving and manipulation & aiming solving are completed in simulation system. Then the performance testing is realized through comparing the resolution results with the standard results in database by querying the database.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5976763,no
Arabic calligraphy recognition based on binarization methods and degraded images,2011,"Optical Font Recognition is one of the main challenges in this time. The available methods of optical font recognition are deal with the recent documents and fonts types. However, there are neglected in dealing with the historical and regarded documents. Moreover, they have neglected languages that are not belong into Asian or Latin. Regarding to those types of documents, we proposed a new framework of optical font recognition for Arabic calligraphy. We enhance binarization method based on previous works. By introducing that, we achieve better quality images at the preprocessing stage. Then we generate text block before passing mailing to post-processing stages. Then, we extract the features based on edge direction matrixes. In the classification stage, we apply backpropagation neural network to identify the font type of the calligraphy. We observe that our proposal method achieve better performance in both preprocessing and post processing.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5976913,no
Suffix tree-based approach to detecting duplications in sequence diagrams,2011,"Models are core artefacts in software development and maintenance. Consequently, quality of models, especially maintainability and extensibility, becomes a big concern for most non-trivial applications. For some reasons, software models usually contain some duplications. These duplications had better be detected and removed because the duplications may reduce maintainability, extensibility and reusability of models. As an initial attempt to address the issue, the author propose an approach in this study to detecting duplications in sequence diagrams. With special preprocessing, the author convert 2-dimensional (2-D) sequence diagrams into an 1-D array. Then the author construct a suffix tree for the array. With the suffix tree, duplications are detected and reported. To ensure that every duplication detected with the suffix tree can be extracted as a separate reusable sequence diagram, the author revise the traditional construction algorithm of suffix trees by proposing a special algorithm to detect the longest common prefixes of suffixes. The author also probe approaches to removing duplications. The proposed approach has been implemented in DuplicationDetector. With the implementation, the author evaluated the proposed approach on six industrial applications. Evaluation results suggest that the approach is effective in detecting duplications in sequence diagrams. The main contribution of the study is an approach to detecting duplications in sequence diagrams, a prototype implementation and an initial evaluation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5977133,no
Transmission control policy design for decentralized detection in tree topology sensor networks,2011,"A Wireless Sensor Network (WSN) deployed for detection applications has the distinguishing feature that sensors cooperate to perform the detection task. Therefore, the decoupled design approach that is typically used to design communication networks, where each network layer is designed independently, does not lead to the desired optimal detection performance. Cross-layer design has been recently explored for the design of MAC protocols for parallel topology (single hop) networks, but little work has been done on the integration of communication and information fusion for tree networks. In this work, we design the optimal Transmission Control Policy (TCP) that coordinates the communication between sensor nodes connected in a tree configuration, in order to optimize the detection performance. We integrate the Quality of Information (QoI), Channel State Information (CSI), and Residual Energy Information (REI) for each sensor into the system model. We formulate a constrained nonlinear optimization problem to find the optimal TCP design variables. We solve the optimization problem using a hierarchical approach where smaller local optimization problems are solved by each parent node to find the optimal TCP design variables for its child nodes. We compare our design with the max throughput and decoupled design approaches.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5977719,no
Table of contents,2011,The following topics are dealt with: hybrid semi-blind digital image watermarking technique; face recognition;database schemas; intelligent agent based job search system; retinal image segmentation; intelligent information retrieval lifecycle architecture; illumination-reflectance; unified power quality conditioner; universal scalability law; network performance evaluation; super high voltage transmission line electric field; signal processing methods; cost reduction; pavement pothole detection; SOCKx; application layer network switching framework; automated pavement distress inspection; transmission simulator development; keyword spotting experiment; multipath routing; environmental sensor network data packaging; predictive multichannel feedback active noise control; 2-channel adaptive speech enhancement; permission-based security model; dielectric electroactive polymer energy harvesting system forward path design; sEMG based real-time embedded force control strategy; tree-type wireless acquisition network; aligned aluminum-doped zinc oxide nanorod arrays; conditioning circuit development; thin film transistors fabrication; robust license plate localization; collaborative learning; frequency domain symbol synchronization; agent based TDMA schedule; malicious messages identification; customer survey data processing; trace element detection; fuzzy rule extraction; component packaging footprint; wireless network coding scheme; secure data transmission; model-based software-defined radio; clinical decision support system; low power 6-T CNFET SRAM cell; intelligent extended clustering genetic algorithm; low power process monitoring circuit; distributed intrusion detection scheme; time frequency analysis techniques; collaborative product design; capsule image segmentation; reconfigurable architectures; 9T SRAM design; turbo codec performance evaluation; architectural design tool; application commerce; foetal electrocardiogram extraction; route anomaly detection; yellowj- cket wast nest construction; free space laser communication; energy harvesting system; low power skin sensor; discrete cosine transform-based reconfigurable system design; energy aware adaptive clustering; automated electronic pen; and remote avian monitoring system.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5978560,no
From village greens to volcanoes: Using wireless networks to monitor air quality,2011,"Summary form only given. Air quality mapping is a rapidly growing need as people become aware of the acute and long term risks of poor air quality. Both spatial and temporal mapping is required to understand, predict and improve air quality. The collision of several technologies has led to an opportunity to create affordable wireless networks to monitor air quality: short range radio on a chip; GPS and internet maps; GSM integrated chips; improved batteries and solar power; Python and other internet languages; and low power, low cost gas sensors with ppb resolution. Bringing these technologies together requires collaboration between electronics engineers, mathematicians, software programmers, atmospheric chemists and sensor technology providers. And add in local politicians because wireless networks need permission. We will discuss implementation and results of successful trials and look at what is now underway: Newcastle, Cambridge and London trials (MESSAGE); Cambridge UK real time air quality mapping (2010); Heathrow airport air quality network (2011); volcanic ash mapping; landfill site monitoring networks; indoor air quality studies; and rural air measurements. Each application has its own requirements of power, wireless protocol, air monitoring needs, data analysis and presentation of results.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5978637,no
Quantification of Software Quality Parameters Using Fuzzy Multi Criteria Approach,2011,"Software quality is the measure of appropriateness of the design of the software and how well it adheres to that design. There are some metrics and measurements to determine the software quality. Software quality measurement is possible only by quantifying the characteristics affecting the software quality. For measuring the quality, the parameters or quality factors are considered that vary over a domain of discourse. The quality factors stated in ISO/IEC 9126 model are used in this paper. Due to the unpredictable nature of these factors or attributes fuzzy approach has been used to estimate the software quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5978957,no
Research on the definition and model of software testing quality,2011,"Aim of software testing is to find out software defects and evaluate the software quality. In order to explain the software testing can really elevate the software quality, it is necessary to assess the quality of software testing itself. This paper discusses the definition of software testing quality, and further builds the framework model of software testing quality (STQFM) and the metrics model of software testing quality (SFTMM). The availability of the models is verified by example application.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5979352,no
A dynamic software binary fault injection system for real-time embedded software,2011,"Dynamic fault injection is an efficient technique for reliability evaluation. In this paper, a new fault injection system called DDSFIS (debug-based dynamic software fault injection system) is presented. DDSFIS is designed to be adaptable to real-time embedded systems. Locations of injections and faults are detected and injected automatically without recompiling. The tool is highly portable between different platforms since it relies on the GNU tool chains. The effectiveness and performance of DDSFIS is validated by experiments.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5979375,no
Nonlinear Unsharp Masking for Mammogram Enhancement,2011,"This paper introduces a new unsharp masking (UM) scheme, called nonlinear UM (NLUM), for mammogram enhancement. The NLUM offers users the flexibility 1) to embed different types of filters into the nonlinear filtering operator; 2) to choose different linear or nonlinear operations for the fusion processes that combines the enhanced filtered portion of the mammogram with the original mammogram; and 3) to allow the NLUM parameter selection to be performed manually or by using a quantitative enhancement measure to obtain the optimal enhancement parameters. We also introduce a new enhancement measure approach, called the second-derivative-like measure of enhancement, which is shown to have better performance than other measures in evaluating the visual quality of image enhancement. The comparison and evaluation of enhancement performance demonstrate that the NLUM can improve the disease diagnosis by enhancing the fine details in mammograms with no a priori knowledge of the image contents. The human-visual-system-based image decomposition is used for analysis and visualization of mammogram enhancement.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5981393,no
Performance optimization of error detection based on speculative reconfiguration,2011,"This paper presents an approach to minimize the average program execution time by optimizing the hardware/software implementation of error detection. We leverage the advantages of partial dynamic reconfiguration of FPGAs in order to speculatively place in hardware those error detection components that will provide the highest reduction of execution time. Our optimization algorithm uses frequency information from a counter-based execution profile of the program. Starting from a control flow graph representation, we build the interval structure and the control dependence graph, which we then use to guide our error detection optimization algorithm.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5981768,no
Transmission Control Policy design for decentralized detection in sensor networks,2011,"A Wireless Sensor Network (WSN) deployed for detection applications has the distinguishing feature that sensors cooperate to perform the detection task. Therefore, the decoupled design approach typically used to design communication networks, where each network layer is designed independently, does not lead to the desired optimal detection performance. Recent work on decentralized detection has addressed the design of MAC and routing protocols for detection applications by considering independently the Quality of Information (QoI), Channel State Information (CSI), and Residual Energy Information (REI) for each sensor. However, little attention has been given to integrate the three quality measures (QoI, CSI, REI) in the complete system design. In this work, we pursue a cross-layer approach to design a QoI, CSI, and REI-aware Transmission Control Policy (TCP) that coordinates communication between local sensors and the fusion center, in order to maximize the detection performance. We formulate and solve a constrained nonlinear optimization problem to find the optimal TCP design variables. We compare our design with the decoupled approach, where each layer is designed separately, in terms of the delay for detection and WSN lifetime.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5982139,no
BS-SVM multi-classification model in the application of consumer goods,2011,"Quality and safety of consumer products have drawn wide attention from scholars in related domain, this issue is based on the subject of the quality and safety of consumer goods, in accordance with characteristics of cases, and put forward a hierarchical support vector machine classification algorithm based on the relative separability of the feature space, to solve the low classification performance and high rate of misclassification of the existing algorithms. The weight of Binary Search Tree is the separability of samples, determining the order of categories by a selective set of training samples to construct SVM classifier and the final formation of a binary classification of the larger interval multi-valued SVM classifier tree. Simulation results show that the method has a faster test speed, relatively perfect good classification accuracy and generalization performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5982240,no
Improving rating estimation in recommender using demographic data and expert opinions,2011,"Memory-based collaborative filtering algorithms have been widely adopted in many popular recommender systems, however the rating data are very sparse, which affects prediction accuracy greatly. To solve this problem, we use expert opinions to improve prediction accuracy. Firstly, we propose a novel similarity measure in order to highlight users' background. Then, combining users' ratings with expert opinions, the prediction get a right balance in both expert professional opinions and similar users. Finally, since SVD-based collaborative filtering algorithms shows good performance on the prevention of noise, we use it to smooth the prediction. Experiments of MovieLens have shown that our proposed method improves recommendation quality obviously.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5982269,no
Development modeling of software based on performance process,2011,"In particular, an early integration of performance specifications in the SDP has been recognized during the last few years as an effective approach to improve the overall quality of a software. Performance related problems are becoming more and more strategic in the software development, especially recently with the advent of Web Services and related business-oriented composition techniques (software as a service, Web 2.0, orchestration, choreography, etc.). The goal of our work is the definition of a software development process that integrates performance evaluation and prediction. The software performance engineering development process (SPEDP) we specify is focused on performance, which plays a key role driving the software development process, thus implementing a performance/QoS-driven (software) development process. More specifically, in this paper our aim is to formally define the SPEDP design process, posing particular interest on the basis, on the first step of SPEDP, the software/system architecture design, modeling and/or representation. We define both the diagrams to use and show how to model the structure of the software architecture, its behavior and performance requirements. This is the first mandatory step for the automation of the SPEDP into a specific tool.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5982336,no
Translation of application-level terms to resource-level attributes across the Cloud stack layers,2011,"The emergence of new environments such as Cloud computing highlighted new challenges in traditional fields like performance estimation. Most of the current cloud environments follow the Software, Platform, Infrastructure service model in order to map discrete roles / providers according to the offering in each â€œlayerâ€? However, the limited amount of information passed from one layer to the other has raised the level of difficulty in translating user-understandable application terms from the Software layer to resource specific attributes, which can be used to manage resources in the Platform and Infrastructure layers. In this paper, a generic black box approach, based on Artificial Neural Networks is used in order to perform the aforementioned translation. The efficiency of the approach is presented and validated through different application scenarios (namely FFMPEG encoding and real-time interactive e-Learning) that highlight its applicability even in cases where accurate performance estimation is critical, as in cloud environments aiming to facilitate real-time and interactivity.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5984009,no
Improving quality in a distributed IP telephony system by the use of multiplexing techniques,2011,"Nowadays, many enterprises use Voice over Internet Protocol (VoIP) in their telephony systems. Companies with offices in different countries or geographical areas can build a central managed telephony system sharing the lines of their gateways in order to increase admission probability and to save costs on international calls. So it is convenient to introduce a system to ensure a minimum QoS (Quality of Service) for conferences, and one of these solutions is CAC (Call Admission Control). In this work we study the improvements in terms of admission probability and conversation quality (R-factor) which can be obtained when RTP multiplexing techniques are introduced, as in this scenario there will be multiple conferences with the same origin and destination offices. Simulations have been carried out in order to compare simple RTP and TCRTP (Tunneling Multiplexed Compressed RTP). The results show that these parameters can be improved while maintaining an acceptable quality for the conferences if multiplexing is used.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5984847,no
Influence of traffic management solutions on Quality of Experience for prevailing overlay applications,2011,"Different sorts of peer-to-peer (P2P) applications emerge every day and they are becoming more and more popular. The performance of such applications may be measured by means of Quality of Experience (QoE) metrics. In this paper, the factors that influence these metrics are surveyed. Moreover, the impact of economic traffic management solutions (e.g., proposed by IETF) on perceived QoE for the dominant overlay applications is assessed. The possible regulatory issues regarding QoE for P2P applications are also mentioned.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5985866,no
Evaluating the efficiency of data-flow software-based techniques to detect SEEs in microprocessors,2011,"There is a large set of software-based techniques that can be used to detect transient faults. This paper presents a detailed analysis of the efficiency of dataflow software-based techniques to detect SEU and SET in microprocessors. A set of well-known rules is presented and implemented automatically to transform an unprotected program into a hardened one. SEU and SET are injected in all sensitive areas of MIPS-based microprocessor architecture. The efficiency of each rule and a combination of them are tested. Experimental results are used to analyze the overhead of data-flow techniques allowing us to compare these techniques in the respect of time, resources and efficiency in detecting this type of faults. This analysis allows us to implement an efficient fault tolerance method that combines the presented techniques in such way to minimize memory area and performance overhead. The conclusions can lead designers in developing more efficient techniques to detect these types of faults.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5985914,no
Using an FPGA-based fault injection technique to evaluate software robustness under SEEs: A case study,2011,"Microprocessor-based system's robustness under Single Event Effects is a very current concern. A widely adopted solution to make robust a microprocessor-based system consists in modifying the software application by adding redundancy and fault detection capabilities. The efficiency of the selected software-based solution must be assessed. This evaluation process allows the designers to choose the more suitable robustness technique and check if the hardened system achieves the expected dependability levels. Several approaches with this purpose can be found in the literature, but their efficiency is limited in terms of the number of faults that can be injected, as well as the level of accuracy of the fault injection process. In this paper, we propose FPGA-based fault injection techniques to evaluate software robustness methods under Single Event Upset (SEU) as well as Single Event Transient (SET). Experimental results illustrate the benefits of using the proposed fault injection method, which is able to evaluate a high amount of faults of both types of events.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5985918,no
On-line BIST for performance failure prediction under aging effects in automotive safety-critical applications,2011,"Electronic design of high-performance digital systems in nano-scale CMOS technologies under Process, power supply Voltage, Temperature and Aging (PVTA) variations is a challenging process. Such variations induce abnormal timing delays leading to systems errors, harmful in safety-critical applications. Performance Failure Prediction (PFP), instead of error detection, becomes necessary, particularly in the presence of aging effects. In this paper, an on-line BIST methodology for PFP in a standard cell design flow is proposed. The methodology is based on abnormal delay detection associated with critical signal paths. PVTA-aware aging sensors are designed. Multilevel simulation is used. Functional and structural test pattern generation is performed, targeting the detection of critical path delay faults. A sensor insertion technique is proposed, together with an up-graded version of a proprietary software tool, DyDA. Finally, a novel strategy for gate-level Aging fault injection is proposed, using the concept of an Aging de-rating factor. Results are presented for a Serial Parallel Interface (SPI) controller, designed with commercial UMC 130nm CMOS technology and Faradayâ„?cell library. Only seven sensors are required to monitor unsafe performance operation, due to Negative Bias Thermal Instability (NBTI)-induced aging.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5985919,no
Improved generalized predictive control in polymerization process,2011,"The polymerization process is the basis stage of PAN based carbon fiber production, and its temperature control affects directly the quality and yield of the last products. However, the polymerization process releases a lot of heat rapidly and it has the serious time delay character. These make it is very complex to control the polymerization process. The paper firstly analyzes the polymerization process model, gives its CARIMA parametric equation, provides the original data excited by the generalized binary noise (GBN) signal, and identify the model using the recursive least square algorithm with fading memory. Secondly, the paper introduces an improved generalized predictive control (GPC) method, which has stronger fault tolerance and robustness with little process overshooting. At last, a system is realized in the cascade frame based on the control layer and the monitoring layer. The former realizes the model identification and the recursive computation in the improved generalized predictive control, and sends the results to the latter, and the latter realizes PID with dead-zone control of the mixed water temperature control using the results of the main regulator. The practice shows that the polymerize temperature cascade system runs well and has evident effect with effective control.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5987195,no
Reasonable coal pillar size design in Xiaoming mine,2011,"The pillar size has great influence not only on the stability of surrounding rock, but also on the recovery rate of coal resources, which directly affects the economic benefits of coal. According to XiaoMing tiefa coalmining group geological conditions and the use of coal pillar, it adopts field measurement, numerical simulation and theory calculation methods to get small coal pillar width and reasonable wide size of coal pillar. Use rock stratum detect to find out the fissures, faults, broken area; analyze the coal pillar stress and displacement distribution of the different pillar widths (3, 5, 8, 10,15and20m) by FLAC2D simulation of numerical simulation software, determine a reasonable size finally.lt provides a guarantee of security for the exploitation of the mine.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5987948,no
Design and realization of lake water quality detection system based on wireless sensor networks,2011,"With the increasing pollution of lake water environment and frequent outbreaks of blue-green algae, it is very significant to establish one kind of distributed real-time monitoring network on all typical locations of the lake. In this way, the pollution of lake water environment can be effectively controlled. Nowadays, the stability of data reporting and how to maximize the use of limited power are the biggest problems to the design of real-time monitoring networks. In this paper, according to characteristics of lake monitoring, the design of lake water quality detection system based on wireless sensor network is presented. Key technologies and issues were discussed in the paper based on lake monitoring sensor network topology design, network protocol design and node hardware and software design. Also the application of lake water quality detection system in the Taihu lake was discussed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5988101,no
Performance Evaluation of FACS-MP CAC System Priority Algorithm for Wireless Cellular Networks,2011,"Call Admission Control (CAC) is one of the resource management functions, which regulates network access to ensure QoS provisioning. However, the decision for CAC is very challenging issue due to user mobility, limited radio spectrum, and multimedia traffic characteristics. In our previous work, we implemented a fuzzy based CAC system called FACS-MP, which consider many priorities. In this paper, we evaluate by simulations the performance of priority algorithm which is part of FACS-MP system. From the simulations results, we conclude that the FACS-MP make a good differentiation of different services based on different priorities.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5989029,no
An Atomatic Fundus Image Analysis System for Clinical Diagnosis of Glaucoma,2011,"Glaucoma is a serious ocular disease and leads blindness if it couldn't be detected and treated in proper way. The diagnostic criteria for glaucoma include intraocular pressure measurement, optic nerve head evaluation, retinal nerve fiber layer and visual field defect. The observation of optic nerve head, cup to disc ratio and neural rim configuration are important for early detecting glaucoma in clinical practice. However, the broad range of cup to disc ratio is difficult to identify early changes of optic nerve head, and different ethnic groups possess various features in optic nerve head structures. Hence, it is still important to develop various detection techniques to assist clinicians to diagnose glaucoma at early stages. In this study, we developed an automatic detection system which contains two major phases: the first phase performs a series modules of digital fundus retinal image analysis including vessel detection, vessel in painting, cup to disc ratio calculation, and neuro-retinal rim for ISNT rule, the second phase determines the abnormal status of retinal blood vessels from different aspect of view. In this study, the novel idea of integrating image in painting and active contour model techniques successfully assisted the correct identification of cup and disk regions. Several clinical fundus retinal images containing normal and glaucoma images were applied to the proposed system for demonstration.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5989070,no
Mining unstructured log files for recurrent fault diagnosis,2011,"Enterprise software systems are large and complex with limited support for automated root-cause analysis. Avoiding system downtime and loss of revenue dictates a fast and efficient root-cause analysis process. Operator practice and academic research have shown that about 80% of failures in such systems have recurrent causes; therefore, significant efficiency gains can be achieved by automating their identification. In this paper, we present a novel approach to modelling features of log files. This model offers a compact representation of log data that can be efficiently extracted from large amounts of monitoring data. We also use decision-tree classifiers to learn and classify symptoms of recurrent faults. This representation enables automated fault matching and, in addition, enables human investigators to understand manifestations of failure easily. Our model does not require any access to application source code, a specification of log messages, or deep application knowledge. We evaluate our proposal using fault-injection experiments against other proposals in the field. First, we show that the features needed for symptom definition can be extracted more efficiently than does related work. Second, we show that these features enable an accurate classification of recurrent faults using only standard machine learning techniques. This enables us to identify accurately up to 78% of the faults in our evaluation data set.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5990536,no
Evaluation of Experiences from Applying the PREDIQT Method in an Industrial Case Study,2011,"We have developed a method called PREDIQT for model-based prediction of impacts of architectural design changes on system quality. A recent case study indicated feasibility of the PREDIQT method when applied on a real-life industrial system. This paper reports on the experiences from applying the PREDIQT method in a second and more recent case study - on an industrial ICT system from another domain and with a number of different system characteristics, compared with the previous case study. The analysis is performed in a fully realistic setting. The system analyzed is a critical and complex expert system used for management and support of numerous working processes. The system is subject to frequent changes of varying type and extent. The objective of the case study has been to perform an additional and more structured evaluation of the PREDIQT method and assess its performance with respect to a set of success criteria. The evaluation argues for feasibility and usefulness of the PREDIQT-based analysis. Moreover, the study has provided useful insights into the weaknesses of the method and suggested directions for future research and improvements.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5992012,no
Visual Recognition Using Density Adaptive Clustering,2011,"Visual codebook based texture analysis and image recognition is popular for its robustness to affine transformation and illumination variation. It is based on the affine invariable descriptors of local patches extracted by region detector, and then represents the image by histogram of the codebook constructed by the feature vector quantization. The most commonly used vector quantization method is k-means. But due to the limitations of predefined number of clusters and local minimum update rule, we show that k-means would fail to code the most discriminable descriptors. Another defect of k-means is that the computational complexity is extremely high. In this paper, we proposed a nonparametric vector quantization method based on mean shift, and use locality-sensitive hashing (LSH) to reduce the cost of the nearest neighborhood query in the mean-shift iterations. The performance of proposed method is demonstrated in several image classification tasks. We also show that the Information Gain or Mutual Information based feature selection based on our codebook further improves the performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5992173,no
Statistical approach for finding sensitive positions for condition based monitoring of reciprocating air compressors,2011,"Reciprocating air compressors are one of the most popular and widely used machines in industry today. Timely detection of fault occurring in these machines becomes very critical since it influences the plant performance by virtue of system reliability, operating efficiency and maintenance cost. Monitoring of faults by identifying sensitive positions through sensory output forms vital part of everyday manufacturing. Health monitoring of a reciprocating air compressor using PC based data acquisition system and timely identification of potential faults can prevent failures of the entire system. Various transducers, data acquisition DAQ hardware and relevant software forms the basic components of the health monitoring system. Prior to acquiring the data for health monitoring and consequently the fault diagnosis, it is crucial to locate the sensitive positions on the machine. This paper proposes a scheme to determine the sensitive positions on a machine in healthy condition, based on computation of statistical parameters such as Peak value, Standard Deviation, RMS value, Variance and Cross Correlation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5993411,no
Software energy estimation based on statistical characterization of intermediate compilation code,2011,"Early estimation of embedded software power consumption is a critical issue that can determine the quality and, sometimes, the feasibility of a system. Architecture-specific, cycle-accurate simulators are valuable tools for fine-tuning performance of critical sections of the application but are often too slow for the simulation of entire systems. This paper proposes a fast and statistically accurate methodology to evaluate the energy performance of embedded software and describes the associated toolchain. The methodology is based on a static characterization of the target instruction set to allow estimation on an equivalent, target-independent intermediate code representation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5993659,no
RVC-based time-predictable faulty caches for safety-critical systems,2011,"Technology and Vcc scaling lead to significant faulty bit rates in caches. Mechanisms based on disabling faulty parts show to be effective for average performance but are unacceptable in safety critical systems where worst-case execution time (WCET) estimations must be safe and tight. The Reliable Victim Cache (RVC) deals with this issue for a large fraction of the cache bits. However, replacement bits are not protected, thus keeping the probability of failure still high. This paper proposes two mechanisms to tolerate faulty bits in replacement bits and keep time-predictability by extending the RVC. Our solutions offer different tradeoffs between cost and complexity. In particular, the Extended RVC (ERVC) has low energy and area overheads while keeping complexity at a minimum. The Reliable Replacement Bits (RRB) solution has even lower overheads at the expense of some more wiring complexity.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5993806,no
Software-based control flow error detection and correction using branch triplication,2011,"Ever Increasing use of commercial off-the-shelf (COTS) processors to reduce cost and time to market in embedded systems has brought significant challenges in error detection and recovery methods employing in such systems. This paper presents a software based control flow error detection and correction technique, so called branch TMR (BTMR), suitable for use in COTS-based embedded systems. In BTMR method, each branch instruction is triplicated and a software interrupt routine is invoked to check the correctness of the branch instruction. During the execution of a program, when a branch instruction is executed, it is compared with the second redundant branch in the interrupt routine. If a mismatch is detected, the third redundant branch instruction is considered as the error-free branch instruction; otherwise, no error has occurred. The main advantage of BTMR over previously proposed control flow checking (CFC) methods is its ability to correct CFEs as well as protection of indirect branch instructions. The BTMR method is evaluated on LEON2 embedded processor. The results show that, error correction coverage is about 96%, while memory overhead and performance overhead of BTMR is about 28% and 10%, respectively.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5993847,no
Application research on temperature WSN nodes in switchgear assemblies based on TinyOS and ZigBee,2011,"Temperature detection can timely discover the potential faults in switchgear assemblies. Appling a Wireless Sensor Networks(WSN) in on-line monitoring system is an effective measure to realize Condition Based Maintenance(CBM) for intelligent switchgear assemblies. A design solution of a temperature wireless sensor network node with CC2430 chip based on ZigBee is presented. The thermocouple is used as main sensor unit to detect the temperature-rise of key points in switchgear assemblies in real time, and the digital temperature sensor is adopted as subsidiary sensor unit to detect the environment temperature, so that the temperature of the points at high voltage and high current can be measured effectively and accurately. TinyOS, the embedded operating system adopted widely for WSN node, is researched and transplanted into CC2430, therefore the node software can be programmed and updated remotely. On this basis, the temperature wireless sensor prototypes based on ZigBee and/or TinyOS with CC2430 are designed and developed, and tested via the WSN experimental platform which is built by us. The experimental result shows that the temperature wireless sensor node can meet the requirement of the system function, and has the features such as low power dissipation, small volume, stable performance and long lifespan etc., and can be broadly applied in on-line temperature measuring and monitoring of high-voltage and low-voltage switchgear assemblies.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5993950,no
Towards improved survivability in safety-critical systems,2011,"Performance demand of Critical Real-Time Embedded (CRTE) systems implementing safety-related system features grows at an exponential rate. Only modern semiconductor technologies can satisfy CRTE systems performance needs efficiently. However, those technologies lead to high failure rates, thus lowering survivability of chips to unacceptable levels for CRTE systems. This paper presents SESACS architecture (Surviving Errors in SAfety-Critical Systems), a paradigm shift in the design of CRTE systems. SESACS is a new system design methodology consisting of three main components: (i) a multicore hardware/firmware platform capable of detecting and diagnosing hardware faults of any type with minimal impact on the worst-case execution time (WCET), recovering quickly from errors, and properly reconfiguring the system so that the resulting system exhibits a predictable and analyzable degradation in WCET; (ii) a set of analysis methods and tools to prove the timing correctness of the reconfigured system; and (iii) a white-box methodology and tools to prove the functional safety of the system and compliance with industry standards. This new design paradigm will deliver huge benefits to the embedded systems industry for several decades by enabling the use of more cost-effective multicore hardware platforms built on top of modern semiconductor technologies, thereby enabling higher performance, and reducing weight and power dissipation. This new paradigm will further extend the life of embedded systems, therefore, reducing warranty and early replacement costs.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5994536,no
Optimal placement of power quality monitors in distribution systems using the topological monitor reach area,2011,"Installation of power quality monitors (PQMs) at all buses in a power distribution network is uneconomical and it needs to be minimized. This paper presents a PQM positioning technique which finds the optimal number and location of PQMs in power distribution systems for voltage sag detection. The IEEE 34-node and 69 bus test systems were modeled in DIgSILENT software to obtain the topological monitor reach area matrix for various types of faults. Then, an optimization problem is formulated and solved using genetic algorithm to find the minimum number of PQMs in the distribution system which can guarantee the observability of the whole system. Finally, the sag severity index has been used to find the best location to install monitors in the distribution system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5994627,no
Integrating the S-PQDA software tool in the utility power quality management system,2011,"This paper presents a smart power quality data a analyzer (S-PQDA) or power quality diagnosis software (PQDS) tool that performs power quality (PQ) diagnosis on the PQ disturbance data recorded by an online PQ monitoring system. The software tool enables power utility engineers to perform automatic PQ disturbance detection, classification and diagnosis of the disturbances. The PQDS also assists the power utility engineers in identifying the existence of incipient faults due to partial discharges in the cable compartment. The overall accuracy of the software in performing PQ diagnosis is 96.4%.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5994947,no
Software Testing and Verification in Climate Model Development,2011,"Over the past 30 years, most climate models have grown from relatively simple representations of a few atmospheric processes to complex multidisciplinary systems. Computer infrastructure over that period has gone from punchcard mainframes to modern parallel clusters. Model implementations have become complex, brittle, and increasingly difficult to extend and maintain. Verification processes for model implementations rely almost exclusively on some combination of detailed analyses of output from full climate simulations and system-level regression tests. Besides being costly in terms of developer time and computing resources, these testing methodologies are limited in the types of defects they can detect, isolate, and diagnose. Mitigating these weaknesses of coarse-grained testing with finer-grained unit tests has been perceived as cumbersome and counterproductive. Recent advances in commercial software tools and methodologies have led to a renaissance of systematic fine-grained testing. This opens new possibilities for testing climate-modeling-software methodologies.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5999647,no
A dynamic workflow simulation platform,2011,"In numeric optimization algorithms errors at application level considerably affect the performance of their execution on distributed infrastructures. Hours of execution can be lost only due to bad parameter configurations. Though current grid workflow systems have facilitated the deployment of complex scientific applications on distributed environments, the error handling mechanisms remain mostly those provided by the middleware. In this paper, we propose a collaborative platform for the execution of scientific experiments in which we integrate a new approach for treating application errors, using the dynamicity and exception handling mechanisms of the YAWL workflow management system. Thus, application errors are correctly detected and appropriate handling procedures are triggered in order to save as much as possible of the work already executed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5999841,no
A dependable system based on adaptive monitoring and replication,2011,"A multi agent system (MAS) has recently gained public attention as a method to solve competition and cooperation in distributed systems. However, MAS's vulnerability due to the propagation of failures prevents it from being applied to a large-scale system. This paper proposes a method to improve the reliability and efficiency of distributed systems. Specifically, the paper deals with the issue of fault tolerance. Distributed systems are characterized by a large number of agents, who interact according to complex patterns. The effects of a localized failure may spread across the whole network, depending on the structure of the interdependences between agents. The method monitors messages between agents to detect undesirable behaviors such as failures. Collecting the information, the method generates global information of interdependence between agents and expresses it in a graph. This interdependence graph enables us to detect or predict undesirable behaviors. This paper also shows that the method can optimize performance of a MAS and improve adoptively its reliability under complicated and dynamic environment by applying the global information acquired from the interdependence graph to a replication system. The advantages of the proposed method are illustrated through simulation experiments based on a virtual auction market.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5999843,no
Software design of rail transit train braking system fault diagnosis based on MATLAB/VB,2011,"With the requirements of safety and reliability in brake system increasingly enhanced in urban transit train. For brake system the performance of prediction system and fault diagnosis must be increased at the same time. MATLAB is a high performance mathematics software integrated data analysis, numeric processing and graphic display, combined with the high powered software in VB graphical user interface, the convenience of developing fault diagnosis system not only has shorter time but also has good performance characteristics. This paper focus on the software design of brake system fault diagnosis in urban rail transit train, attempt to do some theoretical research for timely and effectively avoid for brake system in operation of the train traffic accident caused by failure.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6003313,no
Model-Driven Design of Performance Requirements,2011,"Obtaining the expected performance of a workflow is much simpler if the requirements for each of its tasks are well defined. However, most of the time, not all tasks have well-defined requirements, and these must be derived by hand. This can be an error-prone and time consuming process for complex workflows. In this work, we present an algorithm which can derive a time limit for each task in a workflow, using the available task and workflow expectations. The algorithm assigns the minimum time required by each task and distributes the slack according to the weights set by the user, while checking that the task and workflow expectations are consistent with each other. The algorithm avoids having to evaluate every path in the workflow by building its results incrementally over each edge. We have implemented the algorithm in a model handling language and tested it against a naive exhaustive algorithm which evaluates all paths. Our incremental algorithm reports equivalent results in much less time than the exhaustive algorithm.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004314,no
Improving the Modifiability of the Architecture of Business Applications,2011,"In the current rapidly changing business environment, organizations must keep on changing their business applications to maintain their competitive edges. Therefore, the modifiability of a business application is critical to the success of organizations. Software architecture plays an important role in ensuring a desired modifiability of business applications. However, few approaches exist to automatically assess and improve the modifiability of software architectures. Generally speaking, existing approaches rely on software architects to design software architecture based on their experience and knowledge. In this paper, we build on our prior work on automatic generation of software architectures from business processes and propose a collection of model transformation rules to automatically improve the modifiability of software architectures. We extend a set of existing product metrics to assess the modifiability impact of the proposed model transformation rules and guide the quality improvement process. Eventually, we can generate software architecture with desired modifiability from business processes. We conduct a case study to illustrate the effectiveness of our transformation rules.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004325,no
A Hierarchical Security Assessment Model for Object-Oriented Programs,2011,"We present a hierarchical model for assessing an object-oriented program's security. Security is quantified using structural properties of the program code to identify the ways in which `classified' data values may be transferred between objects. The model begins with a set of low-level security metrics based on traditional design characteristics of object-oriented classes, such as data encapsulation, cohesion and coupling. These metrics are then used to characterise higher-level properties concerning the overall readability and writ ability of classified data throughout the program. In turn, these metrics are then mapped to well-known security design principles such as `assigning the least privilege' and `reducing the size of the attack surface'. Finally, the entire program's security is summarised as a single security index value. These metrics allow different versions of the same program, or different programs intended to perform the same task, to be compared for their relative security at a number of different abstraction levels. The model is validated via an experiment involving five open source Java programs, using a static analysis tool we have developed to automatically extract the security metrics from compiled Java byte code.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004330,no
A Reliability Model for Complex Systems,2011,"A model of software complexity and reliability is developed. It uses an evolutionary process to transition from one software system to the next, while complexity metrics are used to predict the reliability for each system. Our approach is experimental, using data pertinent to the NASA satellite systems application environment. We do not use sophisticated mathematical models that may have little relevance for the application environment. Rather, we tailor our approach to the software characteristics of the software to yield important defect-related predictors of quality. Systems are tested until the software passes defect presence criteria and is released. Testing criteria are based on defect count, defect density, and testing efficiency predictions exceeding specified thresholds. In addition, another type of testing efficiency-a directed graph representing the complexity of the software and defects embedded in the code-is used to evaluate the efficiency of defect detection in NASA satellite system software. Complexity metrics were found to be good predictors of defects and testing efficiency in this evolutionary process.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004508,no
A Methodology of Model-Based Testing for AADL Flow Latency in CPS,2011,"AADL (Architecture Analysis and Design Language) is a kind of model-based real-time CPS (Cyber-Physical System) modeling language, which has been widely used in avionics and space areas. The current challenges have been raised up on how to test CPS model described in AADL dynamically and find design fault at the design phase to iterate and refine the model architecture. This paper mainly tests the flow latency in design model based on PDA (Push-Down Automata). It abstracts the properties of flow latency in CPS model, and translates them into PDA in order to assess the latency in simulation. Meanwhile, this paper presents a case study of pilotless aircraft cruise control system to prove the feasibility of dynamic model-based testing on model performances and achieve the architecture iteration and refining aim.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004510,no
Automatic Regression Test Selection Based on Activity Diagrams,2011,"Regression testing is the most common method for ensuring the quality of changed software. As UML models are widely used as design blueprints, model-based test case generation techniques can be used in regression test cases generation. However, test cases obtained from these techniques are usually represented as sequences of actions in abstract models, and heavy human efforts are needed to transform them into test cases accepted by software for execution. To reduce this effort, we present an approach to automatically generate executable test cases for regression testing based on activity diagrams. It combines a technique of activity diagrams based regression test cases classification to pick up retestable test cases, and a technique of feedback-directed test cases generation to generate test cases for new behaviors in changed software. We implement the tool MDRTGen to show the performance of the approach. The experiments show the good performance of the approach in improving the efficiency of test cases generation and decreasing the cost in regression testing.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004519,no
Towards Synthesizing Realistic Workload Traces for Studying the Hadoop Ecosystem,2011,"Designing cloud computing setups is a challenging task. It involves understanding the impact of a plethora of parameters ranging from cluster configuration, partitioning, networking characteristics, and the targeted applications' behavior. The design space, and the scale of the clusters, make it cumbersome and error-prone to test different cluster configurations using real setups. Thus, the community is increasingly relying on simulations and models of cloud setups to infer system behavior and the impact of design choices. The accuracy of the results from such approaches depends on the accuracy and realistic nature of the workload traces employed. Unfortunately, few cloud workload traces are available (in the public domain). In this paper, we present the key steps towards analyzing the traces that have been made public, e.g., from Google, and inferring lessons that can be used to design realistic cloud workloads as well as enable thorough quantitative studies of Hadoop design. Moreover, we leverage the lessons learned from the traces to undertake two case studies: (i) Evaluating Hadoop job schedulers, and (ii) Quantifying the impact of shared storage on Hadoop system performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6005384,no
An Energy-Aware SaaS Stack,2011,"We present a multi-agent system on top of the IaaS layer consisting of a scheduler agent and multiple worker agents. Each job is controlled by an autonomous worker agent, which is equipped with application specific knowledge (e.g., performance functions) allowing it to estimate the type and number of necessary resources. During runtime, the worker agent monitors the job and adapts its resources to ensure the specified quality of service - even in noisy clouds where the job instances are influenced by other jobs. All worker agents interact with the scheduler agent, which takes care of limited resources and does a cost-aware scheduling by assigning jobs to times with low energy costs. The whole architecture is self-optimizing and able to use public or private clouds.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6005416,no
Power-Aware Resource Scheduling in Base Stations,2011,"Base band stations for Long Term Evolution (LTE) communication processing tend to rely on over-provisioned resources to ensure that peak demands can be met. These systems must meet user Quality of Service expectations, but during non-peak workloads, for instance, many of the cores could be placed in low-power modes. One key property of such application-specific systems is that they execute frequent, short-lived tasks. Sophisticated resource management and task scheduling approaches suffer intolerable overhead costs in terms of time and expense, and thus lighter-weight and more efficient strategies are essential to both saving power and meeting performance expectations. To this end, we develop a flexible, non-propietary LTE workload model to drive our resource management studies. Here we describe our experimental infrastructure and present early results that underscore the promise of our approach along with its implications on future hardware/software codesign.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6005419,no
"""You Want to do What?"" Breaking the Rules to Increase Customer Satisfaction",2011,Customer satisfaction. Every customer support organization strives to increase and maintain a higher level of customer satisfaction. This is a story of how one software organization attempted to increase customer satisfaction by increasing predictability and response times to reported defects. Along the way they were able to establish trust with the customer support organization by implementing a process that seemed counterintuitive to its stakeholders.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6005515,no
Reverse Seam Carving,2011,"Seam carving is an effective operator supporting content-aware resizing for both image reduction and expansion. However, repeated seam removing and inserting processes lead to excessively distortion image when imposed on seam insertion then removal operations or the other way around. With considering the relationship between seam removing and inserting processes, we present an ameliorated energy function to minimize aliasing. ""Forward Energy"" is an effective improvement only to image reduction. Moreover, we propose a novel ""Visual Points"" structure which distinguish the ""Forward Energy"" of seam insertion from that of seam removal, and improve seam insertion operations greatly. Qualitative and quantitative experiments show that the proposed method can achieve high quality as compared to existed methods.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6005531,no
A pattern-oriented methodology for conceptual modeling evaluation and improvement,2011,"Conceptual models are of prime importance to ensure a high level of quality in designing information systems. It has been witnessed that the majority of information systems (IS) change requests result due to deficient functionalities in the information systems. Therefore, a good analysis and design method should guarantee that conceptual models are correct and complete and easy to understand, as they are the communicating mediator between the users and the development team. Similarly, if models are complex then their extension or the incorporation of missing requirements gets very difficult for the designers. Our approach evaluates the conceptual models on multiple levels of granularity in addition to providing the corrective actions or transformations for improvement. We propose quality patterns to help the non-expert users in evaluating their models with respect to their quality goal. This paper also illustrates our approach by describing an evaluation and improvement process using a case study.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6006840,no
Flexible Distributed Capacity Allocation and Load Redirect Algorithms for Cloud Systems,2011,"In Cloud computing systems, resource management is one of the main issues. Indeed, in any time instant resources have to be allocated to handle effectively workload fluctuations, while providing Quality of Service (QoS) guarantees to the end users. In such systems, workload prediction-based autonomic computing techniques have been developed. In this paper we propose capacity allocation techniques able to coordinate multiple distributed resource controllers working in geographically distributed cloud sites. Furthermore, capacity allocation solutions are integrated with a load redirection mechanism which forwards incoming requests between different domains. The overall goal is to minimize the costs of the allocated virtual machine instances, while guaranteeing QoS constraints expressed as a threshold on the average response time. We compare multiple heuristics which integrate workload prediction and distributed non-linear optimization techniques. Experimental results show how our solutions significantly improve other heuristics proposed in the literature (5-35% on average), without introducing significant QoS violations.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6008706,no
Database-Agnostic Transaction Support for Cloud Infrastructures,2011,"In this paper, we present and empirically evaluate the performance of database-agnostic transaction (DAT) support for the cloud. Our design and implementation of DAT is scalable, fault-tolerant, and requires only that the data store provide atomic, row-level access. Our approach enables applications to employ a single transactional data store API that can be used with a wide range of cloud data store technologies. We implement DAT in AppScale, an open-source implementation of the Google App Engine cloud platform, and use it to evaluate DAT's performance and the performance of a number of popular key-value stores.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6008772,no
SkelCL - A Portable Skeleton Library for High-Level GPU Programming,2011,"While CUDA and OpenCL made general-purpose programming for Graphics Processing Units (GPU) popular, using these programming approaches remains complex and error-prone because they lack high-level abstractions. The especially challenging systems with multiple GPU are not addressed at all by these low-level programming models. We propose SkelCL - a library providing so-called algorithmic skeletons that capture recurring patterns of parallel computation and communication, together with an abstract vector data type and constructs for specifying data distribution. We demonstrate that SkelCL greatly simplifies programming GPU systems. We report the competitive performance results of SkelCL using both a simple Mandelbrot set computation and an industrial-strength medical imaging application. Because the library is implemented using OpenCL, it is portable across GPU hardware of different vendors.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6008967,no
Detection and Correction of Silent Data Corruption for Large-Scale High-Performance Computing,2011,"Faults have become the norm rather than the exception for high-end computing on clusters with 10s/100s of thousands of cores, and this situation will only become more dire as we reach exascale computing. Exacerbating this situation, some of these faults will not be detected, manifesting themselves as silent errors that will corrupt memory while applications continue to operate but report incorrect results. This paper introduces RedMPI, an MPI library residing in the profiling layer of any standards-compliant MPI implementation. RedMPI is capable of both online detection and correction of soft errors that occur in MPI applications without requiring code changes to application source code. By providing redundancy, RedMPI is capable of transparently detecting corrupt messages from MPI processes that become faulted during execution. Furthermore, with triple redundancy RedMPI ""votes'' out MPI messages of a faulted process by replacing corrupted results with corrected results from unfaulted processes. We present an evaluation of RedMPI on an assortment of applications to demonstrate the effectiveness and assess associated overheads. Fault injection experiments establish that RedMPI is not only capable of successfully detecting injected faults, but can also correct these faults while carrying a corrupted application to successful completion without propagating invalid data.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6009019,no
Analyzing Fault-Impact Region of Composite Service for Supporting Fault Handling Process,2011,"A fault situation occurs to a service needs to be well analyzed and handled in order to ensure the reliability of composite service. The analysis can be driven by understanding the impact caused by the faulty service on the other services as well as the entire composition. Existing works have given less attention to this issue, in particular, the temporal impact situation caused by the fault. Thus, we propose an approach to analyzing the temporal impact and generating the impact region. The region can be utilized by the handling mechanism to prioritize the services to be repaired. The approach begins by estimating the updated temporal behavior of the composite service after the fault situation occurs, followed by identifying the potential candidates of the impact region. The concept of temporal negative impact is introduced to support the identification activity. Intuitively, the approach can assist in reducing the number of service changes in handling the fault situation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6009273,no
Reputation-Driven Web Service Selection Based on Collaboration Network,2011,"Most of trustworthy web service selection simply focus on individual reputation and ignore the collaboration reputation between services. To enhance the collaboration trust during web service selection, a reputation model called collaboration reputation is proposed. The reputation model is built on web service collaboration network(WSCN), which is constructed in terms of the composite service execution log. Thus, the WSCN aims to maintain the trustworthy collaboration alliance among web services, In WSCN, the collaboration reputation can be assessed by two metrics, one called invoking reputation is computed by recommendation, which is selected from the community structure hiding in WSCN, the other is assessed by the invoked web service. In addition, the web service selection based on WSCN is designed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6009461,no
Measuring robustness of Feature Selection techniques on software engineering datasets,2011,"Feature Selection is a process which identifies irrelevant and redundant features from a high-dimensional dataset (that is, a dataset with many features), and removes these before further analysis is performed. Recently, the robustness (e.g., stability) of feature selection techniques has been studied, to examine the sensitivity of these techniques to changes in their input data. In this study, we investigate the robustness of six commonly used feature selection techniques as the magnitude of change to the datasets and the size of the selected feature subsets are varied. All experiments were conducted on 16 datasets from three real-world software projects. The experimental results demonstrate that Gain Ratio shows the least stability on average while two different versions of ReliefF show the most stability. Results also show that making smaller changes to the datasets has less impact on the stability of feature ranking techniques applied to those datasets.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6009565,no
An image process way of globular transfer in MIG welding,2011,"This paper carried out an image process way about feature extraction of droplet transfer. The adopted materials are that welding wire diameter is 1.2mm and sheet steel thickness is 3.0mm. In order to obtain high quality images of dynamic droplet, a special filtering lens for image filtering with CCD was designed and then an algorithm of image was developed. Under MATLAB software platform, the algorithm compared median filtering, images sharpening, morphological operation, threshold segmentation, edge detection and so on. Because of images' magnification scale coefficients of original image, this paper calculated the coefficient according to characteristic parameters such as wire's diameter. Results proved that the image processing method could deal with the obtained droplet images and get their critical features effectively.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6009890,no
Research on fault pattern recognition for aircraft fuel system with its performance simulation,2011,"This paper presents application research on fault pattern recognition for aircraft fuel system based on its performance simulation. Fault pattern recognition method is used to perform fault detection, fault isolation, fault prognostics and so on in complicated system, which is basic research contents of prognostics and health management (PHM). Since the hardware of fuel system for an aircraft is too expensive in laboratory, it is better to build simulation software to research on the method of fault pattern recognition for fuel system of an aircraft. The simulation software of fault pattern recognition is able to carry out performance simulation for the aircraft fuel flow parameters such as pressure variation, flow rate in the fuel system pipe network both on the fuel system normal condition and its malfunction condition. When a fault pattern of a component such as radial pump, ball valve happens, the nodes in which maximum pressure variation has happen in the fuel system pipe network are selected, and the virtual sensor is arranged in these nodes. Through the signal correlation comparison of the virtual pressure sensors under the normal condition and malfunction condition, the fault pattern is recognized. Since the fault pattern recognition methods is at its very initiative stage, the false alarm rate is not low. Nevertheless, the simulation software is able to verify and study the new concept fault pattern recognition methods, which will supply technical support for the onboard aircraft PHM system in the future.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6010100,no
Automatic Library Generation for BLAS3 on GPUs,2011,"High-performance libraries, the performance-critical building blocks for high-level applications, will assume greater importance on modern processors as they become more complex and diverse. However, automatic library generators are still immature, forcing library developers to manually tune library to meet their performance objectives. We are developing a new script-controlled compilation framework to help domain experts reduce much of the tedious and error-prone nature of manual tuning, by enabling them to leverage their expertise and reuse past optimization experiences. We focus on demonstrating improved performance and productivity obtained through using our framework to tune BLAS3 routines on three GPU platforms: up to 5.4x speedups over the CUBLAS achieved on NVIDIA GeForce 9800, 2.8x on GTX285, and 3.4x on Fermi Tesla C2050. Our results highlight the potential benefits of exploiting domain expertise and the relations between different routines (in terms of their algorithms and data structures).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6012842,no
Mesh-based overlay enhancing live video quality in pull-based P2P systems,2011,"Nowadays, Peer-to-Peer (P2P) live streaming applications have attracted great interest. Despite the fact that numerous systems have been proposed in the past, there are still problems concerning delay and quality requirements of live video distribution. In this paper, we consider a pull-based P2P live video streaming system where the video is disseminated over an overlay network. We propose a mesh-based overlay construction mechanism that enhances the received video quality while minimizing, as much as possible, the play-out delay. The principal feature provides each newcomer with a set of neighbors holding almost the same video segments and enough available transmission capacities to deal with its requests. A particular algorithm has been designed to estimate the peer's available capacities. The results of simulations show our mechanism efficiency in heterogeneous systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6013604,no
A public cryptosystem from R-LWE,2011,"Recently Vadim Lyubashevsky etc. built LWE problem on ring and proposed a public cryptosystem based on R-LWE, which, to a certain extent, solved the defect of large public key of this kind, but it didn't offer parameter selections and performance analysis in detail. In this paper an improved scheme is proposed by sharing a ring polynomial vector that makes public key as small as 1/m of the original scheme in multi-user environments. In additions, we introduce a parameter r to control both the private key space size and decryption errors probability, which greatly enhances the flexibly and practicality. The correctness, security and efficiency are analyzed in detail and choice of parameters is studied, at last concrete parameters are recommended for the new scheme.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6013644,no
A fast fuzzy stator condition monitoring algorithm using FPGA,2011,"This paper presents a new methodology for realtime detection of stator faults in induction motors. It is based on the evaluation of the magnitudes of three phase current signals. The severity of fault is indicated by designed fuzzy system. The algorithm consists of two stages: feature extraction and classification. All acquired current signals are based on IEEE-754 single precision floating point format. In the feature extraction stage, floating point based features were obtained and then converted to the integer format. In the classification stage, all coding are written in VHDL using integer data type format. The proposed method classifies the stator related faults at higher level speed with better accuracy than conventional offline method. This is the first time that FPGA based fuzzy system has been employed to induction motor faults. With the proposed method, it is clearly identified the motor condition as either healthy or damaged, allowing one to perform a simple real time detection. A light computation load is required in order to perform the calculation of the algorithm. In addition, the implemented FPGA design allows the system reconfiguration to attain specific measurement conditions on different types of motors without hardware modifications.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6013711,no
Application of virtual instrument techonoly in electric courses teaching,2011,"This paper analyzed the problems existed in practice teaching process of electric courses and put forward a new view which is to apply virtual instrument (VI) technology in theses courses. On the basis of a simple instruction for labVIEW's VI design function, an instance which designed by labVIEW to emulate and detect harmonic signals was described in detail. The instance has been used in the course of power quality and obtained a good result. More and more instances were designed based VI and used in electric courses, it can promote the combination between theory and practice and improve teaching level of these courses.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6013887,no
Using Residue Number Systems for improving QoS and error detection & correction in Wireless Sensor Networks,2011,"Wireless Sensor Networks have potential of significantly enhancing our ability to monitor and interact with our physical environment. Realizing a fault tolerant operation is critical to the success of WSNs. The integrity of data has tremendous effects on performance of any data acquisition system. Noise and other disturbances can often degrade the information or data acquired from these systems. Devising a fault-tolerant mechanism in wireless sensor networks is very important due to the construction and deployment characteristics of these low powered sensing devices. Moreover, due to the low computation and communication capabilities of the sensor nodes, the fault-tolerant mechanism should have a very low computation overhead. In this paper, we present a novel method to improve Performance and Fault Detection & Correction in Wireless Sensor Networks by using of Residue Number Systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6013936,no
Localized approach to distributed QoS routing with a bandwidth guarantee,2011,"Localized Quality of Service (QoS) routing has been recently proposed as a promising alternative to the currently deployed global routing schemes. In localized routing schemes, routing decisions are taken solely based on locally collected statistical information rather than global state information. This approach significantly reduces the overheads associated with maintaining global state information at each node, which in turn improves the overall routing performance. In this paper we introduce a Localized Distributed QoS Routing algorithm (LDR), which integrates the localized routing scheme into distributed routing. We compare the performance of our algorithm against other existing localized routing algorithm, namely Credit Based Routing (CBR); and against the contemporary global routing algorithm, the Widest Shortest Path (WSP). With the aid of simulations, we show that the proposed algorithm outperforms the others in terms of the overall network blocking probability.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014049,no
Performance comparison of Multiple Description Coding and Scalable video coding,2011,"For a video server, providing a good quality of service to highly diversified users is a challenging task because different users have different link conditions and different requirement of quality and demand. Multiple Description Coding (MDC) and Scalable video coding (SVC) are the two technical methods for quality adaptation to operate over a wide range of quality of service in heterogeneous requirements. Both are techniques of coding a video sequence in a way that multiple levels of quality can be obtained depending on the parts of the video bit stream that are received. For Scalable video coding special protection is mad for the base layer using Forward Error Protection while the streams (descriptions) in multiple description coding has been tested to simulate the advantages of diversity systems where each sub-stream has an equal probability of being correctly decoded. In this paper, the performance comparison of the two coding approaches is made by using DCT coefficients in generating the base layer and enhancement Layers for SVC and Descriptions for MDC with respect to their perspective achievements in image quality and Compression Ratio. Simulation results show that MDC out performs SVC in both cases.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014090,no
ANN observer for on-line estimation of synchronous generator dynamic parameters,2011,"This paper presents a new method for implementing Artificial Neural Network (ANN) observers in estimating and identifying synchronous generator dynamic parameters based on one statistic feature extraction from the operating data using obtained measurements from time zone information. Required data for training the neural network observers are obtained through off-line simulations of a synchronous generator operating in a one-machine-infinite-bus environment. Optimal components of the patterns are segregated from many learning patterns based on a new method called ""normalized variance"". Nominal values of parameters are used as a deviance index in the machine model. Finally, neural network is tested through online simulated measurements in order to estimate and indentify synchronous generator dynamic parameters.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014179,no
Statistical prediction modeling for software development process performance,2011,"With the advent of the information age and more intense competitions among IT companies, it is more important to assess the quality of software development processes and products by not only measuring outcomes but also predicting outcomes. On the basis of analysis and experiments about software development processes and products, a study of the process performance modeling has been conducted with statistical analyzing past and current process performance data. In this paper, we present a statistical prediction modeling for Software Development Process Performance. For predicting delivered defects effectively, a simple case study is illustrated, and several suggestions on planning and controlling management brought from this model are analyzed in detail. At the end, the conclusions with a discussion of future research consideration are pointed out in this paper.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014187,no
High-precision detection device of motor speed sensor based image recognition research,2011,"A device is developed in this paper, which is used to detect whether the motor speed sensor meets the technical specifications, and has the ability of real-time displaying the current actual speed and automatic centering. In order to achieve the high-precision automatic centering of the device, image recognition technology is used in the device, and using ARM and servo control system to detect, recognize, high precision position control and high speed completion. It provides a strong protection for the sensor of speed to enter the market, gives a strong technical basis for the motor manufacturing and motor efficiency, and offers a reliable technical support and quality supervision for the measurement of industry and quality inspection departments in China.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014273,no
A novel method of topic detection and tracking for BBS,2011,"Topic Detection and Tracking (TDT) has been studied for years, but most existing research is oriented to news web pages. Compared to news web pages, texts in Bulletin Board System (BBS) are more complicated and filled with user participation. In this paper, we propose a novel method of TDT for BBS, which mainly includes: a representation posts selection procedure based on post quality ranking and an efficient topic clustering algorithm based on candidate topic set. Experiment results demonstrate that our method significantly improves the performance of TDT in BBS environment on both accuracy and time complexity.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014309,no
Classifying feature description for software defect prediction,2011,"To overcome the limitation of numeric feature description of software modules in Software defect prediction, we propose a novel module description technology, which employs the classifying feature, rather than numerical feature to describe the software module. Firstly, we construct independent classifier on each software metric. Then the classifying results in each feature are used to represent every module. We apply two different feature classifier algorithms (based on mean criterion and minimum error rate criterion, respectively) to obtain the classifying feature description of software modules. By using the proposed description technology, the discrimination of each metric is enlarged distinctly. Also, classifying feature description is simpler compared to numeric description, which would accelerate the speed of prediction model learning and reduce the storage space of massive data sets. Experiment results on four NASA data sets (CM1, KC1, KC2 and PC1) demonstrate the effectiveness of classifying feature description, and our algorithms can significantly improve the performance of software defect prediction.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014475,no
Design of an interactive 3D medical visualization system,2011,"Recent advances in computer technologies in terms of both hardware and software have already made the development of an interactive 3D medical visualization system viable. Volume rendering as an important visualization technique can truly represent the information within the 3D datasets. However, it lacks of desirable interactivity. In order to deal with this problem, we introduce an interactive framework which is capable of the medical image visualization based on high-quality volume rendering. The framework implements various interactive functions with volume rendering techniques, such as displaying arbitrary cross sections and picking objects in 3D space. The system is evaluated by 4 continuous clinical patient data from Computed Tomography Angiograph (CTA) scans, and its performance is estimated by sophisticated surgeons.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014485,no
A novel method based on adaptive median filtering and wavelet transform in noise images,2011,"An image is often corrupted by much noise visible or invisible while being collected, acquired, coded and transmitted. Noise impairs the quality of the received image severely and may cause a big problem for further image processing. In order to improve the quality of an image, the noise must be removed when the image is preprocessed, and the important signal features should be retained as much as possible. The methodology of image denoising is studied in this paper base on median filter and wavelet theory, in the setting of additive salt and pepper noise or white Gaussian noise. In the first phase, Canny edge detection is used in edge detection of the noise image to get the basic outline; In the second phase, an ameliorative adaptive median filter(adaptive variational threshold value) is used to remove salt noise; In the third phase, Coiflet wavelet system used to remove pepper noise; at last, processing images linking to get final images.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014648,no
The research on the breakup of river ice cover prediction based on artificial neural network model,2011,"According to the urgent need of ice protection and reduction in Inner Mongolia section, the breakup of river ice cover prediction model is set up with artificial neural network theory. Focusing on the BaYanGaoLe, SanHuHeKou and TouDaoGuai, the specific breakup of river ice cover dates are forecasted, the predictive results meet ice prevention requirements, attain the prediction standards, provide references for command scheduling and decision in the Inner Mongolia section of Yellow River.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014665,no
Set diagnosis on the power station's devices by fuzzy neural network,2011,"Current situation for energy system which is consist from thermo power station faced on diagnosis of generator fault set without break down maintenance. In ageing thermo power plants, large turbo generator's retrofits or main drive to upgrade was poor reliability associated with increase maintenance cost. Introduction of higher competition in the power market and subsequent introduction of new environmental constraints in our energy system, installed plants life time extension with overall performance improvement through upgrade or retrofit of main components is today a valuable. Also, set fault diagnosis during life time and predictive maintenance can be defined as collecting information from machines as they operate to aid in making decisions about their health, repair and possible improvements in order to reach maximum reliability, before any unplanned break down. As the turbo-generator fault set occurs when sensors should be put on bearing of these to detect vibration signal for extracting fault symptoms, but the relationships between faults and fault symptoms are too complex to get enough accuracy for industry application. In this paper, a new diagnosis method based on fuzzy neural network is proposed and a fuzzy neural network system is structured by associating the fuzzy set theory with neural network technology.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014767,no
Depth map optimization based on adaptive chrominance image segmentation,2011,"The development of Free Viewpoint Video (FVV) technique is the most promising multimedia processing fields in recent years. Depth map prediction is the key method in FVV. There are many methods to estimate depth map. Among them, combined temporal and interview prediction method is the most practical which can save much data processing bandwidth, without additional hardware resources and computation time consumption. But block estimation based depth map has much block effect and prediction noises, especially at the object boundary and large continuous areas. This paper presents an adaptive chrominance image segmentation method to process the depth map prediction noises getting by combined temporal and interview prediction method. Experiment results show that this method can enhance the depth map quality efficiently with less calculation complexity, and is practical for hardware circuit realization and high performance real time processing.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014813,no
R-largest order statistics for the prediction of bursts and serious deteriorations in network traffic,2011,"Predicting bursts and serious deteriorations in Internet traffic is important. It enables service providers and users to define robust quality of service metrics to be negotiated in service level agreements (SLA). Traffic exhibits the heavy tail property for which extreme value theory is the perfect setting for the analysis and modeling. Traditionally, methods from EVT, such as block maxima and peaks over threshold were applied, each treating a different aspect of the prediction problem. In this work, the r-largest order statistics method is applied to the problem. This method is an improvement over the block maxima method and makes more efficient use of the available data by selecting the r largest values from each block to model. As expected, the quality of estimation increased with the use of this method; however, the fit diagnostics cast some doubt about the applicability of the model, possibly due to the dependence structure in the data.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014960,no
Project Management Methodologies: Are they sufficient to develop quality software,2011,"This paper considers whether the use of a project management methodology PRINCE is sufficient to achieve quality information systems. Without the additional use of effort estimation methods and the user of measure that can predict and help control quality during system development. How to make sure the management of software quality to be effective is a critical issue that software development organizations have to face. Software quality management is a series of activities to direct and control the software quality, includes establishment of the quality policy and quality goals, quality planning, quality control, quality assurance and quality improvement. Professional bodies have paid more attention to software standards. Meanwhile, many countries are participating in significant consolidation and coordination effort.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6015648,no
Automated generation of FRU devices inventory records for xTCA devices,2011,"The Advanced Telecommunications Computing Architecture (ATCA) and Micro Telecommunications Computing Architecture (Î¼TCA) standards, intended for high-performance applications, offer an array of features that are compelling from the industry use perspective, like high reliability (99,999%) or hot-swap support. The standards incorporate the Intelligent Platform Management Interface (IPMI) for the purpose o advanced diagnostics and operation control. This standard imposes support for non-volatile Field Replaceable Unit (FRU) information for specific components of an ATCA/Î¼TCA-based system, which would typically include description of a given component. The Electronic Keying (EK) mechanism is capable of using this information for ensuring more reliable cooperation of the components. The FRU Information for the ATCA/Î¼TCA implementation elements may be of sophisticated structure. This paper focuses on a software tool facilitating the process of assembling this information, the goal of which is to make it more effective and less error-prone.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6016055,no
"Virtual instrument based online monitoring, real-time detecting and automatic diagnosis management system for multi-fiber lines",2011,"Along with the popularity of the optical fiber communication, virtual instrument based online monitoring real-time detecting and automatic diagnosis management system on multi-fiber is proposed in this paper. To manage fiber lines and landmarks, simplified landmark map are presented based on the landmark list. Multi-fiber lines are online monitored, which may have the different or same parameters. When the optical power of any monitored line exceeds the set threshold, the subsystem gives a low power alarm, detecting subsystem will be started automatically to detect alarmed line. After data processing and analysis of detection results, then draws the result curve in the virtual instrument panel. User can locate plot of the curve, and zoom in event curve based on the event list. It utilizes detection results, event list, landmark list and simplified landmarks map synthetically, and presents comprehensive diagnosis conclusion of detection based on the map. To avoid fault, the system predicts fault in future through the analysis of a period of the historical detection result. This system has the advantages of excellent stability, powerful analysis, friendly interface, and convenient operation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6016391,no
Fault diagnosis of automobile engine based on support vector machine,2011,"Support vector machine (SVM) based on classification is applied for fault diagnosis of the automotive engine. The basic idea is to identify the information by using the trained SVM model to classify new fault samples. The data from the engine simulation model by AMESim software are fault features extracted, and these fault characteristic parameters have statistical property and specific physical meaning. Principal component analysis (PCA) is used to reduce the dimensions and redundancy of the data, and then these data are normalized as the input of SVM. The proposed method achieves accurate fault classification because SVM has good performance of classification and generalization ability, which is verified by the result of combining MATLAB/SIMULIK and AMESim. And the simulation results indicates that the proposed SVM based fault diagnosis method has achieved the better performance than the Artificial Neural Network, meeting the requirements of real-time diagnosis of the automotive engine.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6016423,no
Operation parameters optimization of butadiene extraction distillation based on neural network,2011,"In this paper, based on the material balance, we used Aspen plus software to make sensitivity analysis of separation performance of the key component and the operational parameters. We quantitatively analyze the influence between the component parameters and the process. At last, we used the neural network to forecast the solvent ratio on diffident C4 feed and calculated the optimization solvent. At the end of the paper, we used the actual production data to test the validity of the model. On the view of reducing the energy consumption and ensuring the product quality, the research result told us that solvent ratio could be reduced nearly one percentage point.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6017562,no
Fuzzy models with GIS for water quality diatom-indicator classification,2011,"The level of certain or set of physico-chemical parameter(s) determinates the optimal living condition for the organism. These thresholds in biology are express using categories of classes. One such category which consists from five classes is water quality (WQ) category based on Saturated Oxygen. Due to natural or human-made pressure on the ecosystem, bottom line as that we have to monitor the levels. Many of the diatoms are very sensitive to certain parameters and for these diatoms ecological indicator reference are known in the literature. But, there are many other unknown ecological references to be identified especially for the newly discovered diatoms. The diatom measurement data is noisy, usually comprises from several dimensions and also has non-linear relationship between the environmental parameters and the diatoms abundance. All these properties of the diatom dataset do not meet the assumptions of conventional statistical procedures. To overcome these problems, this research aims to a fuzzy classification approach to describe this diatom-indicator relationship. Using the approach, each diatom will be classified and together with the Geographic Information System (GIS) software, will be represented on map based on Saturated Oxygen parameter. The results from the model are verified with the known ecological reference found in the literature. Even more important, the proposed method has added some new ecological references that do not exist. This fuzzy approach can be modified for defining new WQ category classes based not only for Saturated Oxygen parameters, but also for metals and other physico-chemical parameters.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6019732,no
H.264/AVC rate control scheme with frame coding complexity optimized selection,2011,"Rate control regulates the output bit rate of a video encoder to obtain optimum visual quality within the available network bandwidth and maintain buffer fullness within a specified tolerance range. The quadratic rate-distortion model is widely adopted in H.264/AVC rate control. However the quadratic model is not precise in terms of the estimation and computation of frame coding complexity. To achieve excellent rate control performance, the paper proposes a novel rate control scheme based on the optimized selection for frame coding complexity. Simulation results indicate that, compared with JM13.2, the new rate control algorithm gets more accurate rate regulation, provides robust buffer control, and efficiently reduces frame skipping. Furthermore, the PSNR gains for QCIF sequences are high up to 0.99dB under all inter-frame encoding formats.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6020033,no
Effects of hydraulic pressure loading paths on the forming of automobile panels by hydro-mechanical deep drawing based on numerical simulation,2011,"As a part of automobile panel, outer door has a few characteristics, such as complex shape, big structure size and high quality forming needs. As a result, outer door needs multiple process and high costs by conventional drawing. In order to get rid of crack and forming deficiency in the stamping process, the numerical simulations of hydro-mechanical deep drawing (HDD) process was carried out. By using large nonlinear dynamic explicit analytical software ETA/Dynaform5.6, the effects of chamber pressure variations on the formability, and the effects of different loading paths on the wall-thickness distribution of the part were analyzed. The results indicate that under the reasonable chamber pressure loading path, the HDD can effectively control crack and forming deficiency, improve the thickness distribution uniformity, increase the die fittingness and stiffness. Complicated shapes car panel can be formed by one process with high quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6020977,no
An activity-based genetic algorithm approach to multiprocessor scheduling,2011,"In parallel and distributed computing, development of an efficient static task scheduling algorithm for directed acyclic graph (DAG) applications is an important problem. The static task scheduling problem is NP-complete in its general form. The complexity of the problem increase when task scheduling is to be done in a heterogeneous environment, where the processors in the network may not be identical and take different amounts of time to execute the same task. This paper presents an activity-based genetic task scheduling algorithm for the tasks run on the network of heterogeneous systems and represented by Directed Acyclic Graphs (DAGs). First, a list scheduling algorithm is incorporated in the generation of the initial population of a GA to represent feasible operation sequences and diminish coding space when compared to permutation representation. Second, the algorithm assigns an activity to each task which is assigned on the processor, and then the quality of the solution will be improved by adding the activity and the random probability in the crossover and mutation operator. The performance of the algorithm is illustrated by comparing with the existing effectively scheduling algorithms.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6022236,no
Improving system health monitoring with better error processing,2011,"To help identify unexpected software events and impending hardware failures, developers typically incorporate error-checking code in their software to detect and report them. Unfortunately, implementing checks with reporting capabilities that give the most useful results comes at a price. Such capabilities should report the exact nature of impending failures and additionally limit reporting to only the first occurrence of an error to prevent flooding the error log with the same message. They must report when an existing error or fault is replaced by another error of a different nature or value. They must recognize what makes occasional faults allowable and they must reset themselves upon recovery from a reported failure so the checking process can begin anew. They must also report recovery from previously reported failures that appear to have healed themselves. Since the price associated with providing all these features is limited by budget and schedule, system reliability and health monitoring often suffer. However, there are practical techniques that can simplify the effort associated with incorporating such error detection and reporting. When done properly, they can greatly improve system reliability and health monitoring by finding potentially hidden problems during development and can also greatly improve system maintainability by providing concise running descriptions of problems when things go wrong particularly when minor errors might otherwise go unnoticed. In addition, preventative maintenance can be greatly aided by applying error detection techniques to performance monitoring in the absence of errors. Many of the techniques described in this paper take advantage of simple classes to do bookkeeping tasks such as updating and tracking statistical analysis of errors and error reporting. The paper highlights several of these classes and gives examples from actual applications.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6024322,no
A novel and fast moving window based technique for transformer differential protection,2011,"In this paper, a new technique is presented to discriminate between magnetizing inrush currents and internal fault currents in power transformers. The proposed technique uses two moving windows which both of them estimate magnitude of differential current of power transformer by different methods. The first moving window method is based on full-cycle Fourier algorithm (long window) and the second one is originated from least error square method that has five sample point input data (short window). The proposed technique presents a new criterion for discrimination between inrush currents and internal fault currents using the difference between output values of the two moving windows. Obtained results demonstrate precise operation of the proposed algorithm for different conditions such as CT saturation condition. PSCAD/EMTDC software is used for evaluating performance of the proposed algorithm.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6024874,no
Voltage sag and swell compensation with DVR based on asymmetrical cascade multicell converter,2011,"This paper deals with a dynamic voltage restorer (DVR) as a solution to compensate the voltage sags and swells and to protect sensitive loads. In order to apply the DVR in the distribution systems with voltage in range of kilovolts, series converter as one of the important components of DVR should be implemented based on the multilevel converters which have the capability to handle voltage in the range of kilovolts and power of several megawatts. So, in this paper a configuration of DVR based on asymmetrical cascade multicell converter is proposed. The main property of this asymmetrical CM converter is increase in the number of output voltage levels with reduced number of switches. Also, the pre-sag compensation strategy and the proposed voltage sag/swell detection and DVR reference voltages determination methods based on synchronous reference frame (SRF) are adopted as the control system. The proposed DVR is simulated using PSCAD/EMTDC software and simulation results are presented to validate its effectiveness.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6024896,no
Numerical simulation of springback base on formability index for auto panel surface,2011,"In order to emerge the traditional measurement's shortage of auto body panel, we proposed the method based on formability index. According to the above mentioned programs and the springback defect diagnosis, we developed a CAE module for the springback defect analysis based on VC++ environment, and solved the problems which the traditional sheet metal forming CAE software can not accurately predict. And then a forming process of U-beam is simulated by applying the proposed method which shows the method we proposed is accurate, and then we introduced some methods to optimize the adjustment amount of metal flow and the stamping dies for springback. Some suggestions are given by investigating the adjustment amount and modification of the stamping die.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6026028,no
Keynote talk 2: How to build an industrial R&D center in Vietnam: A case study,2011,"Summary form only given. To be competitive in today's market, the IT industry faces many challenges in the development and maintenance of enterprise information systems. Engineering these largescaled systems efficiently requires making decisions about a number of issues. In addition, customers expectations imply continuous software delivery in predictable quality. The operation such systems demands for transparency of the software in regard to lifecycle, change and incident management as well as cost efficiency. Addressing these challenges, we learned how to benefit from traditional industries. Contrary to the fact that the IT business calls itself gladly an industry, the industrialization of software engineering in most cases moves on a rather modest level. Industrialization means not only to build a solution or product on top of managed and well-defined processes, but also to have access to structured information about the current conditions of manufacturing at any time. Comparably with test series and assembly lines of the automobile industry, each individual component and each step from the beginning of manufacturing up to the final product should be equipped with measuring points for quality and progress. Even one step further the product itself, after it has left the factory, should be able to continuously provide analytic data for diagnostic reasons. Information is automatically collected and builds the basic essentials for process control, optimization and continuous improvement of the software engineering process. This presentation shows by means of a practical experience report how AdNovum managed to build its software engineering based on a well-balanced system of processes, continuous measurement and control â€?as well as a healthy portion of pragmatism. We implemented an efficient and predictable software delivery pipeline",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6027420,no
HPLC-fingerprint study on medicinal Folium Fici Microcarpae of Guangxi,2011,"HPLC-fingerprint of Folium Fici Microcarpae of guangxi was studied in the paper. By reverse-phase high performance liquid chromatography, the herbal leaves of Ficus microcarpa taken from 10 sites in Guangxi province were analyzed. The analysis was conducted under the following conditions: an Ultimate ODS (5 Î¼m, 4.6 mm Ã— 250 mm) chromatographic column and acetonitrile-0.05% phosphoric acid solution were selected for gradient elution; detection wavelength was at 280 nm; flow rate at 0.5 mL/min; and analysis period of 120 min. Twelve chromatographic peaks were determined through similarity analysis under the chosen chromatographic conditions to identify the characteristic fingerprint peaks of Folium Fici Microcarpae. The method used to establish the fingerprints is deemed of good stability and reproducibility. Not only is it applicable for quality control of Folium Fici Microcarpae, but also an effective means for fundamental study of valid substances.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6028395,no
A Fast and Effective Control Scheme for the Dynamic Voltage Restorer,2011,"A novel control scheme for the dynamic voltage restorer (DVR) is proposed to achieve fast response and effective sag compensation capabilities. The proposed method controls the magnitude and phase angle of the injected voltage for each phase separately. Fast least error squares digital filters are used to estimate the magnitude and phase of the measured voltages. The utilized least error squares estimated filters considerably reduce the effects of noise, harmonics, and disturbances on the estimated phasor parameters. This enables the DVR to detect and compensate voltage sags accurately, under linear and nonlinear load conditions. The proposed control system does not need any phase-locked loops. It also effectively limits the magnitudes of the modulating signals to prevent overmodulation. Besides, separately controlling the injected voltage in each phase enables the DVR to regulate the negative- and zero-sequence components of the load voltage as well as the positive-sequence component. Results of the simulation studies in the PSCAD/EMTDC software environment indicate that the proposed control scheme 1) compensates balanced and unbalanced voltage sags in a very short time period, without phase jump and 2) performs satisfactorily under linear and nonlinear load conditions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6029403,no
Governance and Cost Reduction through Multi-tier Preventive Performance Tests in a Large-Scale Product Line Development,2011,"Experience has shown that maintaining software system performance in a complex product line development is a constant challenge, already achieved performance is often degraded over time because proper quality gates are rarely defined or implemented. The established practice of performance verification tests on an integrated software baseline is essential to ensure final quality of the delivered products, but is late if performance degradations already crept in. Maintenance of performance in software baselines requires an additional preventive approach. The faulty software changes that degrade performance can be identified (performance quality gates) before these changes can flow into the baseline and subsequently get rejected. This ensures that the software baseline maintains a consistent performance leading to more predictable schedules and development costs. For a complex software family involving parallel and dependent sub-projects of domain platforms and end user applications, these performance quality gates need to be established at multiple levels.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6030072,no
Realtime diagnostic prognostic solution for life cycle management of thermomechanical system,2011,Data based approach and methodology for real time diagnosis and prognosis solutions for thermomechanical systems is discussed. Coated turbine blade operation is emulated to sensor online temperature data as the real-time inputs for the software code developed. An algorithm is presented first and extended sampling based statistical hypothesis tests are used for anomaly detection tests. Paired t-test and rank sum hypotheses test are found to be appropriate for different data set combinations. Matlab statistical package is used for the software code. The algorithm and methodology work well with laboratory temperature data and seeded faults. Few limitations of the developed system code are identified.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6030610,no
Unit test case design metrics in test driven development,2011,"Testing is a validation process that determines the conformance of the software's implementation to its specification. It is an important phase in unit test case design and is even more important in object-oriented systems. We want to develop test case designing criteria that give confidence in unit testing of object-oriented system. The main aim testing can be viewed as a means of assessing the existing quality of the software to probe the software for defect and fix them. We also want to develop and executing our test case automatically since this decreases the effort (and cost) of software development cycle (maintenance), and provide re-usability in Test Driven Development framework. We believe such approach is necessary for reaching the levels of confidence required in unit testing. The main goal of this paper is to assist the developers/testers to improve the quality of the ATCUT by accurately design the test case for unit testing of object-oriented software based on the test results. A blend of unit testing assisted by the domain knowledge of the test case designer is used in this paper to improve the design of test case. This paper outlines a solution strategy for deriving Automated Test Case for Unit Testing (ATCUT) metrics from object-oriented metrics via TDD concept.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6031205,no
Software reliability growth models based on Marshall-Olkin generalized exponential families of distributions,2011,"Software has been becoming an important and inevitable tool in our modern day to day life. It finds numerous applications such as space, telecommunications technology, military, nuclear plants, air traffic and medical monitoring control. To meet the continuing demand for high quality software, an enormous multitude of Software reliability Growth models have been proposed and adopted in recent years. A new family of distributions has been introduced by incorporating an additional parameter and applied to yield a new two parameter extension of exponential distribution [1]. Parikh et al. [2] call such a family of distributions as Marshall and Olkin Generalized Exponential (MOGE) distributions and studied its inferential problems. In this paper we propose NHPP software reliability Growth models based on MOGE and validate them through different metrics using real data sets.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6031704,no
An improved model of software reliability growth under time-dependent learning effects,2011,"Over the last two decades, various software reliability growth models (SRGM) have been proposed, and there has been a gradual but marked shift in the balance between software reliability and software testing cost in recent years. Chiu and Huang (2008) provided a Software Reliability Growth Model from the Perspective of Learning Effects, which is able to reasonably describe the S-shaped and exponential-shaped types of behaviors simultaneously, and offers better performance when fitting different data with consideration of the learning effects. However, this earlier model assumes that the learning effects are constant. In contrast, this paper discusses a software reliability growth model with time-dependent learning effects.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6031707,no
A System for Nuclear Fuel Inspection Based on Ultrasonic Pulse-Echo Technique,2011,"Nuclear Pressurized Water Reactor (PWR) technology has been widely used for electric energy generation. The follow-up of the plant operation has pointed out the most important items to optimize the safety and operational conditions. The identification of nuclear fuel failures is in this context. The adoption of this operational policy is due to recognition of the detrimental impact that fuel failures have on operating cost, plant availability, and radiation exposure. In this scenario, the defect detection in rods, before fuel reloading, has become an important issue. This paper describes a prototype of an ultrasonic pulse-echo system designed to inspect failed rods (with water inside) from PWR. This system combines development of hardware (ultrasonic transducer, mechanical scanner and pulser-receiver instrumentation) as well as of software (data acquisition control, signal processing and data classification). The ultrasonic system operates at center frequency of 25 MHz and failed rod detection is based on the envelope amplitude decay of successive echoes reverberating inside the clad wall. The echoes are classified by three different methods. Two of them (Linear Fisher Discriminant and Neural Network) have presented 93% of probability to identify failed rods, which is above the current accepted level of 90%. These results suggest that a combination of a reliable data acquisition system with powerful classification methods can improve the overall performance of the ultrasonic method for failed rod detection.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6031787,no
Test-Driving Static Analysis Tools in Search of C Code Vulnerabilities,2011,"Recently, a number of tools for automated code scanning came in the limelight. Due to the significant costs associated with incorporating such a tool in the software lifecycle, it is important to know what defects are detected and how accurate and efficient the analysis is. We focus specifically on popular static analysis tools for C code defects. Existing benchmarks include the actual defects in open source programs, but they lack systematic coverage of possible code defects and the coding complexities in which they arise. We introduce a test suite implementing the discussed requirements for frequent defects selected from public catalogues. Four open source and two commercial tools are compared in terms of their effectiveness and efficiency of their detection capability. A wide range of C constructs is taken into account and appropriate metrics are computed, which show how the tools balance inherent analysis tradeoffs and efficiency. The results are useful for identifying the appropriate tool, in terms of cost-effectiveness, while the proposed methodology and test suite may be reused.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032220,no
Predicting Timing Performance of Advanced Mechatronics Control Systems,2011,"Embedded control is a key product technology differentiator for many high-tech industries, including ASML. The strong increase in complexity of embedded control systems, combined with the occurrence of late changes in control requirements, results in many timing performance problems showing up only during the integration phase. The fallout of this is extremely costly design iterations, severely threatening the time-to-market and time-to-quality constraints. This paper reports on the industrial application at ASML of the Y-chart method to attack this problem. Through the largely automated construction of executable models of a wafer scanner's mechatronics control application and platform, ASML was able to obtain high-level overview early on in the development process. The system wide insight in timing bottlenecks gained this way resulted in more than a dozen improvement proposals yielding significant performance gains. These insights also led to a new development roadmap of the mechatronics control execution platform.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032239,no
On the Consensus-Based Application of Fault Localization Techniques,2011,"A vast number of software fault localization techniques have been proposed recently with the growing realization that manual debugging is time-consuming, tedious and error-prone, and fault localization is one of the most expensive debugging activities. While some of these techniques perform better than one another on a large number of data sets, they do not do so on all data sets and therefore, the actual quality of fault localization can vary considerably by using just one technique. This paper proposes the use of a consensus-based strategy that combines the results of multiple fault localization techniques, to consistently provide high quality performance, irrespective of data set. Empirical evidence based on case studies conducted on three sets of programs (the seven programs of the Siemens suite, and the gzip and make programs) and three different fault localization techniques suggests that the consensus-based strategy holds merit and generally provides close to the best, if not the best, results. Additionally the consensus-based strategy makes use of techniques that all operate on the same set of input data, minimizing the overhead. It is also simple to include or exclude techniques from consensus, making it an easily extensible, or alternatively, tractable strategy.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032290,no
Ontology-Based Reliability Evaluation for Web Service,2011,"Reliability has become a major quality metric for Web service. However, current reliability evaluation approaches lack a formal semantic representation and the support of incomplete or uncertain information. We propose a Web service reliability ontology (WSRO) serving as a basis to characterize the knowledge of Web service. And based on WSRO, a mapping to the probability graphical model is constructed. The Web service reliability evaluation results are obtained by the causality reasoning. Some evaluation results reveal that our approach is applicable and effective.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032363,no
Software Reliability Prediction for Open Source Software Adoption Systems Based on Early Lifecycle Measurements,2011,"Various OSS(Open Source Software)s are being modified and adopted into software products with their own quality level. However, it is difficult to measure the quality of an OSS before use and to select the proper one. These difficulties come from OSS features such as a lack of bug information, unknown development schedules, and variable documentations. Conventional software reliability models are not adequate to assess the reliability of a software system in which an OSS is being adopted as a new add-on feature because the OSS can be modified while Commercial Off-The-Shelf (COTS) software cannot. This paper provides an approach to assessing the software reliability of OSS adopted software system in the early stage of a software life cycle. We identify the software factors that affect the reliability of software system using the COCOMOII modeling methodology and define the module usage as a module coupling measure. We build the fault count models using the multivariate linear regression and performed the model evaluation. Early software reliability assessment in OSS adoption helps to make an effective development and testing strategies for improving the reliability of the whole system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032366,no
Evaluating an Interactive-Predictive Paradigm on Handwriting Transcription: A Case Study and Lessons Learned,2011,"Transcribing handwritten text is a laborious task which currently is carried out manually. As the accuracy of automatic handwritten text recognizers improves, post-editing the output of these recognizers could be foreseen as a possible alternative. Alas, the state-of-the-art technology is not suitable to perform this kind of work, since current approaches are not accurate enough and the process is usually both inefficient and uncomfortable for the user. As alternative, an interactive-predictive paradigm has gained recently an increasing popularity, mainly due to promising empirical results that estimate considerable reductions of user effort. In order to assess whether these empirical results can lead indeed to actual benefits, we developed a working prototype and conducted a field study remotely. Thirteen regular computer users tested two different transcription engines through the above-mentioned prototype. We observed that the interactive-predictive version allowed to transcribe better (less errors and fewer iterations to achieve a high-quality output) in comparison to the manual engine. Additionally, participants ranked higher such an interactive-predictive system in a usability questionnaire. We describe the evaluation methodology and discuss our preliminary results. While acknowledging the known limitations of our experimentation, we conclude that the interactive-predictive paradigm is an efficient approach for transcribing handwritten text.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032407,no
Towards quantitative software reliability assessment in incremental development processes,2011,"The iterative and incremental development is becoming a major development process model in industry, and allows us for a good deal of parallelism between development and testing. In this paper we develop a quantitative software reliability assessment method in incremental development processes, based on the familiar non-homogeneous Poisson processes. More specifically, we utilize the software metrics observed in each incremental development and testing, and estimate the associated software reliability measures. In a numerical example with a real incremental developmental project data, it is shown that the estimate of software reliability with a specific model can take a realistic value, and that the reliability growth phenomenon can be observed even in the incremental development scheme.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032443,no
The impact of fault models on software robustness evaluations,2011,"Following the design and in-lab testing of software, the evaluation of its resilience to actual operational perturbations in the field is a key validation need. Software-implemented fault injection (SWIFI) is a widely used approach for evaluating the robustness of software components. Recent research [24, 18] indicates that the selection of the applied fault model has considerable influence on the results of SWIFI-based evaluations, thereby raising the question how to select appropriate fault models (i.e. that provide justified robustness evidence). This paper proposes several metrics for comparatively evaluating fault models's abilities to reveal robustness vulnerabilities. It demonstrates their application in the context of OS device drivers by investigating the influence (and relative utility) of four commonly used fault models, i.e. bit flips (in function parameters and in binaries), data type dependent parameter corruptions, and parameter fuzzing. We assess the efficiency of these models at detecting robustness vulnerabilities during the SWIFI evaluation of a real embedded operating system kernel and discuss application guidelines for our metrics alongside.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032444,no
Assessing programming language impact on development and maintenance: a study on c and c++,2011,"Billions of dollars are spent every year for building and maintaining software. To reduce these costs we must identify the key factors that lead to better software and more productive development. One such key factor, and the focus of our paper, is the choice of programming language. Existing studies that analyze the impact of choice of programming language suffer from several deficiencies with respect to methodology and the applications they consider. For example, they consider applications built by different teams in different languages, hence fail to control for developer competence, or they consider small-sized, infrequently-used, short-lived projects. We propose a novel methodology which controls for development process and developer competence, and quantifies how the choice of programming language impacts software quality and developer productivity. We conduct a study and statistical analysis on a set of long-lived, widely-used, open source projects - Firefox, Blender, VLC, and MySQL. The key novelties of our study are: (1) we only consider projects which have considerable portions of development in two languages, C and C++, and (2) a majority of developers in these projects contribute to both C and C++ code bases. We found that using C++ instead of C results in improved software quality and reduced maintenance effort, and that code bases are shifting from C to C++. Our methodology lays a solid foundation for future studies on comparative advantages of particular programming languages.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032456,no
Socio-technical developer networks: should we trust our measurements?,2011,"Software development teams must be properly structured to provide effectiv collaboration to produce quality software. Over the last several years, social network analysis (SNA) has emerged as a popular method for studying the collaboration and organization of people working in large software development teams. Researchers have been modeling networks of developers based on socio-technical connections found in software development artifacts. Using these developer networks, researchers have proposed several SNA metrics that can predict software quality factors and describe the team structure. But do SNA metrics measure what they purport to measure? The objective of this research is to investigate if SNA metrics represent socio-technical relationships by examining if developer networks can be corroborated with developer perceptions. To measure developer perceptions, we developed an online survey that is personalized to each developer of a development team based on that developer's SNA metrics. Developers answered questions about other members of the team, such as identifying their collaborators and the project experts. A total of 124 developers responded to our survey from three popular open source projects: the Linux kernel, the PHP programming language, and the Wireshark network protocol analyzer. Our results indicate that connections in the developer network are statistically associated with the collaborators whom the developers named. Our results substantiate that SNA metrics represent socio-technical relationships in open source development projects, while also clarifying how the developer network can be interpreted by researchers and practitioners.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032467,no
Dealing with noise in defect prediction,2011,"Many software defect prediction models have been built using historical defect data obtained by mining software repositories (MSR). Recent studies have discovered that data so collected contain noises because current defect collection practices are based on optional bug fix keywords or bug report links in change logs. Automatically collected defect data based on the change logs could include noises. This paper proposes approaches to deal with the noise in defect data. First, we measure the impact of noise on defect prediction models and provide guidelines for acceptable noise level. We measure noise resistant ability of two well-known defect prediction algorithms and find that in general, for large defect datasets, adding FP (false positive) or FN (false negative) noises alone does not lead to substantial performance differences. However, the prediction performance decreases significantly when the dataset contains 20%-35% of both FP and FN noises. Second, we propose a noise detection and elimination algorithm to address this problem. Our empirical study shows that our algorithm can identify noisy instances with reasonable accuracy. In addition, after eliminating the noises using our algorithm, defect prediction accuracy is improved.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032487,no
Bringing domain-specific languages to digital forensics,2011,"Digital forensics investigations often consist of analyzing large quantities of data. The software tools used for analyzing such data are constantly evolving to cope with a multiplicity of versions and variants of data formats. This process of customization is time consuming and error prone. To improve this situation we present DERRIC, a domain-specific language (DSL) for declaratively specifying data structures. This way, the specification of structure is separated from data processing. The resulting architecture encourages customization and facilitates reuse. It enables faster development through a division of labour between investigators and software engineers. We have performed an initial evaluation of DERRIC by constructing a data recovery tool. This so-called carver has been automatically derived from a declarative description of the structure of JPEG files. We compare it to existing carvers, and show it to be in the same league both with respect to recovered evidence, and runtime performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032508,no
An industrial case study on quality impact prediction for evolving service-oriented software,2011,"Systematic decision support for architectural design decisions is a major concern for software architects of evolving service-oriented systems. In practice, architects often analyse the expected performance and reliability of design alternatives based on prototypes or former experience. Model-driven prediction methods claim to uncover the tradeoffs between different alternatives quantitatively while being more cost-effective and less error-prone. However, they often suffer from weak tool support and focus on single quality attributes. Furthermore, there is limited evidence on their effectiveness based on documented industrial case studies. Thus, we have applied a novel, model-driven prediction method called Q-ImPrESS on a large-scale process control system consisting of several million lines of code from the automation domain to evaluate its evolution scenarios. This paper reports our experiences with the method and lessons learned. Benefits of Q-ImPrESS are the good architectural decision support and comprehensive tool framework, while one drawback is the time-consuming data collection.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032519,no
Topic-based defect prediction: NIER track,2011,"Defects are unavoidable in software development and fixing them is costly and resource-intensive. To build defect prediction models, researchers have investigated a number of factors related to the defect-proneness of source code, such as code complexity, change complexity, or socio-technical factors. In this paper, we propose a new approach that emphasizes on technical concerns/functionality of a system. In our approach, a software system is viewed as a collection of software artifacts that describe different technical concerns/-aspects. Those concerns are assumed to have different levels of defect-proneness, thus, cause different levels of defectproneness to the relevant software artifacts. We use topic modeling to measure the concerns in source code, and use them as the input for machine learning-based defect prediction models. Preliminary result on Eclipse JDT shows that the topic-based metrics have high correlation to the number of bugs (defect-proneness), and our topic-based defect prediction has better predictive performance than existing state-of-the-art approaches.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032555,no
Pragmatic prioritization of software quality assurance efforts,2011,"A plethora of recent work leverages historical data to help practitioners better prioritize their software quality assurance efforts. However, the adoption of this prior work in practice remains low. In our work, we identify a set of challenges that need to be addressed to make previous work on quality assurance prioritization more pragmatic. We outline four guidelines that address these challenges to make prior work on software quality assurance more pragmatic: 1) Focused Granularity (i.e., small prioritization units), 2) Timely Feedback (i.e., results can be acted on in a timely fashion), 3) Estimate Effort (i.e., estimate the time it will take to complete tasks), and 4) Evaluate Generality (i.e., evaluate findings across multiple projects and multiple domains). We present two approaches, at the code and change level, that demonstrate how prior approaches can be more pragmatic.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032601,no
Using software evolution history to facilitate development and maintenance,2011,"Much research in software engineering have been focused on improving software quality and automating the maintenance process to reduce software costs and mitigating complications associated with the evolution process. Despite all these efforts, there are still high cost and effort associated with software bugs and software maintenance, software still continues to be unreliable, and software bugs can wreak havoc on software producers and consumers alike. My dissertation aims to advance the state-of-art in software evolution research by designing tools that can measure and predict software quality and to create integrated frameworks that helps in improving software maintenance and research that involves mining software repositories.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032605,no
Test blueprint: an effective visual support for test coverage,2011,"Test coverage is about assessing the relevance of unit tests against the tested application. It is widely acknowledged that a software with a ""good"" test coverage is more robust against unanticipated execution, thus lowering the maintenance cost. However, insuring a coverage of a good quality is challenging, especially since most of the available test coverage tools do not discriminate software components that require a ""strong"" coverage from the components that require less attention from the unit tests. HAPAO is an innovative test covepage tool, implemented in the Pharo Smalltalk programming language. It employs an effective and intuitive graphical representation to visually assess the quality of the coverage. A combination of appropriate metrics and relations visually shapes methods and classes, which indicates to the programmer whether more effort on testing is required. This paper presents the essence of HAPAO using a real world case study.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032614,no
Fifth international workshop on software clones: (IWSC 2011),2011,"Software clones are identical or similar pieces of code, design or other artifacts. Clones are known to be closely related to various issues in software engineering, such as software quality, complexity, architecture, refactoring, evolution, licensing, plagiarism, and so on. Various characteristics of software systems can be uncovered through clone analysis, and system restructuring can be performed by merging clones. The goals of this workshop are to bring together researchers and practitioners from around the world to evaluate the current state of research and applications, discuss common problems, discover new opportunities for collaboration, exchange ideas, envision new areas of research and applications, and explore synergies with similarity analysis in other areas and disciplines.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032640,no
zEnergy: An open source project for power quality assessment and monitoring,2011,"In this paper, a new open source project focused on power quality assessment and monitoring for low voltage power systems is presented. Power quality (PQ) is a crucial matter for proper and reliable operation of industrial or home electrical appliances. In order to improve PQ techniques, efforts are made to develop smart sensors that can report near real-time data. Proprietary software and hardware on dedicated computers or servers processes these data and shows relevant information through tables or graphics. In this situation, interoperability, compatibility and scalability are not possible because of the lack of open protocols. In our work, we introduce zEnergy, an open source platform for computing, storing and managing all of the information generated from smart sensors. We apply the most up-to-date algorithms developed for power quality, event detection, and harmonic analysis or power metering. zEnergy makes use of cutting-edge web technologies such as HTML5, CSS3 and Javascript to provide user-friendly interaction and powerful capabilities for the analysis, measurement and monitoring of power systems. All software used in our work is open source, running on Linux.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6036474,no
Modular Fault Injector for Multiple Fault Dependability and Security Evaluations,2011,"The increasing level of integration and decreasing size of circuit elements leads to greater probabilities of operational faults. More sensible electronic devices are also more prone to external influences by energizing radiation. Additionally not only natural causes of faults are a concern of today's chip designers. Especially smart cards are exposed to complex attacks through which an adversary tries to extract knowledge from a secured system by putting it into an undefined state. These problems make it increasingly necessary to test a new design for its fault robustness. Several previous publications propose the usage of single bit injection platforms, but the limited impact of these campaigns might not be the right choice to provide a wide fault attack coverage. This paper first introduces a new in-system fault injection strategy for automatic test pattern injection. Secondly, an approach is presented that provides an abstraction of the internal fault injection structures to a more generic high level view. Through this abstraction it is possible to support the task separation of design and test-engineers and to enable the emulation of physical attacks on circuit level. The controller's generalized interface provides the ability to use the developed controller on different systems using the same bus system. The high level of abstraction is combinable with the advantage of high performance autonomous emulations on high end FPGA-platforms.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6037460,no
Compatibility Study of Compile-Time Optimizations for Power and Reliability,2011,"Historically compiler optimizations have been used mainly for improving embedded systems performance. However, for a wide range of today's power restricted, battery operated embedded devices, power consumption becomes a crucial problem that is addressed by modern compilers. Biomedical implants are one good example of such embedded systems. In addition to power, such devices need to also satisfy high reliability levels. Therefore, performance, power and reliability optimizations should all be considered while designing and programming implantable systems. Various software optimizations, e.g., during compilation, can provide the necessary means to achieve this goal. Additionally the system can be configured to trade-off between the above three factors based on the specific application requirements. In this paper we categorize previous works on compiler optimizations for low power and fault tolerance. Our study considers differences in instruction count and memory overhead, fault coverage and hardware modifications. Finally, the compatibility of different methods from both optimization classes is assessed. Five compatible pairs that can be combined with few or no limitations have been identified.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6037493,no
Measurement system of Î± Î² surface radioactive contamination based on Virtual Training,2011,"A Virtual Training System of Î± Î² surface radioactive contamination measurement is constructed based on Virtual Reality. Firstly, 3-d geometric models about radioactive environment, radioactive environment of surface device, Î± Î² detector, surface contamination instrument are constructed, the general radioactive law based on face radioactive source is presented. These models are driving in general program platform using human-computer interaction technology. Furthermore, there are several functions about database sharing, fault diagnosis, examining evaluation and on-time help which realize the whole system virtual training. Program show this training system avoid â€œnuclear fearâ€?psychology and improve staff's training effect.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6037772,no
Design of nuclear measuring instrument fault diagnosis system based on circuit characteristic test,2011,"The circuits of nuclear measuring instruments are very complicated, and the testing and fault inspection of the nuclear measuring instrument system is difficult. To solve this problem, the fault diagnosis system was designed by combing circuit characteristic test technology with virtual instrument, virtual testing, test data analysis and data management technology. The method based on circuit characteristic test, which is comparing the characteristic curves of tested circuit with the one of the corresponding normal circuit, is applied to the fault diagnosis of nuclear measuring instrument. The fault diagnosis system consists of hardware part and software part. The hardware part includes computer, data acquisition module, arbitrary waveform generator (AWG) module, interface circuit module, electric relay module, etc; the software is made up of computer management software module, control software module and testing functional software module and so on. By the fault diagnosis system, it can test the circuit characteristic of any circuit module of the nuclear measuring instruments, diagnoses and finds out the faulted parts of the nuclear measuring instruments quickly and shows the diagnosing results on the display.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6037965,no
The study on stability and reliability of the nuclear detector,2011,"Because of measurement environment and its own performance defect, the poor stability of nuclear radiation detector used on portable nuclear instrument leads to spectrum drift and decrease of energy resolution, which causes reduction of the detect efficiency. This results in lower measurement precision and accuracy of the system. Generally, the stabilization method based on hardware and software was applied. It is difficult to solve the problem about Spectrum drift caused by multiple factors. To slove the nonlinear problems caused by interference source, Multi-sensor Data Fusion Technique is adopt to design the intelligent nuclear detector with the Self-compensation Technique. It shows that the results of the project can improve the stability and reliability of the fieldwork measurement in system of the nuclear instruments. Except that it will improve the adaptive ability of the nuclear instruments to measuring environment.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6038005,no
Optimal Model-Based Policies for Component Migration of Mobile Cloud Services,2011,"Two recent trends are major motivators for service component migration: the upcoming use of cloud-based services and the increasing number of mobile users accessing Internet-based services via wireless networks. While cloud-based services target the vision of Software as a Service, where services are ubiquitously available, mobile use leads to varying connectivity properties. In spite of temporary weak connections and even disconnections, services should remain operational. This paper investigates service component migration between the mobile client and the infrastructure-based cloud as a means to avoid service failures and improve service performance. Hereby, migration decisions are controlled by policies. To investigate component migration performance, an analytical Markov model is introduced. The proposed model uses a two-phased approach to compute the probability to finish within a deadline for a given reconfiguration policy. The model itself can be used to determine the optimal policy and to quantify the gain that is obtained via reconfiguration. Numerical results from the analytic model show the benefit of reconfigurations and the impact of different reconfigurations applied to three service types, as immediate reconfigurations are in many cases not optimal, a threshold on time before reconfiguration can take place is introduced to control reconfiguration.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6038602,no
Real-time ampacity and ground clearance software for integration into smart grid technology,2011,"Output from a real-time sag, tension and ampacity program was compared with measurements collected on an outdoor test span. The test site included a laser range-finder, load cells and weather station. A fiber optic distributed temperature sensing system was routed along the conductor and thermocouples were attached to the conductor's surface. Nearly 40 million data points were statistically compared with the computer output. The program provided results with a 95% confidence interval for conductor temperatures within Â±10Â°C and sags within Â±0.3m for a conductor temperature of 75Â°C. Test data were also used to determine the accuracy of the IEEE Standard 738 models. The computer program and the Standard 738 transient model gave comparable temperatures for temperatures up to 160Â°C. Measured temperatures were used to estimate the radial and axial temperature gradients in the ACSR conductor. The effect of insulators and instrumentation attached to the conductor on the local conductor temperature was determined. The real-time rating program is an alternative to installing instrumentation on the conductor for measuring tension, sag or temperature. The program avoids the problems of installing and maintaining expensive instrumentation on the conductor, and it will provide accurate information on the conductor's temperature and ground clearance in real-time.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6038884,no
The Study of OFDM ICI Cancellation Schemes in 2.4 GHz Frequency Band Using Software Defined Radio,2011,"In Orthogonal Frequency Division Multiplexing (OFDM), frequency offset is a common problem that causes inter-carrier-interference (ICI) that degrades the quality of the transmitted signal. Many theoretical studies of the different ICI-cancellation schemes have been reported earlier by many authors. The need for experimental verification of the theoretically predicted results in 2.4 GHz frequency band is important. One of most widely used systems is Wi-Fi (IEE 802.11b) that makes use of this frequency band for short range wireless communication with throughput as high as 11 Mbps. In this work, several new ICI cancellation schemes have been tested in 2.4 GHz frequency using open source Software Defined Radio (SDR) namely GNU Radio. The GNU Radio system used in the experiment had two Universal Software Radio Peripheral (USRP N210) modules connected to a computer. Both the USRP units had one-daughterboard (XCVR2450) each for transmission and reception of radio signals. The input data to the USRP was prepared in compliance with IEEE-802.11b specification. The experimental results were compared with the theoretical results of the new Inter-Carrier Interference (ICI) cancellation schemes. The comparison of the results revealed that the new schemes are suitable for high performance transmission. The results of this paper open up new opportunities of using OFDM in heavily congested 2.4 GHz and 5 GHz bands (WiFi5: IEEE 802.a) for error free data transmission. The schemes also can be used in other frequencies where channels are heavily congested.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6040059,no
An Online Fault Management Method for Live Media Applications,2011,"It is an important task to provide the QoS-insured live media application in dealing with the fault effectively. The paper suggests a classification method to perform online fault management in the Live Media Service Grid (LMSN), which adopts online fault management, and can achieve low-overhead online learning, and adaptively adjust measurement sampling rates based on the states of monitored components, and maintain a limited size of historical training data to achieve efficient real-time fault control. The method can achieve more efficient fault management than conventional reactive and proactive approaches.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6040138,no
A Novel Multipath Routing Protocol for MANETs,2011,"This paper proposes a novel multipath routing protocol for MANETs. The proposed protocol is a variant of the single path AODV routing protocol. The proposed multipath routing protocol establishes node-disjoint paths that have the lowest delays based on the interaction of many factors from different layers. Other delay aware MANETs routing protocols don't consider the projected contribution of the source node that is requesting a path into the total network load. The implication is that end to end delay obtained through the RREQ is not accurate any more. On the contrary of its predecessors, the proposed protocol takes into consideration the projected contribution of the source node into the computation of end to end delay. To obtain an accurate estimate of path delay, the proposed multipath routing protocol employs cross-layer communications across three layers; PHY, MAC and Routing layers to achieve link and channel-awareness and creates an update packet to keep the up to date status of the paths in terms of lowest delay. The performance of the proposed protocol investigated and compared against the single path AODV and multipath AOMDV protocols through simulation using OPNET. Results have shown that our multipath routing protocol outperforms both protocols in terms of average throughput, end to end delay and packet dropped.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6040394,no
A New Face Detection Method with GA-BP Neural Network,2011,"In this paper, the BP neural network improved by the genetic algorithm (GA) is applied to the problem of human face detection. GA is used to optimize the initial weights of the BP neural network to make full use of its global optimization and local accurate searching of the BP algorithm. Matlab Software and its neural network toolbox are used to simulate and compute. The experiment results show that the GA-BP neural network has a good performance for face detection. Furthermore, compared with the conventional BP algorithm, the GA-BP learning algorithm has more rapid convergence and better assessment accuracy of detecting quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6040617,no
A self-healing architecture for web services based on failure prediction and a multi agent system,2011,"Failures during web service execution may depend on a wide variety of causes. One of those is loss of Quality of Service (QoS). Failures during web service execution impose heavy costs on services-oriented architecture (SOA). In this paper, we seek to achieve a self-healing architecture to reduce failures in web services. We believe that failure prediction prevents the occurrence of failures and enhances the performance of SOA. The proposed architecture consists of three agents: Monitoring, Diagnosis and Repair. Monitoring agent measures quality parameters in communication level and predicts future values of quality parameter by Time Series Forecasting (TSF) with the help of Neural Network (NN). Diagnosis agent analyzes current and future QoS parameters values for diagnose web service failures. Based on its algorithm, the Diagnosis agent detects failures and faults in web services executions. Repair agent manages repair actions by using Selection agent.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6041420,no
TMM Appraisal Assistant Tool,2011,"Software testing is an important component in the software development life cycle, which leads to have the high quality of software product. Therefore, software industry has focused on improving the testing process for better performance. The Testing Maturity Model (TMM) is one choice to apply for improving the testing process. It guides organization about framework of software testing. The TMM Assessment Model (TMM-AM) is a test process assessment model following the TMM. The TMM-AM consists of processes to assess test capability of organizations. Currently, each organization has various limitations such as cost, effort, time, and know-how. The assessment process lacks of tools to be performed. How to help them to improve their testing process is our key point. This paper proposes a supporting tool based on the TMM-AM which each organization can assess its testing process by itself. The tool can identify test maturity level for an organization and suggest procedures to reach its goal.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6041564,no
Advances in noise radar design,2011,"Applicability of Noise Radar Technology for design of advanced radar sensors has been shown. Pulse-coherent mode and high quality SAR imaging is realized with the help of Noise Radar. Software defined Noise Radar and stepped-delay concept were also implemented. Noise Radar Technology addresses the increasing performance demands placed on both military and civilian radar systems by enhancing their reliable operation in congested, unfriendly or hostile environments without performance degradations caused by electromagnetic interference. We have shown that part of these requirements may be met already today and we hope that our further research will show its new capabilities in design of advanced radar.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6042159,no
Next-generation massively parallel short-read mapping on FPGAs,2011,"The mapping of DNA sequences to huge genome databases is an essential analysis task in modern molecular biology. Having linearized reference genomes available, the alignment of short DNA reads obtained from the sequencing of an individual genome against such a database provides a powerful diagnostic and analysis tool. In essence, this task amounts to a simple string search tolerating a certain number of mismatches to account for the diversity of individuals. The complexity of this process arises from the sheer size of the reference genome. It is further amplified by current next-generation sequencing technologies, which produce a huge number of increasingly short reads. These short reads hurt established alignment heuristics like BLAST severely. This paper proposes an FPGA-based custom computation, which performs the alignment of short DNA reads in a timely manner by the use of tremendous concurrency for reasonable costs. The special measures to achieve an extremely efficient and compact mapping of the computation to a Xilinx FPGA architecture are described. The presented approach also surpasses all software heuristics in the quality of its results. It guarantees to find all alignment locations of a read in the database while also allowing a freely adjustable character mismatch threshold. On the contrary, advanced fast alignment heuristics like Bowtie and Maq can only tolerate small mismatch maximums with a quick deterioration of the probability to detect existing valid alignments. The performance comparison with these widely used software tools also demonstrates that the proposed FPGA computation achieves its guaranteed exact results in very competitive time.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6043268,no
Design of a high performance FPGA based fault injector for real-time safety-critical systems,2011,"Fault injection methods have long been used to assess fault tolerance and safety. However, many conventional fault injection methods face significant shortcomings, which hinder their ability to execute fault injections on target real-time safety-critical systems. We demonstrate a novel fault injection system implemented on a commercial Field-Programmable Gate Array board. The fault injector is unobtrusive to the target system as it utilizes only standardized On-Chip-Debugger (OCD) interfaces present on most current processors. This effort resulted in faults being injected orders of magnitude faster than by utilizing a commercial OCD debugger, while incorporating novel features such as concurrent injection of faults into distinct target processors. The effectiveness of this high performance fault injector was successfully demonstrated on a tightly synchronized commercial real-time safety-critical system used in nuclear power applications.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6043278,no
Smarter Architecture & Engineering: Game changer for requirements management: A position paper,2011,"This paper addresses the challenges of large commercial and government that struggle with requirements understanding, increasing delivery quality and estimating costs. The paper discusses Smarter Architecture & Engineering (SmarterAE) as an approach that applies scrutiny, metrics and analysis to architecture, requirements management and engineering as is historically given to development, testing and operations. SmarterAE facilitates business agility by capturing the essence of the enterprise business into an actionable business architecture. It treats IT architecture, requirements, engineering as key business processes and accelerates acceptance among users, business stakeholders, development, testing and operations. The approach is based on Best Practices and encourages asset reuse and management.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6043935,no
Networked fault detection of nonlinear systems,2011,"This paper addresses Fault Detection (FD) problem of a class of nonlinear systems which are monitored via the communications networks. A sufficient condition is derived which guarantees exponential mean-square stability of the proposed nonlinear NFD systems in the presence of packet drop, quantization error and unwanted exogenous inputs such as disturbance and noise. A Linear Matrix Inequality (LMI) is obtained for the design of the fault detection filter parameters. Finally, the effectiveness of the proposed NFD technique is extensively assessed by using an experimental testbed that has been built for performance evaluation of such systems with the use of IEEE 802.15.4 Wireless Sensor Networks (WSNs) technology. An algorithm is presented to handle floating point calculus when connecting the WSNs to the engineering design softwares such as Matlab.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6044434,no
Software/Hardware Framework for Generating Parallel Long-Period Random Numbers Using the WELL Method,2011,"The Well Equidistributed Long-period Linear (WELL) algorithm is proven to have better characteristics than the Mersenne Twister (MT), one of the most widely used long-period pseudo-random number generators (PRNGs). In this paper, we propose a hardware architecture for efficient implementation of WELL. Our design achieves a throughput of 1 sample-per-cycle and runs as fast as 449.4 MHz on a Xilinx XC6VLX240T FPGA. This performance is 7.6-fold faster than a dedicated software implementation, and is comparable to a MT hardware generator built on the same device. It takes up 633 LUTs, 537 Flip-Flops and 4 BRAMs, which is only 0.5% of the device. Furthermore, we design a software/hardware framework that is capable of dividing the WELL stream into an arbitrary number of independent parallel sub-streams. With support from software, this framework can obtain speedup roughly proportional to the number of parallel cores. The quality of the random numbers generated by our design is verified by the standard statistical test suites Diehard and TestU01. We also apply our framework to a Monte-Carlo simulation for estimating p. Experimental results verify the correctness of our framework as well as the better characteristics of the WELL algorithm.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6044793,no
Detecting and diagnosing application misbehaviors in â€˜on-demandâ€?virtual computing infrastructures,2011,"Numerous automated anomaly detection and application performance modeling and management tools are available to detect and diagnose faulty application behavior. However, these tools have limited utility in `on-demand' virtual computing infrastructures because of the increased tendencies for the applications in virtual machines to migrate across un-comparable hosts in virtualized environments and the unusually long latency associated with the training phase. The relocation of the application subsequent to the training phase renders the already collected data meaningless and the tools need to re-initiate the learning process on the new host afresh. Further, data on several metrics need to be correlated and analyzed in real time to infer application behavior. The multivariate nature of this problem makes detection and diagnosis of faults in real time all the more challenging as any suggested approach must be scalable. In this paper, we provide an overview of a system architecture for detecting and diagnosing anomalous application behaviors even as applications migrate from one host to another and discuss a scalable approach based on Hotelling's T<sup>2</sup> statistic and MYT decomposition. We show that unlike existing methods, the computations in the proposed fault detection and diagnosis method is parallelizable and hence scalable.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6045060,no
Calibration and validation of software performance models for pedestrian detection systems,2011,"In recent years, road vehicles have seen a tremendous increase on driver assistance systems like lane departure warning, traffic sign recognition, or pedestrian detection. The development of efficient and cost-effective electronic control units that meet the necessary real-time performance for these systems is a complex challenge. Often, Electronic System-Level design tackles the challenge by simulation-based performance evaluation, although, the quality of system-level performance simulation approaches is not yet evaluated in detail. In this paper, we present the calibration and validation of a system-level performance simulation model. For evaluation, an automotive pedestrian detection algorithm is studied. Especially the varying number of pedestrians has a significant impact to the system performance and makes the prediction of execution time difficult. As test cases we used typical sequences and corner cases recorded by an experimental car. Our evaluation results indicate that prediction of execution times with an average error of 3.1% and a maximum error of 7.9% can be achieved. Thereby, simulated and measured execution times of a software implementation are compared.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6045460,no
System-Level Online Power Estimation Using an On-Chip Bus Performance Monitoring Unit,2011,"Quality power estimation is a basis of efficient power management of electronic systems. Indirect power measurement, such as power estimation using a CPU performance monitoring unit (PMU), is widely used for its low cost and area overheads. However, the existing CPU PMUs only monitor the core and cache activities, which result in a significant accuracy limitation in the system-wide power estimation including off-chip memory devices. In this paper, we propose an on-chip bus (OCB) PMU that directly captures on-chip and off-chip component activities by snooping the OCB. The OCB PMU stores the activity information in separate counters, and online software converts counter values into actual power values with simple first-order linear power models. We also introduce an optimization algorithm that minimizes the energy model to reduce the number of counters in the OCB PMU. We compare the accuracy of the power estimation using the proposed OCB PMU with real hardware measurement and cycle-accurate system-level power estimation, and demonstrate high estimation accuracy compared with CPU PMU-based estimation method.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6046165,no
A Framework to Manage Knowledge from Defect Resolution Process,2011,"This paper presents a framework for the management, the processing and the reuse, of information relative to defects. This framework is based on the fact that each defect triggers a resolution process in which information about the detected incident (i.e. the problem) and about the applied protocol to resolve it (i.e. the solution) is collected. These different types of information are the cornerstone of the optimization of corrective and preventive processes for new defects. Experimentations show that our prototype provides a very satisfactory quality of results with good performances.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6046949,no
A Discrete Event Simulation Model to Evaluate Changes to a Software Project Delivery Process,2011,"Process simulation modeling has been successfully applied to software development processes to support management decisions on issues related to process management and process improvement. In this work, we develop a discrete event simulation model to study an IT department's project delivery process and estimate the effect on its performance after applying a change to it. We focus on three important performance indicators of project management processes: cost, time and quality. The model uses statistical data analysis techniques and concepts of earned value management to simulate the performance of process activities, model the linkages between them, as well as capture the uncertainty that exists in real-life software processes. Design of experiments is used to organize the experiments and evaluate the results. Finally, two use case scenarios are presented to demonstrate how the simulation model can support decision making during a process improvement initiative. The project delivery process of a major mobile telecommunications provider was used as a template to design the simulation, calibrate parameters, and test the validity of the model.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6046962,no
Virtual Machine Provisioning Based on Analytical Performance and QoS in Cloud Computing Environments,2011,"Cloud computing is the latest computing paradigm that delivers IT resources as services in which users are free from the burden of worrying about the low-level implementation or system administration details. However, there are significant problems that exist with regard to efficient provisioning and delivery of applications using Cloud-based IT resources. These barriers concern various levels such as workload modeling, virtualization, performance modeling, deployment, and monitoring of applications on virtualized IT resources. If these problems can be solved, then applications can operate more efficiently, with reduced financial and environmental costs, reduced under-utilization of resources, and better performance at times of peak load. In this paper, we present a provisioning technique that automatically adapts to workload changes related to applications for facilitating the adaptive management of system and offering end-users guaranteed Quality of Services (QoS) in large, autonomous, and highly dynamic environments. We model the behavior and performance of applications and Cloud-based IT resources to adaptively serve end-user requests. To improve the efficiency of the system, we use analytical performance (queueing network system model) and workload information to supply intelligent input about system requirements to an application provisioner with limited information about the physical infrastructure. Our simulation-based experimental results using production workload models indicate that the proposed provisioning technique detects changes in workload intensity (arrival pattern, resource demands) that occur over time and allocates multiple virtualized IT resources accordingly to achieve application QoS targets.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6047198,no
Safe software processing by concurrent execution in a real-time operating system,2011,"The requirements for safety-related software systems increases rapidly. To detect arbitrary hardware faults, there are applicable coding mechanism, that add redundancy to the software. In this way it is possible to replace conventional multi-channel hardware and so reduce costs. Arithmetic codes are one possibility of coded processing and are used in this approach. A further approach to increase fault tolerance is the multiple execution of certain critical parts of software. This kind of time redundancy is easily realized by the parallel processing in an operating system. Faults in the program flow can be monitored. No special compilers, that insert additional generated code into the existing program, are required. The usage of multi-core processors would further increase the performance of such multi-channel software systems. In this paper we present the approach of program flow monitoring combined with coded processing, which is encapsulated in a library of coded data types. The program flow monitoring is indirectly realized by means of an operating system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6049061,no
On human analyst performance in assisted requirements tracing: Statistical analysis,2011,"Assisted requirements tracing is a process in which a human analyst validates candidate traces produced by an automated requirements tracing method or tool. The assisted requirements tracing process splits the difference between the commonly applied time-consuming, tedious, and error-prone manual tracing and the automated requirements tracing procedures that are a focal point of academic studies. In fact, in software assurance scenarios, assisted requirements tracing is the only way in which tracing can be at least partially automated. In this paper, we present the results of an extensive 12 month study of assisted tracing, conducted using three different tracing processes at two different sites. We describe the information collected about each study participant and their work on the tracing task, and apply statistical analysis to study which factors have the largest effect on the quality of the final trace.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6051649,no
Fast 3-D fingertip reconstruction using a single two-view structured light acquisition,2011,"Current contactless fingertip recognition systems based on three-dimensional finger models mostly use multiple views (N >; 2) or structured light illumination with multiple patterns projected over a period of time. In this paper, we present a novel methodology able to obtain a fast and accurate three-dimensional reconstruction of the fingertip by using a single two-view acquisition and a static projected pattern. The acquisition setup is less constrained than the ones proposed in the literature and requires only that the finger is placed according to the depth of focus of the cameras, and in the overlapping field of views. The obtained pairs of images are processed in order to extract the information related to the fingertip and the projected pattern. The projected pattern permits to extract a set of reference points in the two images, which are then matched by using a correlation approach. The information related to a previous calibration of the cameras is then used in order to estimate the finger model, and one input image is wrapped on the resulting three-dimensional model, obtaining a three-dimensional pattern with a limited distortion of the ridges. In order to obtain data that can be treated by traditional algorithms, the obtained three-dimensional models are then unwrapped into bidimensional images. The quality of the unwrapped images is evaluated by using a software designed for contact-based fingerprint images. The obtained results show that the methodology is feasible and a realistic three-dimensional reconstruction can be achieved with few constraints. These results also show that the fingertip models computed by using our approach can be processed by both specific three-dimensional matching algorithms and traditional matching approaches. We also compared the results with the ones obtained without using structured light techniques, showing that the use of a projector achieves a faster and more accurate fingertip reconstruction.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6053684,no
TrendTV: An architecture for automatic change of TV channels based on social networks with multiple device support,2011,"In this work, we present the TrendTV architecture, a set of software layers that links various TV show viewers, producing a personalized recommendation system. In this architecture is possible to indicate the quality of a TV show in real time through the interaction between viewers using a social network that can be accessed through several different ways. Associated with a personalized Electronic Programming Guide, this social network allows the viewer to perform filtering on a particular subject from the indication made by other viewers through the interactivity over the Web, interactive TV or through a mobile device. The result is a dynamic database containing the classification of several TV programs built from that architecture, and an application that change automatically to the best channel at the moment.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6053843,no
On performance of combining methods for three-node half-duplex cooperative diversity network,2011,"We analysis the performance of the ad-hoc network with a base station, a mobile and a third station acting as a relay. Three combining methods for the Amplify-and-Forward (AF) protocol and the Decode-and-Forward (DF) protocol are compared. Simulations indicate that the Amplifyand-Forward (AF) protocol beats the Decode-and-Forward (DF) protocol under all these three combining methods. To combine the incoming signals the channel quality should be estimated as well as possible, more estimation accuracy requires more resource. A very simple combining method can obtain the performance compared with that by optimal combining methods approximately. At the same time, all three combining methods for both diversity protocols can achieve the maximum diversity order.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6057427,no
Automatic measurement of electrical parameters of signal relays,2011,"The manufacturing process of Metal to Carbon relays used in railway signaling systems for configuring various circuits of signals / points / track circuits etc. consists of seven phases from raw material to finished goods. To ensure in-process quality, the electrical parameters are measured manually after each stage. Manual measurement process is tedious, error prone and involves lot of time, effort and manpower. Besides, it is susceptible to manipulation and may lead to inferior quality products being passed, either due to deliberation or due to malefic intentions. Due to erroneous measurement of electrical parameters, the functional reliability of relays is adversely affected. To enhance the trustworthiness of measurement of electrical parameters & to make the process faster, an automated measurement system having proprietary application software and a testing jig attachment has been developed. When the relay is fixed on the testing jig, the software scans all the relay contacts and measures all the electrical parameters viz. operating voltage / current, contact resistance, release voltage / current, coil resistance etc. The results are displayed on the computer screen and stored in a database file.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6057433,no
A three-dimensional reconstruction system based on panoramic vision and two-dimensional ranging laser,2011,"With respect to the limited field of view in the visual environment sensory perceptual system of the traditional three-dimensional reconstruction system, as well as such defects as high price, big size, cumbersome shape and low rate of imaging, a three-dimensional reconstruction system is designed based on panoramic vision and a two-dimensional ranging laser. By adopting the linear laser technology, a non-contact hardware system of laser scanning and measuring system is constructed based on panoramic vision, so as to achieve real-time scanning of objects; by adopting the corner detection technology to abstract image features, and to perform image matching and camera calibration, a three-dimensional model of objects is constructed. System software tests developed in the programming environment of Visual C++ show that this system could realize an excellent three-dimensional reconstruction based on panoramic vision and two-dimensional ranging laser.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6058090,no
Modelling 3D camera movement for vibration characterisation and multiple object identification with application to lighting assessment,2011,"Utilising cameras as a means to survey the surrounding environment is becoming increasingly popular in a number of different research areas and applications. Central to using camera sensors as input to a vision system, is the need to be able to manipulate and process the information captured in these images. One such application, is the use of cameras to monitor the quality of airport landing lighting at aerodromes where a camera is placed inside an aircraft and used to record images of the lighting pattern during the landing phase of a flight. The images are processed to determine a performance metric. This requires the development of custom software for the localisation and identification of luminaires within the image data. However, because of the necessity to keep airport operations functioning as efficiently as possible, it is difficult to collect enough image data to develop, test and validate any developed software. In this paper, we present a technique to model a virtual landing lighting pattern. A mathematical model is postulated which represents the glide path of the aircraft including random deviations from the expected path. A morphological method has been developed to localise and track the luminaires under different operating conditions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6058530,no
Large-Scale Simulator for Global Data Infrastructure Optimization,2011,"IT infrastructures in global corporations are appropriately compared with nervous systems, in which body parts (interconnected datacenters) exchange signals (request responses) in order to coordinate actions (data visualization and manipulation). A priori inoffensive perturbations in the operation of the system or the elements composing the infrastructure can lead to catastrophic consequences. Downtime disables the capability of clients reaching the latest versions of the data and/or propagating their individual contributions to other clients, potentially costing millions of dollars to the organization affected. The imperative need of guaranteeing the proper functioning of the system not only forces to pay particular attention to network outages, hot-objects or application defects, but also slows down the deployment of new capabilities, features and equipment upgrades. Under these circumstances, decision cycles for these modifications can be extremely conservative, and be prolonged for years, involving multiple authorities across departments of the organization. Frequently, the solutions adopted are years behind state-of-the art technologies or phased out compared to leading research on the IT infrastructure field. In this paper, the utilization of a large-scale data infrastructure simulator is proposed, in order to evaluate the impact of "" what if"" scenarios on the performance, availability and reliability of the system. The goal is to provide data center operators a tool that allows understanding and predicting the consequences of the deployment of new network topologies, hardware configurations or software applications in a global data infrastructure, without affecting the service. The simulator was constructed using a multi-layered approach, providing a granularity down to the individual server component and client action, and was validated against a downscaled version of the data infrastructure of a Fortune 500 company.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6061065,no
The Fading Boundary between Development Time and Run Time,2011,"Summary form only given. Modern software applications are often embedded in highly dynamic contexts. Changes may occur in the requirements, in the behavior of the environment in which the application is embedded, in the usage profiles that characterize interactive aspects. Changes are difficult to predict and anticipate, and are out of control of the application. Their occurrence, however, may be disruptive, and therefore the software must also change accordingly. In many cases, changes to the software cannot be handled off-line, but require the software to self react by adapting its behavior dynamically, in order to continue to ensure the required quality of service. The big challenge in front of us is how to achieve the necessary degrees of flexibility and dynamism required in this setting without compromising dependability of the applications. To achieve dependability, a software engineering paradigm shift is needed. The traditional focus on quality, verification, models, and model transformations must extend from development time to run time. Not only software development environments (SDEs) are important for the software engineer to develop better software. Feature-full Software Run-time Environments (SREs) are also key. SREs must be populated by a wealth of functionalities that support on-line monitoring of the environment, inferring significant changes through machine learning methods, keeping models alive and updating them accordingly, reasoning on models about requirements satisfaction after changes occur, and triggering model-driven self-adaptive reactions, if necessary. In essence, self adaptation must be grounded on the firm foundations provided by formal methods and tools in a seamless SDE SRE setting. The talk discusses these concepts by focusing on non-functional requirements-reliability and performance-that can be expressed in quantitative probabilistic requirements. In particular, it shows how probabilistic model checking can help reasoning about re- - quirements satisfaction and how it can be made run-time efficient. The talk reports on some results of research developed within the SMScom project, funded by the European Commission, Programme IDEAS-ERC, Project 227977 (http://www.erc-smscom.org/).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6061095,no
High Performance Dense Linear System Solver with Soft Error Resilience,2011,"As the scale of modern high end computing systems continues to grow rapidly, system failure has become an issue that requires a better solution than the commonly used scheme of checkpoint and restart (C/R). While hard errors have been studied extensively over the years, soft errors are still under-studied especially for modern HPC systems, and in some scientific applications C/R is not applicable for soft error at all due to error propagation and lack of error awareness. In this work, we propose an algorithm based fault tolerance (ABFT) for high performance dense linear system solver with soft error resilience. By adapting a mathematical model that treats soft error during LU factorization as rank-one perturbation, the solution of Ax=b can be recovered with the Sherman-Morrison formula. Our contribution includes extending error model from Gaussian elimination and pair wise pivoting to LU with partial pivoting, and we provide a practical numerical bound for error detection and a scalable check pointing algorithm to protect the left factor that is needed for recovering x from soft error. Experimental results on cluster systems with ScaLAPACK show that the fault tolerance functionality adds little overhead to the linear system solving and scales well on such systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6061145,no
Reliable software for unreliable hardware: Embedded code generation aiming at reliability,2011,"A compilation technique for reliability-aware software transformations is presented. An instruction-level reliability estimation technique quantifies the effects of hardware-level faults at the instruction-level while considering spatial and temporal vulnerabilities. It bridges the gap between hardware - where faults occur according to our fault model - and software (the abstraction level where we aim to increase reliability). For a given tolerable performance overhead, an optimization algorithm compiles an application software with respect to a tradeoff between performance and reliability. Compared to performance-optimized compilation, our method incurs 60%-80% lower application failures, averaged over various fault injection scenarios and fault rates.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062295,no
Feature Selection of Imbalanced Gene Expression Microarray Data,2011,"Gene expression data is a very complex data set characterised by abundant numbers of features but with a low number of observations. However, only a small number of these features are relevant to an outcome of interest. With this kind of data set, feature selection becomes a real prerequisite. This paper proposes a methodology for feature selection for an imbalanced leukaemia gene expression data based on random forest algorithm. It presents the importance of feature selection in terms of reducing the number of features, enhancing the quality of machine learning and providing better understanding for biologists in diagnosis and prediction. Algorithms are presented to show the methodology and strategy for feature selection taking care to avoid over fitting. Moreover, experiments are done using imbalanced Leukaemia gene expression data and special measurement is used to evaluate the quality of feature selection and performance of classification.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6063547,no
Diagnostic system for on-line detection of rotor faults in induction motor drives,2011,"The paper presents an on-line condition monitoring and diagnostic system for induction motor drives. It enables detection of many different faults, which may arise during the lifetime of the motor, although special attention was devoted to identify broken rotor bars at an early stage of the fault propagation. The method is based on the analysis of stator current frequency spectrum, which can be measured without disturbing normal motor operation, therefore it is completely non-invasive and easy to implement in industrial environments. The presented diagnostic system is being applied on 17 high-voltage motors (range of power 1000 - 6400 kW) in two thermal power plants to increase operational reliability of induction motors and to reduce costs of their maintenance. Concepts of hardware and software configurations are explained in details as well as some of the numerous monitoring results during the last five-years period. The paper also discusses two examples of a timely and efficient detection of rotor electric asymmetry due to cracked end-ring segments of induction motors, which exhibited no obvious problems during operation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6063605,no
A simple fault detection of induction motor by using parity equations,2011,"In this paper, a fault detection technique by using parity equations applied to an induction motor is presented. The nonlinear model of A.C. motor is matched with the lineal model of D.C. motor in synchronous reference frame in order to generate a relative large change in the residual obtained with parity equations in presence of fault, which allows the simplicity and reliability of the fault detection. The good performance of the fault detection system is validated by using a simulator software of power electronics and motor control applications (PSIM).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6063681,no
From Boolean to quantitative synthesis,2011,"Motivated by improvements in constraint-solving technology and by the increase of routinely available computational power, partial-program synthesis is emerging as an effective approach for increasing programmer productivity. The goal of the approach is to allow the programmer to specify a part of her intent imperatively (that is, give a partial program) and a part of her intent declaratively, by specifying which conditions need to be achieved or maintained. The task of the synthesizer is to construct a program that satisfies the specification. As an example, consider a partial program where threads access shared data without using any synchronization mechanism, and a declarative specification that excludes data races and deadlocks. The task of the synthesizer is then to place locks into the program code in order for the program to meet the specification. In this paper, we argue that quantitative objectives are needed in partial-program synthesis in order to produce higher-quality programs, while enabling simpler specifications. Returning to the example, the synthesizer could construct a naive solution that uses one global lock for shared data. This can be prevented either by constraining the solution space further (which is error-prone and partly defeats the point of synthesis), or by optimizing a quantitative objective that models performance. Other quantitative notions useful in synthesis include fault tolerance, robustness, resource (memory, power) consumption, and information flow.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064521,no
On the Interplay between Structural and Logical Dependencies in Open-Source Software,2011,"Structural dependencies have long been explored in the context of software quality. More recently, software evolution researchers have investigated logical dependencies between artifacts to assess failure-proneness, detect design issues, infer code decay, and predict likely changes. However, the interplay between these two kinds of dependencies is still obscure. By mining 150 thousand commits from the Apache Software Foundation repository and employing object-oriented metrics reference values, we concluded that 91% of all established logical dependencies involve non-structurally related artifacts. Furthermore, we found some evidence that structural dependencies do not lead to logical dependencies in most situations. These results suggest that dependency management methods and tools should rely on both kinds of dependencies, since they represent different dimensions of software evolvability.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065158,no
What You See is What You Asked for: An Effort-Based Transformation of Code Analysis Tasks into Interactive Visualization Scenarios,2011,"We propose an approach that derives interactive visualization scenarios from descriptions of code analysis tasks. The scenario derivation is treated as an optimization process. In this context, we evaluate different possibilities of using a given visualization tool to perform the analysis task, and select the scenario that requires the least effort from the analyst. Our approach was applied successfully to various analysis tasks such as design defect detection and feature location.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065178,no
Are the Clients of Flawed Classes (Also) Defect Prone?,2011,"Design flaws are those characteristics of design entities (e.g., methods, classes) which make them harder to maintain. Existing studies show that classes revealing particular design flaws are more change and defect prone than the other classes. Since various collaborations are found among the instances of classes, classes are not isolated within the source code of object-oriented systems. In this paper we investigate if classes using classes revealing design flaws are more defect prone than classes which do not use classes revealing design flaws. We detect four design flaws in three releases of Eclipse and investigate the relation between classes that use/do not use flawed classes and defects. The results show that classes that use flawed classes are defect prone and this does not depend on the number of the used flawed classes. This findings show a new type of correlation between design flaws and defects, bringing evidence related to an increased likelihood of exhibiting defects for classes that use classes revealing design flaws. Based on the provided evidence, practitioners are advised once again about the negative impact design flaws have at a source code level.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065198,no
An MRF Model for Binarization of Natural Scene Text,2011,"Inspired by the success of MRF models for solving object segmentation problems, we formulate the binarization problem in this framework. We represent the pixels in a document image as random variables in an MRF, and introduce a new energy (or cost) function on these variables. Each variable takes a foreground or background label, and the quality of the binarization (or labelling) is determined by the value of the energy function. We minimize the energy function, i.e. find the optimal binarization, using an iterative graph cut scheme. Our model is robust to variations in foreground and background colours as we use a Gaussian Mixture Model in the energy function. In addition, our algorithm is efficient to compute, and adapts to a variety of document images. We show results on word images from the challenging ICDAR 2003 dataset, and compare our performance with previously reported methods. Our approach shows significant improvement in pixel level accuracy as well as OCR accuracy.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065267,no
Distortion Measurement for Automatic Document Verification,2011,"Document forgery detection is important as techniques to generate forgeries are becoming widely available and easy to use even for untrained persons. In this work, two types of forgeries are considered: forgeries generated by re-engineering a document and forgeries that are generated using scanning and printing a genuine document. An unsupervised approach is presented to automatically detect forged documents of these types by detecting the geometric distortions introduced during the forgery process. Using the matching quality between all pairs of documents, outlier detection is performed on the summed matching quality to identify the tampered document. Quantitative evaluation is done on two public data sets, reporting a true positive rate from to 0.7 to 1.0.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065321,no
Automatic band detection of 1D-gel images,2011,"Many gel electrophoresis image segmentation methods have been proposed. However, most of the proposed methods cannot provide a satisfying performance since a gel electrophoresis image is a manually captured image often with various noise and poor quality. This paper presents a 1D-gel image segmentation method (1DGISM) to cut the bands from a one-dimension gel electrophoresis image (1D-gel image). The experimental results showed that 1DGISM can give an impressive segmentation performance even if the band boundaries are indistinct.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6067657,no
GPRS-based data real-time transmission system of water-quality monitoring,2011,"In order to meet the current water resource management automation and remote monitoring needs, a GPRS-based real-time management system is designed,which includes terminal subsystem,master station subsystem and communication service subsystem. The terminal subsystem is developed based on STM32F103X microprocessor and GPRS network. The GPRS module of MC55 is used to realize the remote wireless data transmission between terminal and GPRS network, and on this basis to achieve a wide range of water resource management monitoring. The Software of the management system is based on Windows platform, the operational parameters and real-time water-quality data are acquired and processed for statistics, analysis, curves and classification under the SQL2Server + VC++ environment. The designed system meets the requirements of strong real-time performance, networkization and intelligentization.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6067920,no
Autonomic Configuration Adaptation Based on Simulation-Generated State-Transition Models,2011,"Configuration management is a complex task, even for experienced system administrators, which makes self-managing systems a particularly desirable solution. This paper describes a novel contribution to self-managing systems, including an autonomic configuration self-optimization methodology. Our solution involves a systematic simulation method that develops a state-transition model of the behavior of a service-oriented system in terms of its configuration and performance. At run time, the system's behavior is monitored and classified in one of the model states. If this state may lead to futures that violate service level agreements, the system configuration is changed toward a safer future state. Similarly, a satisfactory state that is over-provisioned may be transitioned to a more economical satisfactory state. Aside from the typical benefits of self-optimization, our approach includes an intuitive, explainable decision model, the ability to predict the future with some accuracy avoiding trial-and-error, offline training, and the ability to improve the model at run-time. We demonstrate this methodology in an experiment where Amazon EC2 instances are added and removed to handle changing request volumes to a real service-oriented application. We show that a knowledge base generated entirely in simulation can be used to make accurate changes to a real-world application.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6068341,no
The Relevance of Assumptions and Context Factors for the Integration of Inspections and Testing,2011,"Integrating inspection processes with testing processes promises to deliver several benefits, including reduced effort for quality assurance or higher defect detection rates. Systematic integration of these processes requires knowledge regarding the relationships between these processes, especially regarding the relationship between inspection defects and test defects. Such knowledge is typically context-dependent and needs to be gained analytically or empirically. If such kind of knowledge is not available, assumptions need to be made for a specific context. This article describes the relevance of assumptions and context factors for integrating inspection and testing processes and provides mechanisms for deriving assumptions in a systematic manner.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6068373,no
Empirical Evaluation of Mixed-Project Defect Prediction Models,2011,"Defect prediction research mostly focus on optimizing the performance of models that are constructed for isolated projects. On the other hand, recent studies try to utilize data across projects for building defect prediction models. We combine both approaches and investigate the effects of using mixed (i.e. within and cross) project data on defect prediction performance, which has not been addressed in previous studies. We conduct experiments to analyze models learned from mixed project data using ten proprietary projects from two different organizations. We observe that code metric based mixed project models yield only minor improvements in the prediction performance for a limited number of cases that are difficult to characterize. Based on existing studies and our results, we conclude that using cross project data for defect prediction is still an open challenge that should only be considered in environments where there is no local data collection activity, and using data from other projects in addition to a project's own data does not pay off in terms of performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6068375,no
Read rate profile monitoring for defect detection in RFID Systems,2011,"RFID technologies are sometimes used into critical domains where the on-line detection of RFID system defects is a must. Moreover, as RFID systems are based on low cost tags which are often used into harsh environment, they do not always ensure robust functioning. This article proposes a new on-line monitoring approach allowing the detection of system defects to enhance system reliability and availability. This approach is based on the characterization of a statistical system parameter - the tag read rate profile - to perform the on-line detection of faulty RFID components. This monitoring approach is compared to classical monitoring approaches for a real case study and through several fault simulations. Results show that the proposed read rate profile monitoring is more efficient than the existing approaches. In addition, results show that this approach should be combined with an existing approach to maximize the fault detection.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6068621,no
E-Quality: A graph based object oriented software quality visualization tool,2011,"Recently, with increasing maintenance costs, studies on software quality are becoming increasingly important and widespread because high quality software means more easily maintainable software. Measurement plays a key role in quality improvement activities and metrics are the quantitative measurement of software design quality. In this paper, we introduce a graph based object-oriented software quality visualization tool called ""E-Quality"". E-Quality automatically extracts quality metrics and class relations from Java source code and visualizes them on a graph-based interactive visual environment. This visual environment effectively simplifies comprehension and refactoring of complex software systems. Our approach assists developers in understanding of software quality attributes by level categorization and intuitive visualization techniques. Experimental results show that the tool can be used to detect software design flaws and refactoring opportunities.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6069454,no
A visual analysis and design tool for planning software reengineerings,2011,"Reengineering complex software systems represents a non-trivial process. As a fundamental technique in software engineering, reengineering includes (a) reverse engineering the as-is system design, (b) identifying a set of transformations to the design, and (c) applying these transformations. While methods a) and c) are widely supported by existing tools, identifying possible transformations to improve architectural quality is not well supported and, therefore, becomes increasingly complex in aged and large software systems. In this paper we present a novel visual analysis and design tool to support software architects during reengineering tasks in identifying a given software's design and in visually planning quality-improving changes to its design. The tool eases estimating effort and change impact of a planned reengineering. A prototype implementation shows the proposed technique's feasibility. Three case studies conducted on industrial software systems demonstrate usage and scalability of our approach.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6069458,no
Computing indicators of creativity,2011,"Currently, the most common measurement of creativity is based on tests of divergence. These creativity tests include divergent thinking, divergent feeling, etc. In most cases the evaluation criteria is a subjective appraisal by a trained ""rater"" to assess the amount of divergence from the ""norm"" a particular submitted solution has to a presented or discovered task. The larger the divergence from the standard, the more creative the solution is. Although the quality and quantity of the solutions to the task must be considered, divergence from the accepted ""norm"" is a significant indicator of creativity. Using the current model for showing creative divergence, a method for evaluating the divergence of programming solutions as compared to the standard tutorial solution, in order to indicate creativity should be in line with current creativity research. Instead of subjective ""rater evaluations"" a method of calculating numerical divergence from programming solutions was devised. This method was employed on three separate class conditions and yielded three separate divergence patterns, indicating that the divergence calculation appears to demonstrate, not only that creativity can be shown to exist in programming solutions, but that the calculation is sensitive enough to differentiate between different class learning conditions of the same teacher. So based on the idea that creativity can be shown through divergence in thinking and feeling, it stands to reason that creativity in programming could be revealed through a similar divergence to a standard norm through calculating the divergence to that norm. Consequently, this divergence calculation method shows promising indicators to inform the measurement of creativity within programming and possibly other scientific areas.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6070407,no
Research of artificial neural network's component of software quality evaluation and prediction method,2011,This paper represents the artificial neural network's method of design results evaluation and software quality characteristics prediction (NMEP) and the analysis of results of realized artificial neural network (ANN) training and functioning.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6072916,no
An alternative optical method for acoustic resonance detection in HID lamps,2011,Acoustic resonances are observed in high-pressure discharge lamps operated with ac input modulated power frequencies in the kHz range. This paper describes an optical acoustic resonance detection method for HID lamps using computer controlled webcams and image processing software. Quality indexes are proposed to evaluate lamp performance and experimental results are presented.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6074368,no
A metric suite for early estimation of software testing effort using requirement engineering document and its validation,2011,"Software testing is one of the most important and critical activity of software development life cycle which ensures software quality and directly influences the development cost and success of the software. This paper empirically proposes a test metric for the estimation of the software testing effort using IEEE-Software Requirement Specification (SRS) document in order to avoid budget overshoot, schedule escalation etc., at very early stage of software development. Further the effort required to develop or test the software will also depend on the complexity of the proposed software. Therefore the proposed test metric computes the requirement based complexity for yet to be developed software on the basis of SRS document. Later the proposed measure is also compared with other proposals for test effort estimation like code based, cognitive value based and use case based measures. The result obtained validates that the proposed test metric is a comprehensive one and compares well with various other prominent measures. The computation of proposed test effort estimation involves least overhead as compared to others.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6075150,no
Impact of attribute selection on defect proneness prediction in OO software,2011,"Defect proneness prediction of software modules always attracts the developers because it can reduce the testing efforts as well as software development time. In the current context, with the piling up of constraints like requirement ambiguity and complex development process, developing fault free reliable software is a daunting task. To deliver reliable software, software engineers are required to execute exhaustive test cases which become tedious and costly for software enterprises. To ameliorate the testing process one can use a defect prediction model so that testers can focus their efforts on defect prone modules. Building a defect prediction model becomes very complex task when the number of attributes is very large and the attributes are correlated. It is not easy even for a simple classifier to cope with this problem. Therefore, while developing a defect proneness prediction model, one should always be careful about feature selection. This research analyzes the impact of attribute selection on Naive Bayes (NB) based prediction model. Our results are based on Eclipse and KC1 bug database. On the basis of experimental results, we show that careful combination of attribute selection and machine learning apparently useful and, on the Eclipse data set, yield reasonable good performance with 88% probability of detection and 49% false alarm rate.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6075151,no
On Protecting Cryptographic Applications Against Fault Attacks Using Residue Codes,2011,"We propose a new class of error detection codes, {em quadratic dual residue codes}, to protect cryptographic computations running on general-purpose processor cores against fault attacks. The assumed adversary model is a powerful one, whereby the attacker can inject errors anywhere in the data path of a general-purpose microprocessor by bit flipping. We demonstrate that quadratic dual residue codes provide a much better protection under this powerful adversary model compared to similar codes previously proposed for the same purpose in the literature. The adopted strategy aims to protect the single-precision arithmetic operations, such as addition and multiplication, which usually dominate the execution time of many public key cryptography algorithms in general-purpose microprocessors. Two so called {em robust} units for addition and multiplication operations, which provide a protection against faults attacks, are designed and tightly integrated into the data path of a simple, embedded re-configurable processor. We report the implementation results that compare the proposed error detection codes favorably with previous proposals of similar type in the literature. In addition, we present performance evaluations of the software implementations of Montgomery multiplication algorithm using the robust execution units. Implementation results clearly show that it is feasible to implement robust arithmetic units with relatively low overhead even for a simple embedded processor.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6076469,no
Autonomic Resource Management with Support Vector Machines,2011,"The use of virtualization technology makes data centers more dynamic and easier to administrate. Today, cloud providers offer customers access to complex applications running on virtualized hardware. Nevertheless, big virtualized data centers become stochastic environments and the implification on the user side leads to many challenges for the provider. He has to find cost-efficient configurations and has to deal with dynamic environments to ensure service guarantees. In this paper, we introduce a software solution that reduces the degree of human intervention to manage cloud services. We present a multi-agent system located in the Software as a Service (SaaS) layer. Agents allocate resources, configure applications, check the feasibility of requests, and generate cost estimates. The agents learn behavior models of the services via Support Vector Machines (SVMs) and share their experiences via a global knowledge base. We evaluate our approach on real cloud systems with three different applications, a brokerage system, a high-performance computing software, and a web server.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6076511,no
Piggy-Backing Link Quality Measurements to IEEE 802.15.4 Acknowledgements,2011,"In this paper we present an approach to piggyback link quality measurements to IEEE 802.15.4 acknowledgement frames by generating acknowledgements in software instead of relying on hardware support. We show that the software-generated ACKs can be sent meeting the timing constraints of IEEE 802.15.4. This allows for a standard conforming, energy neutral dissemination of link quality related information in IEEE 802.15.4 networks. This information is available at no cost when transmitting data and can be used as input for various schemes for adaptive transmission power control and to assess the current channel quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6076690,no
Analyzing Performance of Lease-Based Schemes under Failures,2011,"Leases have proved to be an effective concurrency control technique for distributed systems that are prone to failures. However, many benefits of leases are only realized when leases are granted for approximately the time of expected use. Correct assessment of lease duration has proven difficult for all but the simplest of resource allocation problems. In this paper, we present a model that captures a number of lease styles and semantics used in practice. We consider a few performance characteristics for lease-based systems and analytically derive how they are affected by lease duration. We confirm our analytical findings by running a set of experiments with the OO7 benchmark suite using a variety of workloads and fault loads.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6076777,no
Measurement methods for QoS in VoIP review,2011,"In the last years there is a merging trend towards unified communication systems over IP protocols from desktop to handheld devices. However this trend brings forth the limited QoS control existing in these types of networks. The lack of cost-effective QoS strategies is felt negatively directly by the end user in terms of both communication quality and increasing costs. Therefore, this paper analyses which are the main methods available for measuring the QoS in VoIP networks for both audio and video calls and how neural networks can be used to predict the quality as perceived from the end user perspective.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6078907,no
Real time acquisition of ESPI specklegram and automated analysis for NDE,2011,"Electronic Speckle Pattern Interferometry (ESPI) is a fast developing whole field optical technique used for the measurement of optical phase changes under object deformations and has evolved as a powerful on-line nondestructive evaluation (NDE) tool. In ESPI the speckle interferometry data of the object under different loading conditions are stored as digital images and then converted into interference fringes by digital subtraction or by addition method. These interference patterns correspond to surface displacements, which in turn relates to the strain variation in the specimen. Defects in the specimen produce a strain concentration on loading and generate a fringe anomaly in the interferogram. These anomalies in the interferogram are analyzed for the characterisation of defects in the material. This technique is used as an effective tool in Non-Destructive Evaluation (NDE). In this paper a method for on-line analysis of ESPI fringe patterns for the qualitative evaluation in the materials is presented. Continuous acquisition and simultaneous digital processing of specklegrams has been developed for generating the fringe patterns. The image acquisition and processing system used for the present study is an algorithm developed by using Imaging Graphs software. Image processing techniques are introduced for contrast enhancement, noise reduction and fringe sharpening for the on-line detection of different types of defects in the material.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6079003,no
The design of a software fault prone application using evolutionary algorithm,2011,"Most of the current project management software's are utilizing resources on developing areas in software projects. This is considerably essential in view of the meaningful impact towards time and cost-effective development. One of the major areas is the fault proneness prediction, which is used to find out the impact areas by using several approaches, techniques and applications. Software fault proneness application is an application based on computer aided approach to predict the probability that the software contains faults. The application will uses object oriented metrics and count metrics values from open source software as input values to the genetic algorithm for generation of the rules to classify the software modules in the categories of Faulty and Non Faulty modules. At the end of the process, the result will be visualized using genetic algorithm applet, bar and pie chart. This paper will discussed the detail design of software fault proneness application by using genetic algorithm based on the object oriented approach and will be presented using the Unified Modeling Language (UML). The aim of the proposed design is to develop an automated tool for software development group to discover the most likely software modules to be high problematic in the future.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6079246,no
LTE CoS/QoS Harmonization Emulator,2011,"There are many challenges in the QoS controls and traffic handling mechanisms in the deployment of Long Term Evolution (LTE). Simulations are crucial for performance evaluation of end-to-end QoS (Quality of Service) management across LTE network elements over heterogeneous networks. The LTE CoS/QoS Harmonization Emulator presented in this paper will allow the user to determine whether the QoS design strategy specifically and logically fits in a variety of LTE network topologies. The Emulator is capable of evaluating effectiveness of CoS/QoS mapping, performance of QoS controls (classification/policing/mark/queuing&dropping/ shaping) and traffic handling mechanisms that supports specific LTE applications. This module is portable and can be integrated with the performance management (PM) system as a part of business/operations impact analysis to support testing during the system development and quality assurance phases. The Emulator is offered for free under the terms of an academic, non-commercial use license.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6079418,no
Low-Complexity Encoding Method for H.264/AVC Based on Visual Perception,2011,"H.264/AVC standard achieved excellent encoding performance of at the cost of increased computational complexity and falling encoding speed. In order to overcome poor real-time encoding performance of H.264/AVC, aiming at computing redundancy, based on the integration of visual selective attention mechanism and low complexity encoding of information analysis and visual perception, making use of the distribution of motion vector and the relationship between the mode decision probability and the human visual attention a low-complexity H.264/AVC video coding scheme is implemented in this paper. The simulation results show that the approach encoding effectively resolves the conflict between coding accuracy and speed, saving about 80% coding time on average, which effectively maintains good video quality and overall improves the encoding performance of H.264/AVC.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6079599,no
"iProblems - An Integrated Instrument for Reporting Design Flaws, Vulnerabilities and Defects",2011,"In the current demonstration we present a new instrument that provides for each existing class in an analyzed system information related to the problems the class reveals. We associate different types of problems to a class: design flaws, vulnerabilities and defects. In order to validate its usefulness, we perform some experiments on a suite of object-oriented systems and some results are briefly presented in the last part of the demo.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6079824,no
Approximate Code Search in Program Histories,2011,"Very often a defect must be corrected not only in the current version of a program at one particular place but in many places and many other versions -- possibly even in different development branches. Consequently, we need a technique to efficiently locate all approximate matches of an arbitrary defective code fragment in the program's history as they may need to be fixed as well. This paper presents an approximate whole-program code search in multiple releases and branches. We evaluate this technique for real-world defects of various large and realistic programs having multiple releases and branches. We report runtime measurements and recall using varying levels of allowable differences of the approximate search.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6079833,no
Impact of Installation Counts on Perceived Quality: A Case Study on Debian,2011,"Software defects are generally used to indicate software quality. However, due to the nature of software, we are often only able to know about the defects found and reported, either following the testing process or after being deployed. In software research studies, it is assumed that a higher amount of defect reports represents a higher amount of defects in the software system. In this paper, we argue that widely deployed programs have more reported defects, regardless of their actual number of defects. To address this question, we perform a case study on the Debian GNU/Linux distribution, a well-known free / open source software collection. We compare the defects reported for all the software packages in Debian with their popularity. We find that the number of reported defects for a Debian package is limited by its popularity. This finding has implications on defect prediction studies, showing that they need to consider the impact of popularity on perceived quality, otherwise they might be risking bias.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6079845,no
An Empirical Validation of the Benefits of Adhering to the Law of Demeter,2011,"The Law of Demeter formulates the rule-of-thumb that modules in object-oriented program code should ""only talk to their immediate friends"". While it is said to foster information hiding for object-oriented software, solid empirical evidence confirming the positive effects of following the Law of Demeter is still lacking. In this paper, we conduct an empirical study to confirm that violating the Law of Demeter has a negative impact on software quality, in particular that it leads to more bugs. We implement an Eclipse plugin to calculate the amount of violations of both the strong and the weak form of the law in five Eclipse sub-projects. Then we discover the correlation between violations of the law and the bug-proneness and perform a logistic regression analysis of three sub-projects. We also combine the violations with other OO metrics to build up a model for predicting the bug-proneness for a given class. Empirical results show that violations of the Law of Demeter indeed highly correlate with the number of bugs and are early predictor of the software quality. Based on this evidence, we conclude that obeying the Law of Demeter is a straight-forward approach for developers to reduce the number of bugs in their software.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6079847,no
Assessing Software Quality by Program Clustering and Defect Prediction,2011,"Many empirical studies have shown that defect prediction models built on product metrics can be used to assess the quality of software modules. So far, most methods proposed in this direction predict defects by class or file. In this paper, we propose a novel software defect prediction method based on functional clusters of programs to improve the performance, especially the effort-aware performance, of defect prediction. In the method, we use proper-grained and problem-oriented program clusters as the basic units of defect prediction. To evaluate the effectiveness of the method, we conducted an experimental study on Eclipse 3.0. We found that, comparing with class-based models, cluster-based prediction models can significantly improve the recall (from 31.6% to 99.2%) and precision (from 73.8% to 91.6%) of defect prediction. According to the effort-aware evaluation, the effort needed to review code to find half of the total defects can be reduced by 6% if using cluster-based prediction models.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6079848,no
Modularization Metrics: Assessing Package Organization in Legacy Large Object-Oriented Software,2011,"There exist many large object-oriented software systems consisting of several thousands of classes that are organized into several hundreds of packages. In such software systems, classes cannot be considered as units for software modularization. In such context, packages are not simply classes containers, but they also play the role of modules: a package should focus to provide well identified services to the rest of the software system. Therefore, understanding and assessing package organization is primordial for software maintenance tasks. Although there exist a lot of works proposing metrics for the quality of a single class and/or the quality of inter-class relationships, there exist few works dealing with some aspects for the quality of package organization and relationship. We believe that additional investigations are required for assessing package modularity aspects. The goal of this paper is to provide a complementary set of metrics that assess some modularity principles for packages in large legacy object-oriented software: Information-Hiding, Changeability and Reusability principles. Our metrics are defined with respect to object-oriented dependencies that are caused by inheritance and method call. We validate our metrics theoretically through a careful study of the mathematical properties of each metric.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6079866,no
ImpactScale: Quantifying change impact to predict faults in large software systems,2011,"In software maintenance, both product metrics and process metrics are required to predict faults effectively. However, process metrics cannot be always collected in practical situations. To enable accurate fault prediction without process metrics, we define a new metric, ImpactScale. ImpactScale is the quantified value of change impact, and the change propagation model for ImpactScale is characterized by probabilistic propagation and relation-sensitive propagation. To evaluate ImpactScale, we predicted faults in two large enterprise systems using the effort-aware models and Poisson regression. The results showed that adding ImpactScale to existing product metrics increased the number of detected faults at 10% effort (LOC) by over 50%. ImpactScale also improved the predicting model using existing product metrics and dependency network measures.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080771,no
Identifying distributed features in SOA by mining dynamic call trees,2011,"Distributed nature of web service computing imposes new challenges on software maintenance community for localizing different software features and maintaining proper quality of service as the services change over time. In this paper, we propose a new approach for identifying the implementation of web service features in a service oriented architecture (SOA) by mining dynamic call trees that are collected from distributed execution traces. The proposed approach addresses the complexities of SOA-based systems that arise from: features whose locations may change due to changing of input parameters; execution traces that are scattered throughout different service provider platforms; and trace files that contain interleaving of execution traces related to different concurrent service users. In this approach, we execute different groups of feature-specific scenarios and mine the resulting dynamic call trees to spot paths in the code of a service feature, which correspond to a specific user input and system state. This allows us to focus on a the implementation of a specific feature in a distributed SOA-based system for different maintenance tasks such as bug localization, structure evaluation, and performance analysis. We define a set of metrics to assess structural properties of a SOA-based system. The effectiveness and applicability of our approach is demonstrated through a case study consisting of two service-oriented banking systems.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080774,no
Structural conformance checking with design tests: An evaluation of usability and scalability,2011,"Verifying whether a software meets its functional requirements plays an important role in software development. However, this activity is necessary, but not sufficient to assure software quality. It is also important to check whether the code meets its design specification. Although there exists substantial tool support to assure that a software does what it is supposed to do, verifying whether it conforms to its design remains as an almost completely manual activity. In a previous work, we proposed design tests - test-like programs that automatically check implementations against design rules. Design test is an application of the concept of test to design conformance checking. To support design tests for Java projects, we developed DesignWizard, an API that allows developers to write and execute design tests using the popular JUnit testing framework. In this work, we present a study on the usability and scalability of DesignWizard to support structural conformance checking through design tests. We conducted a qualitative usability evaluation of DesignWizard using the Think Aloud Protocol for APIs. In the experiment, we challenged eleven developers to compose design tests for an open-source software project. We observed that the API meets most developers' expectations and that they had no difficulties to code design rules as design tests. To assess its scalability, we evaluated DesignWizard's use of CPU time and memory consumption. The study indicates that both are linear functions of the size of software under verification.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080781,no
A probabilistic software quality model,2011,"In order to take the right decisions in estimating the costs and risks of a software change, it is crucial for the developers and managers to be aware of the quality attributes of their software. Maintainability is an important characteristic defined in the ISO/IEC 9126 standard, owing to its direct impact on development costs. Although the standard provides definitions for the quality characteristics, it does not define how they should be computed. Not being tangible notions, these characteristics are hardly expected to be representable by a single number. Existing quality models do not deal with ambiguity coming from subjective interpretations of characteristics, which depend on experience, knowledge, and even intuition of experts. This research aims at providing a probabilistic approach for computing high-level quality characteristics, which integrate expert knowledge, and deal with ambiguity at the same time. The presented method copes with â€œgoodnessâ€?functions, which are continuous generalizations of threshold based approaches, i.e. instead of giving a number for the measure of goodness, it provides a continuous function. Two different systems were evaluated using this approach, and the results were compared to the opinions of experts involved in the development. The results show that the quality model values change in accordance with the maintenance activities, and they are in a good correlation with the experts' expectations.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080791,no
Predicting post-release defects using pre-release field testing results,2011,"Field testing is commonly used to detect faults after the in-house (e.g., alpha) testing of an application is completed. In the field testing, the application is instrumented and used under normal conditions. The occurrences of failures are reported. Developers can analyze and fix the reported failures before the application is released to the market. In the current practice, the Mean Time Between Failures (MTBF) and the Average usage Time (AVT) are metrics that are frequently used to gauge the reliability of the application. However, MTBF and AVT cannot capture the whole pattern of failure occurrences in the field testing of an application. In this paper, we propose three metrics that capture three additional patterns of failure occurrences: the average length of usage time before the occurrence of the first failure, the spread of failures to the majority of users, and the daily rates of failures. In our case study, we use data derived from the pre-release field testing of 18 versions of a large enterprise software for mobile applications to predict the number of post-release defects for up to two years in advance. We demonstrate that the three metrics complement the traditional MTBF and AVT metrics. The proposed metrics can predict the number of post-release defects in a shorter time frame than MTBF and AVT.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080792,no
Using source code metrics to predict change-prone Java interfaces,2011,"Recent empirical studies have investigated the use of source code metrics to predict the change- and defect-proneness of source code files and classes. While results showed strong correlations and good predictive power of these metrics, they do not distinguish between interface, abstract or concrete classes. In particular, interfaces declare contracts that are meant to remain stable during the evolution of a software system while the implementation in concrete classes is more likely to change. This paper aims at investigating to which extent the existing source code metrics can be used for predicting change-prone Java interfaces. We empirically investigate the correlation between metrics and the number of fine-grained source code changes in interfaces of ten Java open-source systems. Then, we evaluate the metrics to calculate models for predicting change-prone Java interfaces. Our results show that the external interface cohesion metric exhibits the strongest correlation with the number of source code changes. This metric also improves the performance of prediction models to classify Java interfaces into change-prone and not change-prone.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080797,no
A clustering approach to improving test case prioritization: An industrial case study,2011,"Regression testing is an important activity for controlling the quality of a software product, but it accounts for a large proportion of the costs of software. We believe that an understanding of the underlying relationships in data about software systems, including data correlations and patterns, could provide information that would help improve regression testing techniques. We conjecture that if test cases have common properties, then test cases within the same group may have similar fault detection ability. As an initial approach to investigating the relationships in massive data in software repositories, in this paper, we consider a clustering approach to help improve test case prioritization. We implemented new prioritization techniques that incorporate a clustering approach and utilize code coverage, code complexity, and history data on real faults. To assess our approach, we have designed and conducted empirical studies using an industrial software product, Microsoft Dynamics Ax, which contains real faults. Our results show that test case prioritization that utilizes a clustering approach can improve the effectiveness of test case prioritization techniques.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080805,no
Code Hot Spot: A tool for extraction and analysis of code change history,2011,"Commercial software development teams have limited time available to focus on improvements to their software. These teams need a way to quickly identify areas of the source code that would benefit from improvement, as well as quantifiable data to defend the selected improvements to management. Past research has shown that mining configuration management systems for change information can be useful in determining faulty areas of the code. We present a tool named Code Hot Spot, which mines change records out of Microsoft's TFS configuration management system and creates a report of hot spots. Hot spots are contiguous areas of the code that have higher values of metrics that are indicators of faulty code. We present a study where we use this tool to study projects at ABB to determine areas that need improvement. The resulting data have been used to prioritize areas for additional code reviews and unit testing, as well as identifying change prone areas in need of refactoring.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080806,no
Source code comprehension strategies and metrics to predict comprehension effort in software maintenance and evolution tasks - an empirical study with industry practitioners,2011,"The goal of this research was to assess the consistency of source code comprehension strategies and comprehension effort estimation metrics, such as LOC, across different types of modification tasks in software maintenance and evolution. We conducted an empirical study with software development practitioners using source code from a small paint application written in Java, along with four semantics-preserving modification tasks (refactoring, defect correction) and four semantics-modifying modification tasks (enhancive and modification). Each task has a change specification and corresponding source code patch. The subjects were asked to comprehend the original source code and then judge whether each patch meets the corresponding change specification in the modification task. The subjects recorded the time to comprehend and described the comprehension strategies used and their reason for the patch judgments. The 24 subjects used similar comprehension strategies. The results show that the comprehension strategies and effort estimation metrics are not consistent across different types of modification tasks. The recorded descriptions indicate the subjects scanned through the original source code and the patches when trying to comprehend patches in the semantics-modifying tasks while the subjects only read the source code of the patches in semantics-preserving tasks. An important metric for estimating comprehension efforts of the semantics-modifying tasks is the Code Clone Subtracted from LOC(CCSLOC), while that of semantics-preserving tasks is the number of referred variables.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080814,no
The misuse of the NASA metrics data program data sets for automated software defect prediction,2011,"Background: The NASA Metrics Data Program data sets have been heavily used in software defect prediction experiments. Aim: To demonstrate and explain why these data sets require significant pre-processing in order to be suitable for defect prediction. Method: A meticulously documented data cleansing process involving all 13 of the original NASA data sets. Results: Post our novel data cleansing process; each of the data sets had between 6 to 90 percent less of their original number of recorded values. Conclusions: One: Researchers need to analyse the data that forms the basis of their findings in the context of how it will be used. Two: Defect prediction data sets could benefit from lower level code metrics in addition to those more commonly used, as these will help to distinguish modules, reducing the likelihood of repeated data points. Three: The bulk of defect prediction experiments based on the NASA Metrics Data Program data sets may have led to erroneous findings. This is mainly due to repeated data points potentially causing substantial amounts of training and testing data to be identical.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6083167,no
Further thoughts on precision,2011,"Background: There has been much discussion amongst automated software defect prediction researchers regarding use of the precision and false positive rate classifier performance metrics. Aim: To demonstrate and explain why failing to report precision when using data with highly imbalanced class distributions may provide an overly optimistic view of classifier performance. Method: Well documented examples of how dependent class distribution affects the suitability of performance measures. Conclusions: When using data where the minority class represents less than around 5 to 10 percent of data points in total, failing to report precision may be a critical mistake. Furthermore, deriving the precision values omitted from studies can reveal valuable insight into true classifier performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6083171,no
Predicting software defects: A cost-sensitive approach,2011,"Find software defects is a complex and slow task which consumes most of the development budgets. In order to try reducing the cost of test activities, many researches have used machine learning to predict whether a module is defect-prone or not. Defect detection is a cost-sensitive task whereby a misclassification is more costly than a correct classification. Yet, most of the researches do not consider classification costs in the prediction models. This paper introduces an empirical method based in a COCOMO (COnstructive COst MOdel) that aims to assess the cost of each classifier decision. This method creates a cost matrix that is used in conjunction with a threshold-moving approach in a ROC (Receiver Operating Characteristic) curve to select the best operating point regarding cost. Public data sets from NASA (National Aeronautics and Space Administration) IV&V (Independent Verification & Validation) Facility Metrics Data Program (MDP) are used to train the classifiers and to provide some development effort information. The experiments are carried out through a methodology that complies with validation and reproducibility requirements. The experimental results have shown that the proposed method is efficient and allows the interpretation of the classifier performance in terms of tangible cost values.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6084055,no
Inline magnet inspection using fast high resolution MagCam magnetic field mapping and analysis,2011,"Driven by a clear industrial need for advanced inspection equipment for high-end permanent magnets for drives, sensors, medical applications, consumer electronics and other applications, we present a new magnetic measurement technology, called a `magnetic field camera', a powerful and unique measurement platform for fast and accurate live inspection of both uniaxial and multipole permanent magnets. It is based on high resolution - high speed 2D mapping of the magnetic field distribution, using a patented semiconductor chip with an integrated 2D array of over 16000 microscopic Hall sensors. The measured magnetic field maps are analyzed by powerful data analysis software which uses advanced algorithms to extract unprecedented magnet information from the MagCam measurements. All these quantitative magnet properties can be tested against user-defined quality tolerances for immediate and automated pass/fail or classification analyses, all in real time. The system can be remotely operated using standard industrial communication protocols for integration in production lines. Its applications include inspection of incoming magnets, automated inline magnet quality control and research and development of magnets and magnetic systems and more.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6085537,no
KOMPSAT-5 SAR data processing: Design drivers and key performance,2011,"A general overview of KOMPSAT-5 SAR Processors chain and Software Infrastructure is provided, focusing on design drivers and key performance, mainly in terms of computation time and image quality parameters. An overview of the SAR Standard Products generated in the frame of KOMPSAT-5 mission is also depicted along with a sketch of the whole SAR Processors chain architecture. The End to End (E2E) test approach relies upon the generation of SAR Payload simulated RAW data combined with final image quality parameter estimation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6086950,no
The Method of Medical Named Entity Recognition Based on Semantic Model and Improved SVM-KNN Algorithm,2011,"In the medical field, a lot of unstructured information which is expressed by natural language exists in medical literature, technical documentation and medical records. IE (Information Extraction) as one of the most important research directions in natural language process aims to help humans extract concerned information automatically. NER (Named Entity Recognition) is one of the subsystems of IE and has direct influence on the quality of IE. Nowadays NER of medical field has not reached ideal precision largely due to the knowledge complexity in medical field. It is hard to describe the medical entity definitely in feature selection and current selected features are not rich and lack of semantic information. Besides that, different medical entities which have the similar characters more easily cause classification algorithm making wrong judgment. Combination multi classification algorithms such as SVM-KNN can overcome its own disadvantages and get higher performance. But current SVM-KNN classification algorithm may provide wrong categories results due to setting inappropriate K value and unbalanced examples distribution. In this paper, we introduce two-level modelling tool to help specialists to build semantic models and select features from them. We design and implement medical named entity recognition analysis engine based on UIMA framework and adopt improved SVM-KNN algorithm called EK-SVM-KNN (Extending K SVM-KNN) in classification. We collect experiment data from SLE(Systemic Lupus Erythematosus) clinical information system in Renji Hospital. We adopt 50 Pathology reports as training data and provide 1000 Pathology as test data. Experiment shows medical NER based on semantic model and improved SVM-KNN algorithm can enhance the quality of NER and we get the precision, recall rate and F value up to 86%.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6088087,no
Quality-Driven Hierarchical Clustering Algorithm for Service Intelligence Computation,2011,"Clustering is an important technique for intelligence computation such as trust, recommendation, reputation, and requirement elicitation. With the user centric nature of service and the user's lack of prior knowledge on the distribution of the raw data, one challenge is on how to associate user quality requirements on the clustering results with the algorithmic output properties (e.g. number of clusters to be targeted). In this paper, we focus on the hierarchical clustering process and propose two quality-driven hierarchical clustering algorithms, HBH (homogeneity-based hierarchical) and HDH (homogeneity-driven hierarchical) clustering algorithms, with minimum acceptable homogeneity and relative population for each cluster output as their input criteria. Furthermore, we also give a HDH-approximation algorithm in order to address the time performance issue. Experimental study on data sets with different density distribution and dispersion levels shows that the HDH gives the best quality result and HDH-approximation can significantly improve the execution time.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6088098,no
Towards identifying OS-level anomalies to detect application software failures,2011,"The next generation of critical systems, namely complex Critical Infrastructures (LCCIs), require efficient runtime management, reconfiguration strategies, and the ability to take decisions on the basis of current and past behavior of the system. Anomaly-based detection, leveraging information gathered at Operating System (OS) level (e.g., number of system call errors, signals, and holding semaphores in the time unit), seems to be a promising approach to reveal online application faults. Recently an experimental campaign to evaluate the performance of two anomaly detection algorithms was performed on a case study from the Air Traffic Management (ATM) domain, deployed under the popular OS used in the production environment, i.e., Red Hat 5 EL. In this paper we investigate the impact of the OS and the monitored resources on the quality of the detection, by executing experiments on Windows Server 2008. Experimental results allow identifying which of the two operating systems provides monitoring facilities best suited to implement the anomaly detection algorithms that we have considered. Moreover numerical sensitivity analysis of the detector parameters is carried out to understand the impact of their setting on the performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6088494,no
A Reliable and Self-Aware Clock for reference time failure detection in internal synchronization environment,2011,"A reliable source of time is a fundamental requirement for safety critical applications. For this class of applications, synchronization protocols like the IEEE1588 standard, can be adopted. This type of synchronization protocols was designed to achieve very precise clock synchronization, but they may not be sufficient to ensure safety of the whole system. For example an instability of the local oscillator of a reference node could influence the quality of synchronization of all nodes. In the recent years a new software clock, the Reliable and Self-Aware Clock (R&SAClock), which is designed to estimate the quality of synchronization through statistical analysis, was developed and tested. This paper describe and evaluate the design of a protocol for timing failure detection for internal synchronization based on a revised version of the R&SAClock software suitably modified to cross exploit the information on the quality of synchronization among all the nodes of the system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6088502,no
Remote diagnostics and performance analysis for a wireless sensor network,2011,"Wireless Sensor Networks (WSNs) comprise embedded sensor nodes that operate autonomously in a multi-hop topology. The challenges are unreliable wireless communications, harsh environment, and limited energy and computation resources. To ensure the desired level of service, it is essential to diagnose performance issues e.g. due to low quality links or energy depletion. This paper presents remote diagnostics and performance analysis that comprise self-diagnostics on embedded sensor nodes, the remote collection of diagnostics, and the design of a diagnostics analysis tool. Unlike the related proposals, our approach allows correcting detected problems by identifying the reasons for misbehavior. The diagnostics is verified with a practical WSN implementation. It has only a small overhead, less than 18 B/min per node in the implementation, allowing the use in bandwidth and energy constrained WSNs.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6088951,no
Link quality aware code dissemination in wireless sensor networks,2011,"Wireless reprogramming is a crucial technique for software deployment in wireless sensor networks (WSNs). Code dissemination is a basic building block to enable wireless repro-gramming. We present ECD, an Efficient Code Dissemination protocol leveraging 1-hop link quality information. Compared to prior works, ECD has three salient features. First, it supports dynamically configurable packet sizes. By increasing the packet size for high PHY rate radios, it significantly improves the transmission efficiency. Second, it employs an accurate sender selection algorithm to mitigate transmission collisions and transmissions over poor links. Third, it employs a simple impact-based backoff timer design to shorten the time spent in coordinating multiple eligible senders so that the largest impact sender is most likely to transmit. We implement ECD based on TinyOS and evaluate its performance extensively. Testbed experiments show that ECD outperforms state-of-the-art protocols, Deluge and MNP, in terms of completion time and data traffic. (e.g., about 20% less traffic and 20-30% shorter completion time compared to Deluge).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6089086,no
QoS-oriented Service Management in clouds for large scale industrial activity recognition,2011,"Motivated by the need of industrial enterprises for supervision services for quality, security and safety guarantee, we have developed an Activity Recognition Framework based on computer vision and machine learning tools, attaining good recognition rates. However, the deployment of multiple cameras to exploit redundancies, the large training set requirements of our time series classification models, as well as general resource limitations together with the emphasis on real-time performance, pose significant challenges and lead us to consider a decentralized approach. We thus adapt our application to a new and innovative real-time enabled framework for service-based infrastructures, which has developed QoS-oriented Service Management mechanisms in order to allow cloud environments to facilitate real-time and interactivity. Deploying the Activity Recognition Framework in a cloud infrastructure can therefore enable it for large scale industrial environments.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6089156,no
An automated retinal image quality grading algorithm,2011,"This paper introduces an algorithm for the automated assessment of retinal fundus image quality grade. Retinal image quality grading assesses whether the quality of the image is sufficient to allow diagnostic procedures to be applied. Automated quality analysis is an important preprocessing step in algorithmic diagnosis, as it is necessary to ensure that images are sufficiently clear to allow pathologies to be visible. The algorithm is based on standard recommendations for quality analysis by human screeners, examining the clarity of retinal vessels within the macula region. An evaluation against a reference standard data-set is given; it is shown that the algorithm's performance correlates closely with that of clinicians manually grading image quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6091472,no
Mosaicing of optical microscope imagery based on visual information,2011,"Tools for high-throughput high-content image analysis can simplify and expedite different stages of biological experiments, by processing and combining different information taken at different time and in different areas of the culture. Among the most important in this field, image mosaicing methods provide the researcher with a global view of the biological sample in a unique image. Current approaches rely on known motorized x-y stage offsets and work in batch mode, thus jeopardizing the interaction between the microscopic system and the researcher during the investigation of the cell culture. In this work we present an approach for mosaicing of optical microscope imagery, based on local image registration and exploiting visual information only. To our knowledge, this is the first approach suitable to work on-line with non-motorized microscopes. To assess our method, the quality of resulting mosaics is quantitatively evaluated through on-purpose image metrics. Experimental results show the importance of model selection issues and confirm the soundness of our approach.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6091522,no
Multiresolution texture models for brain tumor segmentation in MRI,2011,"In this study we discuss different types of texture features such as Fractal Dimension (FD) and Multifractional Brownian Motion (mBm) for estimating random structures and varying appearance of brain tissues and tumors in magnetic resonance images (MRI). We use different selection techniques including KullBack - Leibler Divergence (KLD) for ranking different texture and intensity features. We then exploit graph cut, self organizing maps (SOM) and expectation maximization (EM) techniques to fuse selected features for brain tumors segmentation in multimodality T1, T2, and FLAIR MRI. We use different similarity metrics to evaluate quality and robustness of these selected features for tumor segmentation in MRI for real pediatric patients. We also demonstrate a non-patient-specific automated tumor prediction scheme by using improved AdaBoost classification based on these image features.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6091766,no
Telephone-quality pathological speech classification using empirical mode decomposition,2011,"This paper presents a computationally simple and effective methodology based on empirical mode decomposition (EMD) for classification of telephone quality normal and pathological speech signals. EMD is used to decompose continuous normal and pathological speech signals into intrinsic mode functions, which are analyzed to extract physically meaningful and unique temporal and spectral features. Using continuous speech samples from a database of 51 normal and 161 pathological speakers, which has been modified to simulate telephone quality speech under different levels of noise, a linear classifier is used with the feature vector thus obtained to obtain a high classification accuracy, thereby demonstrating the effectiveness of the methodology. The classification accuracy reported in this paper (89.7% for signal-to-noise ratio 30 dB) is a significant improvement over previously reported results for the same task, and demonstrates the utility of our methodology for cost-effective remote voice pathology assessment over telephone channels.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6091793,no
Intelligent diagnostics of devices with the use of infrared mapping images,2011,"A correct diagnostics together with the early prediction of failure or malfunction of the system are the major issues in modern maintenance. Nowadays, the time-honored diagnostics may be inadequate and lead to omitting failures, resulting in higher costs of repairing damaged equipment. That is why the interest in intelligent diagnostics increases due to the possibility of better interpretation of the component status and early failure prediction. One of the ways of determining the device condition is to measure and analyze the temperature of multiple points on the device. Thermographics may show a beginning of significant wear of a component, and enable a repair or replacing before the failure appears. The paper presents the key aspects of the diagnostics with thermal images, including: technology mapping, mapping algorithms, together with a presentation of available software solutions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092135,no
Combating class imbalance problem in semi-supervised defect detection,2011,"Detection of defect-prone software modules is an important topic in software quality research, and widely studied under enough defect data circumstance. An improved semi-supervised learning approach for defect detection involving class imbalanced and limited labeled data problem has been proposed. This approach employs random under-sampling technique to resample the original training set and updating training set in each round for co-train style algorithm. In comparison with conventional machine learning approaches, our method has significant superior performance in the aspect of AUC (area under the receiver operating characteristic) metric. Experimental results also show that with the proposed learning approach, it is possible to design better method to tackle the class imbalanced problem in semi-supervised learning.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092260,no
Software defect prediction using transfer method,2011,"Traditional machine learning works well within company defect prediction. Unlike these works, we consider the scenario where source and target data are drawn from different companies, recently referred to as cross-company defect prediction. In this paper, we proposed a novel algorithm based on transfer method, called Transfer Naive Bayes (TNB). Our solution transferred the information of test data to the weights of the training data. The theoretical analysis and experiment results indicate that our algorithm is able to get more accurate result within less runtime cost than the state of the art algorithm.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092261,no
On the Effectiveness of Contracts as Test Oracles in the Detection and Diagnosis of Race Conditions and Deadlocks in Concurrent Object-Oriented Software,2011,"The idea behind Design by Contract (DbC) is that a method defines a contract stating the requirements a client needs to fulfill to use it, the precondition, and the properties it ensures after its execution, the post condition. Though there exists ample support for DbC for sequential programs, applying DbC to concurrent programs presents several challenges. We have proposed a solution to these challenges in the context of Java as programming language and the Java Modeling language as specification language. This paper presents our findings when applying our DbC technique on an industrial case study to evaluate the ability of contract-based, runtime assertion checking code at detecting and diagnosing race conditions and deadlocks during system testing. The case study is a highly concurrent industrial system from the telecommunications domain, with actual faults. It is the first work to systematically investigate the impact of contract assertions for the detection of race conditions and deadlocks, along with functional properties, in an industrial system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092549,no
Measuring the Efficacy of Code Clone Information in a Bug Localization Task: An Empirical Study,2011,"Much recent research effort has been devoted to designing efficient code clone detection techniques and tools. However, there has been little human-based empirical study of developers as they use the outputs of those tools while performing maintenance tasks. This paper describes a study that investigates the usefulness of code clone information for performing a bug localization task. In this study 43 graduate students were observed while identifying defects in both cloned and non-cloned portions of code. The goal of the study was to understand how those developers used clone information to perform this task. The results of this study showed that participants who first identified a defect then used it to look for clones of the defect were more effective than participants who used the clone information before finding any defects. The results also show a relationship between the perceived efficacy of the clone information and effectiveness in finding defects. Finally, the results show that participants who had industrial experience were more effective in identifying defects than those without industrial experience.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092550,no
A Qualitative Study of Open Source Software Development: The Open EMR Project,2011,"Open Source software is competing successfully in many areas. The commercial sector is recognizing the benefits offered by Open Source development methods that lead to high quality software. Can these benefits be realized in specialized domains where expertise is rare? This study examined discussion forums of an Open Source project in a particular specialized application domain - electronic medical records - to see how development roles are carried out, and by whom. We found through a qualitative analysis that the core developers in this system include doctors and clinicians who also use the product. We also found that the size of the community associated with the project is an order of magnitude smaller than predicted, yet still maintains a high degree of responsiveness to issues raised by users. The implication is that a few experts and a small core of dedicated programmers can achieve success using an Open Source approach in a specialized domain.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092551,no
Exploring Software Measures to Assess Program Comprehension,2011,"Software measures are often used to assess program comprehension, although their applicability is discussed controversially. Often, their application is based on plausibility arguments, which, however, is not sufficient to decide whether software measures are good predictors for program comprehension. Our goal is to evaluate whether and how software measures and program comprehension correlate. To this end, we carefully designed an experiment. We used four different measures that are often used to judge the quality of source code: complexity, lines of code, concern attributes, and concern operations. We measured how subjects understood two comparable software systems that differ in their implementation, such that one implementation promised considerable benefits in terms of better software measures. We did not observe a difference in program comprehension of our subjects as the software measures suggested it. To explore how software measures and program comprehension could correlate, we used several variants of computing the software measures. This brought them closer to our observed result, however, not as close as to confirm a relationship between software measures and program comprehension. Having failed to establish a relationship, we present our findings as an open issue to the community and initiate a discussion on the role of software measures as comprehensibility predictors.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092561,no
Survey Reproduction of Defect Reporting in Industrial Software Development,2011,"Context: Defect reporting is an important part of software development in-vivo, but previous work from open source context suggests that defect reports often have insufficient information for defect fixing. Objective: Our goal was to reproduce and partially replicate one of those open source studies in industrial context to see how well the results could be generalized. Method: We surveyed developers from six industrial software development organizations about the defect report information, from three viewpoints: concerning quality, usefulness and automation possibilities of the information. Seventy-four developers out of 142 completed our survey. Results: Our reproduction confirms the results of the prior study in that ""steps to reproduce"" and ""observed behaviour"" are highly important defect information. Our results extend the results of the prior study as we found that ""part of the application"", ""configuration of the application"", and ""operating data"" are also highly important, but they were not surveyed in the prior study. Finally, we classified defect information as ""critical problems"", ""solutions"", ""boosters"", and ""essentials"" based on the survey answers. Conclusion: The quality of defect reports is a problem in the software industry as well as in the open source community. Thus, we suggest that a part of the defect reporting should be automated since many of the defect reporters lack technical knowledge or interest to produce high-quality defect reports.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092568,no
Mining Static Code Metrics for a Robust Prediction of Software Defect-Proneness,2011,"Defect-proneness prediction is affected by multiple aspects including sampling bias, non-metric factors, uncertainty of models etc. These aspects often contribute to prediction uncertainty and result in variance of prediction. This paper proposes two methods of data mining static code metrics to enhance defect-proneness prediction. Given little non-metric or qualitative information extracted from software codes, we first suggest to use a robust unsupervised learning method, shared nearest neighbors (SNN) to extract the similarity patterns of the code metrics. These patterns indicate similar characteristics of the components of the same cluster that may result in introduction of similar defects. Using the similarity patterns with code metrics as predictors, defect-proneness prediction may be improved. The second method uses the Occam's windows and Bayesian model averaging to deal with model uncertainty: first, the datasets are used to train and cross-validate multiple learners and then highly qualified models are selected and integrated into a robust prediction. From a study based on 12 datasets from NASA, we conclude that our proposed solutions can contribute to a better defect-proneness prediction.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092569,no
Network Versus Code Metrics to Predict Defects: A Replication Study,2011,"Several defect prediction models have been proposed to identify which entities in a software system are likely to have defects before its release. This paper presents a replication of one such study conducted by Zimmermann and Nagappan on Windows Server 2003 where the authors leveraged dependency relationships between software entities captured using social network metrics to predict whether they are likely to have defects. They found that network metrics perform significantly better than source code metrics at predicting defects. In order to corroborate the generality of their findings, we replicate their study on three open source Java projects, viz., JRuby, ArgoUML, and Eclipse. Our results are in agreement with the original study by Zimmermann and Nagappan when using a similar experimental setup as them (random sampling). However, when we evaluated the metrics using setups more suited for industrial use -- forward-release and cross-project prediction -- we found network metrics to offer no vantage over code metrics. Moreover, code metrics may be preferable to network metrics considering the data is easier to collect and we used only 8 code metrics compared to approximately 58 network metrics.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092570,no
Measuring Architectural Change for Defect Estimation and Localization,2011,"While there are many software metrics measuring the architecture of a system and its quality, few are able to assess architectural change qualitatively. Given the sheer size and complexity of current software systems, modifying the architecture of a system can have severe, unintended consequences. We present a method to measure architectural change by way of structural distance and show its strong relationship to defect incidence. We show the validity and potential of the approach in an exploratory analysis of the history and evolution of the Spring Framework. Using other, public datasets, we corroborate the results of our analysis.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092571,no
Handling Estimation Uncertainty with Bootstrapping: Empirical Evaluation in the Context of Hybrid Prediction Methods,2011,"Reliable predictions are essential for managing software projects with respect to cost and quality. Several studies have shown that hybrid prediction models combining causal models with Monte Carlo simulation are especially successful in addressing the needs and constraints of today's software industry: They deal with limited measurement data and, additionally, make use of expert knowledge. Moreover, instead of providing merely point estimates, they support the handling of estimation uncertainty, e.g., estimating the probability of falling below or exceeding a specific threshold. Although existing methods do well in terms of handling uncertainty of information, we can show that they leave uncertainty coming from imperfect modeling largely unaddressed. One of the consequences is that they probably provide over-confident uncertainty estimates. This paper presents a possible solution by integrating bootstrapping into the existing methods. In order to evaluate whether this solution does not only theoretically improve the estimates but also has a practical impact on the quality of the results, we evaluated the solution in an empirical study using data from more than sixty projects and six estimation models from different domains and application areas. The results indicate that the uncertainty estimates of currently used models are not realistic and can be significantly improved by the proposed solution.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092573,no
Inferring Skill from Tests of Programming Performance: Combining Time and Quality,2011,"The skills of software developers are important to the success of software projects. Also, when studying the general effect of a tool or method, it is important to control for individual differences in skill. However, the way skill is assessed is often ad hoc, or based on unvalidated methods. According to established test theory, validated tests of skill should infer skill levels from well-defined performance measures on multiple, small, representative tasks. In this respect, we show how time and quality, which are often analyzed separately, can be combined as task performance and subsequently be aggregated as an approximation of skill. Our results show significant positive correlations between our proposed measures of skill and other variables, such as seniority, lines of code written, and self-evaluated expertise. The method for combining time and quality is a promising first step to measuring programming skill in both industry and research settings.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092579,no
Modeling the Number of Active Software Users,2011,"More and more software applications are developed within a software ecosystem (SECO), such as the Face book ecosystem and the iPhone AppStore. A core asset of a software ecosystem is its users, and the behavior of the users strongly affects the decisions of software vendors. The number of active users reflects user satisfaction and quality of the applications in a SECO. However, we can hardly find any literature about the number of active software users. Because software users are one of the most important assets of a software business, this information is very sensitive. In this paper, we analyzed the traces of software application users within a large scale software ecosystem with millions of active users. We identified useful patterns of user behavior, and proposed models that help to understand the number of active application users. The model we proposed better predicts the number of active users than just looking at the traditional retention rate. It also provides a fast way to monitor user satisfaction of online software applications. We have therefore provided an alternative way for SECO platform vendors to identify rising or falling applications, and for third party application vendors to identify risks and opportunity of their products.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092592,no
What are Problem Causes of Software Projects? Data of Root Cause Analysis at Four Software Companies,2011,"Root cause analysis (RCA) is a structured investigation of a problem to detect the causes that need to be prevented. We applied ARCA, an RCA method, to target problems of four medium-sized software companies and collected 648 causes of software engineering problems. Thereafter, we applied grounded theory to the causes to study their types and related process areas. We detected 14 types of causes in 6 process areas. Our results indicate that development work and software testing are the most common process areas, whereas lack of instructions and experiences, insufficient work practices, low quality task output, task difficulty, and challenging existing product are the most common types of the causes. As the types of causes are evenly distributed between the cases, we hypothesize that the distributions could be generalizable. Finally, we found that only 2.5% of the causes are related to software development tools that are widely investigated in software engineering research.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092595,no
Composite Release Values for Normalized Product-level Metrics,2011,"For metrics normalized using field data, variability is often high, since different classes of customers use different features of large releases. It is important to understand the quality health of the entire release, so we need to have a way to estimate the overall normalized metric value for the entire release, across all product lines. This paper looks at three different ways to calculate release values for a key normalized metric, software defects per million usage hours per month (SWDPMH). The 'Aggregate,' 'Averaging,' and 'Indexing' approaches are defined and examined for two major IOS release variants. Each of these approaches has general strengths and weaknesses, and these are described. Each of the three approaches has been found to be useful in particular situations, and these scenarios are also described. The primary objective of this study is to find an accurate method to estimate SWDPMH for software releases that can be used to improve release management operations, corporate goaling, and best practices evaluation for successor feature releases. Since the methods described are not particular to SWDPMH, we believe they may be useful for other normalized metrics-this is an area of future work.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092601,no
Obtaining Thresholds for the Effectiveness of Business Process Mining,2011,"Business process mining is a powerful tool to retrieve the valuable business knowledge embedded in existing information systems. The effectiveness of this kind of proposal is usually evaluated using recall and precision, which respectively measure the completeness and exactness of the retrieved business processes. Since the effectiveness assessment of business process mining is a difficult and error-prone activity, the main hypothesis of this work studies the possibility of obtaining thresholds to determine when recall and precision values are appropriate. The business process mining technique under study is MARBLE, a model-driven framework to retrieve business processes from existing information systems. The Bender method was applied to obtain the thresholds of the recall and precision measures. The experimental data used as input were obtained from a set of 44 business processes retrieved with MARBLE through a family of case studies carried out over the last two years. The study provides thresholds for recall and precision measures, which facilitates the interpretation of their values by means of five linguistic labels that range from low to very high. As a result, recall must be high (with at least a medium precision above 0.56), and precision must also be high (with at least a low recall of 0.70) to ensure that business processes were recovered (by using MARBLE) with an effectiveness value above 0.65. The thresholds allowed us to ascertain with more confidence whether MARBLE can effectively mine business processes from existing information systems. In addition, the provided results can be used as reference values to compare MARBLE with other similar business process mining techniques.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092604,no
Scrum + Engineering Practices: Experiences of Three Microsoft Teams,2011,"The Scrum methodology is an agile software development process that works as a project management wrapper around existing engineering practices to iteratively and incrementally develop software. With Scrum, for a developer to receive credit for his or her work, he or she must demonstrate the new functionality provided by a feature at the end of each short iteration during an iteration review session. Such a short-term focus without the checks and balances of sound engineering practices may lead a team to neglect quality. In this paper we present the experiences of three teams at Microsoft using Scrum with an additional nine sound engineering practices. Our results indicate that these teams were able to improve quality, productivity, and estimation accuracy through the combination of Scrum and nine engineering practices.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092605,no
Design for testability in embedded software projects,2011,"The purpose of this white paper is to focus on design techniques or methodologies that add testability features to embedded software which is an integral and important process for verification of any safety critical system. This paper presents testability in two forms: software testability i.e. testability at code level and design testability: i.e. testability at software requirements level. The two testability forms are further classified into subtypes explained in detail with examples which cite issues that are faced by engineers while performing verification. In this paper I have also come up with metrics which could become an important criterion in calculating the testability of a system considering the number of inputs that can be driven and the outputs that can be observed in the software during the verification process. Practical usage of these metrics may help designers understand how testable the system they have designed is, thus reducing verification effort and project cost. Good testability features, if not present in a system, may lead to increased cost and completion period of the project at a time when cost reduction and deadline chasing is the key to winning future projects. This paper endeavors to provide software developers guidance to incorporate important testability features into the software during the design and coding phase.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6096129,no
Automated extraction of architecture-level performance models of distributed component-based systems,2011,"Modern enterprise applications have to satisfy increasingly stringent Quality-of-Service requirements. To ensure that a system meets its performance requirements, the ability to predict its performance under different configurations and workloads is essential. Architecture-level performance models describe performance-relevant aspects of software architectures and execution environments allowing to evaluate different usage profiles as well as system deployment and configuration options. However, building performance models manually requires a lot of time and effort. In this paper, we present a novel automated method for the extraction of architecture-level performance models of distributed component-based systems, based on monitoring data collected at run-time. The method is validated in a case study with the industry-standard SPECjEnterprise2010 Enterprise Java benchmark, a representative software system executed in a realistic environment. The obtained performance predictions match the measurements on the real system within an error margin of mostly 10-20 percent.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6100052,no
Software process evaluation: A machine learning approach,2011,"Software process evaluation is essential to improve software development and the quality of software products in an organization. Conventional approaches based on manual qualitative evaluations (e.g., artifacts inspection) are deficient in the sense that (i) they are time-consuming, (ii) they suffer from the authority constraints, and (iii) they are often subjective. To overcome these limitations, this paper presents a novel semi-automated approach to software process evaluation using machine learning techniques. In particular, we formulate the problem as a sequence classification task, which is solved by applying machine learning algorithms. Based on the framework, we define a new quantitative indicator to objectively evaluate the quality and performance of a software process. To validate the efficacy of our approach, we apply it to evaluate the defect management process performed in four real industrial software projects. Our empirical results show that our approach is effective and promising in providing an objective and quantitative measurement for software process evaluation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6100070,no
Ecological inference in empirical software engineering,2011,"Software systems are decomposed hierarchically, for example, into modules, packages and files. This hierarchical decomposition has a profound influence on evolvability, maintainability and work assignment. Hierarchical decomposition is thus clearly of central concern for empirical software engineering researchers; but it also poses a quandary. At what level do we study phenomena, such as quality, distribution, collaboration and productivity? At the level of files? packages? or modules? How does the level of study affect the truth, meaning, and relevance of the findings? In other fields it has been found that choosing the wrong level might lead to misleading or fallacious results. Choosing a proper level, for study, is thus vitally important for empirical software engineering research; but this issue hasn't thus far been explicitly investigated. We describe the related idea of ecological inference and ecological fallacy from sociology and epidemiology, and explore its relevance to empirical software engineering; we also present some case studies, using defect and process data from 18 open source projects to illustrate the risks of modeling at an aggregation level in the context of defect prediction, as well as in hypothesis testing.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6100074,no
Towards dynamic backward slicing of model transformations,2011,"Model transformations are frequently used means for automating software development in various domains to improve quality and reduce production costs. Debugging of model transformations often necessitates identifying parts of the transformation program and the transformed models that have causal dependence on a selected statement. In traditional programming environments, program slicing techniques are widely used to calculate control and data dependencies between the statements of the program. Here we introduce program slicing for model transformations where the main challenge is to simultaneously assess data and control dependencies over the transformation program and the underlying models of the transformation. In this paper, we present a dynamic backward slicing approach for both model transformation programs and their transformed models based on automatically generated execution trace models of transformations.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6100084,no
Automatically detecting the quality of the query and its implications in IR-based concept location,2011,"Concept location is an essential task during software maintenance and in particular program comprehension activities. One of the approaches to this task is based on leveraging the lexical information found in the source code by means of Information Retrieval techniques. All IR-based approaches to concept location are highly dependent on the queries written by the users. An IR approach, even though good on average, might fail when the input query is poor. Currently there is no way to tell when a query leads to poor results for IR-based concept location, unless a considerable effort is put into analyzing the results after the fact. We propose an approach based on recent advances in the field of IR research, which aims at automatically determining the difficulty a query poses to an IR-based concept location technique. We plan to evaluate several models and relate them to IR performance metrics.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6100144,no
Using Formal Concept Analysis to support change analysis,2011,"Software needs to be maintained and changed to cope with new requirement, existing faults and change requests as software evolves. One particular issue in software maintenance is how to deal with a change proposal before change implementation? Changes to software often cause unexpected ripple effects. To avoid this and alleviate the risk of performing undesirable changes, some predictive measurement should be conducted and a change scheme of the change proposal should be presented. This research intends to provide a unified framework for change analysis, which includes dependencies extraction, change impact analysis, changeability assessment, etc. We expect that our change analysis framework will contribute directly to the improvement of the accuracy of these predictive measures before change implementation, and thus provide more accurate change analysis results for software maintainers, improve quality of software evolution and reduce the software maintenance effort and cost.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6100146,no
Texture feature based fingerprint recognition for low quality images,2011,"Fingerprint-based identification is one of the most well-known and publicized biometrics for personal identification. Extracting features out of poor quality prints is the most challenging problem faced in this area. In this paper, the texture feature based approach for fingerprint recognition using Discrete Wavelet Transform (DWT) is developed to identify the low quality fingerprint from inked-printed images on paper. The fingerprint image from paper is very poor quality image and sometimes it is complex with fabric background. Firstly, a center point area of the fingerprint is detected and keeping the Core Point as center point, the image of size w x w is cropped. Gabor filtering is applied for fingerprint enhancement over the orientation image. Finally, the texture features are extracted by analyzing the fingerprint with Discrete Wavelet Transform (DWT) and Euclidean distance metric is used as similarity measure. The accuracy is improved up to 98.98%.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6102204,no
Management towards reducing cloud usage costs,2011,"Summary form only given. Many organizations are attracted to cloud computing as an ICT (information and communications technology) sourcing model that can improve flexibility and total cost of ICT systems. However, it can be difficult for a prospective cloud customer to determine and manage cloud usage costs. We present an overview of several NICTA research projects that aim at providing information that can help ICT professionals determine various cloud usage costs and make decisions that are appropriate from the business viewpoint. Before migrating an application into a cloud, it is necessary to choose to which cloud to migrate, because there is a huge variety of cloud offerings, with significantly different pricing models. To accurately capture projected operating costs of an application in a particular cloud and enable side-by-side comparison of cloud offerings from different providers, NICTA developed a cost estimation tool that calculates the costs based on usage patterns and other characteristics of the application. This tool can also be used during runtime as an input into making adaptation/control decisions. To collect various runtime metrics (e.g., about the amount of transferred data or received quality of service - QoS) that are necessary for operational management and assessment of cloud usage costs, NICTA developed an innovative tool for flexible and integrated monitoring of applications in clouds and (in case of hybrid clouds) related local data centers. To help determine which runtime adaptation/control decisions are best from the business viewpoint (e.g., incur lowest cost), we extended the WS-Policy4MASC language and MiniZnMASC middleware for autonomic business-driven IT management with events and adaptation actions relevant for cloud management. The tools from the presented projects can be used separately or as parts of a powerful integrated cloud management system (which contains several additional tools).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6102258,no
System Monitoring with Metric-Correlation Models,2011,"Modern software systems expose management metrics to help track their health. Recently, it was demonstrated that correlations among these metrics allow errors to be detected and their causes localized. Prior research shows that linear models can capture many of these correlations. However, our research shows that several factors may prevent linear models from accurately describing correlations, even if the underlying relationship is linear. Common phenomena we have observed include relationships that evolve, relationships with missing variables, and heterogeneous residual variance of the correlated metrics. Usually these phenomena can be discovered by testing for heteroscedasticity of the underlying linear models. Such behaviour violates the assumptions of simple linear regression, which thus fail to describe system dynamics correctly. In this paper we address the above challenges by employing efficient variants of Ordinary Least Squares regression models. In addition, we automate the process of error detection by introducing the Wilcoxon Rank-Sum test after proper correlations modeling. We validate our models using a realistic Java-Enterprise-Edition application. Using fault-injection experiments we show that our improved models capture system behavior accurately.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6102277,no
Thermal analysis and experimental validation on cooling efficiency of thin film transistor liquid crystal display (TFT-LCD) panels,2011,"This research explored the thermal analysis and modeling of a 32â€?thin film transistor liquid crystal display (TFT-LCD) panel, in the purpose of making possible improvements in cooling efficiencies. The illumination of the panel was insured by 180 light emitting diodes (LEDs) located at the top and bottom edges of the panels. These LEDs dissipate high heat flux at low thermal resistance. Hence, in order to insure good image quality in panels and long service life, an adequate thermal management is necessary. For this purpose, a commercially available computational fluid dynamics (CFD) simulation software â€œFloEFDâ€?was used to predict the temperature distribution. This thermal prediction by computational method was validated by an experimental thermal analysis by attaching 10 thermocouples on the back cover of the panel and measuring the temperatures. Also, thermal camera images of the panel by FLIR Thermacam SC 2000 test device were also analyzed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6102689,no
Performance Analysis and Performance Modeling of Web-Applications,2011,"We study the performance of coupled web servers and database servers of certain web-applications. We focus on the performance bottleneck of one of the most pressing applications and their average response time. Therefore we present the software architecture of this web-application, called ""Study-Portal"" of the University of Mannheim, and we identify typical workload scenarios. We discuss our performance measurements and document the software and hardware infrastructure used during the measurements. Our goal is the identification of the most influential parameters which determine the timing behavior of the combined web-application. For the modeling we will use standard stochastic methods, even if it will mean over simplification. Our results then provide a simple estimate for the considered web-application: a doubling of application servers leads to half of the response time in case of sufficiently many clients. The developed model fits well with the observations in the operating environment of an IT-center.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6103147,no
Machine-Learning Models for Software Quality: A Compromise between Performance and Intelligibility,2011,"Building powerful machine-learning assessment models is an important achievement of empirical software engineering research, but it is not the only one. Intelligibility of such models is also needed, especially, in a domain, software engineering, where exploration and knowledge capture is still a challenge. Several algorithms, belonging to various machine-learning approaches, are selected and run on software data collected from medium size applications. Some of these approaches produce models with very high quantitative performances, others give interpretable, intelligible, and ""glass-box"" models that are very complementary. We consider that the integration of both, in automated decision-making systems for assessing software product quality, is desirable to reach a compromise between performance and intelligibility.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6103446,no
Impact of Data Sampling on Stability of Feature Selection for Software Measurement Data,2011,"Software defect prediction can be considered a binary classification problem. Generally, practitioners utilize historical software data, including metric and fault data collected during the software development process, to build a classification model and then employ this model to predict new program modules as either fault-prone (fp) or not-fault-prone (nfp). Limited project resources can then be allocated according to the prediction results by (for example) assigning more reviews and testing to the modules predicted to be potentially defective. Two challenges often come with the modeling process: (1) high-dimensionality of software measurement data and (2) skewed or imbalanced distributions between the two types of modules (fp and nfp) in those datasets. To overcome these problems, extensive studies have been dedicated towards improving the quality of training data. The commonly used techniques are feature selection and data sampling. Usually, researchers focus on evaluating classification performance after the training data is modified. The present study assesses a feature selection technique from a different perspective. We are more interested in studying the stability of a feature selection method, especially in understanding the impact of data sampling techniques on the stability of feature selection when using the sampled data. Some interesting findings are found based on two case studies performed on datasets from two real-world software projects.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6103463,no
Decimal Hamming: A Software-Implemented Technique to Cope with Soft Errors,2011,"A low-overhead technique for correction of induced errors affecting algorithms and their data based on the concepts behind Hamming code is presented and evaluated. We go beyond Hamming code by computing the check digits as decimal sums, and using a checker algorithm to perform single error detection and correction and double error detection. This generalization allows for the protection of complex data structures, providing lower performance overhead than classic approaches based on algorithm redundancy, and has been successfully applied to different benchmarking algorithms and their associated data. Comparison of the resulting overheads with those imposed by the classic duplication with comparison shows that the proposed technique, named Decimal Hamming, imposes lower execution time and program memory overheads, while providing enhanced error correction capabilities.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6104504,no
Assuring application-level correctness against soft errors,2011,"Traditionally, research in fault tolerance has required architectural state to be numerically perfect for program execution to be correct. However, in many programs, even if execution is not 100% numerically correct, the program can still appear to execute correctly from the user's perspective. To quantify user satisfaction, application-level fidelity metrics (such as PSNR) can be used. The output for such applications is defined to be correct if the fidelity metrics satisfy a certain threshold. However, such applications still contain instructions whose outputs are critical - i.e. their correctness decides if the overall quality of the program output is acceptable. In this paper, we present an analysis technique for identifying such critical program segments. More importantly, our technique is capable of guaranteeing application-level correctness through a combination of static analysis and runtime monitoring. Our static analysis consists of data flow analysis followed by control flow analysis to find static critical instructions which affect several instructions. Critical instructions are further refined into likely non-critical and likely critical sets in a profiling phase. At runtime, we use a monitoring scheme to monitor likely non-critical instructions and take remedial actions if some likely non-critical instructions become critical. Based on this analysis, we minimize the number of instructions that are duplicated and checked at runtime using a software-based fault detection and recovery technique [20]. Put together, our approach can lead to 22% average energy savings for multimedia applications while guaranteeing application-level correctness, when compared to a recent work [9], which cannot guarantee application-level correctness. Comparing to the approach proposed in [20] which guarantees both application-level and numerical correctness, our method achieves 79% energy reduction.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6105319,no
Computing Properties of Large Scalable and Fault-Tolerant Logical Networks,2011,"As the number of processors embedded in high performance computing platforms becomes higher and higher, it is vital to force the developers to enhance the scalability of their codes in order to exploit all the resources of the platforms. This often requires new algorithms, techniques and methods for code development that add to the application code new properties: the presence of faults is no more an occasional event but a challenge. Scalability and Fault-Tolerance issues are also present in hidden part of any platform: the overlay network that is necessary to build for controlling the application or in the runtime system support for messaging which is also required to be scalable and fault tolerant. In this paper, we focus on the computational challenges to experiment with large scale (many millions of nodes) logical topologies. We compute Fault-Tolerant properties of different variants of Binomial Graphs (BMG) that are generated at random. For instance, we exhibit interesting properties regarding the number of links regarding some desired Fault-Tolerant properties and we compare different metrics with the Binomial Graph structure as the reference structure. A software tool has been developed for this study and we show experimental results with topologies containing 21000 nodes. We also explain the computational challenge when we deal with such large scale topologies and we introduce various probabilistic algorithms to solve the problems of computing the conventional metrics.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6106018,no
Design and development of the CO<inf>2</inf> enriched Seawater Distribution System,2011,"The kinetics of the reaction that occurs when CO<sub>2</sub> and seawater are in contact is a complex function of temperature, alkalinity, final pH and TCO<sub>2</sub> which taken together determine the time required for complete equilibrium. This reaction is extremely important to the study of Ocean Acidification (OA) and is the critical technical driver in the Monterey Bay Aquarium Research Institute's (MBARI) Free Ocean CO<sub>2</sub> Enrichment (FOCE) experiments. The deep water FOCE science experiments are conducted at depths beyond scuba diver reach and demand that a valid perturbation experiment operate at a stable yet naturally fluctuating lower pH condition and avoid large or rapid pH variation as well as incomplete reactions, when we expose an experimental region or sample. Therefore, the technical requirement is to create a CO<sub>2</sub> source in situ that is stable and well controlled. After extensive research and experimentation MBARI has developed the ability to create an in situ source of CO<sub>2</sub> enriched seawater (ESW) for distribution and subsequent use in an ocean acidification experiment. The system mates with FOCE, but can be used in conjunction with other CO<sub>2</sub> experimental applications in deep water. The ESW system is completely standalone from FOCE. While the chemical changes induced by the addition of fossil fuel CO<sub>2</sub> on the ocean are well known and easily predicted, the biological consequences are less clear and the subject of considerable debate. Experiments have been successfully carried out on land to investigate the effects of elevated atmospheric CO<sub>2</sub> levels in various areas around the globe but only limited work on CO<sub>2</sub> impacts to ocean environmental systems have been carried out to date. With rising concern over the long-term reduction in ocean pH, there is a need for viable in situ techniques to carry out experiments on marine biological systems. Previous investigations have used aqua- ia that compromise these studies because of reduced ecological complexity and buffering capacity. Additionally, aquaria use tightly controlled experimental conditions such as temperature, artificial light, and water quality that do not represent the natural ocean variability. In order to study the future effects of ocean acidification, scientists and engineers at MBARI have developed a technique and apparatus for an in situ perturbation experiment, the FOCE experimental platform. At the time of this writing the FOCE system and associated ESW are attached to the Monterey Accelerated Research System (MARS) cabled observatory. Engineering validation and tuning experiments using remote control and real time experimental feedback are underway. Additionally, an extensive instrumentation suite provides all of the necessary data for pH calculation and experimental control. The ESW is a separately deployed system that stores and distributes CO<sub>2</sub> enriched seawater. It receives power and communications via an underwater mateable electrical tether. The CO<sub>2</sub> enriched seawater is pumped into the FOCE sections from the ESW. This paper describes the design, development, and testing of the underwater ESW Distribution System as well as the software control algorithms as applied to FOCE. The paper covers the initial prototype, lessons learned, and the final operational version.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6107095,no
3-dimensional analysis of Ground Penetrating Radar image for non-destructive road inspection,2011,"Regular maintenance of highway is an important issue to ensure safety of the vehicles using the road. Most of existing method of highway inspections are destructive, which take much times, efforts, and costs. In this paper, we propose GPR (Ground Penetrating Radar) imaging to detect possible defect of the road. GPR scanning on a plane parallel to the road yields 3D images, so that slice-by-slice images can be generated for a comprehensive evaluation. First, we simulate the subsurface-scanning with GPR-Max software, by setting up the parameters similar to expected real-condition. Then, we set up the experiment in our GPR Test-Range, in which a Network Analyzer is employed as a GPR. We compare and analyze both of the simulation and Test-Range results, including slice analysis, to asses the quality of the method. Our results indicates implementability of such 3D GPR imaging for road inspection.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6108603,no
Measuring firewall security,2011,"In the recent years, more attention is given to firewalls as they are considered the corner stone in Cyber defense perimeters. The ability to measure the quality of protection of a firewall policy is a key step to assess the defense level for any network. To accomplish this task, it is important to define objective metrics that are formally provable and practically useful. In this work, we propose a set of metrics that can objectively evaluate and compare the hardness and similarities of access policies of single firewalls based on rules tightness, the distribution of the allowed traffic, and security requirements. In order to analyze firewall polices based on the policy semantic, we used a canonical representation of firewall rules using Binary Decision Diagrams (BDDs) regardless of the rules format and representation. The contribution of this work comes in measuring and comparing firewall security deterministically in term of security compliance and weakness in order to optimize security policy and engineering.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6111669,no
Critiquing Rules and Quality Quantification of Development-Related Documents,2011,"As the development of embedded systems grows in scale, it is becoming more important for engineers to share development documents such as requirements, design specifications and testing specifications, and to accurately circulate and understand the information necessary for development. Also, many defects that can be originated in the surface expression of the documents are reported through investigations of causes of defects in embedded systems development, In this paper, we highlight improper surface expressions of Japanese documents, and define quality criteria and critiquing rules to detect problems such as ambiguous expressions or omissions of information. We also carry out visual quality inspections and evaluate detection performance, correlations and working time. Then, we verify the validity of the critiquing rules we have defined and apply them to the document critiquing tool to evaluate the quality of the actual documents used in the development of embedded systems. And we quantify the quality of these documents by automatically detecting improper expression. We also apply supplemental critiquing rules to the document critiquing tool for use by non-native speakers of Japanese, and verify its efficacy at improving the quality of Japanese documents created by foreigners.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113041,no
A Proposal of NHPP-Based Method for Predicting Code Change in Open Source Development,2011,"This paper proposes a novel method for predicting the amount of source code changes (changed lines of code: changed-LOC) in the open source development (OSD). While the software evolution can be observed through the public code repository in OSD, it is not easy to understand and predict the state of the whole development because of the huge amount of less-organized information.The method proposed in the paper predicts the code changes by using only data freely available from the code repository the code-change time stamp and the changed-LOC.The method consists of two steps: 1) to predict the number of occurrences of code changes by using a non-homogeneous Poisson process (NHPP)-based model, and 2) to predict the amount of code changes by using the outcome of the step-1 and the previously changed-LOC.The empirical work shows that the proposed method has an ability to predict the changed-LOC in the next 12 months with less than 10% error.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113042,no
Enabling Analysis and Measurement of Conventional Software Development Documents Using Project-Specific Formalism,2011,"We describe a new approach to modeling and analyzing software development documents that are typically written using conventional office applications. Our approach brings automation to content extraction, quality checking and measurement of massive document artifacts that tend to be handled by labor-intensive manual work in industry today. Rather than seeking an approach based on creation or rewriting of contents using more rigid, machine-friendly representations such as standardized formal models and restricted languages, we provide a method to deal with the diversity of document artifacts by making use of project-specific formalism that exists in target documents. We demonstrate that such project-specific formalism often tends to ""naturally"" exist at syntactic levels, and it is possible to define a ""document model"", a logical data representation gained by transformation rule from the physical, syntactic structure to the logical, semantic structure. With this transformation, various quality checking rules for completeness, consistency, traceability, etc., are realized by evaluating constraints for data items in the logical structure, and measurement of these quality aspects is automated. We developed a tool to allow a user to easily define document models and checking rules, and provide the insights on transformations when defining document models for various industry specification documents written in word processor files, spreadsheets and presentations. We also demonstrate the use of natural language processing can improve document modeling and quality checking by compensating for a weakness of formalism and applying analysis to specific parts of the target documents.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113043,no
An Empirical Study of Fault Prediction with Code Clone Metrics,2011,"In this paper, we present a replicated study to predict fault-prone modules with code clone metrics to follow Baba's experiment. We empirically evaluated the performance of fault prediction models with clone metrics using 3 datasets from the Eclipse project and compared it to fault prediction without clone metrics. Contrary to the original Baba's experiment, we could not significantly support the effect of clone metrics, i.e., the result showed that F1-measure of fault prediction was not improved by adding clone metrics to the prediction model. To explain this result, this paper analyzed the relationship between clone metrics and fault density. The result suggested that clone metrics were effective in fault prediction for large modules but not for small modules.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113044,no
Quantifying the Effectiveness of Testing Efforts on Software Fault Detection with a Logit Software Reliability Growth Model,2011,"Quantifying the effects of software testing metrics such as the number of test runs on the fault detection ability is quite important to design and manage effective software testing. This paper focuses on the regression model which represents the causal relationship between the software testing metrics and the fault detection probability. In a numerical experiment, we perform the quantitative estimation of the causal relationship through the quantization of software testing metrics.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113045,no
Analyzing Involvements of Reviewers through Mining a Code Review Repository,2011,"In order to assure the quality of software, early detection of defects is highly recommended. Code review is one of effective way for such early detection of defects in software. Code review activities must contain various useful insights for software quality. However, especially in open source software developments, records of code review merely exist. In this study, we try to analyze a code review repository of an open source software, Chromium, which adopts a code review tool in its development.Before analyzing the code review data, we address 7 research questions. We can find interesting answers for these questions by repositories mining.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113052,no
Using Efficient Machine-Learning Models to Assess Two Important Quality Factors: Maintainability and Reusability,2011,"Building efficient machine-learning assessment models is an important achievement of empirical software engineering research. Their integration in automated decision-making systems is one of the objectives of this work. It aims at empirically verify the relationships between some software internal artifacts and two quality attributes: maintainability and reusability. Several algorithms, belonging to various machine-learning approaches, are selected and run on software data collected from medium size applications. Some of these approaches produce models with very high quantitative performances; others give interpretable and ""glass-box"" models that are very complementary.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113057,no
Evidence-Based Evaluation of Effort Estimation Methods,2011,"In the selection of appropriate effort estimation methods, there are different questions, depending on the situation in the concrete IT environment. The main goal is of course the best and most accurate estimate of the efforts and costs for their own IT department in conjunction with least cost to the estimate itself. On the other hand, questions of comparability within the company or (internationally distributed) business areas seems to be relevant in a global marketplace of software production. Furthermore another target is to compare themselves with competitors and make the effort estimation transparent outside. This paper deals with these issues, examining the various cost estimation methods for their importance or ""evidence"" and the resulting selection criteria for individual applications. Our paper is based on our theoretical and industrial experience in effort estimation over the last twenty years in our software measurement community.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113060,no
An Exploratory Study on the Impact of Usage of Screenshot in Software Inspection Recording Activity,2011,"This paper describes an exploratory study on theuse of screenshots for recording software inspection activities such as defect reproduction and correction. Although detected defects are usually recorded in writing, using screenshots to record detected defects should ecrease the percentage of irreproducible defects and the time needed to reproduce defects during the defect orrection phase. An experiment was conducted to clarify the efficiency of using screenshots to record detected defects. One practitioner group and two student groups participated in the experiment. The recorder in each group used a prototype support tool for capturing screenshots during the experiment. Each group conducted two trials: one with a general spreadsheet application to support recording, the other with the prototype tool that supportsrecording inspection activities. After the inspection meeting, the recorder was asked to reproduce the recorded defects. The percentage of reproduce defects and time to reproduce defects was measured. The results of the experiment show that use of screenshots increases the percentage of reproduced defects and decreases the time needed to reproduce the defects. The results also indicate that use of the recording tool affected the types of defects.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113068,no
Fault Prediction Capability of Program File's Logical-Coupling Metrics,2011,"Frequent changes in logically coupled source files induced bugs in software. Metrics have been used to identify source files which are logically coupled. In this paper, we propose an approach to compute a set of eight metrics, which measure logical-couplings among source files. We compute these metrics using the historical data of software changes which are related to the fixing of post release bugs. To validate that our propose set of metrics is highly correlated with the number bugs and are more capable to construct a bug prediction model, we performed an experiment. Our experimental results show that our propose set of metrics is highly correlated with the number of bugs, and hence can be used to construct a bug prediction model. In our experiment, the obtained accuracy of our bug predictor model is 97%.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113069,no
Software Metrics Based on Coding Standards Violations,2011,"Software metrics is one of promise technique to capture the size and quality of products, development process in order to assess a software development. Many software metrics based on various aspects of a product and/or a process have been proposed. There is some research which discuss the relation between software metrics and faults to use these metrics as the indicator of quality. Most of these software metrics are based on structural features of products or process information related to explicit fault. In this paper, we focus on latent faults detected by static analysis techniques. The coding checker is widely used to find coding standards violations which are strongly relating to latent faults. In this paper, we propose new software metrics based on coding standards violations to capture latent faults in a development. We analyze two open source projects by using proposed metrics and discuss the effectiveness.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113072,no
Approach to Introducing a Statistical Quality Control,2011,"This paper describes some examples and points about implementing a statistical quality control in developing business systems in Sumitomo Electric Industries, Ltd. and Sumitomo Electric Information Systems Co., Ltd. Although the X-R chart is often used in statistical quality control, it is recommended to introduce the u-chart if defects are to be controlled in future. If a defect detection process such as a review or a test does not reach a statistical steady state that indicates a process is stable on the control chart, an appropriate control could be conducted by not only further standardizing the process, but also reviewing the definition of size indicators. Quality prediction can be made possible by accumulating defect data collected to create a control chart and analyzing the distributions of introduced defect densities at an organization level. If the accuracy of quality prediction is low, it can be enhanced by carrying out improvements to narrow the widths of distributions of introduced defect densities.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113076,no
Internal and External Software Benchmark Repository Utilization for Effort Estimation,2011,"Data repositories play critical role in software management practices. Construction of the estimation models, benchmark of software performance indicators, identification of the process improvement opportunities, and quality assessment are major utilization areas of software repositories. This paper presents the observed difficulties in utilization of external and multi-organizational software benchmark repositories for effort estimation model construction for a software organization in finance domain. ISBSG, Albrecht, China, Desharnais, Finnish, Maxwell and Kemerer repositories' data were utilized in this study. The approach was the utilization of these repositories and organization's own repository to estimate the software development effort and evaluate whether external and multi-organizational data be used for effort estimation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113077,no
Common Practices and Problems in Effort Data Collection in the Software Industry,2011,"Effort data is crucial for project management activities in software projects. This data is utilized in estimations that are required for project planning, in the formation of benchmarking data sets and as a main input for project monitoring and control activities. However, there are known problems regarding effort data collection in the industry. In this study we investigate the effort data collection practices in the industry and factors that lead to inaccurate effort data collection. A pilot study was conducted to observe problems in effort data collection and a survey is conducted to observe the existence of these problems in the industry as well as causes of these problems. In this paper, findings of these studies are presented along with certain suggestions to improve the quality of effort data.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113078,no
OpenMDSP: Extending OpenMP to Program Multi-Core DSP,2011,"Multi-core Digital Signal Processors (DSP) are widely used in wireless telecommunication, core network transcoding, industrial control, and audio/video processing etc. Comparing with general purpose multi-processors, the multi-core DSPs normally have more complex memory hierarchy, such as on-chip core-local memory and non-cache-coherent shared memory. As a result, it is very challenging to write efficient multi-core DSP applications. The current approach to program multi-core DSPs is based on proprietary vendor SDKs, which only provides low-level, non-portable primitives. While it is acceptable to write coarse-grained task level parallel code with these SDKs, it is very tedious and error prone to write fine-grained data parallel code with them. We believe it is desired to have a high-level and portable parallel programming model for multi-core DSPs. In this paper, we propose Open MDSP, an extension of Open MP designed for multi-core DSPs. The goal of Open MDSP is to fill the gap between Open MP memory model and the memory hierarchy of multi-core DSPs. We propose three class of directives in Open MDSP: (1) data placement directives allow programmers to control the placement of global variables conveniently, (2) distributed array directives divide whole array into sections and promote them into core-local memory to improve performance, and (3) stream access directives promote big array into core-local memory section by section during a parallel loop's processing. We implement the compiler and runtime system for Open MDSP on Free Scale MSC8156. Benchmarking result shows that seven out of nine benchmarks achieve a speedup of more than 5 with 6 threads.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113837,no
An Evaluation of Vectorizing Compilers,2011,"Most of today's processors include vector units that have been designed to speedup single threaded programs. Although vector instructions can deliver high performance, writing vector code in assembly language or using intrinsics in high level languages is a time consuming and error-prone task. The alternative is to automate the process of vectorization by using vectorizing compilers. This paper evaluates how well compilers vectorize a synthetic benchmark consisting of 151 loops, two application from Petascale Application Collaboration Teams (PACT), and eight applications from Media Bench II. We evaluated three compilers: GCC (version 4.7.0), ICC (version 12.0) and XLC (version 11.01). Our results show that despite all the work done in vectorization in the last 40 years 45-71% of the loops in the synthetic benchmark and only a few loops from the real applications are vectorized by the compilers we evaluated.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113845,no
Using Automated Control Charts for the Runtime Evaluation of QoS Attributes,2011,"As modern software systems operate in a highly dynamic context, they have to adapt their behaviour in response to changes in their operational environment or/and requirements. Triggering adaptation depends on detecting quality of service (QoS) violations by comparing observed QoS values to predefined thresholds. These threshold-based adaptation approaches result in late adaptations as they wait until violations have occurred. This may lead to undesired consequences such as late response to critical events. In this paper we introduce a statistical approach CREQA - Control Charts for the Runtime Evaluation of QoS Attributes. This approach estimates at runtime capability of a system, and then it monitors and provides early detection of any changes in QoS values allowing timely intervention in order to prevent undesired consequences. We validated our approach using a series of experiments and response time datasets from real world web services.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113911,no
Full-system analysis and characterization of interactive smartphone applications,2011,"Smartphones have recently overtaken PCs as the primary consumer computing device in terms of annual unit shipments. Given this rapid market growth, it is important that mobile system designers and computer architects analyze the characteristics of the interactive applications users have come to expect on these platforms. With the introduction of high-performance, low-power, general purpose CPUs in the latest smartphone models, users now expect PC-like performance and a rich user experience, including high-definition audio and video, high-quality multimedia, dynamic web content, responsive user interfaces, and 3D graphics. In this paper, we characterize the microarchitectural behavior of representative smartphone applications on a current-generation mobile platform to identify trends that might impact future designs. To this end, we measure a suite of widely available mobile applications for audio, video, and interactive gaming. To complete this suite we developed BBench, a new fully-automated benchmark to assess a web-browser's performance when rendering some of the most popular and complex sites on the web. We contrast these applications' characteristics with those of the SPEC CPU2006 benchmark suite. We demonstrate that real-world interactive smartphone applications differ markedly from the SPEC suite. Specifically the instruction cache, instruction TLB, and branch predictor suffer from poor performance. We conjecture that this is due to the applications' reliance on numerous high level software abstractions (shared libraries and OS services). Similar trends have been observed for UI-intensive interactive applications on the desktop.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6114205,no
Towards optimal performance and resource management in web systems via model predictive control,2011,"Management of the performance quality attributes and shared computing resources in a web system environment is vital to many business domains in order to achieve business objectives. These systems need to provide agreed levels of quality of service to their clients while allocating limited available resources among them. This paper proposes a new runtime management scheme based on predictive control to manage such systems within defined constraints. It firstly addresses the issue of building a multi-input multi-output (MIMO) system model when the equality constraints on the total amount of resources are considered. Then, the same performance management and resource allocation problem is reformulated with inequality constraints to improve the system model identification and runtime control. With the dynamic model built, the resource management problem in a shared resource environment is solved using model predictive control and constraint optimization. The performance of the proposed control system is validated on a prototype system implementing a real world scenario under different operation conditions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6114342,no
Evaluating the viability of process replication reliability for exascale systems,2011,"As high-end computing machines continue to grow in size, issues such as fault tolerance and reliability limit application scalability. Current techniques to ensure progress across faults, like checkpoint-restart, are increasingly problematic at these scales due to excessive overheads predicted to more than double an application's time to solution. Replicated computing techniques, particularly state machine replication, long used in distributed and mission critical systems, have been suggested as an alternative to checkpoint-restart. In this paper, we evaluate the viability of using state machine replication as the primary fault tolerance mechanism for upcoming exascale systems. We use a combination of modeling, empirical analysis, and simulation to study the costs and benefits of this approach in comparison to check-point/restart on a wide range of system parameters. These results, which cover different failure distributions, hardware mean time to failures, and I/O bandwidths, show that state machine replication is a potentially useful technique for meeting the fault tolerance demands of HPC applications on future exascale platforms.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6114406,no
Large scale debugging of parallel tasks with AutomaDeD,2011,"Developing correct HPC applications continues to be a challenge as the number of cores increases in today's largest systems. Most existing debugging techniques perform poorly at large scales and do not automatically locate the parts of the parallel application in which the error occurs. The over head of collecting large amounts of runtime information and an absence of scalable error detection algorithms generally cause poor scalability. In this work, we present novel, highly efficient techniques that facilitate the process of debugging large scale parallel applications. Our approach extends our previous work, AutomaDeD, in three major areas to isolate anomalous tasks in a scalable manner: (i) we efficiently compare elements of graph models (used in AutomaDeD to model parallel tasks) using pre-computed lookup-tables and by pointer comparison; (ii) we compress per-task graph models before the error detection analysis so that comparison between models involves many fewer elements; (iii) we use scalable sampling-based clustering and nearest-neighbor techniques to isolate abnormal tasks when bugs and performance anomalies are manifested. Our evaluation with fault injections shows that AutomaDeD scales well to thousands of tasks and that it can find anomalous tasks in under 5 seconds in an online manner.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6114453,no
An adaptive H.264 video protection scheme for video conferencing,2011,"Real-time video communication such as Internet video conferencing is often afflicted by packet loss over the network. To improve the quality of video, error protection schemes have been introduced based on FMO in H.264 whose encoding efficiency is unacceptable. This paper presents a novel region of interest (ROI) protection scheme that can accurately extract ROI area using facial recognition and greatly speedup video encoding based on feedback using x264 codec implementation. In this scheme, the video receiver uses a packet loss prediction model to predict whether to send feedback to the video sender that dynamically adjust the ROI protecting scheme. Experiments prove that the quality of the ROI area can be effectively improved by the scheme whose encoding performance increases by 50 times compared with FMO based algorithms.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6115954,no
Spam diagnosis infrastructure for individual cyberspace,2011,"The theory, methods and the architecture of parallel information's analysis is presented by the form of analytical, graph and table forms of associative relations for the search, recognition, diagnosis of destructive components and the decision making in n-dimensional vector cybernetic individual space. Vector -logical processes-models of actual oriented tasks are considered. They include the diagnostic of spam and the recovery of serviceability, the hardware-software components of computer systems and the decision quality is estimated by the interactions of non-arithmetic metrics of Boolean vectors. The concept of self-development information of computer ecosystem is offered. It repeats the evolution of the functionality of the person. Original processes-models of associative-logical information analysis are represented on the basis of high-speed multiprocessor in n-dimensional vector discrete space.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6116408,no
"The evidential independent verification of software of information and control systems, critical to safety: Functional model of scenario",2011,"The results of development of the techniques which form the scenario of target technology â‰ªEvidential independent verification of I&C Systems Software of critical applicationâ‰?and utilities of the scenario support at information, analytical and organizational levels are presented in the article. The result of the scenario implementation is the quantitative definition of latent faults probability and completeness of test coverage for critical software. This technology can be used by I&C systems developers, certification and regulation bodies to carry out independent verification (or certification) during modernization and modification of critical software directly on client objects without intruding (interrupting) in technological processes.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6116420,no
High performance H.264/AVC encoding motion prediction algorithm,2011,"The ITU-T H.264 can be considered currently as the most efficient professional video coding standard. This codec combines distinct complex techniques in order to encapsulate raw video into efficient compressed streams. Unfortunately, the improved H.264 coding efficiency increases significantly the computational complexity, making real-time applications difficult to be achieved. Particularly the motion prediction is an important and complex algorithm used to remove temporal redundancy of image sequences. Considering that, we propose an optimized high performance small diamond topology zonal search motion prediction algorithm. The proposed method has been designed to operate with orthogonal linear vectors, which are specially aligned with 4:1 sub sampled memories organization reducing the number of operations and data manipulations. The proposed solution was implemented in the H.264 JM reference software obtaining results at least two times faster than other available fast algorithms with negligible quality losses.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6116720,no
A stochastic formulation of successive software releases with faults severity,2011,"Software companies are coming with multiple add-ons to survive in the pure competitive environment. Each succeeding up-gradation offers some performance enhancement and distinguishing itself from the past release. If the size of the software system is large, the number of faults detected during the testing phase becomes large, and the number of faults, which are removed through each debugging, becomes small compared to initial fault content at the beginning of the testing phase. In such a situation, we can model the software fault detection process as a stochastic process with continuous state space. In this paper, we propose a multi-release software reliability growth model based on ItoÌ‚'s type of differential equation. The model categorizes Faults in two categories: simple and hard with respect to time which they take for isolation and removal after their observation. The model developed is validated on real data set.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6117894,no
Assessing Measurements of QoS for Global Cloud Computing Services,2011,"Many global distributed cloud computing applications and services running over the Internet, between globally dispersed clients and servers, will require certain levels of Quality of Service (QoS) in order to deliver and give a sufficiently smooth user experience. This would be essential for real-time streaming multimedia applications like online gaming and watching movies on a pay as you use basis hosted in a cloud computing environment. However, guaranteeing or even predicting QoS in global and diverse networks supporting complex hosting of application services is a very challenging issue that needs a stepwise refinement approach to be solved as the technology of cloud computing matures. In this paper, we investigate if latency in terms of simple Ping measurements can be used as an indicator for other QoS parameters such as jitter and throughput. The experiments were carried out on a global scale, between servers placed in universities in Denmark, Poland, Brazil and Malaysia. The results show some correlation between latency and throughput, and between latency and jitter, even though the results are not completely consistent. As a side result, we were able to monitor the changes in QoS parameters during a number of 24-hour periods. This is also a first step towards defining QoS parameters to be included in Service Level Agreements for cloud computing in the foreseeable future.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6118936,no
Analysis of rotor fault detection in inverter fed induction machines at no load by means of finite element method,2011,"This paper analyzes a new method for detecting defective rotor bars at zero load and standstill by means of modeling using the finite element method (FEM). The detection method uses voltage pulses generated by the switching of the inverter to excite the machine and measures the corresponding reaction of the machine phase currents, which can be used to identify a modulation of the transient leakage inductance caused by asymmetries within the machine. The presented 2D finite element model and the simulation procedure are oriented towards this approach and are developed by means of the FEM software ANSYS. The analysis shows how the transient flux linkage imposed by voltage pulses is influenced by a broken bar leading to very distinct rotor-fixed modulation, that can be clearly exploited for monitoring. Simulation results are presented to show the transient flux paths. These simulation results are supported by measurements on a specially manufactured induction machine.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6119572,no
Decoupled recursive-least-squares technique for extraction of instantaneous synchronized symmetrical components under fault conditions,2011,"This paper presents the decoupled recursive-least-squares (DRLS) technique for extraction of instantaneous synchronized symmetrical components under fault conditions in power grids. The proposed DRLS technique demonstrates outstanding robustness during faults, subsequent circuit breakers operation, and transmission line reclosing. The proposed DRLS technique also significantly reduces the computational burden of the conventional RLS technique for implementation on digital signal processors (DSPs). The performance of the proposed DRLS technique has been evaluated through presetting selected simulation results in MATLAB-Simulink software. DSP implementation of the DRLS technique also confirms considerable computational efficiency improvement in comparison with that of the conventional RLS technique. The DRLS technique also shows better efficiency in comparison with the enhanced phase-locked loop (EPLL) technique under highly distorted power system due to harmonics according to our implementation on two different R&D DSP platforms.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6120081,no
System Failure Forewarning Based on Workload Density Cluster Analysis,2011,"Each computer system contains design objectives for long-term usage, so the operator must conduct a continuous and accurate assessment of system performance in order to detect the potential factors that will degrade system performance. Condition indicators are the basic components of diagnosis. It is important to select feature vectors that meet the criteria in order to provide true accuracy and powerful diagnostic routines. Our goal is to indicate the actual system status according to the workload, and use clustering techniques to analyze the workload distribution density to build diagnostic templates. Such templates can be used for system failure forewarning. In the proposed system, we present an approach, based on workload density cluster analysis to automatically monitor the health of software systems and system failure forewarning. Our approach consists of tracking the workload density of metric clusters. We employ the statistical template model to automatically identify significant changes in cluster moving, therefore enabling robust fault detection. We observed two circumstances from the experiment results. First, under most normal status, the lowest accuracy value is approximate our theoretical minimum threshold of 84%. Such result implies a close correlation between our measured and real system status. Second, the command data used by the system could predict 90% of events announced, which reveals the prediction effectiveness of this proposed system. Although it is infeasible for the system to process the largest possible fault events in the deployment of resources, we could apply statistics to characterize the anomalous behaviors to understand the nature of emergencies and to test system service under such scenarios.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6120749,no
A hybrid method for constructing High Level Architecture of BBS user network,2011,"It is useful to understand the High Level Architecture (HLA) of the user network of Bulletin Board Systems (BBS) for some applications. In this paper, we construct the HLA of a BBS user network through hybrid static and dynamic analysis of the quantitative temporal graphs that are extracted from the BBS entries. We detect the HLA framework first though the static structural analysis of the aggregation of the temporal graphs. Then, we identify the HLA components including communities, community cores, and hubs elaborately through the dynamic analysis of the quantitative temporal attributes of nodes. The hybrid method guarantees the HLA quality as it removes the false components from the HLA. It controls the computational cost at a low level also. A metric is proposed to evaluate the HLA efficiency in information transmission. The experiments show that the HLA constructed by the hybrid method outperforms that constructed by the comparative method.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6122657,no
Algorithm analyzer to check the efficiency of codes,2011,Efficiency of codes developed is always an issue in software development. Software can be said to be of good quality if the measurable features of the software can be quantitatively checked for adoption of standards or following certain set rules. Software metrics can therefore come into play in the sense of helping to measure certain characteristics of software. The issue and factors pertaining to efficiency of a code will be addressed by software metrics. Existing tools that are used to analyze several software metrics have come a long way in helping to assess this very important part of software development. This paper described how software metrics can be used in analyzing efficiency of the developed code in early stage of development. A tool (algorithm analyzer) was developed to enable analyze a given code to check its efficiency level and produce efficiency reports based on the analysis. The system is able to help the code checking whilst maintaining the standards of coding for its users. With the reports that are generated it would be easy for users to determine the efficiency of their object oriented codes.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6122740,no
Design of the mechanical condition monitoring system for Molded Case Circuit Breakers,2011,"In this paper, a mechanical condition monitoring system of Molded Case Circuit Breaker (MCCB) is designed and developed. The operation principle of the monitoring system, the hardware design and the software design are introduced in detail. The three-phase voltage, the voltage between auxiliary normally closed contacts and the motor current are detected during the opening operation, closing operation and reclosing operation of MCCB, wherein these operations are driven by the motor. The mechanical condition characteristic parameters, including closing time, opening time, reclosing time, three phases asynchronous time, closing speed, opening speed, reclosing speed and the force on the handle, can be calculated by analyzing the voltage signals and current signals. The test results show that the system has a good performance. Moreover the characteristic parameters of circuit breaker, obtained in the test, provide test data for the theory research on remaining life prediction of MCCB.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6122986,no
A Novel Framework for Monitoring and Analyzing Quality of Data in Simulation Workflows,2011,"In recent years scientific workflows have been used for conducting data-intensive and long running simulations. Such simulation workflows have processed and produced different types of data whose quality has a strong influence on the final outcome of simulations. Therefore being able to monitor and analyze quality of this data during workflow execution is of paramount importance, as detection of quality problems will enable us to control the execution of simulations efficiently. Unfortunately, existing scientific workflow execution systems do not support the monitoring and analysis of quality of data for multi-scale or multi-domain simulations. In this paper, we examine how quality of data can be comprehensively measured within workflows and how the measured quality can be used to control and adapt running workflows. We present a quality of data measurement process and describe a quality of data monitoring and analysis framework that integrates this measurement process into a workflow management system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6123266,no
Exploiting Text-Related Features for Content-based Image Retrieval,2011,"Distinctive visual cues are of central importance for image retrieval applications, in particular, in the context of visual location recognition. While in indoor environments typically only few distinctive features can be found, outdoors dynamic objects and clutter significantly impair the retrieval performance. We present an approach which exploits text, a major source of information for humans during orientation and navigation, without the need for error-prone optical character recognition. To this end, characters are detected and described using robust feature descriptors like SURF. By quantizing them into several hundred visual words we consider the distinctive appearance of the characters rather than reducing the set of possible features to an alphabet. Writings in images are transformed to strings of visual words termed visual phrases, which provide significantly improved distinctiveness when compared to individual features. An approximate string matching is performed using N-grams, which can be efficiently combined with an inverted file structure to cope with large datasets. An experimental evaluation on three different datasets shows significant improvement of the retrieval performance while reducing the size of the database by two orders of magnitude compared to state-of-the-art. Its low computational complexity makes the approach particularly suited for mobile image retrieval applications.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6123328,no
Towards Energy Consumption Measurement in a Cloud Computing Wireless Testbed,2011,"The evolution of the Next Generation Networks, especially the wireless broadband access technologies such as Long Term Evolution (LTE) and Worldwide Interoperability for Microwave Access (WiMAX), have increased the number of ""all-IP"" networks across the world. The enhanced capabilities of these access networks has spearheaded the cloud computing paradigm, where the end-users aim at having the services accessible anytime and anywhere. The services availability is also related with the end-user device, where one of the major constraints is the battery lifetime. Therefore, it is necessary to assess and minimize the energy consumed by the end-user devices, given its significance for the user perceived quality of the cloud computing services. In this paper, an empirical methodology to measure network interfaces energy consumption is proposed. By employing this methodology, an experimental evaluation of energy consumption in three different cloud computing access scenarios (including WiMAX) were performed. The empirical results obtained show the impact of accurate network interface states management and application network level design in the energy consumption. Additionally, the achieved outcomes can be used in further software-based models to optimized energy consumption, and increase the Quality of Experience (QoE) perceived by the end-users.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6123444,no
Implementation and Usability Evaluation of a Cloud Platform for Scientific Computing as a Service (SCaaS),2011,"Scientific computing requires simulation and visualization involving large data sets among collaborating teams. Cloud platforms offer a promising solution via ScaaS. We report on the architecture, implementation and User Experience (UX) evaluation of one such SCaaS platform implementing TOUGH2V2.0, a numerical simulator for sub-surface fluid and heat flow, offered as a service. Results from example simulations, with virtualization of workloads in a multi-tenant, Virtual Machine (VM)-based cloud platform, are presented. These include fluid production from a geothermal reservoir, diffusive and advective spreading of contaminants, radial flow from a CO2 injection well and gas diffusion of a chemical through porous media. Prepackaged VM pools deployed autonomically ensure that sessions are provisioned elastically on demand. Users can access data-intensive visualizations via a web-browser. Authentication, user state and sessions are managed securely via an Access Gateway, to autonomically redirect and manage the workflows when multiple concurrent users are accessing their own sessions. Usability in the cloud and the traditional desktop are comparatively assessed, using several UX metrics. Simulated network conditions of different quality were imposed using a WAN emulator. Usability was found to be good for all the simulations under even moderately degraded network quality, as long as latency was not well above 100 ms. Hosting of a complex scientific computing application on an actual, global Enterprise cloud platform (as opposed to earlier remoting platforms) and its usability assessment, both presented for the first time, are the essential contributions of this work.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6123522,no
Optimal Sizing of Combined Heat & Power (CHP) Generation in Urban Distribution Network (UDN),2011,"The capacity of Combine Heat and Power (CHP) generation connected to Urban Distribution Network (UDN) will increase significantly as a result of EU government targets and initiatives. CHP generation can have significant impact on the power flow, voltage profile, fault current level and the power quality for customers and electricity suppliers. The connection of CHP plant at UDN creates a number of welldocumented impacts with voltage rise and fault current level being the dominant effects. A range of options have traditionally been used to mitigate adverse impacts but these generally revolve around network upgrades, the cost of which may be considerable. Connection of CHP generation can fundamentally alter the operation of UDN. Where CHP plant capacity is comparable to or larger than local demand there are likely to be observable impacts on network power flows, voltage regulation and fault current level. New connection of CHP schemes must be evaluated to identify and quantify any adverse impact on the security and quality of local electricity supplies. The impacts that arise from an individual CHP scheme are assessed in details when the developer makes an application for the connection of the CHP plant. The objective of this paper is to use static method to develop techniques that provide means of determining the optimum capacity of a CHP plant that may be accommodate within UDN. The main tool used in this paper is ERAC power analyzing software incorporating load flow and fault current level analysis. These analysis are demonstrated on 15 busbar network resembling part of typical UDN. In order to determine optimal placement and sizing of a CHP plant that could be connected at any particular busbar on UDN without causing a significant adverse impact on performance of the UDN, the multiple linear regression model is created and demonstrated using the data obtain by the analysis performed by ERAC power analyzing software.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6125628,no
A practical approach for channel problem detection and adaptation in tactical radio systems,2011,"Soldier-based tactical radio systems in the battlefield have tendencies to be severely affected by various phenomenon such as signal jamming and environment deterioration. This is because many types of wireless handheld and manpack devices carried by soldiers may not have efficient algorithms or the hardware for adapting the device to these conditions. Taking on the advantage that most of these devices use software-defined radios but do not fully utilize its functionalities, we present channel quality estimation and channel adaptation algorithms that can solve the problems of these devices. We utilize simple and practical channel problem detection methods by constantly monitoring the channel quality parameters such as signal-to-interference-noise ratio and clear channel assessment value while inducing low overhead. Furthermore, we propose efficient channel adaptation methods that can quickly sense and avoid deteriorated channel conditions. Via the experimental studies, we implement our proposed scheme on actual testbeds and show that it can improve the performance and mission-criticality of the tactical networks.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6127435,no
Computational resiliency for distributed applications,2011,"In recent years, computer network attacks have decreased overall reliability of computer systems and undermined confidence in mission-critical software. These robustness issues are magnified in distributed applications, which provide multiple points of failure and attack. The notion of resiliency is concerned with constructing applications that are able to operate through a wide variety of failures, errors, and malicious attacks. A number of approaches have been proposed in the literature based on fault tolerance achieved through replication of resources. In general, these approaches provide graceful degradation of performance to the point of failure but do not guarantee progress in the presence of multiple cascading and recurrent failures. Our approach is to dynamically replicate message-passing processes, detect inconsistencies in their behavior, and restore the level of fault tolerance as a computation proceeds. This paper describes a novel operating system technology for resilient message-passing applications that is automated, scalable, and transparent. The technology provides mechanisms for process replication, process migration, and adaptive failure detection. To quantify the performance overhead of the technology, we benchmark a distributed application exemplar to represent a broader class of applications.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6127514,no
Adaptive Failure Detection via Heartbeat under Hadoop,2011,"Hadoop has become one popular framework to process massive data sets in a large scale cluster. However, it is observed that the detection of the failed worker is delayed, which may result in a significant increase in the completion time of jobs with different workload. To cope with it, we present two mechanisms: Adaptive interval and Reputation-based Detector that support Hadoop to detect the failed worker in the shortest time. The Adaptive interval is trying to dynamically configure the expiration time which is adaptive to the job size. The Reputation-based Detector is trying to evaluate the reputation of each worker. Once the reputation of a worker is lower than a threshold, then the worker will be considered as a failed worker. In our experiments, we demonstrate that both of these strategies have achieved great improvement in the detection of the failed worker. Specifically, the Adaptive interval has a relatively better performance with small jobs, while the Reputation-based Detector is more suitable for large jobs.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6127967,no
Software Fault Prediction Framework Based on aiNet Algorithm,2011,"Software fault prediction techniques are helpful in developing dependable software. In this paper, we proposed a novel framework that integrates testing and prediction process for unit testing prediction. Because high fault prone metrical data are much scattered and multi-centers can represent the whole dataset better, we used artificial immune network (aiNet) algorithm to extract and simplify data from the modules that have been tested, then generated multi-centers for each network by Hierarchical Clustering. The proposed framework acquires information along with the testing process timely and adjusts the network generated by aiNet algorithm dynamically. Experimental results show that higher accuracy can be obtained by using the proposed framework.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6128133,no
Adaptive Energy-Efficient Architecture for WCDMA Channel Estimation,2011,"Due to the fast changing wireless communication standards coupled with strict performance constraints, the demand for flexible yet high-performance architectures is increasing. To tackle the flexibility requirement, Software-Defined Radio (SDR) is emerging as an obvious solution, where the underlying hardware implementation is tuned via software layers to the varied standards depending on power-performance and quality requirements leading to adaptable, cognitive radio. To design the hardware architecture for SDR is an interesting challenge, which involves determining the perfect balance of flexibility and performance for the target algorithmic kernels. In this paper, we conduct such a design case study for representatives of two complexity classes of WCDMA channel estimation algorithms. The two algorithms, polynomial channel estimation and weighted multi-slot averaging, differ also significantly in their algorithmic performance, difference which can be exploited in cognitive radio. Furthermore, we propose new design guidelines for highly specialised architectures that provide just enough flexibility to support multiple applications, targeting adaptability, low power and high-performance. Our experiments with various design points show that the resulting architecture meets the performance constraints of WCDMA and offers weak programmability to tune the architecture depending on power/performance constraints of SDR.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6128595,no
Specialist tool for monitoring the measurement degradation process of induction active energy meters,2011,"This paper presents a methodology and a specialist tool for failure probability analysis of induction type watt-hour meters, considering the main variables related to their measurement degradation processes. The database of the metering park of a distribution company, named Elektro Electricity and Services Co., was used for determining the most relevant variables and to feed the data in the software. The modeling developed to calculate the watt-hour meters probability of failure was implemented in a tool through a user friendly platform, written in Delphi language. Among the main features of this tool are: analysis of probability of failure by risk range; geographical localization of the meters in the metering park, and automatic sampling of induction type watt-hour meters, based on a risk classification expert system, in order to obtain information to aid the management of these meters. The main goals of the specialist tool are following and managing the measurement degradation, maintenance and replacement processes for induction watt-hour meters.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6128831,no
Prototype design of low cost four channels digital electroencephalograph for sleep monitoring,2011,"The electrical activity in brain or known as electroencephalogram (EEG) signal is being used in the diagnosis of sleep quality. Based on EEG signal, power of brain wave that related with a sleep quality could be obtained by analysis of power spectral density. The problem in developing countries, for example in Indonesia, EEG instrument is not widely available in each region of the country. This project designed and implemented four channels digital EEG, in which the design of hardware and software concepts was adopted from OpenEEG project. A four channels EEG amplifier operated by battery with average gain magnitude of 6100 times, bandwidth 0.05-60Hz and slope gradient of -60.00 dB/decade is developed. Digital board consists of AT-mega8 and serial interface with optocoupler is used to interface and viewed EEG signal on notebook. The prototype has successfully detected patterns of cardiac signal simultaneously with good SNR. In EEG measurement through monitoring the brain wave sleep, the data generated by PSD (Power Spectral Density) graph show the dominance of the brain signals at 7-9Hz (alpha) and 3-5Hz (theta). From several tests and measurements, this research concludes that the prototype of low cost EEG 4 channels is capable of acquiring satisfactory brain wave monitoring during sleep from healthy volunteer.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6130154,no
Automated Verification of Load Tests Using Control Charts,2011,"Load testing is an important phase in the software development process. It is very time consuming but there is usually little time for it. As a solution to the tight testing schedule, software companies automate their testing procedures. However, existing automation only reduces the time required to run load tests. The analysis of the test results is still performed manually. A typical load test outputs thousands of performance counters. Analyzing these counters manually requires time and tacit knowledge of the system-under-test from the performance engineers. The goal of this study is to derive an approach to automatically verify load tests' results. We propose an approach based on a statistical quality control technique called control charts. Our approach can a) automatically determine if a test run passes or fails and b) identify the subsystem where performance problem originated. We conduct two case studies on a large commercial telecommunication software and an open-source software system to evaluate our approach. Our results warrant further development of control chart based techniques in performance verification.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6130698,no
A New Approach to Evaluate Performance of Component-Based Software Architecture,2011,"Nowadays, by technology developments, software systems enlarge in scale and complexity. In large systems and to overcome complexity, software architecture has been considered as a connected notion with product quality and plays a crucial role in the quality of final system. The aim of the analysis of software architecture is to recognize potential risks and investigating qualitative needs of software design before the process of production and implementation. Achievement to this goal reduces the costs and improves the software quality. In this paper, a new approach is presented to evaluate performance of component-based software architecture for software systems with distributed architecture. In this approach, at first system is modeled as a Discrete Time Markov Chain and then the required parameters are taken from, to produce a Product Form Queueing Network. Limitations of source, like restrictions of the number of threads in a particular machine, are also regarded in the model. The prepared model is solved by the SHARPE software packages. As the result of the solution of the produced model in this approach, throughput and the average response time and bottlenecks in different workloads of system are predicted and some suggestions are presented to improve the system performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6131280,no
On the Effect of the Order of Test Cases in the Modified Exponential Software Reliability Growth Model,2011,"In this paper, we propose a resampling method which is specifically designed for software testing and reduce the bias of estimator caused by the order of test cases. In particular, this paper considers the resampling in the modified exponential software reliability growth model that is modeled by the difficulty of fault detection. In numerical experiments, we investigate the effectiveness of the resampling method.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6131830,no
Automated wafer defect map generation for process yield improvement,2011,"Spatial Signature Analysis (SSA) is used to detect a reoccurring failure signature in today wafer fabrication. In order for SSA to be effective, it must correlate the signature to a wafer defect maps library. However, classifying the signatures for the library is time consuming and tedious. The Manual Visual Inspection (MVI) of several failure bins in a wafer map for multiple lots can lead to fatigue for the operator and resulted in inaccurate representation of the failure signature. Hence, an automated wafer map extraction process is proposed here to replace the MVI while ensuring accuracy of the failure signature library. Clustering tool namely Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is utilized to extract the wafer spatial signature while ignoring the outliners. The appropriate size for the clustered signature is investigated and its performance is compared to the MVI signature. The analysis shows that for 3 selected failure modes, 20% occurrence rate clustered pattern provide similar performance to a 50% MVI signature. The proposed technique leads to a significant reduction in the time required for extracting current and new signatures, allowing faster yield response and improvement.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6131959,no
Simulink-based hardware/software trade-off analysis technique,2011,"Fast analysis of hardware/software trade-offs for cost, performance and power-constrained embedded systems is a key to reduce the time to market and at the same time improve the quality of results. However, this analysis must also be close to the final results of the detailed HW and SW implementation in order to lead to an optimal solution. This requires the use of compilation (for SW) and synthesis (for HW) techniques that ensure the existence of a solution with the estimated cost, and are not too far from what will later be achieved by manual optimization and detailed design. We start from a realistic application domain, namely sound-triggered wireless security cameras, and we show in detail how one can start from an algorithm modeled and validated using Simulink, and using commercial state-of-the-art tools explore various possible hardware and software implementations for the frequency based audio detection front end, with respect to the overall design constraints and goals. We show how rapid estimations of the various aspects of the cost function can be obtained quickly, using directly the C code generated from Simulink, with a few manual refinements in order to increase the efficiency of both software and hardware implementations and bring them closer to the final optimized implementation. We report results showing different points in the design space. The results that we obtained are close to manual hand optimized implementations for both HW and SW, showing that the approach is useful for trade-off analysis in a very short time, and that further manual optimizations can quickly lead to the best implementation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132515,no
"High level synthesis of stereo matching: Productivity, performance, and software constraints",2011,"FPGAs are an attractive platform for applications with high computation demand and low energy consumption requirements. However, design effort for FPGA implementations remains high - often an order of magnitude larger than design effort using high level languages. Instead of this time-consuming process, high level synthesis (HLS) tools generate hardware implementations from high level languages (HLL) such as C/C++/SystemC. Such tools reduce design effort: high level descriptions are more compact and less error prone. HLS tools promise hardware development abstracted from software designer knowledge of the implementation platform. In this paper, we examine several implementations of stereo matching, an active area of computer vision research that uses techniques also common for image de-noising, image retrieval, feature matching and face recognition. We present an unbiased evaluation of the suitability of using HLS for typical stereo matching software, usability and productivity of AutoPilot (a state of the art HLS tool), and the performance of designs produced by AutoPilot. Based on our study, we provide guidelines for software design, limitations of mapping general purpose software to hardware using HLS, and future directions for HLS tool development. For the stereo matching algorithms, we demonstrate between 3.5X and 67.9X speedup over software (but less than achievable by manual RTL design) with a five-fold reduction in design effort vs. manual hardware design.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132716,no
A perfSONAR-based Integrated Multi-domain Network Measurement Platform -- Internet Monitoring as a Service,2011,"The scale, diversity, and decentralized administration of the Internet mean that to continuously acquire the global status of the network and to timely identify the causes of communication performance degradation is reasonably difficult. However, emerging advanced network applications, which are often sensitive to communication quality and bandwidth consumption, as well as increasing security threats, strongly require a higher quality of network measurement and analysis in terms of granularity (spatial and temporal), timeliness, continuity, coverage, and reliability. Integrating multiple-location, diverse-type, and long-term measurements has been considered a key means for coping with such difficulties. In addition, the measured data and analyzed results should be flexibly shared and reused for efficiency. Therefore, a multi-domain network measurement platform should be realized, as it can provide integrated network monitoring and analysis functionality over the Internet on demand and can adapt to the purposes of the individual users (applications) and operators. In this paper, we thus briefly introduce the design principles and software implementation for a perfSONAR-based integrated network measurement system aiming at ""Internet Monitoring As aService"", together with a preliminary experiment using a prototype system on the Internet. With help of a function we newly designed and developed, our system can easily utilize new measurement tools and flexibly integrate and provide the measurement results of those tools to users.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132816,no
Characterizing Time-Varying Behavior and Predictability of Cache AVF,2011,"With the development of information systems, electronic devices are becoming more and more susceptible to soft errors, especially for the tough environment of drastic electromagnetic interference. Architectural Vulnerability Factor (AVF), which is defined as the fraction of soft errors that result in erroneous outputs, has been introduced to quantify the vulnerability of structures to soft errors. Recent studies have shown that the AVF of several micro-architectures (e.g. issue queue) exhibits significant runtime variations and certain predictability for SPEC2K benchmarks, thus motivating the development of AVF-aware fault tolerant techniques. Through accurate AVF prediction, these techniques provide error protection only at the execution points of high AVF rather than the whole execution lifetime of programs, thereby reducing the overheads of soft error protection schemes without sacrificing much reliability. The native motivation of this paper is to see if cache AVF also exhibits such predictability for the further exploration of AVF-aware cache protection techniques. In this paper, we characterize dynamic vulnerability behavior of level1 data (L1D) cache and analyze the correlation between L1D AVF and performance metrics. Based on the analysis of variance and predictability of L1D AVF, we propose a novel hierarchical classification of SPEC2K benchmarks to provide insights into the varying vulnerability behavior and predictability of cache AVF. We develop a new methodology to select the best-suited predictive method for cache AVF. Our results show that accurate cache AVF predictor is available for SPEC2K benchmarks, and most programs are excellent candidates for AVF-aware fault tolerant schemes.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132898,no
The Early Identification of Detector Locations in Dependable Software,2011,"The dependability properties of a software system are usually assessed and refined towards the end of the software development lifecycle. Problems pertaining to software dependability may necessitate costly system redesign. Hence, early insights into the potential for error propagation within a software system would be beneficial. Further, the refinement of the dependability properties of software involves the design and location of dependability components called detectors and correctors. Recently, a metric, called spatial impact, has been proposed to capture the extent of error propagation in a software system, providing insights into the location of detectors and correctors. However, the metric only provides insights towards the end of the software development life cycle. In this paper, our objective is to investigate whether spatial impact can enable the early identification of locations for detectors. To achieve this we first hypothesise that spatial impact is correlated with module coupling, a metric that can be evaluated early in the software development life cycle, and show this relationship to hold. We then evaluate module coupling for the modules of a complex software system, identifying modules with high coupling values as potential locations for detectors. We then enhanced these modules with detectors and perform fault-injection analysis to determine the suitability of these locations. The results presented demonstrate that our approach can permit the early identification of possible detector locations.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132952,no
Statistical Evaluation of Complex Input-Output Transformations,2011,"This paper presents a new, statistical approach to evaluating software products that transform complex inputs into complex outputs. This approach, called multistage stratified input/output (MSIO) sampling, combines automatic clustering of multidimensional I/O data with multistage sampling and manual examination of data elements, in order to accurately and economically estimate summary measures of output data quality. We report results of two case studies in which MSIO sampling was successfully applied to evaluating complex graphical outputs.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132955,no
Uncertainty Propagation through Software Dependability Models,2011,"Stochastic models are often employed to study dependability of critical systems and assess various hardware and software fault-tolerance techniques. These models take into account the randomness in the events of interest (aleatory uncertainty) and are generally solved at fixed parameter values. However, the parameter values themselves are determined from a finite number of observations and hence have uncertainty associated with them (epistemic uncertainty). This paper discusses methods for computing the uncertainty in output metrics of dependability models, due to epistemic uncertainties in the model input parameters. Methods for epistemic uncertainty propagation through dependability models of varying complexity are presented with illustrative examples. The distribution, variance and expectation of model output, due to epistemic uncertainty in model input parameters are derived and analyzed to understand their limiting behavior.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132956,no
An Empirical Study of JUnit Test-Suite Reduction,2011,"As test suites grow larger during software evolution, regression testing becomes expensive. To reduce the cost of regression testing, test-suite reduction aims to select a minimal subset of the original test suite that can still satisfy all the test requirements. While traditional test-suite reduction techniques were intensively studied on C programs with specially generated test suites, there are limited studies for test-suite reduction on programs with real-world test suites. In this paper, we investigate test-suite reduction techniques on Java programs with real-world JUnit test suites. We implemented four representative test-suite reduction techniques for JUnit test suites. We performed an empirical study on 19 versions of four real-world Java programs, ranging from 1.89 KLoC to 80.44 KLoC. Our study investigates both the benefits and the costs of test-suite reduction. The results show that the four traditional test-suite reduction techniques can effectively reduce these JUnit test suites without substantially reducing their fault-detection capability. Based on the results, we provide a guideline for achieving cost-effective JUnit test suite reduction.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132965,no
WSPred: A Time-Aware Personalized QoS Prediction Framework for Web Services,2011,"The exponential growth of Web service makes building high-quality service-oriented applications an urgent and crucial research problem. User-side QoS evaluations of Web services are critical for selecting the optimal Web service from a set of functionally equivalent service candidates. Since QoS performance of Web services is highly related to the service status and network environments which are variable against time, service invocations are required at different instances during a long time interval for making accurate Web service QoS evaluation. However, invoking a huge number of Web services from user-side for quality evaluation purpose is time-consuming, resource-consuming, and sometimes even impractical (e.g., service invocations are charged by service providers). To address this critical challenge, this paper proposes a Web service QoS prediction framework, called WSPred, to provide time-aware personalized QoS value prediction service for different service users. WSPred requires no additional invocation of Web services. Based on the past Web service usage experience from different service users, WSPred builds feature models and employs these models to make personalized QoS prediction for different users. The extensive experimental results show the effectiveness and efficiency of WSPred. Moreover, we publicly release our real-world time-aware Web service QoS dataset for future research, which makes our experiments verifiable and reproducible.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132969,no
Parametric Bootstrapping for Assessing Software Reliability Measures,2011,"The bootstrapping is a statistical technique to replicate the underlying data based on the resampling, and enables us to investigate the statistical properties. It is useful to estimate standard errors and confidence intervals for complex estimators of complex parameters of the probability distribution from a small number of data. In software reliability engineering, it is common to estimate software reliability measures from the fault data (fault-detection time data) and to focus on only the point estimation. However, it is difficult in general to carry out the interval estimation or to obtain the probability distributions of the associated estimators, without applying any approximate method. In this paper, we assume that the software fault-detection process in the system testing is described by a non-homogeneous Poisson process, and develop a comprehensive technique to study the probability distributions on significant software reliability measures. Based on the maximum likelihood estimation, we assess the probability distributions of estimators such as the initial number of software faults remaining in the software, software intensity function, mean value function and software reliability function, via parametric bootstrapping method.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6133060,no
Estimating Software Intensity Function via Multiscale Analysis and Its Application to Reliability Assessment,2011,"Since software fault detection process is well-modeled by a non-homogeneous Poisson process, it is of great interest to estimate accurately the intensity function from observed software-fault data. In the existing work the same authors introduced the wavelet-based techniques for this problem and found that the Haar wavelet transform provided a very powerful performance in estimating software intensity function. In this paper, we also study the Haar-wavelet-transform-based approach to be investigated from the point of view of multiscale analysis. More specifically, a Bayesian multiscale intensity estimation algorithm is employed. In numerical study with real software-fault count data, we compare the Bayesian multiscale intensity estimation with the existing non-Bayesian wavelet-based estimation as well as the conventional maximum likelihood estimation method and least squares estimation method.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6133061,no
Using Dependability Benchmarks to Support ISO/IEC SQuaRE,2011,"The integration of Commercial-Off-The-Shelf (COTS) components in software has reduced time-to-market and production costs, but selecting the most suitable component, among those available, remains still a challenging task. This selection process, typically named benchmarking, requires evaluating the behaviour of eligible components in operation, and ranking them attending to quality characteristics. Most existing benchmarks only provide measures characterising the behaviour of software systems in absence of faults ignoring the hard impact that both accidental and malicious faults have on software quality. However, since using COTS to build a system may motivate the emergence of dependability issues due to the interaction between components, benchmarking the system in presence of faults is essential. The recent ISO/IEC 25045 standard copes with this lack by considering accidental faults when assessing the recoverability capabilities of software systems. This paper proposes a dependability benchmarking approach to determine the impact that faults (noted as disturbances in the standard) either accidental or malicious may have on the quality features exhibited by software components. As will be shown, the usefulness of the approach embraces all evaluator profiles (developers, acquirers and third-party evaluators) identified in the ISO/IEC 25000 ""SQuaRE"" standard. The feasibility of the proposal is finally illustrated through the benchmarking of three distinct software components, which implement the OLSR protocol specification, competing for integration in a wireless mesh network.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6133063,no
Autonomic Resource Management Handling Delayed Configuration Effects,2011,"Today, cloud providers offer customers access to complex applications running on virtualized hardware. Nevertheless, big virtualized data centers become stochastic environments with performance fluctuations. The growing number of cloud services makes a manual steering impossible. An automatism on the provider side is needed. In this paper, we present a software solution located in the Software as a Service layer with autonomous agents that handle user requests. The agents allocate resources and configure applications to compensate performance fluctuations. They use a combination of Support Vector Machines and Model-Predictive Control to predict and plan future configurations. This allows them to handle configuration delays for requesting new virtual machines and to guarantee time-dependent service level objectives (SLOs). We evaluated our approach on a real cloud system with a high-performance software and a three-tier e-commerce application. The experiments show that the agents accurately configure the application and plan horizontal scalings to enforce SLO fulfillments even in the presence of noise.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6133137,no
VM Leakage and Orphan Control in Open-Source Clouds,2011,"Computer systems often exhibit degraded performance due to resource leakage caused by erroneous programming or malicious attacks, and computers can even crash in extreme cases of resource exhaustion. The advent of cloud computing provides increased opportunities to amplify such vulnerabilities, thus affecting a significant number of computer users. Using simulation, we demonstrate that cloud computing systems based on open-source code could be subjected to a simple malicious attack capable of degrading availability of virtual machines (VMs). We describe how the attack leads to VM leakage, causing orphaned VMs to accumulate over time, reducing the pool of resources available to users. We identify a set of orphan control processes needed in multiple cloud components, and we illustrate how such processes detect and eliminate orphaned VMs. We show that adding orphan control allows an open-source cloud to sustain a higher level of VM availability during malicious attacks. We also report on the overhead of implementing orphan control.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6133193,no
Coverability of wireless sensor networks,2011,"The coverability of Wireless Sensor Networks (WSNs) is essentially a Quality of Service (QoS) problem that measures how well the monitored area is covered by one or more sensor nodes. The cover-ability of WSNs was examined by combining existing computational geometry techniques such as the Voronoi diagram and Delaunay triangulation with graph theoretical algorithmic techniques. Three new evaluation algorithms, known as CRM (Comprehensive Risk Minimization), TWS (Threshold Weight Shortest path), and CSM (Comprehensive Support Maximization), were introduced to better measure the coverability. The experimental results show that the CRM and CSM algorithms perform better than the MAM (MAximize Minimum weight) and MIM (MInimize Maximum weight) algorithms, respectively. In addition, the TWS algorithm can provide a lower bound detection possibility that accurately reflects the coverability of the wireless sensor nodes. Both theoretical and experimental analyses show that the proposed CRM, TWS, and CSM algorithms have O(n<sup>2</sup>) complexity.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6133356,no
Valuing quality of experience: A brave new era of user satisfaction and revenue possibilities,2011,"Telecommunication market today is defined by a plethora of innovative products and technologies that constantly raising the bar of technical feasibility in both hardware and software. Meanwhile users constantly demand better quality and improved attributes for all applications, becoming less and less tolerant in errors or inconsistencies. Evaluation methods that were dominant for several years in the field seem to have limited effect on assess end-user satisfaction, leading to unhappy customers and lower revenue for key market players. Ensuring Quality of Service (QoS) proved no longer capable to increase market share therefore a novel evaluation method is necessary. The aim of the present paper is to present a new framework of user-oriented quality assessment that tries to measure the overall experience derived from a telecommunication product. Provided that modern services are based over the principal of sharing an overall experience with others, it seems certain that the new method of estimating Quality of Experience (QoE) will produce much better results, needed by both providers and customers.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6133422,no
A method for copper lines classification,2011,"Recently many end users show attention on the quality of Internet access; in Italy, a national wide measure campaign, sponsored by Italian Communication Regulatory Authority, allows user to evaluate his bandwidth using a licit software. In this context a primary end user need is to have the possibility to measure its bandwidth and to compare it with the parameters declared by ISPs. Assuming the availability of a standard recognized methodology to measure bandwidth on user access link, a problem with this approach arises when the measured performances are lower than the declared quality of service. When this happens, the problem could depend on several factors not directly attributable to the ISP. In this work, we propose a solution by which it is possible to characterize a physical line. The idea is to detect the situations in which the performances are degraded due to an unsatisfactory physical line state. To make this detection some real cases are considerate.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6133444,no
Performance Analysis of Cloud Centers under Burst Arrivals and Total Rejection Policy,2011,"Quality of service, QoS, has a great impact on wider adoption of cloud computing. Maintaining the QoS at an acceptable level for cloud users requires an accurate and well adapted performance analysis approach. In this paper, we describe a new approximate analytical model for performance evaluation of cloud server farms under burst arrivals and solve it to obtain important performance indicators such as mean request response time, blocking probability, probability of immediate service and probability distribution of number of tasks in the system. This model allows cloud operators to tune the parameters such as the number of servers and/or burst size, on one side, and the values of blocking probability and probability that a task request will obtain immediate service, on the other.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6133765,no
Intelligent migration from smart metering to smart grid,2011,"With the growing demand for more energy from the subscribers, and given the issues associated with ecological systems, it is inevitable and indispensable to move toward an efficient, economical, green, clean and self-correcting power system. Implementing this system is a challenge for the managers to tackle. It is a long time that smart metering system have been used by utility companies and implemented in quite a few numbers of them. Necessity for developing from smart metering to smart gird and utilizing it for consumption management, black-out management, demand response, distributed energy resources, Grid optimization, Load management, technical loss, theft detection, quality of service, utilizing the maximum capacity of power line, balancing the distribution network, freeing the distribution network and a great number of other advantages are inevitable. In this paper we have studied how to move toward smartening up the grid.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6134995,no
Electrically detected magnetic resonance study of a near interface trap in 4H SiC MOSFETs,2011,"It is well known that 4H silicon carbide (SiC) based metal oxide silicon field effect transistors (MOSFETs) have great promise in high power and high temperature applications. The reliability and performance of these MOSFETs is currently limited by the presence of SiC/SiO<sub>2</sub> interface and near interface traps which are poorly understood. Conventional electron paramagnetic resonance (EPR) studies of silicon samples have been utilized to argue for carbon dangling bond interface traps [1]. For several years, with several coworkers, we have explored these silicon carbide based MOSFETs with electrically detected magnetic resonance (EDMR), [2,3] establishing a connection between an isotropic EDMR spectrum with g=2.003 and deep level defects in the interface/near interface region of SiC MOSFETs. We tentatively linked the spectrum to a silicon vacancy or closely related defect. This assessment was tentative because we were not previously able to quantitatively evaluate the electron nuclear hyperfine interactions at the site. Through multiple improvements in EDMR hardware and data acquisition software, we have achieved a very large improvement in sensitivity and resolution in EDMR, which allows us to detect side peak features in the EDMR spectra caused by electron nuclear hyperfine interactions. This improved resolution allows far more definitive conclusions to be drawn about defect structure. In this work, we provide extremely strong experimental evidence identifying the structure of that defect. The evidence comes from very high resolution and sensitivity â€œfast passageâ€?(FP) mode [4, 5] electrically detected magnetic resonance (EDMR) or FPEDMR of the ubiquitous EDMR spectrum.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6135142,no
PV system monitoring and performance of a grid connected PV power station located in Manchester-UK,2011,"In the last two decades renewable resources have gained more attention due to continuing energy demand, along with the depletion in fossil fuel resources and their environmental effects to the planet. This paper presents a novel approach in monitoring PV power stations. The monitoring system enables system degradation early detection by calculating the residual difference between the model predicted and the actual measured power parameters. The model being derived using the MATLAB/SIMULINK software package and is designed with a dialog box to enable the user input of the PV system parameters. The performance of the developed monitoring system was examined and validated under different operating condition and faults e.g. dust, shadow and snow. Results were simulated and analyzed using the environmental parameters of irradiance and temperature. The irradiance and temperature data is gathered from a 28.8kW grid connected solar power system located on the tower block within the MMU campus in central Manchester. These real-time parameters are used as inputs of the developed PV model. Repeatability and reliability of the developed model performance were validated over a one and half year's period.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6136072,no
Parallelization of an ultrasound reconstruction algorithm for non destructive testing on multicore CPU and GPU,2011,"The CIVA software platform developed by CEA-LIST offers various simulation and data processing modules dedicated to non-destructive testing (NDT). In particular, ultrasonic imaging and reconstruction tools are proposed, in the purpose of localizing echoes and identifying and sizing the detected defects. Because of the complexity of data processed, computation time is now a limitation for the optimal use of available information. In this article, we present performance results on parallelization of one computationally heavy algorithm on general purpose processors (GPP) and graphic processing units (GPU). GPU implementation makes an intensive use of atomic intrinsics. Compared to initial GPP implementation, optimized GPP implementation runs up to Ã—116 faster and GPU implementation up to Ã—631. This shows that, even with irregular workloads, combining software optimization and hardware improvements, GPU give high performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6136904,no
Efficient Gender Classification Using Interlaced Derivative Pattern and Principal Component Analysis,2011,"With the wealth of image data that is now becoming increasingly accessible through the advent of the world wide web and proliferation of cheap, high quality digital cameras it is becoming ever more desirable to be able to automatically classify Gender into appropriate category such that intelligent agents and other such intelligent software might make better informed decisions regarding them without a need for excessive human intervention. In this paper, we present a new technique which provides superior performance superior than existing gender classification techniques. We first detect the face portion using Voila Jones face detector and then Interlaced Derivative Pattern (IDP)extract discriminative facial features for gender which are passed through Principal Component Analysis (PCA) to eliminate redundant features and thus reduce dimension. Keeping in mind strengths of different classifiers three classifiers K-nearest neighbor, Support Vector Machine and Fisher Discriminant Analysis are combined, which minimizes the classification error rate. We have used Stanford University Medical students (SUMS) face database for our experiment. Comparing our results and performance with existing techniques our proposed method provides high accuracy rate and robustness to illumination change.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6137158,no
Stream prediction using representative episode rules,2011,"Stream prediction based on episode rules of the form ""whenever a series of antecedent event types occurs, another series of consequent event types appears eventually""has received intensive attention due to its broad applications such as reading sequence forecasting, stock trend analyzing, road traffic monitoring, and software fault preventing. Many previous works focus on the task of discovering a full set of episode rules or matching a single predefined episode rule, little emphasis has been attached to the systematic methodology of stream prediction. This paper fills the gap by constructing an efficient and effective episode predictor over an event stream which works on a three-step process of rule extracting, rule matching and result reporting. Aiming at this goal, we first propose an algorithm Extractor to extract all representative episode rules based on frequent closed episodes and their generators, then we introduce an approach Matcher to simultaneously match multiple episode rules by finding the latest minimal and non-overlapping occurrences of their antecedents, and finally we devise a strategy Reporter to report each prediction result containing a prediction interval and a series of event types. Experiments on both synthetic and real-world datasets demonstrate that our methods are efficient and effective in the stream environment.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6137395,no
Risk management assessment using SERIM method,2011,"Software development is a complex process that involved many activities and has a big uncertainty to success. It is also a typical of activity that can be costly if mismanaged. Many factors can lead the success and also can cause software project failure. The failure actually can be detected early if we can adopt the concept of risk management and implemented it into software development project. SERIM is a method to measure risk in software engineering, proposed by Karolak [4]. SERIM is based on the mathematics of probability. SERIM uses some parameters which are derived from risk factors. The factors are: Organization, Estimation, Monitoring, Development Methodology, Tools, Risk Culture, Usability, Correctness, Reliability and Personnel. Each factor then measured by some questions metrics and there is 81 software metric questions for all factors. The factors then related and mapped into SDLC phases and risk management activities to calculate the probability (P). SERIM uses 28 probability variables to assess the risk potentials. The SERIM method then implemented to assess the risk of information system development, TrainSys, which is developed for a training and education unit in an organization. The result is useful to determine the low probability of TrainSys project success factor. The result also show the dominant and highest factor need to address in order to improve the quality of process and product of software development.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6137821,no
Adaptive resource management in PaaS platform using Feedback Control LRU algorithm,2011,"Cloud computing gets more and more popular because of its abilities on offering flexible dynamic IT infrastructure, QoS (Quality of Service) guaranteed computing environments and configurable software services. Cloud computing supports three service models: SaaS (Software as a Service), PaaS (Platform as a Service), and IaaS (Infrastructure as a Service). PaaS provides users with an application development and hosting platform with great reliability, scalability and convenience and it has many advantages in helping customers create applications compared with other service models. However, as a typical distributed system with limited computing resources, PaaS platform has to address the problems of resource management in order to achieve satisfactory QoS as well as efficient resource utilization. This paper presents an adaptive resource management algorithm called Feedback Control LRU (FC-LRU) which integrates the feedback control technique with LRU (Least Recently Used) algorithm. Simulation is conducted to evaluate the performance of FC-LRU and the results demonstrate that FC-LRU achieves satisfactory performance: it enables PaaS platform to maintain a low missed deadline ratio and high CPU utilization under the conditions of multitasks with different workloads.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6138508,no
A data placement algorithm with binary weighted tree on PC cluster-based cloud storage system,2011,"The need and use of scalable storage on cloud has rapidly increased in last few years. Organizations need large amount of storage for their operational data and backups. To address this need, high performance storage servers for cloud computing are the ultimate solution, but they are very expensive. Therefore we propose efficient cloud storage system by using inexpensive and commodity computer nodes. These computer nodes are organized into PC cluster as datacenter. Data objects are distributed and replicated in a cluster of commodity nodes located in the cloud. In the proposed cloud storage system, a data placement algorithm which provides a highly available and reliable storage is proposed. The proposed algorithm applies binary tree to search storage nodes. It supports the weighted allocation of data objects, balancing load on PC cluster with minimum cost. The proposed system is implemented with HDFS and experimental results prove that the proposed algorithm can balance storage load depending on the disk space, expected availability and failure probability of each node in PC cluster.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6138540,no
A quality of service framework for dependability in large-scale distributed systems,2011,"As recognition grows within industry for the advantages that can be gained through the exploitation of large-scale dynamic systems, a need emerges for dependable performance. Future systems are being developed with a requirement to support mission critical and safety critical applications. These levels of criticality require predictable performance and as such have traditionally not been associated with adaptive systems. The software architecture proposed for such systems takes its properties from the service-oriented computing paradigm and the communication model follows a publish/subscribe approach. While adaptive, such architectures do not, however, typically support real-time levels of performance. There is scope, however, for dependability within such architectures through the use of Quality of Service (QoS) methods. QoS is used in systems where the distribution of resources cannot be decided at design time. In this paper a QoS based framework is proposed for providing adaptive and dependable behaviour for future large-scale dynamic systems through the flexible allocation of resources. Simulation results are presented to demonstrate the benefits of the QoS framework and the tradeoffs that occur between negotiation algorithms of varying complexities.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6139124,no
A Software-Based Self-Test methodology for on-line testing of processor caches,2011,"Nowadays, on-line testing is essential for modern high-density microprocessors to detect either latent hardware defects or new defects appearing during lifetime both in logic and memory modules. For cache arrays, the flexibility to apply online different March tests is a critical requirement. For small memory arrays that may lack programmable Memory Built-In Self-Test (MBIST) circuitry, such as L1 cache arrays, Software-Based Self-Test (SBST) can be a flexible and low-cost solution for on-line March test application. In this paper, an SBST program development methodology is proposed for online periodic testing of L1 data and instruction cache, both for tag and data arrays. The proposed SBST methodology utilizes existing special purpose instructions that modern Instruction Set Architectures (ISAs) implement to access caches for debug-diagnostic and performance purposes, termed hereafter Direct Cache Access (DCA) instructions, as well as, performance monitoring mechanisms to overcome testability challenges. The methodology has been applied to 2 processor benchmarks, OpenRISC and LEON3 to demonstrate its high adaptability, and experimental comparison results against previous contributions show that the utilization of DCA instructions significantly improves test code size (83%) and test duration (72%) when applied to the same benchmark (LEON3).",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6139154,no
Performance assessment of ASD team using FPL football rules as reference,2011,"Agile software development (ASD) teams are committed to frequent, regular, high-quality deliverables. Agile team requires to produce high-quality code in short time span. Agile suggests methodologies like extreme programming and scrum to resolve the issues faced by the developers. Extreme programming is a methodology of ASD which suggests pair programming. But for a number of reasons, pairing is the most controversial and least universally-embraced agile programmer practice [1]. The reason for this is that certain task requires lot of deep thinking and so pairing (lack of privacy) does not work here. Certain personalities too do not work well with pairing. In scrum, daily standup-meeting is the method used to resolve impediments. Those impediments that are not resolved are added to product backlog. This adds to cost. There can be online mentors (e-Mentors) to help programmers resolve their domain issues. The selection of such mentors depends on their skill set and availability [3]. In order to sustain e-Mentoring, the experts who act as mentors in the respective domain (application / technology / tools) have to be rewarded for their assists. The mentor could be within the development team or can be part of any other project team. By seeing the similarities between the sports team and the Agile team, a way of recognizing and rewarding these assists is suggested in this paper. The set of rules used in Fantasy Premier League for performance assessment of football players is taken here as a reference for assessing agile team.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6139390,no
Computerized instrumentation â€?Automatic measurement of contact resistance of metal to carbon relays used in railway signaling,2011,"The Contact Resistance of metal to carbon relays used in railway signaling systems is a vital quality parameter. The manual measurement process is tedious, error prone and involves lot of time, effort and manpower. Besides, it is susceptible to manipulation and may adversely affect the functional reliability of relays due to erroneous measurements. To enhance the trustworthiness of measurement of contact resistance & to make the process faster, an automated measurement system having specially designed application software and a testing jig attachment has been developed. When the relay is fixed on the testing jig, the software scans all the relay contacts and measures the CR. The results are displayed on the computer screen and stored in a database file.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6139583,no
Instantaneous within cycle model based fault estimators for SI engines,2011,"The Mean Value Engine Model, commonly used in model based fault estimation of automotive spark ignition gasoline engines describes only the averaged dynamics of the engines, resulting in reduced sensitivity and isolability of faults. In this paper, estimation of faults is done using an instantaneous physics based within cycle model of the gasoline engine. A novel method for estimating certain types of faults is proposed and validated against the standard industrial simulation software, AMESim. The results of fault estimation show the efficacy of the method and also signify the importance of their instantaneously pulsating nature for characterizing the true nature of the fault.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6139610,no
Non-iterative algorithm of analytical synchronization of two-end measurements for transmission line parameters estimation and fault location,2011,This paper presents a new setting free approach to synchronization of two-end current and voltage unsynchronized measurements. Authors propose new non-iterative algorithm processing three-phase currents and voltages measured under normal steady state load condition of overhead line. Using such synchronization line parameters can be estimated and then precise fault location can be performed. The presented algorithm has been tested with ATP-EMTP software by generating great number of simulations with various sets of line parameters proving its usefulness for overhead lines up to 200km length.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140271,no
Pair analysis of requirements in software engineering education,2011,"Requirements Analysis and Design is found to be one of the crucial subjects in Software Engineering education. Students need to have deeper understanding before they could start to analyse and design the requirements, either using models or textual descriptions. However, the outcomes of their analysis are always vague and error-prone. We assume that this issue can be handled if â€œpair analysisâ€?is conducted where all students are assigned with partners following the concept of pair-programming. To prove this, we have conducted a small preliminary evaluation to compare the outcomes of solo work and â€œpair analysisâ€?work for three different groups of students. The performance, efficacy and students' satisfaction and confidence level are evaluated.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140641,no
Adopting Six Sigma approach in predicting functional defects for system testing,2011,"This research focuses on constructing a mathematical model to predict functional defects in system testing by applying Six Sigma approach. The motivation behind this effort is to achieve zero known post release defects of the software delivered to end-user. Besides serving as the indicator of optimizing testing process, predicting functional defects at the start of testing allows testing team to put comprehensive test coverage, find as many defects as possible and determine when to stop testing so that all known defects are contained within testing phase. Design for Six Sigma (DfSS) is chosen as the methodology as it emphasizes on customers' requirement and systematic techniques to build the model. Historical data becomes the crucial elements in this study. Metrics related to potential predictors and their relationships for the model are identified, which focuses on metrics in phases prior to testing phase. Repeatability and capability of testers' consistency in finding defects are analyzed. Type of data required are also identified and collected. The metrics of selected predictors which incorporate testing and development metrics are measured against total functional defects using multiple regression analysis. The best and most significant mathematical model generated by the regression analysis is selected as the proposed prediction model for functional defects in system testing phase. Validation of the model is then conducted to prove the goodness for implementation. Recommendation and future research work are provided at the end of this study.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140677,no
Efficient prediction of software fault proneness modules using support vector machines and probabilistic neural networks,2011,"A software fault is a defect that causes software failure in an executable product. Fault prediction models usually aim to predict either the probability or the density of faults that the code units contain. Many fault prediction models using software metrics have been proposed in the Software Engineering literature. This study focuses on evaluating high-performance fault predictors based on support vector machines (SVMs) and probabilistic neural networks (PNNs). Five public NASA datasets from the PROMISE repository are used to make these predictive models repeatable, refutable, and verifiable. According to the obtained results, the probabilistic neural networks generally provide the best prediction performance for most of the datasets in terms of the accuracy rate.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140679,no
H.264 deblocking filter enhancement,2011,This paper proposes new software-based techniques for speeding and reducing the complexity of the deblocking filter used in the state-of-the-art H.264 international video coding standard to improve the visual quality of the decoded video frames. The proposed techniques are classified as standard-compliant and standard-noncompliant techniques. The standard-compliant techniques optimize the standard filter through optimizing the boundary strength calculation and group filtering of macroblocks. The standard-noncompliant techniques predict the new boundary strength and edge detection conditions from previous values. Experimental results on both an embedded platform and a desktop PC show significant increment in performance improvement that reaches 47% for the standard-compliant techniques and 80% for the standard-noncompliant techniques. They also demonstrate that for standard-noncompliant techniques the quality degradation computed using the Peak Signal to Noise Ratio is insignificant.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6141046,no
Geometric mean based trust management system for WSNs (GMTMS),2011,"The Wireless Sensor Network (WSN) nodes are high-volume in number, and their deployment environment may be hazardous, unattended and/or hostile and sometimes dangerous. The traditional cryptographic and security mechanisms in WSNs cannot detect the node physical capture, and due to the malicious or selfish nodes even total breakdown of network may take place. Also, the traditional security mechanisms in WSNs requires sophisticated software, hardware, large memory, high processing speed and communication bandwidth at node. Hence, they are not sufficient for secure routing of message from source to destination in WSNs. Alternatively, trust management schemes consist a powerful tool for the detection of unexpected node behaviours (either faulty or malicious). In this paper, we propose a new geometric mean based trust management system by evaluating direct trust from the QoS characteristics (trust metrics) and indirect trust from recommendations by neighbour nodes, which allows for trusted nodes only to participate in routing.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6141286,no
Understanding Bohr-Mandel Bugs through ODC Triggers and a Case Study with Empirical Estimations of Their Field Proportion,2011,"This paper uses ODC Triggers as a means to estimate the Bohr-Mandel bug proportions from a software product in production. Specifically, the contributions are: A conceptual articulation of how ODC Triggers can differentiate between Bohr and Mandel bugsA grouping of triggers to estimate the proportion of Bohr-Mandel bugs in the field. A case study that estimates Mandelbug proportions to range ~20%-40% which is comparable to the JPL-NASA empirical study. A measure of the distribution of Mandelbugs across components, impact groups and time.Creates a discussion, and raises questions in greater depth on the Bohr-Mandel definitions, implications, features and manifestations.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6141718,no
A smarter supply chain - end to end Quality Management,2011,"For companies in a â€œSmarter Planetâ€?it is key that they are globally integrated, interconnected and intelligent. For globally integrated supply chains the large enterprises must rely heavily on their suppliers and their suppliers' suppliers for products, services and software. This increased reliance and dependence requires that an extended supply chain have an effective supplier quality management (SQM) system that provides visibility into this complex system. Essential in this SQM system is the ability to provide an end to end (E2E) process that begins with the concept of a new product and manages it through its extended lifecycle. These supply chains of large enterprises are faced with many daunting challenges such as consistent quality, compliance performance, part management (change control, parts obsolescence, counterfeit parts, etc. ), continuous improvement, inventory management, product security, on time delivery, and environment regulation compliances. An added complexity to the extended supply chain is that over the past several years there have been disruptions in the supply chain as a result of raw materials, demand changes, commodity volatility and fuel price increases. Per Manufacturing Insights â€œWorldwide Supply Chain 2010 top 10 Predictionsâ€?the #5 Prediction: â€œintelligent Supply Chains will put Broader Visibility Burden on Supply Chain Organizations, both Owned and Outsourcedâ€?further driving the need for a strong E2E Quality Management System. [1] Many major enterprises', like IBM, are implementing or considering implementing the use of business intelligence in their E2E supply management system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6142379,no
Generation of the power quality disturbances in LabVIEW software environment,2011,"Software controlled procedure for classification and generation of the typical power quality disturbances, is presented in this paper. Generation procedure is functionally based on the virtual instrumentation concept, including software application developed using graphical programming package LabVIEW and D/A data acquisition card NI PCI 6713, installed in standard PC environment. Besides standard undisturbed three-phase voltage signal waveforms, six different categories of the PQ disturbances characteristic for real-time power distribution networks, can be simulated on the basis of developed virtual instruments: voltage swells, sags, interruptions, high-order voltage harmonics, swells with harmonics and sags with harmonics. Each of simulated PQ disturbances can be predefined and easily changed according to user requirements, using various combinations of the knobs and controls implemented on the virtual instrument front panel. Data acquisition 8-channel card NI 6713 provides real-time generation of the disturbances using analog output channels, which can be applied for testing and verification of the instruments developed for measurement and processing of the basic PQ parameters.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143184,no
A method for elimination of phase jitter in software signal demodulation,2011,"This paper describes a new method for phase jitter elimination. It is based on detection of signal passing through zero after demodulation, which helps to determine a residual carrier frequency, and subsequently phase jitter elimination. Classical ways of phase jitter elimination require a frequency search in some range, while this new method is much faster. The method is implemented in the software for offline analysis of modulation quality for 16-QAM signal constellation. It analyzes the signal at receiving IF of radio-relay device. The program consists of several separate functional units, which are described in text. Measurements were made on real signal under the laboratory conditions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143582,no
Signal acquisition and processing in the magnetic defectoscopy of steel wire ropes,2011,"In this paper, the system that resolves the problem of wire rope defects using magnetic method of inspection is presented. Implementation of the system should provide for full monitoring of wire rope condition, according to the prescribed international standards. The purpose of this system, except to identify defects in the rope, is to determine to what extent the damage has been done. The measurement procedure provides for a better understanding of the defects that occur, as well as the rejection criteria of used ropes, that way increasing their security. Hardware and software design of appliance for recording defects and test results are presented in this paper.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143681,no
Measuring the quality characteristics of assembly code on embedded platforms,2011,The paper describes the implementation of programming tool for measuring quality characteristics of assembly code. The aim of this paper is to prove the usability of these metrics for assessing the quality of assembly code generated by C Compiler for DSP architecture in order to improve the Compiler. The analysis of test results showed that the compiler generates good quality assembly code.,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143798,no
Metrics and Antipatterns for Software Quality Evaluation,2011,"In the context of software evolution, many activities are involved and are very useful, like being able to evaluate the design quality of an evolving system, both to locate the parts that need particular refactoring or reengineering efforts, and to evaluate parts that are well designed. This paper aims to give support hints for the evaluation of the code and design quality of a system and in particular we suggest to use metrics computation and antipatterns detection together. We propose metrics computation based on particular kinds of micro-structures and the detection of structural and object-oriented antipatterns with the aim of identifying areas of design improvements. We can evaluate the quality of a system according to different issues, for example by understanding its global complexity, analyzing the cohesion and coupling of system modules and locating the most critical and complex components that need particular refactoring or maintenance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6146907,no
Toward Intelligent Software Defect Detection - Learning Software Defects by Example,2011,"Source code level software defect detection has gone from state of the art to a software engineering best practice. Automated code analysis tools streamline many of the aspects of formal code inspections but have the drawback of being difficult to construct and either prone to false positives or severely limited in the set of defects that can be detected. Machine learning technology provides the promise of learning software defects by example, easing construction of detectors and broadening the range of defects that can be found. Pinpointing software defects with the same level of granularity as prominent source code analysis tools distinguishes this research from past efforts, which focused on analyzing software engineering metrics data with granularity limited to that of a particular function rather than a line of code.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6146920,no
Stability and Classification Performance of Feature Selection Techniques,2011,"Feature selection techniques can be evaluated based on either model performance or the stability (robustness) of the technique. The ideal situation is to choose a feature selection technique that is robust to change, while also ensuring that models built with the selected features perform well. One domain where feature selection is especially important is software defect prediction, where large numbers of metrics collected from previous software projects are used to help engineers focus their efforts on the most faulty modules. This study presents a comprehensive empirical examination of seven filter-based feature ranking techniques (rankers) applied to nine real-world software measurement datasets of different sizes. Experimental results demonstrate that signal-to-noise ranker performed moderately in terms of robustness and was the best ranker in terms of model performance. The study also shows that although Relief was the most stable feature selection technique, it performed significantly worse than other rankers in terms of model performance.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6146960,no
Fault Detection through Sequential Filtering of Novelty Patterns,2011,"Multi-threaded applications are commonplace in today's software landscape. Pushing the boundaries of concurrency and parallelism, programmers are maximizing performance demanded by stakeholders. However, multi-threaded programs are challenging to test and debug. Prone to their own set of unique faults, such as race conditions, testers need to turn to automated validation tools for assistance. This paper's main contribution is a new algorithm called multi-stage novelty filtering (MSNF) that can aid in the discovery of software faults. MSNF stresses minimal configuration, no domain specific data preprocessing or software metrics. The MSNF approach is based on a multi-layered support vector machine scheme. After experimentation with the MSNF algorithm, we observed promising results in terms of precision. However, MSNF relies on multiple iterations (i.e., stages). Here, we propose four different strategies for estimating the number of the requested stages.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6146973,no
New integrated hybrid evaporative cooling system for HVAC energy efficiency improvement,2011,"Cooling systems in buildings are required to be more energy-efficient while maintaining the standard air quality. The aim of this paper is to explore the potential of reducing the energy consumption of a central air-conditioned building taking into account comfort conditions. For this, we propose a new hybrid evaporative cooling system for HVAC efficiency improvement. The integrated system will be modeled and analyzed to accomplish the energy conservation and thermal comfort objectives. Comparisons of the proposed hybrid evaporative cooling approach with current technologies are included to show its advantages. To investigate the potential of energy savings and air quality, a real-world commercial building, located in a hot and dry climate region, together with its central cooling plant is used in the case study. The energy consumption and relevant data of the existing central cooling plant are acquired in a typical summer week. The performance with different cooling systems is simulated by using a transient simulation software package. New modules for the proposed system are developed by using collected experimental data and implemented with the transient tool. Results show that more than 52% power savings can be obtained by this system while maintaining the predicted mean vote (PMV) between -1 to +1 for most of summer time.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6147546,no
Autonomic Computing: Applications of Self-Healing Systems,2011,"Self-Management systems are the main objective of Autonomic Computing (AC), and it is needed to increase the running system's reliability, stability, and performance. This field needs to investigate some issues related to complex systems such as, self-awareness system, when and where an error state occurs, knowledge for system stabilization, analyze the problem, healing plan with different solutions for adaptation without the need for human intervention. This paper focuses on self-healing which is the most important component of Autonomic Computing. Self-healing is a technique that aims to detect, analyze, and repair existing faults within the system. All of these phases are accomplished in real-time system. In this approach, the system is capable of performing a reconfiguration action in order to recover from a permanent fault. Moreover, self-healing system should have the ability to modify its own behavior in response to changes within the environment. Recursive neural network has been proposed and used to solve the main challenges of self-healing, such as monitoring, interpretation, resolution, and adaptation.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6150010,no
Designing Phantoms for Industrial Computed Tomography,2011,"The increasing use of computed tomography (CT) as a diagnostic tool creates the need for an efficient means of evaluating the performance of the CT systems now in use. Usually the metal products have the defects during manufacturing and are inevitable. In the NDT system, ability for evaluate the casting defects such as cracks and pores is also very important. In the present study, several phantoms were made for evaluating CT system and defect analysis tools. The phantoms were CT scanned, and the result of CT scan was discussed. For the accuracy matters, measurements were made and compared with real objects. It is expected that phantoms used in the present study help to validate defect detection softwares.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6150070,no
Tomographic performance characteristics of the IQâ—SPECT system,2011,"The IQÂ·SPECT system was introduced by Siemens in 2010 to significantly improve the efficiency of myocardial perfusion imaging (MPI) using conventional, large field-of-view (FOV) SPECT and SPECTÂ·CT systems. With IQÂ·SPECT, it is possible to perform MPI scans in one-fourth the time or using one-fourth the administered dose as compared to a standard protocol using parallel-hole collimators. This improvement is achieved by means of a proprietary multifocal collimator that rotates around the patient in a cardio-centric orbit resulting in a four-fold magnification of the heart while keeping the entire torso in the FOV. The data are reconstructed using an advanced reconstruction algorithm that incorporates measured values for gantry deflections, collimator-hole angles, and system point response function. This article explores the boundary conditions of IQÂ·SPECT imaging, as measured using the Data Spectrum<sup>Â®</sup> cardiac torso phantom with the cardiac insert. Impact on reconstructed image quality was evaluated for variations in positioning of the myocardium relative to the sweet spot, scan-arc limitations, and for low-dose imaging protocols. Reconstructed image quality was assessed visually using the INVIA 4DMSPECT and quantitatively using Siemens internal IQ assessment software. The results indicated that the IQÂ·SPECT system is capable of tolerating possible mispositioning of the myocardium relative to the sweet spot by the operator, and that no artifacts are introduced by the limited angle coverage. We also found from the study of multiple low dose protocols that the dwell time will need to be adjusted in order to acquire data with sufficient signal-to-noise ratio for good reconstructed image quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6152666,no
Process automation of metal to Carbon relays: On â€?Line measurement of electrical parameters,2011,"The manufacturing process of Metal to Carbon relays used in railway signaling systems for configuring various circuits of signals / points / track circuits etc. consists of seven phases from raw material to finished goods. To ensure in-process quality, the physical, electrical and various other parameters are measured manually with non-automated equipment, after each stage. Manual measurements are tedious, error prone and involve lot of time, effort and manpower. Besides, they are susceptible to manipulation and may lead to inferior quality products being passed, either due to deliberation or due to malefic intentions. Due to erroneous measurement of electrical parameters, the functional reliability of relays is adversely affected. To enhance the trustworthiness of measurement of electrical parameters & to make the process faster, an automated measurement system having proprietary application software and a testing jig attachment has been developed. When the relay was fixed on the testing jig, the software scanned all the relay contacts and measured all the electrical parameters viz. operating voltage / current, contact resistance, release voltage / current, coil resistance etc. The result was stored in a database file and ported on an internet website. Thus, the test results of individual relays were available on-line, with date & time tags and could be easily monitored.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6153311,no
Functionality test of a readout circuit for a 1mm<sup>3</sup> resolution clinical PET system,2011,"We are developing a 1mm<sup>3</sup> resolution Positron Emission Tomography (PET) camera dedicated to breast imaging, which collects high energy photons emitted from radioactively labeled agents injected in the patients to detect molecular signatures of breast cancer. The camera consists of 8 Ã— 8 arrays of 1 Ã— 1 Ã— 1mm<sup>3</sup> lutetium yttrium oxyorthosilicate (LYSO) crystals coupled to position sensitive avalanche photo-diodes (PSAPDs). The camera is built out of 2 panels each having 9 cartridges. A cartridge houses 8 layers of 16 dual LYSO-PSAPD modules and features 1024 readout channels. Amplification, shaping and triggering is done using the 36 channel RENA-3â„?chip. 32 of these chips are needed per cartridge, or 576 for the entire camera. The RENA-3â„?needs functionality and performance validation before assembly in the camera. A Chip Tester board was built to ensure RENA functionality and quality, featuring a 20,000-cycle chip holder and the ability to charge inject each channel individually. Lab View software was written to collect data on all RENA-3â„?channels automatically. Charge was injected using an arbitrary waveform generator. Software was written to validate gain, linearity, cross talk, and timing noise characteristics. Gain is tested using a linear fit, typical values of gain are 13 and -16 for positive and negative injected charges respectively. Timing analysis was based on analyzing the phase shift between U and V timing channels. A phase shift of 90 Â± 5Â° and timing noise of 2ns were considered acceptable.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6153750,no
FSFI: A Full System Simulator-Based Fault Injection Tool,2011,"With VLSI technology advances and the increasing popularity of COTS components and multi-core processor in space, aviation, and military harsh environment, dependability becomes more attractive to overcome the increasing susceptibility to transient, permanent, and wear-out induced intermittent fault. Fault injection is widely used in dependability evaluation and fault emulation. As compared to physical- and software based fault-injection tool, this paper presents a simulation based Fault Injection tool, namely FSFI, to study high-level propagations of faults and system-level manifestations, especially the underlying causes of fault manifestations, namely Symptoms in this paper. The primary contributions of this paper are as follows. First and foremost, FSFI - A Full System simulator based Fault Injection tool is designed and described in great details. FSFI is based on an open source full system simulator (SAM) and it is heavily modified to support different processor component such as ALU, decoder, integer register files, and AGEN (Address Generation Unit). Additional four modules are added to the FSFI-simulator: Fault Injector, Monitor, Analyzer, and Controller. Second, transient faults are injected into different SPARC processor components, such as ALU, decoder, integer register files, and AGEN to deliberately study fault high level manifestations and propagations from processor component, through SPARC architecture level, hyper visor, OS, to application. Third, the underlying causes of fault manifestations, namely Symptoms, such as fatal trap, high OS, high hyper, and hangs, are captured by FSFI Monitor. The distributions of Symptoms for different components against typical benchmarks are investigated, and the underlying reasons are analyzed in detail. Preliminary experiment results show that FSFI is effective for fault high level propagation and Symptom research, and some of our future work is prospected.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6154066,no
A Network Status Evaluation Method under Worm Propagation,2011,"The measurement of worm propagation impact on network status remained an elusive goal. This paper analyzes the worm characteristics and network traffic and service, introduces evaluation metrics and presents a new method to assess the network situation under worm propagation. The applicability of this method is verified by simulated experiments with the network simulation tool LSNEMUlab test bed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6154242,no
The Testing and Diagnostic System on AVR of the Movable Electricity Generating Set,2011,"AVR (Automatic Voltage Regulator) is the most important module, which can control the output voltage of generating sets, guarantee the voltage stability, improve power quality and decide the performance of electricity generating sets, so this paper introduces the design method of the AVR detecting and diagnostic system, which based on the fault database. The paper introduces the platform of the testing and diagnostic system on AVR from the hardware and software design, composed of the industrial computer (main controller), the programmed power supply, the data acquisition unit, and the software programmed by Lab Windows/CVI. The FTA (Fault Tree Analysis) method, establish fault database and analyse fault of AVR, is Applyed. Through testing the certain type's AVR, the method, proposd in the paper, is proved to be feasibile and versatile, and is satisfied with the detection and diagnosis for AVR eventually.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6154271,no
Research on Detection and Diagnosis Technology for Certain Shortwave-Communication-Control Equipment,2011,"Aiming at automatic detection and diagnosis for certain short wave-communication-control equipment, some suitable technology of detection and diagnosis for the equipment was researched. Methods of detection and diagnosis for the system were confirmed based on analyzing equipment characteristic and requirement of detection. The hardware and software platform of automatic detection and diagnosis system were designed. Results show that the proposed detection and diagnosis technology is quick in fault detection speed and high in fault diagnosis.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6154274,no
Online muon reconstruction in the ATLAS Muon Spectrometer at the level-2 stage of the event selection,2011,"To cope with the 40 MHz event production rate of LHC, the ATLAS experiment uses a multi level trigger architecture that selects events in three sequential steps, increasing the complexity of reconstruction algorithms and accuracy of measurements with each step. The Level-1 is implemented with custom hardware and provides a first reduction of the event rate to 75KHz, identifying physics candidates within a small detector region (Region of Interest, RoI). The higher trigger levels, the Level-2 and the Event Filter, are running on dedicated PC farms where the event rate is further reduced to O(400) Hz by software algorithms. At Level-2, the selection of the muon events is initiated by the â€œMuFastâ€?algorithm, which confirms the muon candidates by means of a precise measurement of the muon candidate momentum. Designed to use a negligible fraction of the Level-2 latency, this algorithm exploits fast tracking and Look Up Table (LUT) techniques to perform the muon reconstruction with the precision muon chamber data within the RoI. The quality of the track parameters is good and approaches that of the offline reconstruction in some detector regions, and enables Level-2 to achieve a significant reduction of the event rate. This paper presents the current state of the art of the L2 algorithm â€œMuFastâ€?reviewing its performance on the event selection, and the algorithm optimization achieved by using data taken in 2010.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6154358,no
ATLAS detector data processing on the Grid,2011,"The ATLAS detector is in the second year of continuous LHC running. A starting point for ATLAS physics analysis is data reconstruction. Following the prompt reconstruction, the ATLAS data are reprocessed, which allows reconstruction of the data with updated software and calibrations providing coherence and improving the quality of the reconstructed data for physics analysis. The large-scale data reprocessing campaigns are conducted on the Grid. Computing centers around the world participate in reprocessing providing tens of thousands of CPU-cores for a faster throughput. Reprocessing relies upon underlying ATLAS technologies providing reproducibility of results, scalable database access, orchestrated workflow and performance monitoring, dynamic workload sharing, and petascale data integrity control. These technologies are also empowering ATLAS physics and subsystem groups in further data processing steps on the Grid. We present the experience of large-scale data reprocessing campaigns and group data processing on the Grid.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6154460,no
Prompt reconstruction of ATLAS data in 2010 and 2011,2011,"Excluding a short duration in 2008, the LHC started regular operation in November 2009 and has since then provided billions of collision events that were recorded by the ATLAS experiment and promptly reconstructed at an on-site computing farm. The prompt reconstruction chain takes place in two steps and is designed to provide high-quality data for physics publications with as little delay as possible. The first reconstruction step is used for data quality assessment and determining calibration constants and beam spot position, so that this information can be used in the second reconstruction step to optimize the reconstruction performance. After the technical stop of the LHC at the end of 2010, the prompt reconstruction chain had to deal with greatly increased luminosity and pileup conditions. To allow the computing resources to cope with this increased dataflow, without developing a backlog, recently significant improvements have been made in the ATLAS reconstruction software to reduce CPU time and file sizes for the produced datasets.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6154584,no
Resistivity and mu-tau imager for automatic characterization of semiconductor materials,2011,"Despite intensive research for the improvement of the quality of II-VI crystals, until now no clear method permits to determine quickly the performance of final detector, directly on the as-grown crystal or on the wafers. This situation induces excess cost, which hinders the development of detectors based on these binary and ternary semiconductors. The homogeneity of the semi-insulating crystals will determine the capacity of these semiconductors to be used in gamma and X- rays detectors for imaging in medical, spatial and industrial applications. It appears, that the smaller the pixel unit size is, the better the material uniformity has to be. Fast characterization and non destructive methods become, therefore, imperative to keep the production costs in industry acceptable. The previously developed innovative contactless equipment for the mapping of resistivity of the wafers before contacts are deposited has become of great interest with a rather large domain of resistivity (from 10<sup>6</sup> up to 10<sup>12</sup> Ohm cm). We have introduced an additional contactless wafer mu-tau imaging of electrons on the same wafer automatically. The principle is based on a localized excitation of the wafer without any electrical contact and a particular use of the Hecht relation. In this paper, the main features of this new instrument will be presented and the results obtained on different materials CdTe, CdZnTe or CdMnTe presented and discussed. Very satisfying agreement has been observed between the results obtained with the new system and conventional methods based on the I-V and alpha spectroscopic measurements of detectors produced from these materials.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6154723,no
"Non-invasive, label free, quantitative characterisation of live cells in monolayer culture",2011,"This paper presents the development of an evanescent wave light microscope and predictive software to enable the quantitative study of cellular processes and live cell quality at high resolution and without the use of labels. Using the proposed technique, the current measurement capabilities of live cell light microscopy will be extended as critical data such as cell adhesion, will be extracted and quantified. In particular, mouse neural stem cells are studied using TIRM technique. Results show that TIRM images are high in information content containing details about cell morphology, cell adhesion and through the combination with time-lapse imaging provide information on cell dynamic processes and motility.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6154837,no
Design and FPGA implementation of digital noise generator based on superposition of Gaussian process,2011,"Currently in the design of digital communication system, in order to detect the communication quality, plenty of tests need to be done in a noisy environment. The common method is adding analog noise to the transmitted data on the radio. Adding digital noise has always been a difficulty. A digital noise generator based on superposition of Gaussian process is presented in this paper. And hardware program simulation is carried out using Quartus II combined with Modelsim software, even the performance of the digital noise generator is tested on FPGA, simultaneously, compared with the single Gaussian noise.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6157878,no
Increasing security of supply by the use of a Local Power Controller during large system disturbances,2011,"This paper describes intelligent ways in which distributed generation and local loads can be controlled during large system disturbances, using Local Power Controllers. When distributed generation is available, and a system disturbance is detected early enough, the generation can be dispatched, and its output power can be matched as closely as possible to local microgrid demand levels. Priority-based load shedding can be implemented to aid this process. In this state, the local microgrid supports the wider network by relieving the wider network of the micro-grid load. Should grid performance degrade further, the local microgrid can separate itself from the network and maintain power to the most important local loads, re-synchronising to the grid only after more normal performance is regained. Such an intelligent system would be a suitable for hospitals, data centres, or any other industrial facility where there are critical loads. The paper demonstrates the actions of such Local Power Controllers using laboratory experiments at the 10kVA scale.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6162785,no
Simple scoring system for ECG quality assessment on Android platform,2011,"Work presented in this paper was undertaken in response to the PhysioNet/CinC Challenge 2011: Improving the quality of ECGs collected using mobile phones. For the purpose of this challenge we have developed an algorithm that uses five simple rules, detecting the most common distortions of the ECG signal in the out of hospital environment. Using five if-then rules arranges for easy implementation and reasonably swift code on the mobile device. Our results on test set B were well-outside the top ten algorithms (Best score: 0.932; Our score: 0.828). Nevertheless our algorithm placed second among those providing open-source code for evaluation on the data set C, where neither data nor scores were released to the participants before the end of the challenge. The difference in the scores of the top two algorithms was minimal (Best score: 0.873; Our score: 0.872). As a consequence, relative success of simple algorithm on undisclosed set C raises questions about the over-fitting of more sophisticated algorithms - question that is hovering above many recently published results of automated methods for medical applications.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6164599,no
Mind-mapping: An effective technique to facilitate requirements engineering in agile software development,2011,"Merging agile with more traditional approaches in software development is a challenging task, especially when requirements are concerned: the main temptation is to let two opposite schools of thought become rigid in their own assumptions, without trying to recognize which advantages could come from either side. Mind mapping seems to provide a suitable solution for both parties: those who develop within an agile method and those who advocate proper requirements engineering practice. In this paper, mind mapping has been discussed as a suitable technique to elicit and represent requirements within the SCRUM model: specifically, we have focused on whether and how mind maps could lead to the development of a suitable product backlog, which in SCRUM plays the role of an initial requirements specification document. In order to experimentally assess how effectively practitioners could rely on a product backlog for their first development sprint, we have identified the adoption of mind maps as the independent variable and the quality of the backlog as the dependent variable, the latter being measured against the ""function points"" metric. Our hypothesis (i.e., mind maps are effective in increasing the quality of product backlogs) has been tested within an existing SCRUM project (the development of a digital library by an academic institution), and several promising data have been obtained and further discussed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6164775,no
SOSOF: A fuzzy learning approach for Proportional Slowdown Differentiation control,2011,"The Proportional Slowdown Differentiation (PSD) is an important Quality of Service (QoS) model for services in the Cloud. It aims to maintain consistent pre-specified slowdown ratios between the requests of different service classes. In this paper, a novel online fuzzy learning algorithm is proposed to facilitate the PSD control for distributed services. The advantages of the proposed approach are self-optimization of its fuzzy rules and robust control performance. We built a prototype to realize the fuzzy learning algorithm and the PSD control mechanism based on an open source middleware platform. Comprehensive experiments over a wide range of parameters and workloads have been conducted. Experimental results show that the proposed fuzzy learning algorithm can achieve superior performance over several other controllers.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6166241,no
Towards a performance estimate in semi-structured processes,2011,"Semi-structured processes are business workflows, where the execution of the workflow is not completely controlled by a workflow engine, i.e., an implementation of a formal workflow model. Examples are workflows where actors potentially have interaction with customers reporting the result of the interaction in a process aware information system. Building a performance model for resource management in these processes is difficult since the information required for a performance model is only partially recorded. In this paper we propose a systematic approach for the creation of an event log that is suitable for available process mining tools. This event log is created by an incremental cleansing of data. The proposed approach is evaluated in a case study where the quality of the derived event log i assessed by domain experts.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6166256,no
Design pattern prediction techniques: A comparative analysis,2011,"There are many design patterns available in literature to predict refactoring. However literature gives a comprehensive study to evaluate and compare various design patterns so that quality professionals may select an appropriate design pattern. To find a technique which performs better in general is an undesirable problem because behavior of a design pattern also depends on many other features like pre-deployment of design pattern, structural, behavioral etc. we have conducted an empirical survey of various design pattern in terms of various evaluation software metrics. In this paper we have presented comparison of few design patterns on metrics basis.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6168035,no
Semantic Process Management Environment,2011,"As the knowledge-based society has been constructed, the size of work process grows bigger and the amount of the information that has to be analyzed increases. So the necessity of the process management and improvement has been required highly. This study suggests the process management method to support a company's survival strategy to get the competitive power in difficult situation to predict future business environment. The suggested process management method applies ontology for formalizing and sharing the several generalized process management concept. In ontology, several techniques from Six Sigma and PSP are defined for process definition, execution and measurement. With ontology, we provide formal knowledge base for both process management environment and human stakeholders. Also, we can easily improve our environment by extending our process ontology to adapt new management methods.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6172107,no
Impact of SIPS performance on power systems integrity,2011,"An increasing number of utilities are using System Integrity Protection Schemes (SIPS) to minimize the probability of large disturbances and to enhance power system reliability. This trend leads to the use of an increased number of SIPS resulting in additional risks to system security. This paper proposes a procedure based on Markov Modeling for assessing the risk of a SIPS failure or misoperation. The proposed method takes into consideration failures in the three stages of SIPS operation: arming, activation and implementation. This method is illustrated using an example of a Generation Rejection Scheme (GRS) for preventing cascading outages that may lead to load shedding. In addition, system operators tend to have the SIPS always armed to prevent a failure to operate when required. However, this can result in increased probability of SIPS misoperation (operation when not needed). Therefore, the risk introduced to the system by having the SIPS always armed and ready to initiate actions is examined and compared with the risk of automatic or manual arming of SIPS only when required. Sensitivity analysis is also performed to determine how different factors can affect the ability of the SIPS to operate in a dependable and secure manner.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6180416,no
Design of adaptive line protection under smart grid,2011,"Smart grid will bring new opportunities to development of relay protection, new sensor technology is used in smart grid. Simplified algorithm to protect data, reducing data processing time. With the State Grid Corporation of China launched the construction of smart grid, smart grid caused by the characteristics of network reconfiguration, distributed power access technologies such as micro-network operation, to put forward new demands on relay protection, based on local measurement information and a small amount of regional information makes conventional protection face greater difficulties to solve these problems; the same time, research and application on new technologies (such as new sensor technology, clock synchronization and data synchronization technology, computer technology, optical fiber communication technology, etc.) provided a broad space for development of relay protection. Adaptive protection means that the protection must adapt to changing system conditions, the computer relay protection must have a hierarchical configuration of communication lines to exchange information with computer network of other devices. For now, fiber optic communication lines are best medium in large amounts of information transmission and conversion in adaptive protection. Adaptive protection is a protection theory, according to this theory, which allows adjustment of the various protection functions, making them more adapted to practical power system operation. The key idea is to make certain changes to the protection system to respond due to load changes, such as power failures caused by switching operations or changes in the power system. Adaptive protection under the relevant literature to a definition: â€œAdaptive protection is a basic principle of protection, this principle makes the relay can automatically adjust to various protection functions, or changes to more suitable for a given power system conditions.â€? In addition to conventional protection, b- t also must have a clear adaptive function modules, and only in this case can be called adaptive protection. For the general protection adaptive capacity and detect some aspects of complex fault there are some limitations, hardware circuit of microprocessor line protection device is discussed in this thesis, both hardware and algorithm considers the anti-jamming methods. On the software side, the use of a relatively new method of frequency measurement, dynamically tracking changes of frequency, real-time adjustment of sampling frequency for sampling; and using the new algorithm, improving data accuracy and simplifying the hardware circuit. The adaptive principle is applied to microprocessor line protection, adaptive instantaneous overcurrent protection, overcurrent protection principles, etc, to meet the requirements of rapid change operation mode to improve the performance of line protection. Relay protection needs to adapt to frequent changes in the power system operating mode, correctly remove various failure and equipment, and adaptive relay protection maintains a system of standard features in case of parameter changes. The simulation results show that it is an effective adaptive method.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6180471,no
A method for online analyzing excitation systems performance based on PMU measurements,2011,"A method based on synchronized phasor measurement technology for analyzing and evaluating the dynamic regulating performance of excitation system using dynamic electrical data acquired by phasor measurement unit (PMU) is proposed. Combined with an engineering processing of corresponding excitation system performance parameters, the method realizes the calculation and analysis of the main excitation system performance indexes through detecting and extracting the course of a generator disturbance and its excitation system response. Meanwhile, its whole software system functions are designed and realized based on the system structure of a WAMS. It is concluded through the method introduction and practical project applications that compared with conventional analysis methods, this method has the advantages of online analysis, offline research, simpleness and practicality, convenient use, and its computed results can be treated as an important reference for the evaluation of dynamic regulating performances of a excitation system.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6180748,no
Hierarchical index system for crash-stop service failure detection,2011,"Service Availability is an important method meeting adaptive and general requirements of service failure detection and largely affected by failure detection indexes. Therefore, how to select appropriate indicators of service failure detection is an important issue. However, only small number of indicators in the existing service availability mechanism can't accurately describe the complex situation of a distributed system, thus difficult to meet the QoS requirements of applications. After researching on existing service failure detection algorithms, applications' failures and network survivability, this paper proposes a hierarchical index system for crash-stop service failure detection that considers ability to provide services, network performance, and load brought by failure detection.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6181934,no
A finite queuing model with generalized modified Weibull testing effort for software reliability,2011,"This study incorporates a generalized modified Weibull (GMW) testing effort function (TEF) into failure detection process (FDP) and fault correction process (FCP). Although some researchers have been devoted to model these two processes, the influence of the amount of resources on lag of these two processes is not discussed. The amount of resources consumed can be depicted as testing effort function (TEF), and can largely influence failure detection speed and the time to correct a detected failure. Thus, in this paper, we will integrate a TEF into FDP and FCP. Further, we show that a GMW TEF can be expressed as a TEF curve, and present a finite server queuing (FSQ) model which permits a joint study of FDP and FCP two processes. An actual software failure data set is analyzed to illustrate the effectiveness of proposed model. Experimental results show that the proposed model has a fairly accurate predication capability.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6181985,no
A study on cooling efficiency improvement of thin film transistor Liquid Crystal Display (TFT-LCD) modules,2011,"In recent years, LCD (Liquid Crystal Display) TVs are taking the place of CRT (Cathode Ray Tube) TVs very fast by bringing new display technologies into use. LCD module technology is divided into two main groups; the first one is CCFL (Cold Cathode Fluorescent Lamp) display which was the first type used in LCD TV, the other one is the LED (Light Emitting Diode) module which is the newest display technology comes to make slim TV design. There is a thermal challenge making slim TV design. The purpose of this paper is to investigate the thermal analysis and modeling of a 32"" TFT-LCD LED module, The performance of LCD TV is strongly dependant on thermal effects such as temperature and its distribution on LCD displays The illumination of the display was insured by 180 light emitting diodes (LEDs) located at the top and bottom edges of the modules. Hence, in order to insure good image quality in display and long service life, an adequate thermal management is necessary. For this purpose, a commercially available computational fluid dynamics (CFD) simulation software â€œFloEFDâ€?was used to predict the temperature distribution. This thermal prediction by computational method was validated by an experimental thermal analysis by attaching 10 thermocouples on the back cover of the modules and measuring the temperatures. Also, thermal camera images of the display by FLIR Thermacam SC 2000 test device were also analyzed.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6184479,no
UIO-based diagnosis of aircraft engine control systems using scilab,2011,"Fault diagnosis is of significant importance to the robustness of aeroengine control systems. This paper makes use of full-order unknown input observers (UIOs) to facilitate the diagnosis of sensor/actuator faults in engine control systems. The built-in â€œui-observerâ€?function in Scilab, however, can not give satisfying performance, in terms of observer realization. Hence we rewrite this UIO program in standard Scilab scripts and decouple the effect of unknown disturbances upon state estimation to improve the sensitivity to engine faults. An evaluation platform is created on the basis of the Xcos tool in a Simulink-like manner. All the above work is accomplished in the Scilab environment. Experimental results on an aircraft turbofan engine demonstrate that the suggested UIO diagnostic method has good anti-disturbance ability and can effectively detect and isolate sensor/actuator faults under various fault conditions.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6184696,no
Assessing integrated measurement and evaluation strategies: A case study,2011,"This paper presents a case study aimed at understanding and comparing integrated strategies for software measurement and evaluation, considering a strategy as a resource from the assessed entity standpoint. The evaluation focus is on the quality of the capabilities of a measurement and evaluation strategy taking into account three key aspects: i) the conceptual framework, centered on a terminological base, ii) the explicit specification of the process, and iii) the methodological/technological support. We consider a strategy is integrated if to great extent these three capabilities are met simultaneously. In the illustrated case study two strategies i.e. GQM<sup>+</sup>Strategies (Goal-Question-Metric), and GOCAME (Goal-Oriented Context-Aware Measurement and Evaluation) are evaluated. The given results allowed us the understanding of strengths and weaknesses for both strategies, and planning improvement actions as well.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6188462,no
Research on the relationship between curvature radius of deflection basin and stress state on bottom of semi-rigid type base,2011,"In China'current specification for the design of asphalt pavement, tensile stress of asphalt layer bottom is one of design indexes. However, the design index can not be detected and verified in practical engineering application. Research and analysis of deflection bowl in this paper show that there is a relationship between tensile stress of each layer bottom and deflections in different positions. The dynamic analysis model of rigid pavement under falling weight deflectometer load was established by utilizing ANSYS for researching the relationship between tensile stress of semi-rigid layer bottom and the curvature radius of deflection basin. This paper tries to seek a testing method to characterize pavement design indexes. It would be helpful to establish a relationship between theoretical calculation and the actual test of engineering. It also contribute to evaluate the performance of pavement and riding quality.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6199608,no
Sampling + DMR: Practical and low-overhead permanent fault detection,2011,"With technology scaling, manufacture-time and in-field permanent faults are becoming a fundamental problem. Multi-core architectures with spares can tolerate them by detecting and isolating faulty cores, but the required fault detection coverage becomes effectively 100% as the number of permanent faults increases. Dual-modular redundancy(DMR) can provide 100% coverage without assuming device-level fault models, but its overhead is excessive. In this paper, we explore a simple and low-overhead mechanism we call Sampling-DMR: run in DMR mode for a small percentage (1% of the time for example) of each periodic execution window (5 million cycles for example). Although Sampling-DMR can leave some errors undetected, we argue the permanent fault coverage is 100% because it can detect all faults eventually. SamplingDMR thus introduces a system paradigm of restricting all permanent faults' effects to small finite windows of error occurrence. We prove an ultimate upper bound exists on total missed errors and develop a probabilistic model to analyze the distribution of the number of undetected errors and detection latency. The model is validated using full gate-level fault injection experiments for an actual processor running full application software. Sampling-DMR outperforms conventional techniques in terms of fault coverage, sustains similar detection latency guarantees, and limits energy and performance overheads to less than 2%.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6307759,no
QoS-aware multipath routing protocol for delay sensitive applications in MANETs A cross-layer approach,2011,"This paper proposes a QoS multipath routing protocol (QMRP) for MANETs based on the single path AODV routing protocol. QMRP establishes node-disjoint paths that experience the lowest delay. Other delay-aware routing protocols do not factor in the projected contribution of the node requesting a route in the total network load. The implication is that the end-to-end (E2E) delay obtained through RREQ is no longer accurate. Unlike its predecessors, QMRP takes into account the projected contribution of the source node in the calculation of E2E delay. To obtain accurate estimate of path delay, QMRP uses cross-layer communication to achieve link and channel-awareness. Performance evaluation of QMRP and comparison with AODV using OPNET show that QMRP outperforms AODV in terms of average throughput, delay and packet loss has been conducted.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6733036,no
Monitoring high performance data streams in vertical markets: Theory and applications in public safety and healthcare,2011,"Over the last several years, monitoring high performance data stream sources has become very important in various vertical markets. For example, in the public safety sector, monitoring and automatically identifying individuals suspected of terrorist or criminal activity without physically interacting with them has become a crucial security function. In the healthcare industry, noninvasive mechanical home ventilation monitoring has allowed patients with chronic respiratory failure to be moved from the hospital to a home setting without jeopardizing quality of life. In order to improve the efficiency of large data stream processing in such applications, we contend that data stream management systems (DSMS) should be introduced into the monitoring infrastructure. We also argue that monitoring tasks should be performed by executing data stream queries defined in Continuous Query Language (CQL), which we have extended with: 1) new operators that allow creation of a sophisticated event-based alerting system through the definition of threshold schemes and threshold activity scheduling, and 2) multimedia support, which allows manipulation of continuous multimedia data streams using a similarity-based join operator which permits correlation of data arriving in multimedia streams with static content stored in a conventional multimedia database. We developed a prototype in order to assess these proposed concepts and verified the effectiveness of our framework in a lab environment.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6770306,no
Rate-distortion optimized image coding allowing lossless conversion to JPEG compliant bitstreams,2011,"This paper proposes an efficient lossy image coding scheme which has a sort of compatibility with the JPEG standard. In this scheme, our encoder directly compresses an image into a specific bitstream. On the other hand, the reconstructed image is given by the standard JPEG decoder with a help of lossless bitstream conversion from the specific bitstream to the JPEG compliant one. This asymmetric structure of encoding and decoding procedures enables us to utilize modern coding techniques such as intra prediction, rate- distortion optimization and arithmetic coding in encoding process, while allowing almost all image manipulation software to directly import image contents without any loss of quality. In other words, the proposed scheme utilizes the JPEG compliant bitstream as an intermediate format for the decoding process. Simulation results indicate that the rate- distortion performance of the proposed scheme is better than those of the current state-of-the-art lossy coding schemes.",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7074286,no